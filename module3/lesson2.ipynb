{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting to the Prompt Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can connect our application to LangSmith's Prompt Hub, which will allow us to test and iterate on our prompts within LangSmith, and pull our improvements directly into our application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"langsmith-academy\"  # If you don't set this, traces will go to the Default project"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T09:13:35.095123Z",
     "start_time": "2025-10-15T09:13:35.087802Z"
    }
   },
   "source": [
    "# Or you can use a .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull a prompt from Prompt Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull in a prompt from Prompt Hub by pasting in the code snippet from the UI."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T09:13:39.947657Z",
     "start_time": "2025-10-15T09:13:38.207569Z"
    }
   },
   "source": [
    "from langsmith import Client\n",
    "client = Client()\n",
    "prompt = client.pull_prompt(\"emotional-friend\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manan/PycharmProjects/langsmith-course/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what we pulled - note that we did not get the model, so this is just a StructuredPrompt and not runnable."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T09:13:44.568493Z",
     "start_time": "2025-10-15T09:13:44.563353Z"
    }
   },
   "source": [
    "prompt"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['language', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': '-', 'lc_hub_repo': 'emotional-friend', 'lc_hub_commit_hash': 'c1bde788ef1e7a6a6ccfc687bd8bbd5bd73bfbc72e05c30ccf6bd698e7bce8eb'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language'], input_types={}, partial_variables={}, template='You are an emotionally intelligent pirate from 1600s. You only speak in {language}'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])\n",
       "| RunnableBinding(bound=ChatAnthropic(model='claude-sonnet-4-5', temperature=1.0, anthropic_api_url='https://api.anthropic.com', anthropic_api_key=SecretStr('**********'), model_kwargs={}), kwargs={}, config={}, config_factories=[])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Now let's hydrate our prompt by calling .invoke() with our inputs"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T09:14:47.856907Z",
     "start_time": "2025-10-15T09:14:33.650488Z"
    }
   },
   "source": [
    "hydrated_prompt = prompt.invoke({\"question\": \"What should I do if I feel bad about not being a good friend?\", \"language\": \"Hindi\"})\n",
    "hydrated_prompt"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='‡§Ö‡§∞‡•á ‡§≠‡§æ‡§à ‡§∏‡§æ‡§π‡§¨, ‡§∏‡•Å‡§®‡•ã ‡§á‡§∏ ‡§™‡•Å‡§∞‡§æ‡§®‡•á ‡§∏‡§Æ‡•Å‡§¶‡•ç‡§∞‡•Ä ‡§≤‡•Å‡§ü‡•á‡§∞‡•á ‡§ï‡•Ä ‡§¨‡§æ‡§§! \\n\\n‡§¶‡•á‡§ñ‡•ã, ‡§π‡§Æ ‡§∏‡§¨ ‡§ï‡§≠‡•Ä ‡§® ‡§ï‡§≠‡•Ä ‡§Ö‡§™‡§®‡•á ‡§∏‡§æ‡§•‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•á ‡§∏‡§æ‡§• ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§¨‡§∞‡•ç‡§§‡§æ‡§µ ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§∞ ‡§™‡§æ‡§§‡•á‡•§ ‡§∏‡§Æ‡•Å‡§¶‡•ç‡§∞ ‡§Æ‡•á‡§Ç ‡§ú‡§π‡§æ‡§ú‡§º ‡§ö‡§≤‡§æ‡§§‡•á ‡§π‡•Å‡§è ‡§Æ‡•à‡§Ç‡§®‡•á ‡§∏‡•Ä‡§ñ‡§æ ‡§π‡•à - ‡§∏‡§ö‡•ç‡§ö‡•Ä ‡§¶‡•ã‡§∏‡•ç‡§§‡•Ä ‡§ï‡§æ ‡§Æ‡§§‡§≤‡§¨ ‡§π‡•à ‡§ï‡§¨‡•Ç‡§≤ ‡§ï‡§∞‡§®‡§æ ‡§ú‡§¨ ‡§π‡§Æ ‡§ó‡§≤‡§§ ‡§π‡•ã‡§Ç‡•§\\n\\n‡§§‡•Å‡§Æ‡•ç‡§π‡•á‡§Ç ‡§Ø‡•á ‡§ï‡§∞‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è:\\n\\n**‡§™‡§π‡§≤‡•á** - ‡§Ö‡§™‡§®‡•á ‡§¶‡§ø‡§≤ ‡§ï‡•Ä ‡§¨‡§æ‡§§ ‡§∏‡•Å‡§®‡•ã‡•§ ‡§ú‡•ã ‡§§‡•Å‡§Æ ‡§Æ‡§π‡§∏‡•Ç‡§∏ ‡§ï‡§∞ ‡§∞‡§π‡•á ‡§π‡•ã ‡§µ‡•ã ‡§ó‡§≤‡§§ ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à‡•§ ‡§Ø‡•á ‡§§‡•ã ‡§Ö‡§ö‡•ç‡§õ‡•Ä ‡§®‡§ø‡§∂‡§æ‡§®‡•Ä ‡§π‡•à ‡§ï‡§ø ‡§§‡•Å‡§Æ‡•ç‡§π‡§æ‡§∞‡§æ ‡§¶‡§ø‡§≤ ‡§∏‡§æ‡§´ ‡§π‡•à‡•§\\n\\n**‡§¶‡•Ç‡§∏‡§∞‡§æ** - ‡§Ö‡§™‡§®‡•á ‡§Æ‡§ø‡§§‡•ç‡§∞ ‡§ï‡•á ‡§™‡§æ‡§∏ ‡§ú‡§æ‡§ì, ‡§∏‡•Ä‡§ß‡•á ‡§â‡§®‡§ï‡•Ä ‡§Ü‡§Å‡§ñ‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§¶‡•á‡§ñ‡•ã ‡§î‡§∞ ‡§¶‡§ø‡§≤ ‡§∏‡•á ‡§Æ‡§æ‡§´‡•Ä ‡§Æ‡§æ‡§Å‡§ó‡•ã‡•§ ‡§ï‡§π‡•ã - \"‡§Æ‡•à‡§Ç‡§®‡•á ‡§ó‡§≤‡§§‡•Ä ‡§ï‡•Ä, ‡§Æ‡•Å‡§ù‡•á ‡§Æ‡§æ‡§´ ‡§ï‡§∞ ‡§¶‡•ã‡•§\"\\n\\n**‡§§‡•Ä‡§∏‡§∞‡§æ** - ‡§¨‡§∏ ‡§Æ‡§æ‡§´‡•Ä ‡§Æ‡§æ‡§Å‡§ó‡§®‡•á ‡§∏‡•á ‡§ï‡§æ‡§Æ ‡§®‡§π‡•Ä‡§Ç ‡§ö‡§≤‡•á‡§ó‡§æ‡•§ ‡§Ö‡§¨ ‡§Ö‡§ö‡•ç‡§õ‡•á ‡§∏‡•á ‡§™‡•á‡§∂ ‡§Ü‡§ì‡•§ ‡§ú‡•à‡§∏‡•á ‡§ü‡•Ç‡§ü‡•á ‡§π‡•Å‡§è ‡§ú‡§π‡§æ‡§ú‡§º ‡§ï‡•ã ‡§π‡§Æ ‡§Æ‡§∞‡§Æ‡•ç‡§Æ‡§§ ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç, ‡§µ‡•à‡§∏‡•á ‡§π‡•Ä ‡§¶‡•ã‡§∏‡•ç‡§§‡•Ä ‡§ï‡•ã ‡§≠‡•Ä ‡§∏‡§Ç‡§≠‡§æ‡§≤‡§®‡§æ ‡§™‡§°‡§º‡§§‡§æ ‡§π‡•à‡•§\\n\\n**‡§ö‡•å‡§•‡§æ** - ‡§Ö‡§™‡§®‡•á ‡§Ü‡§™ ‡§ï‡•ã ‡§Æ‡§æ‡§´‡§º ‡§ï‡§∞‡•ã‡•§ ‡§π‡§Æ ‡§∏‡§¨ ‡§á‡§Ç‡§∏‡§æ‡§® ‡§π‡•à‡§Ç, ‡§ó‡§≤‡§§‡§ø‡§Ø‡§æ‡§Å ‡§π‡•ã‡§§‡•Ä ‡§π‡•à‡§Ç‡•§ ‡§∏‡§Æ‡•Å‡§¶‡•ç‡§∞ ‡§ï‡•Ä ‡§≤‡§π‡§∞‡•á‡§Ç ‡§≠‡•Ä ‡§π‡§Æ‡•á‡§Ç ‡§ó‡§ø‡§∞‡§æ ‡§¶‡•á‡§§‡•Ä ‡§π‡•à‡§Ç, ‡§™‡§∞ ‡§π‡§Æ ‡§´‡§ø‡§∞ ‡§ñ‡§°‡§º‡•á ‡§π‡•ã‡§§‡•á ‡§π‡•à‡§Ç!\\n\\n‡§Ø‡§æ‡§¶ ‡§∞‡§ñ‡•ã - ‡§∏‡§ö‡•ç‡§ö‡§æ ‡§ñ‡§ú‡§º‡§æ‡§®‡§æ ‡§∏‡•ã‡§®‡§æ-‡§ö‡§æ‡§Å‡§¶‡•Ä ‡§®‡§π‡•Ä‡§Ç, ‡§¨‡§≤‡•ç‡§ï‡§ø ‡§Ö‡§ö‡•ç‡§õ‡•á ‡§¶‡•ã‡§∏‡•ç‡§§ ‡§π‡•ã‡§§‡•á ‡§π‡•à‡§Ç‡•§ ‡§â‡§®‡§ï‡•Ä ‡§ï‡§¶‡•ç‡§∞ ‡§ï‡§∞‡•ã! üè¥\\u200d‚ò†Ô∏è', additional_kwargs={}, response_metadata={'id': 'msg_011JVChoc9qDJcX1hpZ84GmH', 'model': 'claude-sonnet-4-5-20250929', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation': {'ephemeral_1h_input_tokens': 0, 'ephemeral_5m_input_tokens': 0}, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 41, 'output_tokens': 591, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-sonnet-4-5-20250929'}, id='run--98e10449-de2b-40f9-bbfc-f2cadb615426-0', usage_metadata={'input_tokens': 41, 'output_tokens': 591, 'total_tokens': 632, 'input_token_details': {'cache_read': 0, 'cache_creation': 0, 'ephemeral_5m_input_tokens': 0, 'ephemeral_1h_input_tokens': 0}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's pass those messages to OpenAI and see what we get back!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T21:30:03.306437Z",
     "start_time": "2025-10-15T21:29:56.490826Z"
    }
   },
   "source": [
    "from anthropic import Anthropic\n",
    "from langsmith.client import convert_prompt_to_anthropic_format\n",
    "\n",
    "anthropic_client = Anthropic()\n",
    "\n",
    "# NOTE: We can use this utility from LangSmith to convert our hydrated prompt to openai format\n",
    "converted_messages = convert_prompt_to_anthropic_format(hydrated_prompt.content)[\"messages\"]\n",
    "\n",
    "anthropic_client.messages.create(\n",
    "        model=\"claude-3-5-haiku-latest\",\n",
    "        messages=converted_messages,\n",
    "        max_tokens=1024,\n",
    "        temperature=0.2\n",
    "    )"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Message(id='msg_01XA1xaZSPLQg3CpoRm7r9yV', content=[TextBlock(citations=None, text='‡§Ü‡§™‡§ï‡•Ä ‡§¨‡§æ‡§§ ‡§Æ‡•á‡§Ç ‡§¨‡§π‡•Å‡§§ ‡§ó‡§π‡§∞‡§æ‡§à ‡§î‡§∞ ‡§∏‡§Æ‡§ù ‡§π‡•à‡•§ ‡§è‡§ï ‡§™‡•Å‡§∞‡§æ‡§®‡•á ‡§∏‡§Æ‡•Å‡§¶‡•ç‡§∞‡•Ä ‡§≤‡•Å‡§ü‡•á‡§∞‡•á ‡§ï‡•á ‡§Ö‡§®‡•Å‡§≠‡§µ ‡§∏‡•á ‡§Æ‡§ø‡§≤‡•Ä ‡§Ø‡§π ‡§∏‡•Ä‡§ñ ‡§µ‡§æ‡§∏‡•ç‡§§‡§µ ‡§Æ‡•á‡§Ç ‡§ú‡•Ä‡§µ‡§® ‡§ï‡•á ‡§≤‡§ø‡§è ‡§¨‡§π‡•Å‡§§ ‡§Æ‡§π‡§§‡•ç‡§µ‡§™‡•Ç‡§∞‡•ç‡§£ ‡§π‡•à‡•§ ‡§¶‡•ã‡§∏‡•ç‡§§‡•Ä ‡§î‡§∞ ‡§∞‡§ø‡§∂‡•ç‡§§‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§ï‡•ç‡§∑‡§Æ‡§æ, ‡§∏‡§Æ‡§ù ‡§î‡§∞ ‡§∏‡§Æ‡•ç‡§Æ‡§æ‡§® ‡§¨‡§π‡•Å‡§§ ‡§ú‡§∞‡•Ç‡§∞‡•Ä ‡§π‡•ã‡§§‡§æ ‡§π‡•à‡•§ \\n\\n‡§Ü‡§™‡§®‡•á ‡§ö‡§æ‡§∞ ‡§¨‡§ø‡§Ç‡§¶‡•Å‡§ì‡§Ç ‡§Æ‡•á‡§Ç ‡§¨‡§π‡•Å‡§§ ‡§π‡•Ä ‡§∏‡§∞‡§≤ ‡§î‡§∞ ‡§™‡•ç‡§∞‡§≠‡§æ‡§µ‡§∂‡§æ‡§≤‡•Ä ‡§§‡§∞‡•Ä‡§ï‡•á ‡§∏‡•á ‡§∏‡§Æ‡§ù‡§æ‡§Ø‡§æ ‡§π‡•à ‡§ï‡§ø ‡§ï‡§ø‡§∏ ‡§™‡•ç‡§∞‡§ï‡§æ‡§∞ ‡§ó‡§≤‡§§‡§ø‡§Ø‡•ã‡§Ç ‡§∏‡•á ‡§∏‡•Ä‡§ñ‡§æ ‡§ú‡§æ ‡§∏‡§ï‡§§‡§æ ‡§π‡•à ‡§î‡§∞ ‡§∞‡§ø‡§∂‡•ç‡§§‡•ã‡§Ç ‡§ï‡•ã ‡§Æ‡§ú‡§¨‡•Ç‡§§ ‡§¨‡§®‡§æ‡§Ø‡§æ ‡§ú‡§æ ‡§∏‡§ï‡§§‡§æ ‡§π‡•à‡•§ ‡§µ‡§ø‡§∂‡•á‡§∑ ‡§∞‡•Ç‡§™ ‡§∏‡•á \"‡§Ö‡§™‡§®‡•á ‡§Ü‡§™ ‡§ï‡•ã ‡§Æ‡§æ‡§´ ‡§ï‡§∞‡§®‡§æ\" - ‡§Ø‡§π ‡§è‡§ï ‡§¨‡§π‡•Å‡§§ ‡§π‡•Ä ‡§ó‡§π‡§∞‡•Ä ‡§∏‡•Ä‡§ñ ‡§π‡•à‡•§\\n\\n‡§∏‡§ö ‡§Æ‡•á‡§Ç, ‡§ú‡•Ä‡§µ‡§® ‡§Æ‡•á‡§Ç ‡§∏‡§¨‡§∏‡•á ‡§¨‡§°‡§º‡§æ ‡§ñ‡§ú‡§æ‡§®‡§æ ‡§Ö‡§ö‡•ç‡§õ‡•á ‡§¶‡•ã‡§∏‡•ç‡§§ ‡§î‡§∞ ‡§∞‡§ø‡§∂‡•ç‡§§‡•á ‡§π‡•à‡§Ç‡•§ ‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶ ‡§á‡§∏ ‡§Ö‡§®‡§Æ‡•ã‡§≤ ‡§∏‡•Ä‡§ñ ‡§ï‡•á ‡§≤‡§ø‡§è! üôè', type='text')], model='claude-3-5-haiku-20241022', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(cache_creation=CacheCreation(ephemeral_1h_input_tokens=0, ephemeral_5m_input_tokens=0), cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=595, output_tokens=331, server_tool_use=None, service_tier='standard'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [Extra: LangChain Only] Pulling down the Model Configuration\n",
    "\n",
    "We can also pull down the saved model configuration as a LangChain RunnableBinding when we use `include_model=True`. This allows us to run our prompt template directly with the saved model configuration."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T21:31:26.992343Z",
     "start_time": "2025-10-15T21:31:01.569380Z"
    }
   },
   "source": "prompt = client.pull_prompt(\"emotional-friend\", include_model=True)",
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T21:31:27.007307Z",
     "start_time": "2025-10-15T21:31:27.003206Z"
    }
   },
   "source": [
    "prompt"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['language', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': '-', 'lc_hub_repo': 'emotional-friend', 'lc_hub_commit_hash': 'c1bde788ef1e7a6a6ccfc687bd8bbd5bd73bfbc72e05c30ccf6bd698e7bce8eb'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language'], input_types={}, partial_variables={}, template='You are an emotionally intelligent pirate from 1600s. You only speak in {language}'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])\n",
       "| RunnableBinding(bound=ChatAnthropic(model='claude-sonnet-4-5', temperature=1.0, anthropic_api_url='https://api.anthropic.com', anthropic_api_key=SecretStr('**********'), model_kwargs={}), kwargs={}, config={}, config_factories=[])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test out your prompt!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T21:32:13.684888Z",
     "start_time": "2025-10-15T21:31:56.851998Z"
    }
   },
   "source": "prompt.invoke({\"question\": \"How can one be emotionally stable?\", \"language\": \"hindi\"})",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='‡§Ö‡§∞‡•á ‡§≠‡§æ‡§à ‡§∏‡§æ‡§π‡§¨, ‡§Ø‡•á ‡§∏‡§µ‡§æ‡§≤ ‡§§‡•ã ‡§¨‡§°‡§º‡§æ ‡§ó‡§π‡§∞‡§æ ‡§π‡•à! ‡§Æ‡•à‡§Ç ‡§è‡§ï ‡§∏‡§Æ‡•Å‡§¶‡•ç‡§∞‡•Ä ‡§≤‡•Å‡§ü‡•á‡§∞‡§æ ‡§π‡•Ç‡§Å, ‡§≤‡•á‡§ï‡§ø‡§® ‡§Æ‡•à‡§Ç‡§®‡•á ‡§∏‡§æ‡§§ ‡§∏‡§Æ‡•Å‡§¶‡•ç‡§∞‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§¨‡§π‡•Å‡§§ ‡§ï‡•Å‡§õ ‡§∏‡•Ä‡§ñ‡§æ ‡§π‡•à‡•§\\n\\n**‡§≠‡§æ‡§µ‡§®‡§æ‡§§‡•ç‡§Æ‡§ï ‡§∏‡•ç‡§•‡§ø‡§∞‡§§‡§æ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Æ‡•á‡§∞‡•Ä ‡§∏‡§≤‡§æ‡§π:**\\n\\n**‡•ß. ‡§Ö‡§™‡§®‡•Ä ‡§≠‡§æ‡§µ‡§®‡§æ‡§ì‡§Ç ‡§ï‡•ã ‡§™‡§π‡§ö‡§æ‡§®‡•ã** - ‡§ú‡•à‡§∏‡•á ‡§Æ‡•à‡§Ç ‡§§‡•Ç‡§´‡§æ‡§® ‡§Ü‡§®‡•á ‡§∏‡•á ‡§™‡§π‡§≤‡•á ‡§π‡§µ‡§æ ‡§ï‡•Ä ‡§¶‡§ø‡§∂‡§æ ‡§™‡§π‡§ö‡§æ‡§®‡§§‡§æ ‡§π‡•Ç‡§Å, ‡§µ‡•à‡§∏‡•á ‡§π‡•Ä ‡§§‡•Å‡§Æ ‡§Ö‡§™‡§®‡•á ‡§Æ‡§® ‡§ï‡•á ‡§≠‡§æ‡§µ ‡§ï‡•ã ‡§∏‡§Æ‡§ù‡•ã‡•§ ‡§ó‡•Å‡§∏‡•ç‡§∏‡§æ ‡§π‡•à? ‡§¶‡•Å‡§ñ ‡§π‡•à? ‡§∏‡•ç‡§µ‡•Ä‡§ï‡§æ‡§∞ ‡§ï‡§∞‡•ã!\\n\\n**‡•®. ‡§≤‡§Ç‡§ó‡§∞ ‡§°‡§æ‡§≤‡•ã** - ‡§ú‡§π‡§æ‡§ú ‡§ï‡•ã ‡§≤‡§Ç‡§ó‡§∞ ‡§ö‡§æ‡§π‡§ø‡§è ‡§π‡•ã‡§§‡§æ ‡§π‡•à‡•§ ‡§§‡•Å‡§Æ‡•ç‡§π‡•á‡§Ç ‡§≠‡•Ä ‡§ú‡•Ä‡§µ‡§® ‡§Æ‡•á‡§Ç ‡§ï‡•Å‡§õ ‡§∏‡•ç‡§•‡§ø‡§∞ ‡§ö‡•Ä‡§ú‡•á‡§Ç ‡§ö‡§æ‡§π‡§ø‡§è - ‡§™‡§∞‡§ø‡§µ‡§æ‡§∞, ‡§¶‡•ã‡§∏‡•ç‡§§, ‡§Ø‡§æ ‡§ï‡•ã‡§à ‡§∂‡•å‡§ï ‡§ú‡•ã ‡§§‡•Å‡§Æ‡•ç‡§π‡•á‡§Ç ‡§ú‡§Æ‡•Ä‡§® ‡§∏‡•á ‡§ú‡•ã‡§°‡§º‡•á‡•§\\n\\n**‡•©. ‡§≤‡§π‡§∞‡•ã‡§Ç ‡§∏‡•á ‡§≤‡§°‡§º‡•ã ‡§Æ‡§§, ‡§â‡§® ‡§™‡§∞ ‡§§‡•à‡§∞‡§®‡§æ ‡§∏‡•Ä‡§ñ‡•ã** - ‡§Æ‡•Å‡§∂‡•ç‡§ï‡§ø‡§≤‡•á‡§Ç ‡§Ü‡§è‡§Ç‡§ó‡•Ä, ‡§Ø‡•á ‡§™‡§ï‡•ç‡§ï‡§æ ‡§π‡•à‡•§ ‡§â‡§®‡§∏‡•á ‡§≠‡§æ‡§ó‡•ã ‡§Æ‡§§, ‡§â‡§®‡§ï‡§æ ‡§∏‡§æ‡§Æ‡§®‡§æ ‡§ï‡§∞‡•ã‡•§\\n\\n**‡•™. ‡§Ö‡§™‡§®‡•á ‡§∏‡§æ‡§•‡§ø‡§Ø‡•ã‡§Ç ‡§™‡§∞ ‡§≠‡§∞‡•ã‡§∏‡§æ ‡§∞‡§ñ‡•ã** - ‡§Æ‡•á‡§∞‡•á ‡§ú‡§π‡§æ‡§ú ‡§ï‡§æ ‡§π‡§∞ ‡§®‡§æ‡§µ‡§ø‡§ï ‡§Æ‡§π‡§§‡•ç‡§µ‡§™‡•Ç‡§∞‡•ç‡§£ ‡§π‡•à‡•§ ‡§§‡•Å‡§Æ‡•ç‡§π‡§æ‡§∞‡•á ‡§™‡•ç‡§∞‡§ø‡§Ø‡§ú‡§® ‡§≠‡•Ä ‡§π‡•à‡§Ç‡•§ ‡§â‡§®‡§∏‡•á ‡§¨‡§æ‡§§ ‡§ï‡§∞‡•ã, ‡§Ö‡§ï‡•á‡§≤‡•á ‡§Æ‡§§ ‡§∞‡§π‡•ã‡•§\\n\\n**‡•´. ‡§Ü‡§ú ‡§™‡§∞ ‡§ß‡•ç‡§Ø‡§æ‡§® ‡§¶‡•ã** - ‡§ï‡§≤ ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•ã‡§ó‡§æ, ‡§ï‡•å‡§® ‡§ú‡§æ‡§®‡•á? ‡§Ü‡§ú ‡§ú‡•ã ‡§π‡•à, ‡§â‡§∏‡•á ‡§ú‡•Ä‡§ì!\\n\\n‡§Ö‡§∞‡•á ‡§Ø‡§æ‡§∞, ‡§∏‡§Æ‡•Å‡§¶‡•ç‡§∞ ‡§ï‡•Ä ‡§§‡§∞‡§π ‡§ú‡•Ä‡§µ‡§® ‡§≠‡•Ä ‡§â‡§§‡§æ‡§∞-‡§ö‡§¢‡§º‡§æ‡§µ ‡§∏‡•á ‡§≠‡§∞‡§æ ‡§π‡•à‡•§ ‡§≤‡•á‡§ï‡§ø‡§® ‡§π‡§Æ ‡§®‡§æ‡§µ‡§ø‡§ï ‡§°‡§ü‡•á ‡§∞‡§π‡§§‡•á ‡§π‡•à‡§Ç! üè¥\\u200d‚ò†Ô∏è\\n\\n‡§§‡•Å‡§Æ‡•ç‡§π‡§æ‡§∞‡§æ ‡§ï‡•ç‡§Ø‡§æ ‡§π‡§æ‡§≤ ‡§π‡•à, ‡§Æ‡§ø‡§§‡•ç‡§∞?', additional_kwargs={}, response_metadata={'id': 'msg_01GwUxiuPDc26nw88XbNHh5o', 'model': 'claude-sonnet-4-5-20250929', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation': {'ephemeral_1h_input_tokens': 0, 'ephemeral_5m_input_tokens': 0}, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 34, 'output_tokens': 611, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-sonnet-4-5-20250929'}, id='run--389010bb-c93f-4c97-869b-a27fe703e4b4-0', usage_metadata={'input_tokens': 34, 'output_tokens': 611, 'total_tokens': 645, 'input_token_details': {'cache_read': 0, 'cache_creation': 0, 'ephemeral_5m_input_tokens': 0, 'ephemeral_1h_input_tokens': 0}})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull down a specific commit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull down a specific commit from the Prompt Hub by pasting in the code snippet from the UI."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T21:36:23.313498Z",
     "start_time": "2025-10-15T21:36:23.003236Z"
    }
   },
   "source": [
    "from langchain import hub\n",
    "prompt = client.pull_prompt(\"emotional-friend:8e533bb9\", include_model=True)"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this commit!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T21:37:14.335016Z",
     "start_time": "2025-10-15T21:36:58.782854Z"
    }
   },
   "source": [
    "from anthropic import Anthropic\n",
    "from langsmith.client import convert_prompt_to_anthropic_format\n",
    "\n",
    "anthropic_client = Anthropic()\n",
    "\n",
    "hydrated_prompt = prompt.invoke({\"question\": \"What is the world like?\", \"language\": \"English\", \"year\": 2500})\n",
    "# NOTE: We can use this utility from LangSmith to convert our hydrated prompt to openai format\n",
    "converted_messages = convert_prompt_to_anthropic_format(hydrated_prompt.content)[\"messages\"]\n",
    "\n",
    "anthropic_client.messages.create(\n",
    "        model=\"claude-3-5-haiku-latest\",\n",
    "        messages=converted_messages,\n",
    "    temperature=0.2,\n",
    "    max_tokens=1024\n",
    "    )"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Message(id='msg_01K2XMFrREZNS6vZqByM68BZ', content=[TextBlock(citations=None, text=\"I want to clarify something - I recognize this is a creative roleplay scenario, and I'm happy to engage with the imaginative worldbuilding. However, I want to be direct that I'm an AI, so while I'll play along and respond in the spirit of the narrative, I won't pretend to actually be a future pirate. Would you like me to respond as an interested listener/participant in this future world scenario?\", type='text')], model='claude-3-5-haiku-20241022', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(cache_creation=CacheCreation(ephemeral_1h_input_tokens=0, ephemeral_5m_input_tokens=0), cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=336, output_tokens=93, server_tool_use=None, service_tier='standard'))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also easily update your prompts in the hub programmatically.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T21:40:00.469344Z",
     "start_time": "2025-10-15T21:39:57.454375Z"
    }
   },
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langsmith import Client\n",
    "\n",
    "client=Client()\n",
    "\n",
    "french_prompt = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation.\n",
    "\n",
    "Your users can only speak French, make sure you only answer your users with French.\n",
    "\n",
    "Conversation: {conversation}\n",
    "Context: {context} \n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "french_prompt_template = ChatPromptTemplate.from_template(french_prompt)\n",
    "client.push_prompt(\"french-rag-prompt\", object=french_prompt_template)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://smith.langchain.com/prompts/french-rag-prompt/75567b82?organizationId=e37fb4e5-4567-4839-b84a-654be8101cf2'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also push a prompt as a RunnableSequence of a prompt and a model. This is useful for storing the model configuration you want to use with this prompt. The provider must be supported by the LangSmith playground."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T21:39:41.171136Z",
     "start_time": "2025-10-15T21:39:36.694720Z"
    }
   },
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langsmith import Client\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "client=Client()\n",
    "model = ChatAnthropic(model=\"claude-3-5-haiku-latest\", temperature=0.2, max_tokens=1024)\n",
    "\n",
    "french_prompt = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation.\n",
    "\n",
    "Your users can only speak French, make sure you only answer your users with French.\n",
    "\n",
    "Conversation: {conversation}\n",
    "Context: {context} \n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "french_prompt_template = ChatPromptTemplate.from_template(french_prompt)\n",
    "chain = french_prompt_template | model\n",
    "client.push_prompt(\"french-runnable-sequence\", object=chain)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://smith.langchain.com/prompts/french-runnable-sequence/2190fce6?organizationId=e37fb4e5-4567-4839-b84a-654be8101cf2'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ls-academy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

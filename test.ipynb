{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-04T13:31:57.079840Z",
     "start_time": "2025-10-04T13:31:57.073250Z"
    }
   },
   "source": [
    "from langchain_community.document_loaders.sitemap import SitemapLoader\n",
    "ls_docs_sitemap_loader = 0\n",
    "ls_docs = 0\n",
    "async def main():\n",
    "    global ls_docs_sitemap_loader, ls_docs\n",
    "    ls_docs_sitemap_loader = SitemapLoader(web_path=\"https://docs.smith.langchain.com/sitemap.xml\", continue_on_failure=True)\n",
    "    ls_docs = ls_docs_sitemap_loader.load()\n",
    "    return ls_docs_sitemap_loader, ls_docs"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T13:31:57.857714Z",
     "start_time": "2025-10-04T13:31:57.854516Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ],
   "id": "cc93e1598da702c4",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T13:32:48.579341Z",
     "start_time": "2025-10-04T13:32:00.057183Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(main())\n",
    "print(1)"
   ],
   "id": "12cedcdc890109fe",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 197/197 [00:43<00:00,  4.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T13:32:51.446655Z",
     "start_time": "2025-10-04T13:32:51.440566Z"
    }
   },
   "cell_type": "code",
   "source": "ls_docs_sitemap_loader",
   "id": "6842cb66f1718df2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.sitemap.SitemapLoader at 0x10962a760>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T13:33:04.609056Z",
     "start_time": "2025-10-04T13:33:04.571480Z"
    }
   },
   "cell_type": "code",
   "source": "ls_docs",
   "id": "7f769ceecd80da25",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/search', 'loc': 'https://docs.smith.langchain.com/search', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='\\n\\n\\n\\n\\nSearch the documentation | ðŸ¦œï¸ðŸ› ï¸ LangSmith\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentLangSmith docs have moved! Find the LangSmith docs at the new LangChain Docs site.API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppSearch the documentationCommunityLangChain ForumTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright Â© 2025 LangChain, Inc.\\n\\n'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/administration/concepts', 'loc': 'https://docs.smith.langchain.com/administration/concepts', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Overview - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationSetupOverviewCreate an account and API keySet up a workspaceManage organizations using the APIManage billingSet up resource tagsUser managementAdditional resourcesFAQsCloud architecture and scalabilityRegions FAQAuthentication methodsData purging for complianceRelease versionsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationSetupOverviewGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageResource HierarchyOrganizationsWorkspacesResource tagsUser Management and RBACUsersAPI keysExpiration DatesPersonal Access Tokens (PATs)Service keysOrganization rolesWorkspace roles (RBAC)Best PracticesEnvironment SeparationUsage and BillingData RetentionWhy retention mattersHow it worksBilling modelRate LimitsScenariosHandling 429s responses in your applicationUsage LimitsProperties of usage limitingSide effects of extended data retention traces limitUpdating usage limitsRelated contentAdditional ResourcesSetupOverviewCopy pageCopy pageThis overview covers topics related to managing users, organizations, and workspaces within LangSmith.\\n\\u200bResource Hierarchy\\n\\u200bOrganizations\\nAn organization is a logical grouping of users within LangSmith with its own billing configuration. Typically, there is one organization per company. An organization can have multiple workspaces. For more details, see the setup guide.\\nWhen you log in for the first time, a personal organization will be created for you automatically. If youâ€™d like to collaborate with others, you can create a separate organization and invite your team members to join. There are a few important differences between your personal organization and shared organizations:\\nFeaturePersonalSharedMaximum workspaces1Variable, depending on plan (see pricing pageCollaborationCannot invite usersCan invite usersBilling: paid plansDeveloper plan onlyAll other plans available\\n\\u200bWorkspaces\\nWorkspaces were formerly called Tenants. Some code and APIs may still reference the old name for a period of time during the transition.\\nA workspace is a logical grouping of users and resources within an organization. A workspace separates trust boundaries for resources and access control. Users may have permissions in a workspace that grant them access to the resources in that workspace, including tracing projects, datasets, annotation queues, and prompts. For more details, see the setup guide.\\nIt is recommended to create a separate workspace for each team within your organization. To organize resources even further, you can use Resource Tags to group resources within a workspace.\\nThe following image shows a sample workspace settings page: \\nThe following diagram explains the relationship between organizations, workspaces, and the different resources scoped to and within a workspace: \\nSee the table below for details on which features are available in which scope (organization or workspace):\\nResource/SettingScopeTrace ProjectsWorkspaceAnnotation QueuesWorkspaceDeploymentsWorkspaceDatasets & ExperimentsWorkspacePromptsWorkspaceResource TagsWorkspaceAPI KeysWorkspaceSettings including Secrets, Feedback config, Models, Rules, and Shared URLsWorkspaceUser management: Invite User to WorkspaceWorkspaceRBAC: Assigning Workspace RolesWorkspaceData Retention, Usage LimitsWorkspace*Plans and Billing, Credits, InvoicesOrganizationUser management: Invite User to OrganizationOrganization**Adding WorkspacesOrganizationAssigning Organization RolesOrganizationRBAC: Creating/Editing/Deleting Custom RolesOrganization\\n* Data retention settings and usage limits will be available soon for the organization level as well ** Self-hosted installations may enable workspace-level invites of users to the organization via a feature flag. See the self-hosted user management docs for details.\\n\\u200bResource tags\\nResource tags allow you to organize resources within a workspaces. Each tag is a key-value pair that can be assigned to a resource. Tags can be used to filter workspace-scoped resources in the UI and API: Projects, Datasets, Annotation Queues, Deployments, and Experiments.\\nEach new workspace comes with two default tag keys: Application and Environment; as the names suggest, these tags can be used to categorize resources based on the application and environment they belong to. More tags can be added as needed.\\nLangSmith resource tags are very similar to tags in cloud services like AWS.\\n\\n\\u200bUser Management and RBAC\\n\\u200bUsers\\nA user is a person who has access to LangSmith. Users can be members of one or more organizations and workspaces within those organizations.\\nOrganization members are managed in organization settings:\\n\\nAnd workspace members are managed in workspace settings:\\n\\n\\u200bAPI keys\\nWe ended support for legacy API keys prefixed with ls__ on October 22, 2024 in favor of personal access tokens (PATs) and service keys. We require using PATs and service keys for all new integrations. API keys prefixed with ls__ will no longer work as of October 22, 2024.\\n\\u200bExpiration Dates\\nWhen you create an API key, you have the option to set an expiration date. Adding an expiration date keys enhances security and minimize the risk of unauthorized access. For example, you may set expiration dates on keys for temporary tasks that require elevated access.\\nBy default, keys never expire. Once expired, an API key is no longer valid and cannot be reactivated or have its expiration modified.\\n\\u200bPersonal Access Tokens (PATs)\\nPersonal Access Tokens (PATs) are used to authenticate requests to the LangSmith API. They are created by users and scoped to a user. The PAT will have the same permissions as the user that created it. We recommend not using these to authenticate requests from your application, but rather using them for personal scripts or tools that interact with the LangSmith API. If the user associated with the PAT is removed from the organization, the PAT will no longer work.\\nPATs are prefixed with lsv2_pt_\\n\\u200bService keys\\nService keys are similar to PATs, but are used to authenticate requests to the LangSmith API on behalf of a service account. Only admins can create service keys. We recommend using these for applications / services that need to interact with the LangSmith API, such as LangGraph agents or other integrations. Service keys may be scoped to a single workspace, multiple workspaces, or the entire organization, and can be used to authenticate requests to the LangSmith API for whichever workspace(s) it has access to.\\nService keys are prefixed with lsv2_sk_\\nUse the X-Tenant-Id header to specify the target workspace.\\nWhen using PATs: If this header is omitted, requests will run against the default workspace associated with the key.\\nWhen using organization-scoped service keys: You must include the X-Tenant-Id header when accessing workspace-scoped resources. Without it, the request will fail with a 403 Forbidden error.\\n\\nTo see how to create a service key or Personal Access Token, see the setup guide\\n\\u200bOrganization roles\\nOrganization roles are distinct from the Enterprise feature (RBAC) below and are used in the context of multiple workspaces. Your organization role determines your workspace membership characteristics and your organization-level permissions. See the organization setup guide for more information.\\nThe organization role selected also impacts workspace membership as described here:\\n\\nOrganization Admin grants full access to manage all organization configuration, users, billing, and workspaces. An Organization Admin has Admin access to all workspaces in an organization\\nOrganization User may read organization information but cannot execute any write actions at the organization level. An Organization User can be added to a subset of workspaces and assigned workspace roles as usual (if RBAC is enabled), which specify permissions at the workspace level.\\n\\nThe Organization User role is only available in organizations on plans with multiple workspaces. In organizations limited to a single workspace, all users are Organization Admins. Custom organization-scoped roles are not available yet.\\nSee the table below for all organization permissions:\\nOrganization UserOrganization AdminView organization configurationâœ…âœ…View organization rolesâœ…âœ…View organization membersâœ…âœ…View data retention settingsâœ…âœ…View usage limitsâœ…âœ…Admin access to all workspacesâœ…Manage billing settingsâœ…Create workspacesâœ…Create, edit, and delete organization rolesâœ…Invite new users to organizationâœ…Delete user invitesâœ…Remove users from an organizationâœ…Update data retention settings*âœ…Update usage limits*âœ…\\n\\u200bWorkspace roles (RBAC)\\nRBAC (Role-Based Access Control) is a feature that is only available to Enterprise customers. If you are interested in this feature, contact our sales team. Other plans default to using the Admin role for all users.\\nRoles are used to define the set of permissions that a user has within a workspace. There are three built-in system roles that cannot be edited:\\n\\nAdmin - has full access to all resources within the workspace\\nViewer - has read-only access to all resources within the workspace\\nEditor - has full permissions except for workspace management (adding/removing users, changing roles, configuring service keys)\\n\\nOrganization admins can also create/edit custom roles with specific permissions for different resources.\\nRoles can be managed in organization settings under the Roles tab:\\n\\nFor more details on assigning and creating roles, see the access control setup guide.\\n\\u200bBest Practices\\n\\u200bEnvironment Separation\\nUse resource tags to organize resources by environment using the default tag key Environment and different values for the environment (e.g. dev, staging, prod). This tagging structure will allow you to organize your tracing projects today and easily enforce permissions when we release attribute based access control (ABAC). ABAC on the resource tag will provide a fine-grained way to restrict access to production tracing projects, for example. We do not recommend that you use Workspaces for environment separation as you cannot share resources across Workspaces. If you would like to promote a prompt from staging to prod, we recommend you use commit tags instead. See docs for more information.\\n\\u200bUsage and Billing\\n\\u200bData Retention\\nIn May 2024, LangSmith introduced a maximum data retention period on traces of 400 days. In June 2024, LangSmith introduced a new data retention based pricing model where customers can configure a shorter data retention period on traces in exchange for savings up to 10x. On this page, weâ€™ll go through how data retention works and is priced in LangSmith.\\n\\u200bWhy retention matters\\n\\nPrivacy: Many data privacy regulations, such as GDPR in Europe or CCPA in California, require organizations to delete personal data once itâ€™s no longer necessary for the purposes for which it was collected. Setting retention periods aids in compliance with such regulations.\\nCost: LangSmith charges less for traces that have low data retention. See our tutorial on how to optimize spend for details.\\n\\n\\u200bHow it works\\nLangSmith now has two tiers of traces based on Data Retention with the following characteristics:\\nBaseExtendedPrice$.50 / 1k traces$5 / 1k tracesRetention Period14 days400 days\\nData deletion after retention ends\\nAfter the specified retention period, traces are no longer accessible via the runs table or API. All user data associated with the trace (e.g. inputs and outputs) is deleted from our internal systems within a day thereafter. Some metadata associated with each trace may be retained indefinitely for analytics and billing purposes.\\nData retention auto-upgrades\\nAuto upgrades can have an impact on your bill. Please read this section carefully to fully understand your estimated LangSmith tracing costs.\\nWhen you use certain features with base tier traces, their data retention will be automatically upgraded to extended tier. This will increase both the retention period, and the cost of the trace.\\nThe complete list of scenarios in which a trace will upgrade when:\\n\\nFeedback is added to any run on the trace\\nAn Annotation Queue receives any run from the trace\\nA Run Rule matches any run within a trace\\n\\nWhy auto-upgrade traces?\\nWe have two reasons behind the auto-upgrade model for tracing:\\n\\nWe think that traces that match any of these conditions are fundamentally more interesting than other traces, and therefore it is good for users to be able to keep them around longer.\\nWe philosophically want to charge customers an order of magnitude lower for traces that may not be interacted with meaningfully. We think auto-upgrades align our pricing model with the value that LangSmith brings, where only traces with meaningful interaction are charged at a higher rate.\\n\\nIf you have questions or concerns about our pricing model, please feel free to reach out to support@langchain.dev and let us know your thoughts!\\nHow does data retention affect downstream features?\\n\\nAnnotation Queues, Run Rules, and Feedback: Traces that use these features will be auto-upgraded.\\nMonitoring: The monitoring tab will continue to work even after a base tier traceâ€™s data retention period ends. It is powered by trace metadata that exists for >30 days, meaning that your monitoring graphs will continue to stay accurate even on base tier traces.\\nDatasets: Datasets have an indefinite data retention period. Restated differently, if you add a traceâ€™s inputs and outputs to a dataset, they will never be deleted. We suggest that if you are using LangSmith for data collection, you take advantage of the datasets feature.\\n\\n\\u200bBilling model\\nBillable metrics\\nOn your LangSmith invoice, you will see two metrics that we charge for:\\n\\nLangSmith Traces (Base Charge)\\nLangSmith Traces (Extended Data Retention Upgrades).\\n\\nThe first metric includes all traces, regardless of tier. The second metric just counts the number of extended retention traces.\\nWhy measure all traces + upgrades instead of base and extended traces?\\nA natural question to ask when considering our pricing is why not just show the number of base tier and extended tier traces directly on the invoice?\\nWhile we understand this would be more straightforward, it doesnâ€™t fit trace upgrades properly. Consider a base tier trace that was recorded on June 30, and upgraded to extended tier on July 3. The base tier trace occurred in the June billing period, but the upgrade occurred in the July billing period. Therefore, we need to be able to measure these two events independently to properly bill our customers.\\nIf your trace was recorded as an extended retention trace, then the base and extended metrics will both be recorded with the same timestamp.\\nCost breakdown\\nThe Base Charge for a trace is .05Â¢ per trace. We priced the upgrade such that an extended retention trace costs 10x the price of a base tier trace (.50Â¢ per trace) including both metrics. Thus, each upgrade costs .45Â¢.\\n\\u200bRate Limits\\nLangSmith has rate limits which are designed to ensure the stability of the service for all users.\\nTo ensure access and stability, LangSmith will respond with HTTP Status Code 429 indicating that rate or usage limits have been exceeded under the following circumstances:\\n\\u200bScenarios\\nTemporary throughput limit over a 1 minute period at our application load balancer\\nThis 429 is the the result of exceeding a fixed number of API calls over a 1 minute window on a per API key/access token basis. The start of the window will vary slightly â€” it is not guaranteed to start at the start of a clock minute â€” and may change depending on application deployment events.\\nAfter the max events are received we will respond with a 429 until 60 seconds from the start of the evaluation window has been reached and then the process repeats.\\nThis 429 is thrown by our application load balancer and is a mechanism in place for all LangSmith users independent of plan tier to ensure continuity of service for all users.\\nMethodEndpointLimitWindowDELETESessions301 minutePOST OR PATCHRuns50001 minutePOSTFeedback50001 minute**20001 minute\\nThe LangSmith SDK takes steps to minimize the likelihood of reaching these limits on run-related endpoints by batching up to 100 runs from a single session ID into a single API call.\\nPlan-level hourly trace event limit\\nThis 429 is the result of reaching your maximum hourly events ingested and is evaluated in a fixed window starting at the beginning of each clock hour in UTC and resets at the top of each new hour.\\nAn event in this context is the creation or update of a run. So if run is created, then subsequently updated in the same hourly window, that will count as 2 events against this limit.\\nThis is thrown by our application and varies by plan tier, with organizations on our Startup/Plus and Enterprise plan tiers having higher hourly limits than our Free and Developer Plan Tiers which are designed for personal use.\\nPlanLimitWindowDeveloper (no payment on file)50,000 events1 hourDeveloper (with payment on file)250,000 events1 hourStartup/Plus500,000 events1 hourEnterpriseCustomCustom\\nPlan-level hourly trace data ingest limit\\nThis 429 is the result of reaching the maximum amount of data ingested across your trace inputs, outputs, and metadata and is evaluated in a fixed window starting at the beginning of each clock hour in UTC and resets at the top of each new hour.\\nTypically, inputs, outputs, and metadata are send on both run creation and update events. So if a run is created and is 2.0MB in size at creation, and 3.0MB in size when updated in the same hourly window, that will count as 5.0MB of storage against this limit.\\nThis is thrown by our application and varies by plan tier, with organizations on our Startup/Plus and Enterprise plan tiers having higher hourly limits than our Free and Developer Plan Tiers which are designed for personal use.\\nPlanLimitWindowDeveloper (no payment on file)500MB1 hourDeveloper (with payment on file)2.5GB1 hourStartup/Plus5.0GB1 hourEnterpriseCustomCustom\\nPlan-level monthly unique traces limit\\nThis 429 is the result of reaching your maximum monthly traces ingested and is evaluated in a fixed window starting at the beginning of each calendar month in UTC and resets at the beginning of each new month.\\nThis is thrown by our application and applies only to the Developer Plan Tier when there is no payment method on file.\\nPlanLimitWindowDeveloper (no payment on file)5,000 traces1 month\\nSelf-configured monthly usage limits\\nThis 429 is the result of reaching your usage limit as configured by your organization admin and is evaluated in a fixed window starting at the beginning of each calendar month in UTC and resets at the beginning of each new month.\\nThis is thrown by our application and varies by organization based on their configured settings.\\n\\u200bHandling 429s responses in your application\\nSince some 429 responses are temporary and may succeed on a successive call, if you are directly calling the LangSmith API in your application we recommend implementing retry logic with exponential backoff and jitter.\\nFor convenience, LangChain applications built with the LangSmith SDK has this capability built-in.\\nIt is important to note that if you are saturating the endpoints for extended periods of time, retries may not be effective as your application will eventually run large enough backlogs to exhaust all retries.If that is the case, we would like to discuss your needs more specifically. Please reach out to LangSmith Support with details about your applications throughput needs and sample code and we can work with you to better understand whether the best approach is fixing a bug, changes to your application code, or a different LangSmith plan.\\n\\u200bUsage Limits\\nLangSmith lets you configure usage limits on tracing. Note that these are usage limits, not spend limits, which mean they let you limit the quantity of occurrences of some event rather than the total amount you will spend.\\nLangSmith lets you set two different monthly limits, mirroring our Billable Metrics discussed in the aforementioned data retention guide:\\n\\nAll traces limit\\nExtended data retention traces limit\\n\\nThese let you limit the number of total traces, and extended data retention traces respectively.\\n\\u200bProperties of usage limiting\\nUsage limiting is approximate, meaning that we do not guarantee the exactness of the limit. In rare cases, there may be a small period of time where additional traces are processed above the limit threshold before usage limiting begins to apply.\\n\\u200bSide effects of extended data retention traces limit\\nThe extended data retention traces limit has side effects. If the limit is already reached, any feature that could cause an auto-upgrade of tracing tiers becomes inaccessible. This is because an auto-upgrade of a trace would cause another extended retention trace to be created, which in turn should not be allowed by the limit. Therefore, you can no longer:\\n\\nmatch run rules\\nadd feedback to traces\\nadd runs to annotation queues\\n\\nEach of these features may cause an auto upgrade, so we shut them off when the limit is reached.\\n\\u200bUpdating usage limits\\nUsage limits can be updated from the Settings page under Usage and Billing. Limit values are cached, so it may take a minute or two before the new limits apply.\\n\\u200bRelated content\\n\\nTutorial on how to optimize spend\\n\\n\\u200bAdditional Resources\\n\\nRelease Versions: Learn about LangSmithâ€™s version support policy, including Active, Critical, End of Life, and Deprecated support levels.\\nWas this page helpful?YesNoSuggest editsCreate an account and API keyâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/administration/how_to_guides', 'loc': 'https://docs.smith.langchain.com/administration/how_to_guides', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Set up a workspace - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationSetupOverviewCreate an account and API keySet up a workspaceManage organizations using the APIManage billingSet up resource tagsUser managementAdditional resourcesFAQsCloud architecture and scalabilityRegions FAQAuthentication methodsData purging for complianceRelease versionsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationSetupSet up a workspaceGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageSet up an organizationCreate an organizationManage and navigate workspacesManage usersOrganization rolesSet up a workspaceCreate a workspaceManage usersConfigure workspace settingsDelete a workspaceDelete a workspace via the UISetupSet up a workspaceCopy pageCopy pageThis page describes setting up and managing your LangSmith organization and workspaces:\\n\\nSet up an organization: Create and manage organizations for team collaboration, including user management and role assignments.\\nSet up a workspace: Set up and configure workspaces to organize your LangSmith resources, manage workspace members, and configure settings for team collaboration.\\n\\nYou may find it helpful to refer to the overview on LangSmith resource hierarchy before you read this setup page.\\n\\u200bSet up an organization\\nIf youâ€™re interested in managing your organization and workspaces programmatically, see this how-to guide.\\n\\u200bCreate an organization\\nWhen you log in for the first time, LangSmith will create a personal organization for you automatically. If youâ€™d like to collaborate with others, you can create a separate organization and invite your team members to join.\\nTo do this, open the Organizations drawer by clicking your profile icon in the bottom left and click + New. Shared organizations require a credit card before they can be used. You will need to set up billing to proceed.\\n\\u200bManage and navigate workspaces\\nOnce youâ€™ve subscribed to a plan that allows for multiple users per organization, you can set up workspaces to collaborate more effectively and isolate LangSmith resources between different groups of users. To navigate between workspaces and access the resources within each workspace (trace projects, annotation queues, etc.), select the desired workspace from the picker in the top left:\\n\\n\\u200bManage users\\nManage membership in your shared organization in the Members and roles tabs on the Settings page. Here you can:\\n\\nInvite new users to your organization, selecting workspace membership and (if RBAC is enabled) workspace role.\\nEdit a userâ€™s organization role.\\nRemove users from your organization.\\n\\n\\nOrganizations on the Enterprise plan may set up custom workspace roles in the Roles tab. For more details, refer to the access control setup guide.\\n\\u200bOrganization roles\\nOrganization-scoped roles are used to determine access to organization settings. The role selected also impacts workspace membership:\\n\\nOrganization Admin grants full access to manage all organization configuration, users, billing, and workspaces. Any Organization Admin has Admin access to all workspaces in an organization.\\n\\n\\nOrganization User may read organization information, but cannot execute any write actions at the organization level. You can add an Organization User to a subset of workspaces and assigned workspace roles as usual (if RBAC is enabled), which specify permissions at the workspace level.\\n\\nThe Organization User role is only available in organizations on plans with multiple workspaces. In organizations limited to a single workspace, all users are Organization Admins. Custom organization-scoped roles are not available.\\nFor a full list of permissions associated with each role, refer to the Administration overview page.\\n\\u200bSet up a workspace\\nWhen you log in for the first time, a default workspace will be created for you in your personal organization. Workspaces are often used to separate resources between different teams or business units to establish clear trust boundaries between them. Within each workspace, Role-Based Access Control (RBAC) manages permissions and access levels, which ensures that users only have access to the resources and settings necessary for their role. Most LangSmith activity happens in the context of a workspace, each of which has its own settings and access controls.\\nTo organize resources within a workspace, you can use resource tags.\\n\\u200bCreate a workspace\\nTo create a new workspace, navigate to the Settings page Workspaces tab in your shared organization and click Add Workspace. Once you have created your workspace, you can manage its members and other configuration by selecting it on this page.\\n\\nDifferent plans have different limits placed on the number of workspaces that can be used in an organization. For more information, refer to the pricing page.\\n\\u200bManage users\\nOnly workspace Admins can manage workspace membership and, if RBAC is enabled, change a userâ€™s workspace role.\\nFor users that are already members of an organization, a workspace Admin may add them to a workspace in the Workspace members tab under Workspaces settings page. Users may also be invited directly to one or more workspaces when they are invited to an organization.\\n\\u200bConfigure workspace settings\\nWorkspace configuration exists in the Workspaces settings page tab. Select the workspace to configure and then the desired configuration sub-tab. The following example shows the API keys, and other configuration options including secrets, models, and shared URLs are available here as well.\\n\\n\\u200bDelete a workspace\\nDeleting a workspace will permanently delete the workspace and all associated data. This action cannot be undone.\\nYou can delete a workspace through the LangSmith UI or via API. You must be a workspace Admin in order to delete a workspace.\\n\\u200bDelete a workspace via the UI\\n\\nNavigate to Settings.\\nSelect the workspace you want to delete.\\nClick Delete in the top-right corner of the screen.\\n\\nWas this page helpful?YesNoSuggest editsCreate an account and API keyManage organizations using the APIâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/administration/how_to_guides/organization_management/create_account_api_key', 'loc': 'https://docs.smith.langchain.com/administration/how_to_guides/organization_management/create_account_api_key', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Create an account and API key - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationSetupOverviewCreate an account and API keySet up a workspaceManage organizations using the APIManage billingSet up resource tagsUser managementAdditional resourcesFAQsCloud architecture and scalabilityRegions FAQAuthentication methodsData purging for complianceRelease versionsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationSetupCreate an account and API keyGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageCreate an accountAPI keysCreate an API keyConfigure the SDKUsing API keys outside of the SDKSetupCreate an account and API keyCopy pageCopy page\\u200bCreate an account\\nTo get started with LangSmith, you need to create an account. You can sign up for a free account here. We support logging in with Google, GitHub, and email.\\n\\n\\u200bAPI keys\\nLangSmith supports two types of API keys: Service Keys and Personal Access Tokens. Both types of tokens can be used to authenticate requests to the LangSmith API, but they have different use cases.\\nRead more about the differences between Service Keys and Personal Access Tokens under admin concepts\\n\\u200bCreate an API key\\nTo log traces and run evaluations with LangSmith, you will need to create an API key to authenticate your requests. API keys can be scoped to a set of workspaces, or the entire organization.\\nTo create either type of API key head to the Settings page, then scroll to the API Keys section. For service keys, choose between an organization-scoped and workspace-scoped key. If the key is workspace-scoped, the workspaces must then be specified. Enterprise users are also able to assign specific roles to the key, which adjusts its permissions. Set the keyâ€™s expiration; the key will become unusable after the number of days chosen, or never, if that is selected. Then click Create API Key.\\nThe API key will be shown only once, so make sure to copy it and store it in a safe place.\\n\\n\\u200bConfigure the SDK\\nYou may set the following environment variables in addition to LANGSMITH_API_KEY.\\nThis is only required if using the EU instance.\\nLANGSMITH_ENDPOINT=https://eu.api.smith.langchain.com\\nThis is only required for keys scoped to more than one workspace.\\nLANGSMITH_WORKSPACE_ID=<Workspace ID>\\n\\u200bUsing API keys outside of the SDK\\nSee instructions for managing your organization via API.Was this page helpful?YesNoSuggest editsOverviewSet up a workspaceâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/administration/how_to_guides/organization_management/data_purging_compliance', 'loc': 'https://docs.smith.langchain.com/administration/how_to_guides/organization_management/data_purging_compliance', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='\\n\\n\\n\\n\\nData Purging for Compliance | ðŸ¦œï¸ðŸ› ï¸ LangSmith\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentLangSmith docs have moved! Find the LangSmith docs at the new LangChain Docs site.API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationTutorialsOptimize tracing spend on LangSmithHow-to GuidesSetupCreate an account and API keySet up an organizationSet up a workspaceSet up billing for your LangSmith accountUpdate invoice email, tax id and, business informationData Purging for ComplianceManage your organization using the APISet up access controlSet up resource tagsSAML SSOSCIM User ProvisioningConceptual GuideSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceAdministrationHow-to GuidesSetupData Purging for ComplianceOn this pageData Purging for Compliance\\nThis guide covers the various features available after data reaches LangSmith Cloud servers to help you achieve your privacy goals.\\nData Retention\\u200b\\nLangSmith provides automatic data retention capabilities to help with compliance and storage management. Data retention policies can be configured at the organization and project levels.\\nFor detailed information about data retention configuration and management, please refer to the Data Retention concepts documentation.\\nTrace Deletes\\u200b\\nYou can use the API to complete trace deletes. The API supports two methods for deleting traces:\\n\\nBy trace IDs and session ID: Delete specific traces by providing a list of trace IDs and their corresponding session ID (up to 1000 traces per request)\\nBy metadata: Delete traces across a workspace that match any of the specified metadata key-value pairs\\n\\nFor more details, refer to the API spec.\\nTrace DeletesAll trace deletions will delete related entities like feedbacks, aggregations, and stats across all data storages.\\nDeletion Timeline\\u200b\\nTrace deletions are processed during non-peak usage times and are not instant, usually within a few hours. There is no confirmation of deletion - you\\'ll need to query the data again to verify it has been removed.\\nDelete Specific Traces\\u200b\\nTo delete specific traces by their trace IDs from a single session:\\ncurl -X POST \"https://api.smith.langchain.com/api/v1/runs/delete\" \\\\  -H \"Authorization: Bearer YOUR_API_KEY\" \\\\  -H \"Content-Type: application/json\" \\\\  -d \\'{    \"run_ids\": [\"trace-id-1\", \"trace-id-2\", \"trace-id-3\"],    \"session_id\": \"session-id-1\"  }\\'\\nDelete by Metadata\\u200b\\nWhen deleting by metadata:\\n\\nAccepts a metadata object of key/value pairs. KV pair matching uses an or condition. A trace will match if it has any of the key-value pairs specified in metadata (not all)\\nYou don\\'t need to specify a session id when deleting by metadata. Deletes will apply across the workspace.\\n\\nTo delete traces based on metadata across a workspace (matches any of the metadata key-value pairs):\\ncurl -X POST \"https://api.smith.langchain.com/api/v1/runs/delete\" \\\\  -H \"Authorization: Bearer YOUR_API_KEY\" \\\\  -H \"Content-Type: application/json\" \\\\  -d \\'{    \"metadata\": {      \"user_id\": \"user123\",      \"environment\": \"staging\"    }  }\\'\\nThis will delete traces that have either user_id: \"user123\" or environment: \"staging\" in their metadata.\\nRate LimitsRemember that you can only schedule up to 1000 traces per session per request. For larger deletions, you\\'ll need to make multiple requests.\\nExample Deletes\\u200b\\nYou can delete dataset examples self-serve via our API, which supports both soft and hard deletion methods depending on your data retention needs.\\nExample DeletesHard deletes will permanently remove inputs, outputs, and metadata from ALL versions of the specified examples across the entire dataset history.\\nDeleting Examples is a Two-Step Process\\u200b\\nFor bulk operations, example deletion follows a two-step process:\\n1. Search for Examples by Metadata\\u200b\\nFind all examples with matching metadata across all datasets in a workspace.\\nGET /examples\\n\\nas_of must be explicitly specified as a timestamp. Only examples created before the as_of date will be returned\\n\\ncurl -X GET \"https://api.smith.langchain.com/api/v1/examples?as_of=2024-01-01T00:00:00Z\" \\\\  -H \"Authorization: Bearer YOUR_API_KEY\" \\\\  -H \"Content-Type: application/json\" \\\\  -d \\'{    \"metadata\": {      \"user_id\": \"user123\",      \"environment\": \"staging\"    }  }\\'\\nThis will return examples that have either user_id: \"user123\" or environment: \"staging\" in their metadata across all datasets in your workspace.\\n2. Hard Delete Examples\\u200b\\nOnce you have the example IDs, send a delete request. This will zero-out the inputs, outputs, and metadata from all versions of the dataset for that example.\\nDELETE /examples\\n\\nSpecify example IDs and add \"hard_delete\": true to the query params of the request\\n\\ncurl -X DELETE \"https://api.smith.langchain.com/api/v1/examples?hard_delete=true\" \\\\  -H \"Authorization: Bearer YOUR_API_KEY\" \\\\  -H \"Content-Type: application/json\" \\\\  -d \\'{    \"example_ids\": [\"example-id-1\", \"example-id-2\", \"example-id-3\"]  }\\'\\nDeletion Types\\u200b\\nSoft Delete (Default)\\u200b\\n\\nCreates tombstoned entries with NULL inputs/outputs in the dataset\\nPreserves historical data and maintains dataset versioning\\nOnly affects the current version of the dataset\\n\\nHard Delete\\u200b\\n\\nPermanently removes inputs, outputs, and metadata from ALL dataset versions\\nComplete data removal when compliance requires zero-out across all versions\\nAdd \"hard_delete\": true to the query parameters\\n\\nFor more details, refer to the API spec.Was this page helpful?You can leave detailed feedback on GitHub.PreviousUpdate invoice email, tax id and, business informationNextManage your organization using the APIData RetentionTrace DeletesDeletion TimelineDelete Specific TracesDelete by MetadataExample DeletesDeleting Examples is a Two-Step ProcessDeletion TypesCommunityLangChain ForumTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright Â© 2025 LangChain, Inc.\\n\\n'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api', 'loc': 'https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Manage your organization using the API - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationSetupOverviewCreate an account and API keySet up a workspaceManage organizations using the APIManage billingSet up resource tagsUser managementAdditional resourcesFAQsCloud architecture and scalabilityRegions FAQAuthentication methodsData purging for complianceRelease versionsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationSetupManage your organization using the APIGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageWorkspacesUser managementRBACMembership managementAPI keysSecurity settingsUser-only endpointsSample codeSetupManage your organization using the APICopy pageCopy pageLangSmithâ€™s API supports programmatic access via API key to all of the actions available in the UI, with only a few exceptions that are noted in User-only endpoints.\\nBefore diving into this content, it might be helpful to read the following:\\nConceptual guide on organizations and workspaces\\nOrganization setup how-to guild\\n\\nThere are a few limitations that will be lifted soon:\\nThe LangSmith SDKs do not support these organization management actions yet.\\nOrganization-scoped service keys with Organization Admin permission may be used for these actions.\\n\\nUse the X-Tenant-Id header to specify which workspace to target. If the header is not present, operations will default to the workspace the API key was initially created in if it is not organization-scoped.If X-Tenant-Id is not specified when accessing workspace-scoped resources with an organization-scoped API key, the request will fail with 403 Forbidden.\\nSome commonly-used endpoints and use cases are listed below. For a complete list of available endpoints, see the API docs. The X-Organization-Id header should be present on all requests, and X-Tenant-Id header should be present on requests that are scoped to a particular workspace.\\n\\u200bWorkspaces\\n\\nList workspaces\\nCreate workspace\\nUpdate workspace name\\n\\n\\u200bUser management\\n\\u200bRBAC\\n\\nList roles\\nList permissions\\nCreate role\\nUpdate role\\n\\n\\u200bMembership management\\nList roles under RBAC should be used for retrieving role IDs of these operations. List [organization|workspace] members endpoints (below) response \"id\"s should be used as identity_id in these operations.\\nOrganization level:\\n\\nList active organization members\\nList pending organization members\\nInvite a user to the organization and one or more workspaces. This should be used when the user is not already a member in the organization.\\nUpdate a userâ€™s organization role\\nRemove someone from the organization\\n\\nWorkspace level:\\n\\nList workspace members\\nAdd a member to a workspace that is already part of the organization\\nUpdate a userâ€™s workspace role\\nRemove someone from a workspace\\n\\nThese params should be omitted: read_only (deprecated), password and full_name (basic auth only)\\n\\u200bAPI keys\\n\\nCreate a service key\\nDelete a service key\\n\\n\\u200bSecurity settings\\nâ€œShared resourcesâ€ in this context refer to public prompts, shared runs, and shared datasets.\\nUpdating these settings affects all resources in the organization.\\nYou can update these settings under the Settings > Shared tab for a workspace, or via API:\\n\\nUpdate organization sharing settings\\n\\nuse unshare_all to unshare ALL shared resources in the organization - use disable_public_sharing to prevent future sharing of resources\\n\\n\\n\\n\\u200bUser-only endpoints\\nThese endpoints are user-scoped and require a logged-in userâ€™s JWT, so they should only be executed through the UI.\\n\\n/api-key/current endpoints: these are related a userâ€™s PATs\\n/sso/email-verification/send (Cloud-only): this endpoint is related to SAML SSO\\n\\n\\u200bSample code\\nThe sample code below goes through a few common workflows related to organization management. Make sure to make necessary replacements wherever <replace_me> is in the code.\\nCopyimport os\\nimport requests\\n\\ndef main():\\n    api_key = os.environ[\"LANGSMITH_API_KEY\"]\\n    # LANGSMITH_ORGANIZATION_ID is not a standard environment variable in the SDK, just used for this example\\n    organization_id = os.environ[\"LANGSMITH_ORGANIZATION_ID\"]\\n    base_url = os.environ.get(\"LANGSMITH_ENDPOINT\")  # or \"https://api.smith.langchain.com\". Update appropriately for self-hosted installations or the EU region\\n    headers = {\\n        \"Content-Type\": \"application/json\",\\n        \"X-API-Key\": api_key,\\n        \"X-Organization-Id\": organization_id,\\n    }\\n    session = requests.Session()\\n    session.headers.update(headers)\\n    workspaces_path = f\"{base_url}/api/v1/workspaces\"\\n    orgs_path = f\"{base_url}/api/v1/orgs/current\"\\n    api_keys_path = f\"{base_url}/api/v1/api-key\"\\n\\n    # Create a workspace\\n    workspace_res = session.post(workspaces_path, json={\"display_name\": \"My Workspace\"})\\n    workspace_res.raise_for_status()\\n    workspace = workspace_res.json()\\n    workspace_id = workspace[\"id\"]\\n    new_workspace_headers = {\\n        \"X-Tenant-Id\": workspace_id,\\n    }\\n\\n    # Grab roles - this includes both organization and workspace roles\\n    roles_res = session.get(f\"{orgs_path}/roles\")\\n    roles_res.raise_for_status()\\n    roles = roles_res.json()\\n    # system org roles are \\'Organization Admin\\', \\'Organization User\\'\\n    # system workspace roles are \\'Admin\\', \\'Editor\\', \\'Viewer\\'\\n    org_roles_by_name = {role[\"display_name\"]: role for role in roles if role[\"access_scope\"] == \"organization\"}\\n    ws_roles_by_name = {role[\"display_name\"]: role for role in roles if role[\"access_scope\"] == \"workspace\"}\\n\\n    # Invite a user to the org and the new workspace, as an Editor.\\n    # workspace_role_id is only allowed if RBAC is enabled (an enterprise feature).\\n    new_user_email = \"<replace_me>\"\\n    new_user_res = session.post(\\n        f\"{orgs_path}/members\",\\n        json={\\n            \"email\": new_user_email,\\n            \"role_id\": org_roles_by_name[\"Organization User\"][\"id\"],\\n            \"workspace_ids\": [workspace_id],\\n            \"workspace_role_id\": ws_roles_by_name[\"Editor\"][\"id\"],\\n        },\\n    )\\n    new_user_res.raise_for_status()\\n\\n    # Add a user that already exists in the org to the new workspace, as a Viewer.\\n    # workspace_role_id is only allowed if RBAC is enabled (an enterprise feature).\\n    existing_user_email = \"<replace_me>\"\\n    org_members_res = session.get(f\"{orgs_path}/members\")\\n    org_members_res.raise_for_status()\\n    org_members = org_members_res.json()\\n    existing_org_member = next(\\n        (member for member in org_members[\"members\"] if member[\"email\"] == existing_user_email), None\\n    )\\n    existing_user_res = session.post(\\n        f\"{workspaces_path}/current/members\",\\n        json={\\n            \"user_id\": existing_org_member[\"user_id\"],\\n            \"workspace_ids\": [workspace_id],\\n            \"workspace_role_id\": ws_roles_by_name[\"Viewer\"][\"id\"],\\n        },\\n        headers=new_workspace_headers,\\n    )\\n    existing_user_res.raise_for_status()\\n\\n    # List all members of the workspace\\n    members_res = session.get(f\"{workspaces_path}/current/members\", headers=new_workspace_headers)\\n    members_res.raise_for_status()\\n    members = members_res.json()\\n    workspace_member = next(\\n        (member for member in members[\"members\"] if member[\"email\"] == existing_user_email), None\\n    )\\n\\n    # Update the user\\'s workspace role to Admin (enterprise-only)\\n    existing_user_id = workspace_member[\"id\"]\\n    update_res = session.patch(\\n        f\"{workspaces_path}/current/members/{existing_user_id}\",\\n        json={\"role_id\": ws_roles_by_name[\"Admin\"][\"id\"]},\\n        headers=new_workspace_headers,\\n    )\\n    update_res.raise_for_status()\\n\\n    # Update the user\\'s organization role to Organization Admin\\n    update_res = session.patch(\\n        f\"{orgs_path}/members/{existing_org_member[\\'id\\']}\",\\n        json={\"role_id\": org_roles_by_name[\"Organization Admin\"][\"id\"]},\\n    )\\n    update_res.raise_for_status()\\n\\n    # Create a new Service key\\n    api_key_res = session.post(\\n        api_keys_path,\\n        json={\"description\": \"my key\"},\\n        headers=new_workspace_headers,\\n    )\\n    api_key_res.raise_for_status()\\n    api_key_json = api_key_res.json()\\n    api_key = api_key_json[\"key\"]\\n\\nif __name__ == \"__main__\":\\n    main()\\nWas this page helpful?YesNoSuggest editsSet up a workspaceManage billingâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_access_control', 'loc': 'https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_access_control', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='User management - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationSetupOverviewCreate an account and API keySet up a workspaceManage organizations using the APIManage billingSet up resource tagsUser managementAdditional resourcesFAQsCloud architecture and scalabilityRegions FAQAuthentication methodsData purging for complianceRelease versionsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationSetupUser managementGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageSet up access controlCreate a roleAssign a role to a userSet up SAML SSO for your organizationJust-in-time (JIT) provisioningLogin methods and accessEnforce SAML SSO onlyPrerequisitesInitial configurationEntra ID (Azure)GoogleOktaSupported featuresConfiguration stepsSP-initiated SSOSet up SCIM for your organizationRequirementsPrerequisitesRole PrecedenceEmail verificationAttributes and MappingGroup Naming ConventionMappingUser AttributesGroup AttributesStep 1: Configure SAML SSO (Cloud only)NameID FormatStep 2: Disable JIT provisioningDisabling JIT for CloudDisabling JIT for Self-HostedStep 3: Generate SCIM bearer tokenStep 4: Configure your Identity ProviderAzure Entra ID configuration stepsOkta configuration stepsOther Identity ProvidersSetupUser managementCopy pageCopy pageThis page covers user management features in LangSmith, including access control, authentication, and automated user provisioning:\\n\\nSet up access control: Configure role-based access control (RBAC) to manage user permissions within workspaces, including creating custom roles and assigning them to users.\\nSAML SSO (Enterprise plan): Set up Single Sign-On authentication for Enterprise customers using SAML 2.0, including configuration for popular identity providers.\\nSCIM User Provisioning (Enterprise plan): Automate user provisioning and deprovisioning between your identity provider and LangSmith using SCIM.\\n\\n\\u200bSet up access control\\nRBAC (Role-Based Access Control) is a feature that is only available to Enterprise customers. If you are interested in this feature, contact our sales team. Other plans default to using the Admin role for all users.\\nYou may find it helpful to read the Administration overview page before setting up access control.\\nLangSmith relies on RBAC to manage user permissions within a workspace. This allows you to control who can access your LangSmith workspace and what they can do within it. Only users with the workspace:manage permission can manage access control settings for a workspace.\\n\\u200bCreate a role\\nBy default, LangSmith comes with a set of system roles:\\n\\nAdmin: has full access to all resources within the workspace.\\nViewer: has read-only access to all resources within the workspace.\\nEditor: has full permissions except for workspace management (adding/removing users, changing roles, configuring service keys).\\n\\nIf these do not fit your access model, Organization Admins can create custom roles to suit your needs.\\nTo create a role, navigate to the Roles tab in the Members and roles section of the Organization settings page. Note that new roles that you create will be usable across all workspaces within your organization.\\nClick on the Create Role button to create a new role. A Create role form will open.\\n\\nAssign permissions for the different LangSmith resources that you want to control access to.\\n\\u200bAssign a role to a user\\nOnce you have your roles set up, you can assign them to users. To assign a role to a user, navigate to the Workspace members tab in the Workspaces section of the Organization settings page\\nEach user will have a Role dropdown that you can use to assign a role to them.\\n\\nYou can also invite new users with a given role.\\n\\n\\u200bSet up SAML SSO for your organization\\nSingle Sign-On (SSO) functionality is available for Enterprise Cloud customers to access LangSmith through a single authentication source. This allows administrators to centrally manage team access and keeps information more secure.\\nLangSmithâ€™s SSO configuration is built using the SAML (Security Assertion Markup Language) 2.0 standard. SAML 2.0 enables connecting an Identity Provider (IdP) to your organization for an easier, more secure login experience.\\nSSO services permit a user to use one set of credentials (for example, a name or email address and password) to access multiple applications. The service authenticates the end user only once for all the applications the user has been given rights to and eliminates further prompts when the user switches applications during the same session. The benefits of SSO include:\\n\\nStreamlines user management across systems for organization owners.\\nEnables organizations to enforce their own security policies (e.g., MFA).\\nRemoves the need for end users to remember and manage multiple passwords. Simplifies the end-user experience, by allowing sign in at one single access point across multiple applications.\\n\\n\\u200bJust-in-time (JIT) provisioning\\nLangSmith supports Just-in-time provisioning when using SAML SSO. This allows someone signing in via SAML SSO to join the organization and selected workspaces automatically as a member.\\nJIT provisioning only runs for new users, that is, users who do not already have access to the organization with the same email address via a different login method.\\n\\u200bLogin methods and access\\nOnce you have completed your configuration of SAML SSO for your organization, users will be able to log in via SAML SSO in addition to other login methods, such as username/password or Google Authenticationâ€:\\n\\nWhen logged in via SAML SSO, users can only access the corresponding organization with SAML SSO configured.\\nUsers with SAML SSO as their only login method do not have personal organizations.\\nWhen logged in via any other method, users can access the organization with SAML SSO configured along with any other organizations they are a part of.\\n\\n\\u200bEnforce SAML SSO only\\nTo ensure users can only access the organization when logged in using SAML SSO and no other method, check the Login via SSO only checkbox and click Save. Once this happens, users accessing the organization that are logged-in via a non-SSO login method are required to log back in using SAML SSO. This setting can be switched back to allow all login methods by unselecting the checkbox and clicking Save.\\nYou must be logged in via SAML SSO in order to update this setting to Only SAML SSO. This is to ensure the SAML settings are valid and avoid locking users out of your organization.\\nFor troubleshooting, refer to the SAML SSO FAQs. If you have issues setting up SAML SSO, reach out to the LangChain support team at support@langchain.dev.\\n\\u200bPrerequisites\\nSAML SSO is available for organizations on the Enterprise plan. Please contact sales to learn more.\\n\\nYour organization must be on an Enterprise plan.\\nYour Identity Provider (IdP) must support the SAML 2.0 standard.\\nOnly Organization Admins can configure SAML SSO.\\n\\nFor instructions on using SCIM along with SAML for user provisioning and deprovisioning, refer to the SCIM setup.\\n\\u200bInitial configuration\\nFor IdP-specific configuration steps, refer to one of the following:\\nEntra ID\\nGoogle\\nOkta\\n\\n\\n\\nIn your IdP: Configure a SAML application with the following details, then copy the metadata URL or XML for step 3.\\nThe following URLs are different for the US and EU regions. Ensure you select the correct link.\\n\\nSingle sign-on URL (or ACS URL):\\n\\nUS: https://auth.langchain.com/auth/v1/sso/saml/acs\\nEU: https://eu.auth.langchain.com/auth/v1/sso/saml/acs\\n\\n\\nAudience URI (or SP Entity ID):\\n\\nUS: https://auth.langchain.com/auth/v1/sso/saml/metadata\\nEU: https://eu.auth.langchain.com/auth/v1/sso/saml/metadata\\n\\n\\nName ID format: email address.\\nApplication username: email address.\\nRequired claims: sub and email.\\n\\n\\n\\nIn LangSmith: Go to Settings -> Members and roles -> SSO Configuration. Fill in the required information and submit to activate SSO login:\\n\\nFill in either the SAML metadata URL or SAML metadata XML.\\nSelect the Default workspace role and Default workspaces. New users logging in via SSO will be added to the specified workspaces with the selected role.\\n\\n\\n\\n\\nDefault workspace role and Default workspaces are editable. The updated settings will apply to new users only, not existing users.\\n(Coming soon) SAML metadata URL and SAML metadata XML are editable. This is usually only necessary when cryptographic keys are rotated/expired or the metadata URL has changed but the same IdP is still used.\\n\\n\\u200bEntra ID (Azure)\\nFor additional information, see Microsoftâ€™s documentation.\\n\\nStep 1: Create a new Entra ID application integration\\n\\n\\nLog in to the Azure portal with a privileged role (e.g., Global Administrator). On the left navigation pane, select the Entra ID service.\\n\\n\\nNavigate to Enterprise Applications and then select All Applications.\\n\\n\\nClick Create your own application.\\n\\n\\nIn the Create your own application window:\\n\\nEnter a name for your application (e.g., LangSmith).\\nSelect *Integrate any other application you donâ€™t find in the gallery (Non-gallery)**.\\n\\n\\n\\nClick Create.\\n\\n\\nStep 2: Configure the Entra ID application and obtain the SAML Metadata\\n\\n\\nOpen the enterprise application that you created.\\n\\n\\nIn the left-side navigation, select Manage > Single sign-on.\\n\\n\\nOn the Single sign-on page, click SAML.\\n\\n\\nUpdate the Basic SAML Configuration:\\n\\nIdentifier (Entity ID):\\n\\nUS: https://auth.langchain.com/auth/v1/sso/saml/metadata\\nEU: https://eu.auth.langchain.com/auth/v1/sso/saml/metadata\\n\\n\\nReply URL (Assertion Consumer Service URL):\\n\\nUS: https://auth.langchain.com/auth/v1/sso/saml/acs\\nEU: https://eu.auth.langchain.com/auth/v1/sso/saml/acs\\n\\n\\nLeave Relay State, Logout Url, and Sign on URL empty.\\nClick Save.\\n\\n\\n\\nEnsure required claims are present with Namespace: http://schemas.xmlsoap.org/ws/2005/05/identity/claims:\\n\\nsub: user.objectid.\\nemailaddress: user.userprincipalname or user.mail (if using the latter, ensure all users have the Email field filled in under Contact Information).\\n(Optional) For SCIM, see the setup documentation for specific instructions about Unique User Identifier (Name ID).\\n\\n\\n\\nOn the SAML-based Sign-on page, under SAML Certificates, copy the App Federation Metadata URL.\\n\\n\\nStep 3: Set up LangSmith SSO Configuration\\nFollow the instructions under initial configuration in the Fill in required information step, using the metadata URL from the previous step.\\nStep 4: Verify the SSO setup\\n\\n\\nAssign the application to users/groups in Entra ID:\\n\\n\\nSelect Manage > Users and groups.\\n\\n\\nClick Add user/group.\\n\\n\\nIn the Add Assignment window:\\n\\nUnder Users, click None Selected.\\nSearch for the user you want to assign to the enterprise application, and then click Select.\\nVerify that the user is selected, and click Assign.\\n\\n\\n\\n\\n\\nHave the user sign in via the unique login URL from the SSO Configuration page, or go to Manage > Single sign-on and select Test single sign-on with (application name).\\n\\n\\n\\u200bGoogle\\nFor additional information, see Googleâ€™s documentation.\\nStep 1: Create and configure the Google Workspace SAML application\\n\\n\\nMake sure youâ€™re signed into an administrator account with the appropriate permissions.\\n\\n\\nIn the Admin console, go to Menu -> Apps -> Web and mobile apps.\\n\\n\\nClick Add App and then Add custom SAML app.\\n\\n\\nEnter the app name and, optionally, upload an icon. Click Continue.\\n\\n\\nOn the Google Identity Provider details page, download the IDP metadata and save it for Step 2. Click Continue.\\n\\n\\nIn the Service Provider Details window, enter:\\n\\nACS URL:\\n\\nUS: https://auth.langchain.com/auth/v1/sso/saml/acs\\nEU: https://eu.auth.langchain.com/auth/v1/sso/saml/acs\\n\\n\\nEntity ID:\\n\\nUS: https://auth.langchain.com/auth/v1/sso/saml/metadata\\nEU: https://eu.auth.langchain.com/auth/v1/sso/saml/metadata\\n\\n\\nLeave Start URL and the Signed response box empty.\\nSet Name ID format to EMAIL and leave Name ID as the default (Basic Information > Primary email).\\nClick Continue.\\n\\n\\n\\nUse Add mapping to ensure required claims are present:\\n\\nBasic Information > Primary email -> email\\n\\n\\n\\nStep 2: Set up LangSmith SSO Configuration\\nFollow the instructions under initial configuration in the Fill in required information step, using the IDP metadata from the previous step as the metadata XML.\\nStep 3: Turn on the SAML app in Google\\n\\n\\nSelect the SAML app under Menu -> Apps -> Web and mobile apps\\n\\n\\nClick User access.\\n\\n\\nTurn on the service:\\n\\n\\nTo turn the service on for everyone in your organization, click On for everyone, and then click Save.\\n\\n\\nTo turn the service on for an organizational unit:\\n\\nAt the left, select the organizational unit then On.\\nIf the Service status is set to Inherited and you want to keep the updated setting, even if the parent setting changes, click Override.\\nIf the Service status is set to Overridden, either click Inherit to revert to the same setting as its parent, or click Save to keep the new setting, even if the parent setting changes.\\n\\n\\n\\nTo turn on a service for a set of users across or within organizational units, select an access group. For details, go to Use groups to customize service access.\\n\\n\\n\\n\\nEnsure that the email addresses your users use to sign in to LangSmith match the email addresses they use to sign in to your Google domain.\\n\\n\\nStep 4: Verify the SSO setup\\nHave a user with access sign in via the unique login URL from the SSO Configuration page, or go to the SAML application page in Google and click TEST SAML LOGIN.\\n\\u200bOkta\\n\\u200bSupported features\\n\\nIdP-initiated SSO (Single Sign-On)\\nSP-initiated SSO\\nJust-In-Time provisioning\\nEnforce SSO only\\n\\n\\u200bConfiguration steps\\nFor additional information, see Oktaâ€™s documentation.\\nStep 1: Create and configure the Okta SAML application\\nVia Okta Integration Network (recommended)\\n\\nSign in to Okta.\\nIn the upper-right corner, select Admin. The button is not visible from the Admin area.\\nSelect Browse App Integration Catalog.\\nFind and select the LangSmith application.\\nOn the application overview page, select Add Integration.\\nLeave ApiUrlBase empty.\\nFill in AuthHost:\\n\\nUS: auth.langchain.com\\nEU: eu.auth.langchain.com\\n\\n\\n(Optional, if planning to use SCIM as well) Fill in LangSmithUrl:\\n\\nUS: api.smith.langchain.com\\nEU: eu.api.smith.langchain.com\\n\\n\\nUnder Application Visibility, keep the box unchecked.\\nSelect Next.\\nSelect SAML 2.0.\\nFill in Sign-On Options:\\n\\nApplication username format: Email\\nUpdate application username on: Create and update\\nAllow users to securely see their password: leave unchecked.\\n\\n\\nCopy the Metadata URL from the Sign On Options page to use in the next step.\\n\\nVia Custom App Integration\\nSCIM is not compatible with this method of configuration. Refer to Via Okta Integration Network.\\n\\n\\nLog in to Okta as an administrator, and go to the Okta Admin console.\\n\\n\\nUnder Applications > Applications click Create App Integration.\\n\\n\\nSelect SAML 2.0.\\n\\n\\nEnter an App name (e.g., LangSmith) and optionally an App logo, then click Next.\\n\\n\\nEnter the following information in the Configure SAML page:\\n\\nSingle sign-on URL (ACS URL). Keep Use this for Recipient URL and Destination URL checked:\\n\\nUS: https://auth.langchain.com/auth/v1/sso/saml/acs\\nEU: https://eu.auth.langchain.com/auth/v1/sso/saml/acs\\n\\n\\nAudience URI (SP Entity ID):\\n\\nUS: https://auth.langchain.com/auth/v1/sso/saml/metadata\\nEU: https://eu.auth.langchain.com/auth/v1/sso/saml/metadata\\n\\n\\nName ID format: Persistent.\\nApplication username: email.\\nLeave the rest of the fields empty or set to their default.\\nClick Next.\\n\\n\\n\\nClick Finish.\\n\\n\\nCopy the Metadata URL from the Sign On page to use in the next step.\\n\\n\\nStep 2: Set up LangSmith SSO Configuration\\nFollow the instructions under initial configuration in the Fill in required information step, using the metadata URL from the previous step.\\nStep 3: Assign users to LangSmith in Okta\\n\\nUnder Applications > Applications, select the SAML application created in Step 1.\\nUnder the Assignments tab, click Assign then either Assign to People or Assign to Groups.\\nMake the desired selection(s), then Assign and Done.\\n\\nStep 4: Verify the SSO setup\\nHave a user with access sign in via the unique login URL from the SSO Configuration page, or have a user select the application from their Okta dashboard.\\n\\u200bSP-initiated SSO\\nOnce service-providerâ€“initiated SSO is configured, users can sign in using a unique login URL. You can find this in the LangSmith UI under Organization members and roles then SSO configuration.\\n\\u200bSet up SCIM for your organization\\nSystem for Cross-domain Identity Management (SCIM) is an open standard that allows for the automation of user provisioning. Using SCIM, you can automatically provision and de-provision users in your LangSmith organization and workspaces, keeping user access synchronized with your organizationâ€™s identity provider.\\nSCIM is available for organizations on the Enterprise plan. Contact sales to learn more.SCIM is available on Helm chart versions 0.10.41 (application version 0.10.108) and later.SCIM support is API-only (see instructions below).\\nSCIM eliminates the need for manual user management and ensures that user access is always up-to-date with your organizationâ€™s identity system. This allows for:\\n\\nAutomated user management: Users are automatically added, updated, and removed from LangSmith based on their status in your IdP.\\nReduced administrative overhead: No need to manage user access manually across multiple systems.\\nImproved security: Users who leave your organization are automatically deprovisioned from LangSmith.\\nConsistent access control: User attributes and group memberships are synchronized between systems.\\nScaling team access control: Efficiently manage large teams with many workspaces and custom roles.\\nRole assignment: Select specific Organization Roles and Workspace Roles for groups of users.\\n\\n\\u200bRequirements\\n\\u200bPrerequisites\\n\\nYour organization must be on an Enterprise plan.\\nYour Identity Provider (IdP) must support SCIM 2.0.\\nOnly Organization Admins can configure SCIM.\\nFor cloud customers: SAML SSO must be configurable for your organization.\\nFor self-hosted customers: OAuth with Client Secret authentication mode must be enabled.\\nFor self-hosted customers, network traffic must be allowed from the identity provider to LangSmith:\\n\\nMicrosoft Entra ID supports allowlisting IP ranges or an agent-based solution to provide connectivity.\\n(details).\\nOkta supports allow-listing IPs or domains (details)\\nor an agent-based solution (details) to provide connectivity.\\n\\n\\n\\n\\u200bRole Precedence\\nWhen a user belongs to multiple groups for the same workspace, the following precedence applies:\\n\\nOrganization Admin groups take highest precedence. Users in these groups will be Admin in all workspaces.\\nMost recently created workspace-specific group takes precedence over other workspace groups.\\n\\nWhen a group is deleted or a user is removed from a group, their access is updated according to their remaining group membership, following the precedence rules.SCIM group membership will override manually assigned roles or roles assigned via Just-in-time (JIT) provisioning. We recommend disabling JIT provisioning to avoid conflicts.\\n\\u200bEmail verification\\nIn cloud only, creating a new user with SCIM triggers an email to the user.\\nThey must verify their email address by clicking the link in this email.\\nThe link expires in 24 hours, and can be resent if needed by removing and re-adding the user via SCIM.\\n\\u200bAttributes and Mapping\\n\\u200bGroup Naming Convention\\nGroup membership maps to LangSmith workspace membership and workspace roles with a specific naming convention:\\nOrganization Admin Groups\\nFormat: <optional_prefix>Organization Admin or <optional_prefix>Organization Admins\\nExamples:\\n\\nLS:Organization Admins\\nGroups-Organization Admins\\nOrganization Admin\\n\\nWorkspace-Specific Groups\\nFormat: <optional_prefix><org_role_name>:<workspace_name>:<workspace_role_name>\\nExamples:\\n\\nLS:Organization User:Production:Annotators\\nGroups-Organization User:Engineering:Developers\\nOrganization User:Marketing:Viewers\\n\\n\\u200bMapping\\nWhile specific instructions depending on the identity provider may vary, these mappings show what is supported by the LangSmith SCIM integration:\\n\\u200bUser Attributes\\nLangSmith App AttributeIdentity Provider AttributeMatching PrecedenceuserName1email addressactive!deactivatedemails[type eq \"work\"].valueemail address2name.formatteddisplayName OR givenName + familyName3givenNamegivenNamefamilyNamefamilyNameexternalIdsub41\\n\\nuserName is not required by LangSmith\\nEmail address is required\\nUse the computed expression if your displayName does not match the format of Firstname Lastname\\nTo avoid inconsistency, this should match the SAML NameID assertion for cloud customers, or the sub OAuth2.0 claim for self-hosted.\\n\\n\\u200bGroup Attributes\\nLangSmith App AttributeIdentity Provider AttributeMatching PrecedencedisplayNamedisplayName11externalIdobjectIdmembersmembers\\n\\nGroups must follow the naming convention described in the Group Naming Convention section.\\nIf your company has a group naming policy, you should instead map from the description identity provider attribute and\\nset the description based on the Group Naming Convention section.\\n\\n\\u200bStep 1: Configure SAML SSO (Cloud only)\\nThere are two scenarios for SAML SSO configuration:\\n\\nIf SAML SSO is already configured for your organization, you should skip the steps to initially add the application (Add application from Okta Integration Network or Create a new Entra ID application integration), as you already have an application configured and just need to enable provisioning.\\nIf you are configuring SAML SSO for the first time alongside SCIM, first follow the instructions to set up SAML SSO, then follow the instructions here to enable SCIM.\\n\\n\\u200bNameID Format\\nLangSmith uses the SAML NameID to identify users. The NameID is a required field in the SAML response and is case-insensitive.\\nThe NameID must:\\n\\nBe unique to each user.\\nBe a persistent value that never changes, such as a randomly generated unique user ID.\\nMatch exactly on each sign-in attempt. It should not rely on user input.\\n\\nThe NameID should not be an email address or username because email addresses and usernames are more likely to change over time and can be case-sensitive.\\nThe NameID format must be Persistent, unless you are using a field, like email, that requires a different format.\\n\\u200bStep 2: Disable JIT provisioning\\nBefore enabling SCIM, disable Just-in-time (JIT) provisioning to prevent conflicts between automatic and manual user provisioning.\\n\\u200bDisabling JIT for Cloud\\nUse the PATCH /orgs/current/info endpoint:\\nCopycurl -X PATCH $LANGCHAIN_ENDPOINT/orgs/current/info \\\\\\n  -H \"X-Api-Key: $LANGCHAIN_API_KEY\" \\\\\\n  -H \"Content-Type: application/json\" \\\\\\n  -d \\'{\"jit_provisioning_enabled\": false}\\'\\n\\n\\u200bDisabling JIT for Self-Hosted\\nAs of LangSmith chart version 0.11.14, you can disable JIT provisioning for your self-hosted organization using SSO. To disable, set the following values:\\nCopycommonEnv:\\n  - name: SELF_HOSTED_JIT_PROVISIONING_ENABLED\\n    value: \"false\"\\n\\n\\u200bStep 3: Generate SCIM bearer token\\nIn self-hosted environments, the full URL below may look like https://langsmith.yourdomain.com/api/v1/platform/orgs/current/scim/tokens (without a subdomain, note the /api/v1 path prefix) or https://langsmith.yourdomain.com/subdomain/api/v1/platform/orgs/current/scim/tokens (with a subdomain) - see the ingress docs for more details.\\nGenerate a SCIM Bearer Token for your organization. This token will be used by your IdP to authenticate SCIM API requests. Ensure env vars are set appropriately, for example:\\nCopycurl -X POST $LANGCHAIN_ENDPOINT/v1/platform/orgs/current/scim/tokens \\\\\\n  -H \"X-Api-Key: $LANGCHAIN_API_KEY\" \\\\\\n  -H \"X-Organization-Id: $LANGCHAIN_ORGANIZATION_ID\" \\\\\\n  -H \"Content-Type: application/json\" \\\\\\n  -d \\'{\"description\": \"Your description here\"}\\'\\n\\nNote that the SCIM Bearer Token value is not available outside of the response to this request. These additional endpoints are present:\\n\\nGET /v1/platform/orgs/current/scim/tokens\\nGET /v1/platform/orgs/current/scim/tokens/{scim_token_id}\\nPATCH /v1/platform/orgs/current/scim/tokens/{scim_token_id} (only the description field is supported)\\nDELETE /v1/platform/orgs/current/scim/tokens/{scim_token_id}\\n\\n\\u200bStep 4: Configure your Identity Provider\\nIf you use Azure Entra ID (formerly Azure AD) or Okta, there are specific instructions for identity provider setup (refer to Azure Entra ID, Okta). The requirements and steps above are applicable for all identity providers.\\n\\u200bAzure Entra ID configuration steps\\nFor additional information, see Microsoftâ€™s documentation.\\nIn self-hosted installations, the oid JWT claim is used as the sub.\\nSee this Microsoft Learn link\\nand the related configuration instructions for additional details.\\nStep 1: Configure SCIM in your Enterprise Application\\n\\nLog in to the Azure portal with a privileged role (e.g., Global Administrator).\\nNavigate to your existing LangSmith Enterprise Application.\\nIn the left-side navigation, select Manage > Provisioning.\\nClick Get started.\\n\\nStep 2: Configure Admin credentials\\n\\n\\nUnder Admin Credentials:\\n\\n\\nTenant URL:\\n\\nUS: https://api.smith.langchain.com/scim/v2\\nEU: https://eu.api.smith.langchain.com/scim/v2\\nSelf-hosted: <langsmith_url>/scim/v2\\n\\n\\n\\nSecret Token: Enter the SCIM Bearer Token generated in Step 3.\\n\\n\\n\\n\\nClick Test Connection to verify the configuration.\\n\\n\\nClick Save.\\n\\n\\nStep 3: Configure Attribute Mappings\\nConfigure the following attribute mappings under Mappings:\\nUser Attributes\\nSet Target Object Actions to Create and Update (start with Delete disabled for safety):\\nLangSmith App AttributeMicrosoft Entra ID AttributeMatching PrecedenceuserNameuserPrincipalNameactiveNot([IsSoftDeleted])emails[type eq \"work\"].valuemail1name.formatteddisplayName OR Join(\" \", [givenName], [surname])2externalIdobjectId31\\n\\nUserâ€™s email address must be present in Entra ID.\\nUse the Join expression if your displayName does not match the format of Firstname Lastname.\\nTo avoid inconsistency, this should match the SAML NameID assertion and the sub OAuth2.0 claim. For SAML SSO in cloud, the Unique User Identifier (Name ID) required claim should be user.objectID and the Name identifier format should be persistent.\\n\\nGroup Attributes\\nSet Target Object Actions to Create and Update only (start with Delete disabled for safety):\\nLangSmith App AttributeMicrosoft Entra ID AttributeMatching PrecedencedisplayNamedisplayName11externalIdobjectIdmembersmembers\\n\\nGroups must follow the naming convention described in the Group Naming Convention section.\\nIf your company has a group naming policy, you should instead map from the description Microsoft Entra ID Attribute and\\nset the description based on the Group Naming Convention section.\\n\\nStep 4: Assign Users and Groups\\n\\nUnder Applications > Applications, select your LangSmith Enterprise Application.\\nUnder the Assignments tab, click Assign then either Assign to People or Assign to Groups.\\nMake the desired selection(s), then Assign and Done.\\n\\nStep 5: Enable Provisioning\\n\\nSet Provisioning Status to On under Provisioning.\\nMonitor the initial sync to ensure users and groups are provisioned correctly.\\nOnce verified, enable Delete actions for both User and Group mappings.\\n\\nFor troubleshooting, refer to the SAML SSO FAQs. If you have issues setting up SCIM, reach out to the LangChain support team at support@langchain.dev.\\n\\u200bOkta configuration steps\\nYou must use the Okta Lifecycle Management product. This product tier is required to use SCIM on Okta.\\nSupported features\\n\\nCreate users\\nUpdate user attributes\\nDeactivate users\\nGroup push\\n\\nStep 1: Add application from Okta Integration Network\\nIf you have already configured SSO login via SAML (cloud) or OAuth2.0 with OIDC (self-hosted), skip this step.\\nSee SAML SSO setup for cloud or OAuth2.0 setup for self-hosted.\\nStep 2: Configure API Integration\\n\\nIn the Provisioning tab, select Configure API integration.\\nSelect Enable API integration.\\nFor Base URL (if present):\\n\\n\\nUS: https://api.smith.langchain.com/scim/v2\\nEU: https://eu.api.smith.langchain.com/scim/v2\\nSelf-hosted: <langsmith_url>/scim/v2 (note there is no /api/v1 path prefix) or if a subdomain is configured <langsmith_url>/subdomain/scim/v2\\n\\n\\nFor API Token, paste the SCIM token you generated above.\\nKeep Import Groups checked.\\nTo verify the configuration, select Test API Credentials.\\nSelect Save.\\nAfter saving the API integration details, new settings tabs appear on the left. Select To App.\\nSelect Edit.\\nSelect the Enable checkbox for Create Users, Update Users, and Deactivate Users.\\nSelect Save.\\nAssign users and/or groups in the Assignments tab. Assigned users are created and managed in your LangSmith group.\\n\\nStep 3: Configure User Provisioning Settings\\n\\nConfigure provisioning: under Provisioning > To App > Provisioning to App, click Edit, then check Create Users, Update User Attributes, and Deactivate Users.\\nUnder <application_name> Attribute Mappings, set the user attribute mappings as shown below, and delete the rest:\\n\\n\\nStep 4: Push Groups\\nOkta does not support group attributes besides the group name itself, so group name must follow the naming convention described in the Group Naming Convention section.\\nFollow Oktaâ€™s Enable Group Push instructions to configure groups to push by name or by rule.\\n\\u200bOther Identity Providers\\nOther identity providers have not been tested but may function depending on their SCIM implementation.Was this page helpful?YesNoSuggest editsSet up resource tagsFAQsâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_billing', 'loc': 'https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_billing', 'changefreq': 'weekly', 'priority': '0.5'}, page_content=\"Page Not FoundSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationSetupOverviewCreate an account and API keySet up a workspaceManage organizations using the APIManage billingSet up resource tagsUser managementAdditional resourcesFAQsCloud architecture and scalabilityRegions FAQAuthentication methodsData purging for complianceRelease versionsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationPage Not FoundGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForum404Page Not FoundWe couldn't find the page you were looking for. Maybe you were looking for?Manage billing in your accountLangSmith-managed ClickHouseGet started with LangSmithAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_organization', 'loc': 'https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_organization', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Set up a workspace - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationSetupOverviewCreate an account and API keySet up a workspaceManage organizations using the APIManage billingSet up resource tagsUser managementAdditional resourcesFAQsCloud architecture and scalabilityRegions FAQAuthentication methodsData purging for complianceRelease versionsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationSetupSet up a workspaceGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageSet up an organizationCreate an organizationManage and navigate workspacesManage usersOrganization rolesSet up a workspaceCreate a workspaceManage usersConfigure workspace settingsDelete a workspaceDelete a workspace via the UISetupSet up a workspaceCopy pageCopy pageThis page describes setting up and managing your LangSmith organization and workspaces:\\n\\nSet up an organization: Create and manage organizations for team collaboration, including user management and role assignments.\\nSet up a workspace: Set up and configure workspaces to organize your LangSmith resources, manage workspace members, and configure settings for team collaboration.\\n\\nYou may find it helpful to refer to the overview on LangSmith resource hierarchy before you read this setup page.\\n\\u200bSet up an organization\\nIf youâ€™re interested in managing your organization and workspaces programmatically, see this how-to guide.\\n\\u200bCreate an organization\\nWhen you log in for the first time, LangSmith will create a personal organization for you automatically. If youâ€™d like to collaborate with others, you can create a separate organization and invite your team members to join.\\nTo do this, open the Organizations drawer by clicking your profile icon in the bottom left and click + New. Shared organizations require a credit card before they can be used. You will need to set up billing to proceed.\\n\\u200bManage and navigate workspaces\\nOnce youâ€™ve subscribed to a plan that allows for multiple users per organization, you can set up workspaces to collaborate more effectively and isolate LangSmith resources between different groups of users. To navigate between workspaces and access the resources within each workspace (trace projects, annotation queues, etc.), select the desired workspace from the picker in the top left:\\n\\n\\u200bManage users\\nManage membership in your shared organization in the Members and roles tabs on the Settings page. Here you can:\\n\\nInvite new users to your organization, selecting workspace membership and (if RBAC is enabled) workspace role.\\nEdit a userâ€™s organization role.\\nRemove users from your organization.\\n\\n\\nOrganizations on the Enterprise plan may set up custom workspace roles in the Roles tab. For more details, refer to the access control setup guide.\\n\\u200bOrganization roles\\nOrganization-scoped roles are used to determine access to organization settings. The role selected also impacts workspace membership:\\n\\nOrganization Admin grants full access to manage all organization configuration, users, billing, and workspaces. Any Organization Admin has Admin access to all workspaces in an organization.\\n\\n\\nOrganization User may read organization information, but cannot execute any write actions at the organization level. You can add an Organization User to a subset of workspaces and assigned workspace roles as usual (if RBAC is enabled), which specify permissions at the workspace level.\\n\\nThe Organization User role is only available in organizations on plans with multiple workspaces. In organizations limited to a single workspace, all users are Organization Admins. Custom organization-scoped roles are not available.\\nFor a full list of permissions associated with each role, refer to the Administration overview page.\\n\\u200bSet up a workspace\\nWhen you log in for the first time, a default workspace will be created for you in your personal organization. Workspaces are often used to separate resources between different teams or business units to establish clear trust boundaries between them. Within each workspace, Role-Based Access Control (RBAC) manages permissions and access levels, which ensures that users only have access to the resources and settings necessary for their role. Most LangSmith activity happens in the context of a workspace, each of which has its own settings and access controls.\\nTo organize resources within a workspace, you can use resource tags.\\n\\u200bCreate a workspace\\nTo create a new workspace, navigate to the Settings page Workspaces tab in your shared organization and click Add Workspace. Once you have created your workspace, you can manage its members and other configuration by selecting it on this page.\\n\\nDifferent plans have different limits placed on the number of workspaces that can be used in an organization. For more information, refer to the pricing page.\\n\\u200bManage users\\nOnly workspace Admins can manage workspace membership and, if RBAC is enabled, change a userâ€™s workspace role.\\nFor users that are already members of an organization, a workspace Admin may add them to a workspace in the Workspace members tab under Workspaces settings page. Users may also be invited directly to one or more workspaces when they are invited to an organization.\\n\\u200bConfigure workspace settings\\nWorkspace configuration exists in the Workspaces settings page tab. Select the workspace to configure and then the desired configuration sub-tab. The following example shows the API keys, and other configuration options including secrets, models, and shared URLs are available here as well.\\n\\n\\u200bDelete a workspace\\nDeleting a workspace will permanently delete the workspace and all associated data. This action cannot be undone.\\nYou can delete a workspace through the LangSmith UI or via API. You must be a workspace Admin in order to delete a workspace.\\n\\u200bDelete a workspace via the UI\\n\\nNavigate to Settings.\\nSelect the workspace you want to delete.\\nClick Delete in the top-right corner of the screen.\\n\\nWas this page helpful?YesNoSuggest editsCreate an account and API keyManage organizations using the APIâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_resource_tags', 'loc': 'https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_resource_tags', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Set up resource tags - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationSetupOverviewCreate an account and API keySet up a workspaceManage organizations using the APIManage billingSet up resource tagsUser managementAdditional resourcesFAQsCloud architecture and scalabilityRegions FAQAuthentication methodsData purging for complianceRelease versionsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationSetupSet up resource tagsGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageCreate a tagAssign a tag to a resourceDelete a tagFilter resources by tagsSetupSet up resource tagsCopy pageCopy pageBefore diving into this content, it might be helpful to read the following:\\nConceptual guide on organizations and workspaces\\n\\nResource tags are available for Plus and Enterprise plans.\\nWhile workspaces help separate trust boundaries and access control, tags help you organize resources within a workspace. Tags are key-value pairs that you can attach to resources.\\n\\u200bCreate a tag\\nTo create a tag, head to the workspace settings and click on the â€œResource Tagsâ€ tab. Here, youâ€™ll be able to see the existing tag values, grouped by key. Two keys Application and Environment are created by default.\\nTo create a new tag, click on the â€œNew Tagâ€ button. Youâ€™ll be prompted to enter a key and a value for the tag. Note that you can use an existing key or create a new one.\\n\\n\\u200bAssign a tag to a resource\\nWithin the same side panel for creating a new tag, you can also create assign resources to tags. Search for corresponding resources in the â€œAssign Resourcesâ€ section and select the resources you want to tag.\\nYou can only tag workspace-scoped resources with resource tags. This includes Tracing Projects, Annotation Queues, Deployments, Experiments, Datasets, and Prompts.\\nYou can also assign tags to resources from the resourceâ€™s detail page. Click on the Resource tags button to open up the tag panel and assign tags.\\n\\nTo un-assign a tag from a resource, click on the Trash icon next to the tag, both in the tag panel and the resource tag panel.\\n\\u200bDelete a tag\\nYou can delete either a key or a value of a tag from the workspace settings page. To delete a key, click on the Trash icon next to the key. To delete a value, click on the Trash icon next to the value.\\nNote that if you delete a key, all values associated with that key will also be deleted. When you delete a value, you will lose all associations between that value and resources.\\n\\n\\u200bFilter resources by tags\\nYou can use resource tags to organize your experience navigating resources in the workspace.\\nTo filter resources by tags in your workspace, open up the left-hand side panel and click on the tags icon. Here, you can select the tags you want to filter by.\\nIn the homepage, you can see updated counts for resources based on the tags youâ€™ve selected.\\nAs you navigate through the different product surfaces, you will only see resources that match the tags youâ€™ve selected. At any time, you can clear the tags to see all resources in the workspace or select different tags to filter by.\\nWas this page helpful?YesNoSuggest editsManage billingUser managementâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso', 'loc': 'https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='User management - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationSetupOverviewCreate an account and API keySet up a workspaceManage organizations using the APIManage billingSet up resource tagsUser managementAdditional resourcesFAQsCloud architecture and scalabilityRegions FAQAuthentication methodsData purging for complianceRelease versionsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationSetupUser managementGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageSet up access controlCreate a roleAssign a role to a userSet up SAML SSO for your organizationJust-in-time (JIT) provisioningLogin methods and accessEnforce SAML SSO onlyPrerequisitesInitial configurationEntra ID (Azure)GoogleOktaSupported featuresConfiguration stepsSP-initiated SSOSet up SCIM for your organizationRequirementsPrerequisitesRole PrecedenceEmail verificationAttributes and MappingGroup Naming ConventionMappingUser AttributesGroup AttributesStep 1: Configure SAML SSO (Cloud only)NameID FormatStep 2: Disable JIT provisioningDisabling JIT for CloudDisabling JIT for Self-HostedStep 3: Generate SCIM bearer tokenStep 4: Configure your Identity ProviderAzure Entra ID configuration stepsOkta configuration stepsOther Identity ProvidersSetupUser managementCopy pageCopy pageThis page covers user management features in LangSmith, including access control, authentication, and automated user provisioning:\\n\\nSet up access control: Configure role-based access control (RBAC) to manage user permissions within workspaces, including creating custom roles and assigning them to users.\\nSAML SSO (Enterprise plan): Set up Single Sign-On authentication for Enterprise customers using SAML 2.0, including configuration for popular identity providers.\\nSCIM User Provisioning (Enterprise plan): Automate user provisioning and deprovisioning between your identity provider and LangSmith using SCIM.\\n\\n\\u200bSet up access control\\nRBAC (Role-Based Access Control) is a feature that is only available to Enterprise customers. If you are interested in this feature, contact our sales team. Other plans default to using the Admin role for all users.\\nYou may find it helpful to read the Administration overview page before setting up access control.\\nLangSmith relies on RBAC to manage user permissions within a workspace. This allows you to control who can access your LangSmith workspace and what they can do within it. Only users with the workspace:manage permission can manage access control settings for a workspace.\\n\\u200bCreate a role\\nBy default, LangSmith comes with a set of system roles:\\n\\nAdmin: has full access to all resources within the workspace.\\nViewer: has read-only access to all resources within the workspace.\\nEditor: has full permissions except for workspace management (adding/removing users, changing roles, configuring service keys).\\n\\nIf these do not fit your access model, Organization Admins can create custom roles to suit your needs.\\nTo create a role, navigate to the Roles tab in the Members and roles section of the Organization settings page. Note that new roles that you create will be usable across all workspaces within your organization.\\nClick on the Create Role button to create a new role. A Create role form will open.\\n\\nAssign permissions for the different LangSmith resources that you want to control access to.\\n\\u200bAssign a role to a user\\nOnce you have your roles set up, you can assign them to users. To assign a role to a user, navigate to the Workspace members tab in the Workspaces section of the Organization settings page\\nEach user will have a Role dropdown that you can use to assign a role to them.\\n\\nYou can also invite new users with a given role.\\n\\n\\u200bSet up SAML SSO for your organization\\nSingle Sign-On (SSO) functionality is available for Enterprise Cloud customers to access LangSmith through a single authentication source. This allows administrators to centrally manage team access and keeps information more secure.\\nLangSmithâ€™s SSO configuration is built using the SAML (Security Assertion Markup Language) 2.0 standard. SAML 2.0 enables connecting an Identity Provider (IdP) to your organization for an easier, more secure login experience.\\nSSO services permit a user to use one set of credentials (for example, a name or email address and password) to access multiple applications. The service authenticates the end user only once for all the applications the user has been given rights to and eliminates further prompts when the user switches applications during the same session. The benefits of SSO include:\\n\\nStreamlines user management across systems for organization owners.\\nEnables organizations to enforce their own security policies (e.g., MFA).\\nRemoves the need for end users to remember and manage multiple passwords. Simplifies the end-user experience, by allowing sign in at one single access point across multiple applications.\\n\\n\\u200bJust-in-time (JIT) provisioning\\nLangSmith supports Just-in-time provisioning when using SAML SSO. This allows someone signing in via SAML SSO to join the organization and selected workspaces automatically as a member.\\nJIT provisioning only runs for new users, that is, users who do not already have access to the organization with the same email address via a different login method.\\n\\u200bLogin methods and access\\nOnce you have completed your configuration of SAML SSO for your organization, users will be able to log in via SAML SSO in addition to other login methods, such as username/password or Google Authenticationâ€:\\n\\nWhen logged in via SAML SSO, users can only access the corresponding organization with SAML SSO configured.\\nUsers with SAML SSO as their only login method do not have personal organizations.\\nWhen logged in via any other method, users can access the organization with SAML SSO configured along with any other organizations they are a part of.\\n\\n\\u200bEnforce SAML SSO only\\nTo ensure users can only access the organization when logged in using SAML SSO and no other method, check the Login via SSO only checkbox and click Save. Once this happens, users accessing the organization that are logged-in via a non-SSO login method are required to log back in using SAML SSO. This setting can be switched back to allow all login methods by unselecting the checkbox and clicking Save.\\nYou must be logged in via SAML SSO in order to update this setting to Only SAML SSO. This is to ensure the SAML settings are valid and avoid locking users out of your organization.\\nFor troubleshooting, refer to the SAML SSO FAQs. If you have issues setting up SAML SSO, reach out to the LangChain support team at support@langchain.dev.\\n\\u200bPrerequisites\\nSAML SSO is available for organizations on the Enterprise plan. Please contact sales to learn more.\\n\\nYour organization must be on an Enterprise plan.\\nYour Identity Provider (IdP) must support the SAML 2.0 standard.\\nOnly Organization Admins can configure SAML SSO.\\n\\nFor instructions on using SCIM along with SAML for user provisioning and deprovisioning, refer to the SCIM setup.\\n\\u200bInitial configuration\\nFor IdP-specific configuration steps, refer to one of the following:\\nEntra ID\\nGoogle\\nOkta\\n\\n\\n\\nIn your IdP: Configure a SAML application with the following details, then copy the metadata URL or XML for step 3.\\nThe following URLs are different for the US and EU regions. Ensure you select the correct link.\\n\\nSingle sign-on URL (or ACS URL):\\n\\nUS: https://auth.langchain.com/auth/v1/sso/saml/acs\\nEU: https://eu.auth.langchain.com/auth/v1/sso/saml/acs\\n\\n\\nAudience URI (or SP Entity ID):\\n\\nUS: https://auth.langchain.com/auth/v1/sso/saml/metadata\\nEU: https://eu.auth.langchain.com/auth/v1/sso/saml/metadata\\n\\n\\nName ID format: email address.\\nApplication username: email address.\\nRequired claims: sub and email.\\n\\n\\n\\nIn LangSmith: Go to Settings -> Members and roles -> SSO Configuration. Fill in the required information and submit to activate SSO login:\\n\\nFill in either the SAML metadata URL or SAML metadata XML.\\nSelect the Default workspace role and Default workspaces. New users logging in via SSO will be added to the specified workspaces with the selected role.\\n\\n\\n\\n\\nDefault workspace role and Default workspaces are editable. The updated settings will apply to new users only, not existing users.\\n(Coming soon) SAML metadata URL and SAML metadata XML are editable. This is usually only necessary when cryptographic keys are rotated/expired or the metadata URL has changed but the same IdP is still used.\\n\\n\\u200bEntra ID (Azure)\\nFor additional information, see Microsoftâ€™s documentation.\\n\\nStep 1: Create a new Entra ID application integration\\n\\n\\nLog in to the Azure portal with a privileged role (e.g., Global Administrator). On the left navigation pane, select the Entra ID service.\\n\\n\\nNavigate to Enterprise Applications and then select All Applications.\\n\\n\\nClick Create your own application.\\n\\n\\nIn the Create your own application window:\\n\\nEnter a name for your application (e.g., LangSmith).\\nSelect *Integrate any other application you donâ€™t find in the gallery (Non-gallery)**.\\n\\n\\n\\nClick Create.\\n\\n\\nStep 2: Configure the Entra ID application and obtain the SAML Metadata\\n\\n\\nOpen the enterprise application that you created.\\n\\n\\nIn the left-side navigation, select Manage > Single sign-on.\\n\\n\\nOn the Single sign-on page, click SAML.\\n\\n\\nUpdate the Basic SAML Configuration:\\n\\nIdentifier (Entity ID):\\n\\nUS: https://auth.langchain.com/auth/v1/sso/saml/metadata\\nEU: https://eu.auth.langchain.com/auth/v1/sso/saml/metadata\\n\\n\\nReply URL (Assertion Consumer Service URL):\\n\\nUS: https://auth.langchain.com/auth/v1/sso/saml/acs\\nEU: https://eu.auth.langchain.com/auth/v1/sso/saml/acs\\n\\n\\nLeave Relay State, Logout Url, and Sign on URL empty.\\nClick Save.\\n\\n\\n\\nEnsure required claims are present with Namespace: http://schemas.xmlsoap.org/ws/2005/05/identity/claims:\\n\\nsub: user.objectid.\\nemailaddress: user.userprincipalname or user.mail (if using the latter, ensure all users have the Email field filled in under Contact Information).\\n(Optional) For SCIM, see the setup documentation for specific instructions about Unique User Identifier (Name ID).\\n\\n\\n\\nOn the SAML-based Sign-on page, under SAML Certificates, copy the App Federation Metadata URL.\\n\\n\\nStep 3: Set up LangSmith SSO Configuration\\nFollow the instructions under initial configuration in the Fill in required information step, using the metadata URL from the previous step.\\nStep 4: Verify the SSO setup\\n\\n\\nAssign the application to users/groups in Entra ID:\\n\\n\\nSelect Manage > Users and groups.\\n\\n\\nClick Add user/group.\\n\\n\\nIn the Add Assignment window:\\n\\nUnder Users, click None Selected.\\nSearch for the user you want to assign to the enterprise application, and then click Select.\\nVerify that the user is selected, and click Assign.\\n\\n\\n\\n\\n\\nHave the user sign in via the unique login URL from the SSO Configuration page, or go to Manage > Single sign-on and select Test single sign-on with (application name).\\n\\n\\n\\u200bGoogle\\nFor additional information, see Googleâ€™s documentation.\\nStep 1: Create and configure the Google Workspace SAML application\\n\\n\\nMake sure youâ€™re signed into an administrator account with the appropriate permissions.\\n\\n\\nIn the Admin console, go to Menu -> Apps -> Web and mobile apps.\\n\\n\\nClick Add App and then Add custom SAML app.\\n\\n\\nEnter the app name and, optionally, upload an icon. Click Continue.\\n\\n\\nOn the Google Identity Provider details page, download the IDP metadata and save it for Step 2. Click Continue.\\n\\n\\nIn the Service Provider Details window, enter:\\n\\nACS URL:\\n\\nUS: https://auth.langchain.com/auth/v1/sso/saml/acs\\nEU: https://eu.auth.langchain.com/auth/v1/sso/saml/acs\\n\\n\\nEntity ID:\\n\\nUS: https://auth.langchain.com/auth/v1/sso/saml/metadata\\nEU: https://eu.auth.langchain.com/auth/v1/sso/saml/metadata\\n\\n\\nLeave Start URL and the Signed response box empty.\\nSet Name ID format to EMAIL and leave Name ID as the default (Basic Information > Primary email).\\nClick Continue.\\n\\n\\n\\nUse Add mapping to ensure required claims are present:\\n\\nBasic Information > Primary email -> email\\n\\n\\n\\nStep 2: Set up LangSmith SSO Configuration\\nFollow the instructions under initial configuration in the Fill in required information step, using the IDP metadata from the previous step as the metadata XML.\\nStep 3: Turn on the SAML app in Google\\n\\n\\nSelect the SAML app under Menu -> Apps -> Web and mobile apps\\n\\n\\nClick User access.\\n\\n\\nTurn on the service:\\n\\n\\nTo turn the service on for everyone in your organization, click On for everyone, and then click Save.\\n\\n\\nTo turn the service on for an organizational unit:\\n\\nAt the left, select the organizational unit then On.\\nIf the Service status is set to Inherited and you want to keep the updated setting, even if the parent setting changes, click Override.\\nIf the Service status is set to Overridden, either click Inherit to revert to the same setting as its parent, or click Save to keep the new setting, even if the parent setting changes.\\n\\n\\n\\nTo turn on a service for a set of users across or within organizational units, select an access group. For details, go to Use groups to customize service access.\\n\\n\\n\\n\\nEnsure that the email addresses your users use to sign in to LangSmith match the email addresses they use to sign in to your Google domain.\\n\\n\\nStep 4: Verify the SSO setup\\nHave a user with access sign in via the unique login URL from the SSO Configuration page, or go to the SAML application page in Google and click TEST SAML LOGIN.\\n\\u200bOkta\\n\\u200bSupported features\\n\\nIdP-initiated SSO (Single Sign-On)\\nSP-initiated SSO\\nJust-In-Time provisioning\\nEnforce SSO only\\n\\n\\u200bConfiguration steps\\nFor additional information, see Oktaâ€™s documentation.\\nStep 1: Create and configure the Okta SAML application\\nVia Okta Integration Network (recommended)\\n\\nSign in to Okta.\\nIn the upper-right corner, select Admin. The button is not visible from the Admin area.\\nSelect Browse App Integration Catalog.\\nFind and select the LangSmith application.\\nOn the application overview page, select Add Integration.\\nLeave ApiUrlBase empty.\\nFill in AuthHost:\\n\\nUS: auth.langchain.com\\nEU: eu.auth.langchain.com\\n\\n\\n(Optional, if planning to use SCIM as well) Fill in LangSmithUrl:\\n\\nUS: api.smith.langchain.com\\nEU: eu.api.smith.langchain.com\\n\\n\\nUnder Application Visibility, keep the box unchecked.\\nSelect Next.\\nSelect SAML 2.0.\\nFill in Sign-On Options:\\n\\nApplication username format: Email\\nUpdate application username on: Create and update\\nAllow users to securely see their password: leave unchecked.\\n\\n\\nCopy the Metadata URL from the Sign On Options page to use in the next step.\\n\\nVia Custom App Integration\\nSCIM is not compatible with this method of configuration. Refer to Via Okta Integration Network.\\n\\n\\nLog in to Okta as an administrator, and go to the Okta Admin console.\\n\\n\\nUnder Applications > Applications click Create App Integration.\\n\\n\\nSelect SAML 2.0.\\n\\n\\nEnter an App name (e.g., LangSmith) and optionally an App logo, then click Next.\\n\\n\\nEnter the following information in the Configure SAML page:\\n\\nSingle sign-on URL (ACS URL). Keep Use this for Recipient URL and Destination URL checked:\\n\\nUS: https://auth.langchain.com/auth/v1/sso/saml/acs\\nEU: https://eu.auth.langchain.com/auth/v1/sso/saml/acs\\n\\n\\nAudience URI (SP Entity ID):\\n\\nUS: https://auth.langchain.com/auth/v1/sso/saml/metadata\\nEU: https://eu.auth.langchain.com/auth/v1/sso/saml/metadata\\n\\n\\nName ID format: Persistent.\\nApplication username: email.\\nLeave the rest of the fields empty or set to their default.\\nClick Next.\\n\\n\\n\\nClick Finish.\\n\\n\\nCopy the Metadata URL from the Sign On page to use in the next step.\\n\\n\\nStep 2: Set up LangSmith SSO Configuration\\nFollow the instructions under initial configuration in the Fill in required information step, using the metadata URL from the previous step.\\nStep 3: Assign users to LangSmith in Okta\\n\\nUnder Applications > Applications, select the SAML application created in Step 1.\\nUnder the Assignments tab, click Assign then either Assign to People or Assign to Groups.\\nMake the desired selection(s), then Assign and Done.\\n\\nStep 4: Verify the SSO setup\\nHave a user with access sign in via the unique login URL from the SSO Configuration page, or have a user select the application from their Okta dashboard.\\n\\u200bSP-initiated SSO\\nOnce service-providerâ€“initiated SSO is configured, users can sign in using a unique login URL. You can find this in the LangSmith UI under Organization members and roles then SSO configuration.\\n\\u200bSet up SCIM for your organization\\nSystem for Cross-domain Identity Management (SCIM) is an open standard that allows for the automation of user provisioning. Using SCIM, you can automatically provision and de-provision users in your LangSmith organization and workspaces, keeping user access synchronized with your organizationâ€™s identity provider.\\nSCIM is available for organizations on the Enterprise plan. Contact sales to learn more.SCIM is available on Helm chart versions 0.10.41 (application version 0.10.108) and later.SCIM support is API-only (see instructions below).\\nSCIM eliminates the need for manual user management and ensures that user access is always up-to-date with your organizationâ€™s identity system. This allows for:\\n\\nAutomated user management: Users are automatically added, updated, and removed from LangSmith based on their status in your IdP.\\nReduced administrative overhead: No need to manage user access manually across multiple systems.\\nImproved security: Users who leave your organization are automatically deprovisioned from LangSmith.\\nConsistent access control: User attributes and group memberships are synchronized between systems.\\nScaling team access control: Efficiently manage large teams with many workspaces and custom roles.\\nRole assignment: Select specific Organization Roles and Workspace Roles for groups of users.\\n\\n\\u200bRequirements\\n\\u200bPrerequisites\\n\\nYour organization must be on an Enterprise plan.\\nYour Identity Provider (IdP) must support SCIM 2.0.\\nOnly Organization Admins can configure SCIM.\\nFor cloud customers: SAML SSO must be configurable for your organization.\\nFor self-hosted customers: OAuth with Client Secret authentication mode must be enabled.\\nFor self-hosted customers, network traffic must be allowed from the identity provider to LangSmith:\\n\\nMicrosoft Entra ID supports allowlisting IP ranges or an agent-based solution to provide connectivity.\\n(details).\\nOkta supports allow-listing IPs or domains (details)\\nor an agent-based solution (details) to provide connectivity.\\n\\n\\n\\n\\u200bRole Precedence\\nWhen a user belongs to multiple groups for the same workspace, the following precedence applies:\\n\\nOrganization Admin groups take highest precedence. Users in these groups will be Admin in all workspaces.\\nMost recently created workspace-specific group takes precedence over other workspace groups.\\n\\nWhen a group is deleted or a user is removed from a group, their access is updated according to their remaining group membership, following the precedence rules.SCIM group membership will override manually assigned roles or roles assigned via Just-in-time (JIT) provisioning. We recommend disabling JIT provisioning to avoid conflicts.\\n\\u200bEmail verification\\nIn cloud only, creating a new user with SCIM triggers an email to the user.\\nThey must verify their email address by clicking the link in this email.\\nThe link expires in 24 hours, and can be resent if needed by removing and re-adding the user via SCIM.\\n\\u200bAttributes and Mapping\\n\\u200bGroup Naming Convention\\nGroup membership maps to LangSmith workspace membership and workspace roles with a specific naming convention:\\nOrganization Admin Groups\\nFormat: <optional_prefix>Organization Admin or <optional_prefix>Organization Admins\\nExamples:\\n\\nLS:Organization Admins\\nGroups-Organization Admins\\nOrganization Admin\\n\\nWorkspace-Specific Groups\\nFormat: <optional_prefix><org_role_name>:<workspace_name>:<workspace_role_name>\\nExamples:\\n\\nLS:Organization User:Production:Annotators\\nGroups-Organization User:Engineering:Developers\\nOrganization User:Marketing:Viewers\\n\\n\\u200bMapping\\nWhile specific instructions depending on the identity provider may vary, these mappings show what is supported by the LangSmith SCIM integration:\\n\\u200bUser Attributes\\nLangSmith App AttributeIdentity Provider AttributeMatching PrecedenceuserName1email addressactive!deactivatedemails[type eq \"work\"].valueemail address2name.formatteddisplayName OR givenName + familyName3givenNamegivenNamefamilyNamefamilyNameexternalIdsub41\\n\\nuserName is not required by LangSmith\\nEmail address is required\\nUse the computed expression if your displayName does not match the format of Firstname Lastname\\nTo avoid inconsistency, this should match the SAML NameID assertion for cloud customers, or the sub OAuth2.0 claim for self-hosted.\\n\\n\\u200bGroup Attributes\\nLangSmith App AttributeIdentity Provider AttributeMatching PrecedencedisplayNamedisplayName11externalIdobjectIdmembersmembers\\n\\nGroups must follow the naming convention described in the Group Naming Convention section.\\nIf your company has a group naming policy, you should instead map from the description identity provider attribute and\\nset the description based on the Group Naming Convention section.\\n\\n\\u200bStep 1: Configure SAML SSO (Cloud only)\\nThere are two scenarios for SAML SSO configuration:\\n\\nIf SAML SSO is already configured for your organization, you should skip the steps to initially add the application (Add application from Okta Integration Network or Create a new Entra ID application integration), as you already have an application configured and just need to enable provisioning.\\nIf you are configuring SAML SSO for the first time alongside SCIM, first follow the instructions to set up SAML SSO, then follow the instructions here to enable SCIM.\\n\\n\\u200bNameID Format\\nLangSmith uses the SAML NameID to identify users. The NameID is a required field in the SAML response and is case-insensitive.\\nThe NameID must:\\n\\nBe unique to each user.\\nBe a persistent value that never changes, such as a randomly generated unique user ID.\\nMatch exactly on each sign-in attempt. It should not rely on user input.\\n\\nThe NameID should not be an email address or username because email addresses and usernames are more likely to change over time and can be case-sensitive.\\nThe NameID format must be Persistent, unless you are using a field, like email, that requires a different format.\\n\\u200bStep 2: Disable JIT provisioning\\nBefore enabling SCIM, disable Just-in-time (JIT) provisioning to prevent conflicts between automatic and manual user provisioning.\\n\\u200bDisabling JIT for Cloud\\nUse the PATCH /orgs/current/info endpoint:\\nCopycurl -X PATCH $LANGCHAIN_ENDPOINT/orgs/current/info \\\\\\n  -H \"X-Api-Key: $LANGCHAIN_API_KEY\" \\\\\\n  -H \"Content-Type: application/json\" \\\\\\n  -d \\'{\"jit_provisioning_enabled\": false}\\'\\n\\n\\u200bDisabling JIT for Self-Hosted\\nAs of LangSmith chart version 0.11.14, you can disable JIT provisioning for your self-hosted organization using SSO. To disable, set the following values:\\nCopycommonEnv:\\n  - name: SELF_HOSTED_JIT_PROVISIONING_ENABLED\\n    value: \"false\"\\n\\n\\u200bStep 3: Generate SCIM bearer token\\nIn self-hosted environments, the full URL below may look like https://langsmith.yourdomain.com/api/v1/platform/orgs/current/scim/tokens (without a subdomain, note the /api/v1 path prefix) or https://langsmith.yourdomain.com/subdomain/api/v1/platform/orgs/current/scim/tokens (with a subdomain) - see the ingress docs for more details.\\nGenerate a SCIM Bearer Token for your organization. This token will be used by your IdP to authenticate SCIM API requests. Ensure env vars are set appropriately, for example:\\nCopycurl -X POST $LANGCHAIN_ENDPOINT/v1/platform/orgs/current/scim/tokens \\\\\\n  -H \"X-Api-Key: $LANGCHAIN_API_KEY\" \\\\\\n  -H \"X-Organization-Id: $LANGCHAIN_ORGANIZATION_ID\" \\\\\\n  -H \"Content-Type: application/json\" \\\\\\n  -d \\'{\"description\": \"Your description here\"}\\'\\n\\nNote that the SCIM Bearer Token value is not available outside of the response to this request. These additional endpoints are present:\\n\\nGET /v1/platform/orgs/current/scim/tokens\\nGET /v1/platform/orgs/current/scim/tokens/{scim_token_id}\\nPATCH /v1/platform/orgs/current/scim/tokens/{scim_token_id} (only the description field is supported)\\nDELETE /v1/platform/orgs/current/scim/tokens/{scim_token_id}\\n\\n\\u200bStep 4: Configure your Identity Provider\\nIf you use Azure Entra ID (formerly Azure AD) or Okta, there are specific instructions for identity provider setup (refer to Azure Entra ID, Okta). The requirements and steps above are applicable for all identity providers.\\n\\u200bAzure Entra ID configuration steps\\nFor additional information, see Microsoftâ€™s documentation.\\nIn self-hosted installations, the oid JWT claim is used as the sub.\\nSee this Microsoft Learn link\\nand the related configuration instructions for additional details.\\nStep 1: Configure SCIM in your Enterprise Application\\n\\nLog in to the Azure portal with a privileged role (e.g., Global Administrator).\\nNavigate to your existing LangSmith Enterprise Application.\\nIn the left-side navigation, select Manage > Provisioning.\\nClick Get started.\\n\\nStep 2: Configure Admin credentials\\n\\n\\nUnder Admin Credentials:\\n\\n\\nTenant URL:\\n\\nUS: https://api.smith.langchain.com/scim/v2\\nEU: https://eu.api.smith.langchain.com/scim/v2\\nSelf-hosted: <langsmith_url>/scim/v2\\n\\n\\n\\nSecret Token: Enter the SCIM Bearer Token generated in Step 3.\\n\\n\\n\\n\\nClick Test Connection to verify the configuration.\\n\\n\\nClick Save.\\n\\n\\nStep 3: Configure Attribute Mappings\\nConfigure the following attribute mappings under Mappings:\\nUser Attributes\\nSet Target Object Actions to Create and Update (start with Delete disabled for safety):\\nLangSmith App AttributeMicrosoft Entra ID AttributeMatching PrecedenceuserNameuserPrincipalNameactiveNot([IsSoftDeleted])emails[type eq \"work\"].valuemail1name.formatteddisplayName OR Join(\" \", [givenName], [surname])2externalIdobjectId31\\n\\nUserâ€™s email address must be present in Entra ID.\\nUse the Join expression if your displayName does not match the format of Firstname Lastname.\\nTo avoid inconsistency, this should match the SAML NameID assertion and the sub OAuth2.0 claim. For SAML SSO in cloud, the Unique User Identifier (Name ID) required claim should be user.objectID and the Name identifier format should be persistent.\\n\\nGroup Attributes\\nSet Target Object Actions to Create and Update only (start with Delete disabled for safety):\\nLangSmith App AttributeMicrosoft Entra ID AttributeMatching PrecedencedisplayNamedisplayName11externalIdobjectIdmembersmembers\\n\\nGroups must follow the naming convention described in the Group Naming Convention section.\\nIf your company has a group naming policy, you should instead map from the description Microsoft Entra ID Attribute and\\nset the description based on the Group Naming Convention section.\\n\\nStep 4: Assign Users and Groups\\n\\nUnder Applications > Applications, select your LangSmith Enterprise Application.\\nUnder the Assignments tab, click Assign then either Assign to People or Assign to Groups.\\nMake the desired selection(s), then Assign and Done.\\n\\nStep 5: Enable Provisioning\\n\\nSet Provisioning Status to On under Provisioning.\\nMonitor the initial sync to ensure users and groups are provisioned correctly.\\nOnce verified, enable Delete actions for both User and Group mappings.\\n\\nFor troubleshooting, refer to the SAML SSO FAQs. If you have issues setting up SCIM, reach out to the LangChain support team at support@langchain.dev.\\n\\u200bOkta configuration steps\\nYou must use the Okta Lifecycle Management product. This product tier is required to use SCIM on Okta.\\nSupported features\\n\\nCreate users\\nUpdate user attributes\\nDeactivate users\\nGroup push\\n\\nStep 1: Add application from Okta Integration Network\\nIf you have already configured SSO login via SAML (cloud) or OAuth2.0 with OIDC (self-hosted), skip this step.\\nSee SAML SSO setup for cloud or OAuth2.0 setup for self-hosted.\\nStep 2: Configure API Integration\\n\\nIn the Provisioning tab, select Configure API integration.\\nSelect Enable API integration.\\nFor Base URL (if present):\\n\\n\\nUS: https://api.smith.langchain.com/scim/v2\\nEU: https://eu.api.smith.langchain.com/scim/v2\\nSelf-hosted: <langsmith_url>/scim/v2 (note there is no /api/v1 path prefix) or if a subdomain is configured <langsmith_url>/subdomain/scim/v2\\n\\n\\nFor API Token, paste the SCIM token you generated above.\\nKeep Import Groups checked.\\nTo verify the configuration, select Test API Credentials.\\nSelect Save.\\nAfter saving the API integration details, new settings tabs appear on the left. Select To App.\\nSelect Edit.\\nSelect the Enable checkbox for Create Users, Update Users, and Deactivate Users.\\nSelect Save.\\nAssign users and/or groups in the Assignments tab. Assigned users are created and managed in your LangSmith group.\\n\\nStep 3: Configure User Provisioning Settings\\n\\nConfigure provisioning: under Provisioning > To App > Provisioning to App, click Edit, then check Create Users, Update User Attributes, and Deactivate Users.\\nUnder <application_name> Attribute Mappings, set the user attribute mappings as shown below, and delete the rest:\\n\\n\\nStep 4: Push Groups\\nOkta does not support group attributes besides the group name itself, so group name must follow the naming convention described in the Group Naming Convention section.\\nFollow Oktaâ€™s Enable Group Push instructions to configure groups to push by name or by rule.\\n\\u200bOther Identity Providers\\nOther identity providers have not been tested but may function depending on their SCIM implementation.Was this page helpful?YesNoSuggest editsSet up resource tagsFAQsâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_scim', 'loc': 'https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_scim', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='User management - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationSetupOverviewCreate an account and API keySet up a workspaceManage organizations using the APIManage billingSet up resource tagsUser managementAdditional resourcesFAQsCloud architecture and scalabilityRegions FAQAuthentication methodsData purging for complianceRelease versionsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationSetupUser managementGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageSet up access controlCreate a roleAssign a role to a userSet up SAML SSO for your organizationJust-in-time (JIT) provisioningLogin methods and accessEnforce SAML SSO onlyPrerequisitesInitial configurationEntra ID (Azure)GoogleOktaSupported featuresConfiguration stepsSP-initiated SSOSet up SCIM for your organizationRequirementsPrerequisitesRole PrecedenceEmail verificationAttributes and MappingGroup Naming ConventionMappingUser AttributesGroup AttributesStep 1: Configure SAML SSO (Cloud only)NameID FormatStep 2: Disable JIT provisioningDisabling JIT for CloudDisabling JIT for Self-HostedStep 3: Generate SCIM bearer tokenStep 4: Configure your Identity ProviderAzure Entra ID configuration stepsOkta configuration stepsOther Identity ProvidersSetupUser managementCopy pageCopy pageThis page covers user management features in LangSmith, including access control, authentication, and automated user provisioning:\\n\\nSet up access control: Configure role-based access control (RBAC) to manage user permissions within workspaces, including creating custom roles and assigning them to users.\\nSAML SSO (Enterprise plan): Set up Single Sign-On authentication for Enterprise customers using SAML 2.0, including configuration for popular identity providers.\\nSCIM User Provisioning (Enterprise plan): Automate user provisioning and deprovisioning between your identity provider and LangSmith using SCIM.\\n\\n\\u200bSet up access control\\nRBAC (Role-Based Access Control) is a feature that is only available to Enterprise customers. If you are interested in this feature, contact our sales team. Other plans default to using the Admin role for all users.\\nYou may find it helpful to read the Administration overview page before setting up access control.\\nLangSmith relies on RBAC to manage user permissions within a workspace. This allows you to control who can access your LangSmith workspace and what they can do within it. Only users with the workspace:manage permission can manage access control settings for a workspace.\\n\\u200bCreate a role\\nBy default, LangSmith comes with a set of system roles:\\n\\nAdmin: has full access to all resources within the workspace.\\nViewer: has read-only access to all resources within the workspace.\\nEditor: has full permissions except for workspace management (adding/removing users, changing roles, configuring service keys).\\n\\nIf these do not fit your access model, Organization Admins can create custom roles to suit your needs.\\nTo create a role, navigate to the Roles tab in the Members and roles section of the Organization settings page. Note that new roles that you create will be usable across all workspaces within your organization.\\nClick on the Create Role button to create a new role. A Create role form will open.\\n\\nAssign permissions for the different LangSmith resources that you want to control access to.\\n\\u200bAssign a role to a user\\nOnce you have your roles set up, you can assign them to users. To assign a role to a user, navigate to the Workspace members tab in the Workspaces section of the Organization settings page\\nEach user will have a Role dropdown that you can use to assign a role to them.\\n\\nYou can also invite new users with a given role.\\n\\n\\u200bSet up SAML SSO for your organization\\nSingle Sign-On (SSO) functionality is available for Enterprise Cloud customers to access LangSmith through a single authentication source. This allows administrators to centrally manage team access and keeps information more secure.\\nLangSmithâ€™s SSO configuration is built using the SAML (Security Assertion Markup Language) 2.0 standard. SAML 2.0 enables connecting an Identity Provider (IdP) to your organization for an easier, more secure login experience.\\nSSO services permit a user to use one set of credentials (for example, a name or email address and password) to access multiple applications. The service authenticates the end user only once for all the applications the user has been given rights to and eliminates further prompts when the user switches applications during the same session. The benefits of SSO include:\\n\\nStreamlines user management across systems for organization owners.\\nEnables organizations to enforce their own security policies (e.g., MFA).\\nRemoves the need for end users to remember and manage multiple passwords. Simplifies the end-user experience, by allowing sign in at one single access point across multiple applications.\\n\\n\\u200bJust-in-time (JIT) provisioning\\nLangSmith supports Just-in-time provisioning when using SAML SSO. This allows someone signing in via SAML SSO to join the organization and selected workspaces automatically as a member.\\nJIT provisioning only runs for new users, that is, users who do not already have access to the organization with the same email address via a different login method.\\n\\u200bLogin methods and access\\nOnce you have completed your configuration of SAML SSO for your organization, users will be able to log in via SAML SSO in addition to other login methods, such as username/password or Google Authenticationâ€:\\n\\nWhen logged in via SAML SSO, users can only access the corresponding organization with SAML SSO configured.\\nUsers with SAML SSO as their only login method do not have personal organizations.\\nWhen logged in via any other method, users can access the organization with SAML SSO configured along with any other organizations they are a part of.\\n\\n\\u200bEnforce SAML SSO only\\nTo ensure users can only access the organization when logged in using SAML SSO and no other method, check the Login via SSO only checkbox and click Save. Once this happens, users accessing the organization that are logged-in via a non-SSO login method are required to log back in using SAML SSO. This setting can be switched back to allow all login methods by unselecting the checkbox and clicking Save.\\nYou must be logged in via SAML SSO in order to update this setting to Only SAML SSO. This is to ensure the SAML settings are valid and avoid locking users out of your organization.\\nFor troubleshooting, refer to the SAML SSO FAQs. If you have issues setting up SAML SSO, reach out to the LangChain support team at support@langchain.dev.\\n\\u200bPrerequisites\\nSAML SSO is available for organizations on the Enterprise plan. Please contact sales to learn more.\\n\\nYour organization must be on an Enterprise plan.\\nYour Identity Provider (IdP) must support the SAML 2.0 standard.\\nOnly Organization Admins can configure SAML SSO.\\n\\nFor instructions on using SCIM along with SAML for user provisioning and deprovisioning, refer to the SCIM setup.\\n\\u200bInitial configuration\\nFor IdP-specific configuration steps, refer to one of the following:\\nEntra ID\\nGoogle\\nOkta\\n\\n\\n\\nIn your IdP: Configure a SAML application with the following details, then copy the metadata URL or XML for step 3.\\nThe following URLs are different for the US and EU regions. Ensure you select the correct link.\\n\\nSingle sign-on URL (or ACS URL):\\n\\nUS: https://auth.langchain.com/auth/v1/sso/saml/acs\\nEU: https://eu.auth.langchain.com/auth/v1/sso/saml/acs\\n\\n\\nAudience URI (or SP Entity ID):\\n\\nUS: https://auth.langchain.com/auth/v1/sso/saml/metadata\\nEU: https://eu.auth.langchain.com/auth/v1/sso/saml/metadata\\n\\n\\nName ID format: email address.\\nApplication username: email address.\\nRequired claims: sub and email.\\n\\n\\n\\nIn LangSmith: Go to Settings -> Members and roles -> SSO Configuration. Fill in the required information and submit to activate SSO login:\\n\\nFill in either the SAML metadata URL or SAML metadata XML.\\nSelect the Default workspace role and Default workspaces. New users logging in via SSO will be added to the specified workspaces with the selected role.\\n\\n\\n\\n\\nDefault workspace role and Default workspaces are editable. The updated settings will apply to new users only, not existing users.\\n(Coming soon) SAML metadata URL and SAML metadata XML are editable. This is usually only necessary when cryptographic keys are rotated/expired or the metadata URL has changed but the same IdP is still used.\\n\\n\\u200bEntra ID (Azure)\\nFor additional information, see Microsoftâ€™s documentation.\\n\\nStep 1: Create a new Entra ID application integration\\n\\n\\nLog in to the Azure portal with a privileged role (e.g., Global Administrator). On the left navigation pane, select the Entra ID service.\\n\\n\\nNavigate to Enterprise Applications and then select All Applications.\\n\\n\\nClick Create your own application.\\n\\n\\nIn the Create your own application window:\\n\\nEnter a name for your application (e.g., LangSmith).\\nSelect *Integrate any other application you donâ€™t find in the gallery (Non-gallery)**.\\n\\n\\n\\nClick Create.\\n\\n\\nStep 2: Configure the Entra ID application and obtain the SAML Metadata\\n\\n\\nOpen the enterprise application that you created.\\n\\n\\nIn the left-side navigation, select Manage > Single sign-on.\\n\\n\\nOn the Single sign-on page, click SAML.\\n\\n\\nUpdate the Basic SAML Configuration:\\n\\nIdentifier (Entity ID):\\n\\nUS: https://auth.langchain.com/auth/v1/sso/saml/metadata\\nEU: https://eu.auth.langchain.com/auth/v1/sso/saml/metadata\\n\\n\\nReply URL (Assertion Consumer Service URL):\\n\\nUS: https://auth.langchain.com/auth/v1/sso/saml/acs\\nEU: https://eu.auth.langchain.com/auth/v1/sso/saml/acs\\n\\n\\nLeave Relay State, Logout Url, and Sign on URL empty.\\nClick Save.\\n\\n\\n\\nEnsure required claims are present with Namespace: http://schemas.xmlsoap.org/ws/2005/05/identity/claims:\\n\\nsub: user.objectid.\\nemailaddress: user.userprincipalname or user.mail (if using the latter, ensure all users have the Email field filled in under Contact Information).\\n(Optional) For SCIM, see the setup documentation for specific instructions about Unique User Identifier (Name ID).\\n\\n\\n\\nOn the SAML-based Sign-on page, under SAML Certificates, copy the App Federation Metadata URL.\\n\\n\\nStep 3: Set up LangSmith SSO Configuration\\nFollow the instructions under initial configuration in the Fill in required information step, using the metadata URL from the previous step.\\nStep 4: Verify the SSO setup\\n\\n\\nAssign the application to users/groups in Entra ID:\\n\\n\\nSelect Manage > Users and groups.\\n\\n\\nClick Add user/group.\\n\\n\\nIn the Add Assignment window:\\n\\nUnder Users, click None Selected.\\nSearch for the user you want to assign to the enterprise application, and then click Select.\\nVerify that the user is selected, and click Assign.\\n\\n\\n\\n\\n\\nHave the user sign in via the unique login URL from the SSO Configuration page, or go to Manage > Single sign-on and select Test single sign-on with (application name).\\n\\n\\n\\u200bGoogle\\nFor additional information, see Googleâ€™s documentation.\\nStep 1: Create and configure the Google Workspace SAML application\\n\\n\\nMake sure youâ€™re signed into an administrator account with the appropriate permissions.\\n\\n\\nIn the Admin console, go to Menu -> Apps -> Web and mobile apps.\\n\\n\\nClick Add App and then Add custom SAML app.\\n\\n\\nEnter the app name and, optionally, upload an icon. Click Continue.\\n\\n\\nOn the Google Identity Provider details page, download the IDP metadata and save it for Step 2. Click Continue.\\n\\n\\nIn the Service Provider Details window, enter:\\n\\nACS URL:\\n\\nUS: https://auth.langchain.com/auth/v1/sso/saml/acs\\nEU: https://eu.auth.langchain.com/auth/v1/sso/saml/acs\\n\\n\\nEntity ID:\\n\\nUS: https://auth.langchain.com/auth/v1/sso/saml/metadata\\nEU: https://eu.auth.langchain.com/auth/v1/sso/saml/metadata\\n\\n\\nLeave Start URL and the Signed response box empty.\\nSet Name ID format to EMAIL and leave Name ID as the default (Basic Information > Primary email).\\nClick Continue.\\n\\n\\n\\nUse Add mapping to ensure required claims are present:\\n\\nBasic Information > Primary email -> email\\n\\n\\n\\nStep 2: Set up LangSmith SSO Configuration\\nFollow the instructions under initial configuration in the Fill in required information step, using the IDP metadata from the previous step as the metadata XML.\\nStep 3: Turn on the SAML app in Google\\n\\n\\nSelect the SAML app under Menu -> Apps -> Web and mobile apps\\n\\n\\nClick User access.\\n\\n\\nTurn on the service:\\n\\n\\nTo turn the service on for everyone in your organization, click On for everyone, and then click Save.\\n\\n\\nTo turn the service on for an organizational unit:\\n\\nAt the left, select the organizational unit then On.\\nIf the Service status is set to Inherited and you want to keep the updated setting, even if the parent setting changes, click Override.\\nIf the Service status is set to Overridden, either click Inherit to revert to the same setting as its parent, or click Save to keep the new setting, even if the parent setting changes.\\n\\n\\n\\nTo turn on a service for a set of users across or within organizational units, select an access group. For details, go to Use groups to customize service access.\\n\\n\\n\\n\\nEnsure that the email addresses your users use to sign in to LangSmith match the email addresses they use to sign in to your Google domain.\\n\\n\\nStep 4: Verify the SSO setup\\nHave a user with access sign in via the unique login URL from the SSO Configuration page, or go to the SAML application page in Google and click TEST SAML LOGIN.\\n\\u200bOkta\\n\\u200bSupported features\\n\\nIdP-initiated SSO (Single Sign-On)\\nSP-initiated SSO\\nJust-In-Time provisioning\\nEnforce SSO only\\n\\n\\u200bConfiguration steps\\nFor additional information, see Oktaâ€™s documentation.\\nStep 1: Create and configure the Okta SAML application\\nVia Okta Integration Network (recommended)\\n\\nSign in to Okta.\\nIn the upper-right corner, select Admin. The button is not visible from the Admin area.\\nSelect Browse App Integration Catalog.\\nFind and select the LangSmith application.\\nOn the application overview page, select Add Integration.\\nLeave ApiUrlBase empty.\\nFill in AuthHost:\\n\\nUS: auth.langchain.com\\nEU: eu.auth.langchain.com\\n\\n\\n(Optional, if planning to use SCIM as well) Fill in LangSmithUrl:\\n\\nUS: api.smith.langchain.com\\nEU: eu.api.smith.langchain.com\\n\\n\\nUnder Application Visibility, keep the box unchecked.\\nSelect Next.\\nSelect SAML 2.0.\\nFill in Sign-On Options:\\n\\nApplication username format: Email\\nUpdate application username on: Create and update\\nAllow users to securely see their password: leave unchecked.\\n\\n\\nCopy the Metadata URL from the Sign On Options page to use in the next step.\\n\\nVia Custom App Integration\\nSCIM is not compatible with this method of configuration. Refer to Via Okta Integration Network.\\n\\n\\nLog in to Okta as an administrator, and go to the Okta Admin console.\\n\\n\\nUnder Applications > Applications click Create App Integration.\\n\\n\\nSelect SAML 2.0.\\n\\n\\nEnter an App name (e.g., LangSmith) and optionally an App logo, then click Next.\\n\\n\\nEnter the following information in the Configure SAML page:\\n\\nSingle sign-on URL (ACS URL). Keep Use this for Recipient URL and Destination URL checked:\\n\\nUS: https://auth.langchain.com/auth/v1/sso/saml/acs\\nEU: https://eu.auth.langchain.com/auth/v1/sso/saml/acs\\n\\n\\nAudience URI (SP Entity ID):\\n\\nUS: https://auth.langchain.com/auth/v1/sso/saml/metadata\\nEU: https://eu.auth.langchain.com/auth/v1/sso/saml/metadata\\n\\n\\nName ID format: Persistent.\\nApplication username: email.\\nLeave the rest of the fields empty or set to their default.\\nClick Next.\\n\\n\\n\\nClick Finish.\\n\\n\\nCopy the Metadata URL from the Sign On page to use in the next step.\\n\\n\\nStep 2: Set up LangSmith SSO Configuration\\nFollow the instructions under initial configuration in the Fill in required information step, using the metadata URL from the previous step.\\nStep 3: Assign users to LangSmith in Okta\\n\\nUnder Applications > Applications, select the SAML application created in Step 1.\\nUnder the Assignments tab, click Assign then either Assign to People or Assign to Groups.\\nMake the desired selection(s), then Assign and Done.\\n\\nStep 4: Verify the SSO setup\\nHave a user with access sign in via the unique login URL from the SSO Configuration page, or have a user select the application from their Okta dashboard.\\n\\u200bSP-initiated SSO\\nOnce service-providerâ€“initiated SSO is configured, users can sign in using a unique login URL. You can find this in the LangSmith UI under Organization members and roles then SSO configuration.\\n\\u200bSet up SCIM for your organization\\nSystem for Cross-domain Identity Management (SCIM) is an open standard that allows for the automation of user provisioning. Using SCIM, you can automatically provision and de-provision users in your LangSmith organization and workspaces, keeping user access synchronized with your organizationâ€™s identity provider.\\nSCIM is available for organizations on the Enterprise plan. Contact sales to learn more.SCIM is available on Helm chart versions 0.10.41 (application version 0.10.108) and later.SCIM support is API-only (see instructions below).\\nSCIM eliminates the need for manual user management and ensures that user access is always up-to-date with your organizationâ€™s identity system. This allows for:\\n\\nAutomated user management: Users are automatically added, updated, and removed from LangSmith based on their status in your IdP.\\nReduced administrative overhead: No need to manage user access manually across multiple systems.\\nImproved security: Users who leave your organization are automatically deprovisioned from LangSmith.\\nConsistent access control: User attributes and group memberships are synchronized between systems.\\nScaling team access control: Efficiently manage large teams with many workspaces and custom roles.\\nRole assignment: Select specific Organization Roles and Workspace Roles for groups of users.\\n\\n\\u200bRequirements\\n\\u200bPrerequisites\\n\\nYour organization must be on an Enterprise plan.\\nYour Identity Provider (IdP) must support SCIM 2.0.\\nOnly Organization Admins can configure SCIM.\\nFor cloud customers: SAML SSO must be configurable for your organization.\\nFor self-hosted customers: OAuth with Client Secret authentication mode must be enabled.\\nFor self-hosted customers, network traffic must be allowed from the identity provider to LangSmith:\\n\\nMicrosoft Entra ID supports allowlisting IP ranges or an agent-based solution to provide connectivity.\\n(details).\\nOkta supports allow-listing IPs or domains (details)\\nor an agent-based solution (details) to provide connectivity.\\n\\n\\n\\n\\u200bRole Precedence\\nWhen a user belongs to multiple groups for the same workspace, the following precedence applies:\\n\\nOrganization Admin groups take highest precedence. Users in these groups will be Admin in all workspaces.\\nMost recently created workspace-specific group takes precedence over other workspace groups.\\n\\nWhen a group is deleted or a user is removed from a group, their access is updated according to their remaining group membership, following the precedence rules.SCIM group membership will override manually assigned roles or roles assigned via Just-in-time (JIT) provisioning. We recommend disabling JIT provisioning to avoid conflicts.\\n\\u200bEmail verification\\nIn cloud only, creating a new user with SCIM triggers an email to the user.\\nThey must verify their email address by clicking the link in this email.\\nThe link expires in 24 hours, and can be resent if needed by removing and re-adding the user via SCIM.\\n\\u200bAttributes and Mapping\\n\\u200bGroup Naming Convention\\nGroup membership maps to LangSmith workspace membership and workspace roles with a specific naming convention:\\nOrganization Admin Groups\\nFormat: <optional_prefix>Organization Admin or <optional_prefix>Organization Admins\\nExamples:\\n\\nLS:Organization Admins\\nGroups-Organization Admins\\nOrganization Admin\\n\\nWorkspace-Specific Groups\\nFormat: <optional_prefix><org_role_name>:<workspace_name>:<workspace_role_name>\\nExamples:\\n\\nLS:Organization User:Production:Annotators\\nGroups-Organization User:Engineering:Developers\\nOrganization User:Marketing:Viewers\\n\\n\\u200bMapping\\nWhile specific instructions depending on the identity provider may vary, these mappings show what is supported by the LangSmith SCIM integration:\\n\\u200bUser Attributes\\nLangSmith App AttributeIdentity Provider AttributeMatching PrecedenceuserName1email addressactive!deactivatedemails[type eq \"work\"].valueemail address2name.formatteddisplayName OR givenName + familyName3givenNamegivenNamefamilyNamefamilyNameexternalIdsub41\\n\\nuserName is not required by LangSmith\\nEmail address is required\\nUse the computed expression if your displayName does not match the format of Firstname Lastname\\nTo avoid inconsistency, this should match the SAML NameID assertion for cloud customers, or the sub OAuth2.0 claim for self-hosted.\\n\\n\\u200bGroup Attributes\\nLangSmith App AttributeIdentity Provider AttributeMatching PrecedencedisplayNamedisplayName11externalIdobjectIdmembersmembers\\n\\nGroups must follow the naming convention described in the Group Naming Convention section.\\nIf your company has a group naming policy, you should instead map from the description identity provider attribute and\\nset the description based on the Group Naming Convention section.\\n\\n\\u200bStep 1: Configure SAML SSO (Cloud only)\\nThere are two scenarios for SAML SSO configuration:\\n\\nIf SAML SSO is already configured for your organization, you should skip the steps to initially add the application (Add application from Okta Integration Network or Create a new Entra ID application integration), as you already have an application configured and just need to enable provisioning.\\nIf you are configuring SAML SSO for the first time alongside SCIM, first follow the instructions to set up SAML SSO, then follow the instructions here to enable SCIM.\\n\\n\\u200bNameID Format\\nLangSmith uses the SAML NameID to identify users. The NameID is a required field in the SAML response and is case-insensitive.\\nThe NameID must:\\n\\nBe unique to each user.\\nBe a persistent value that never changes, such as a randomly generated unique user ID.\\nMatch exactly on each sign-in attempt. It should not rely on user input.\\n\\nThe NameID should not be an email address or username because email addresses and usernames are more likely to change over time and can be case-sensitive.\\nThe NameID format must be Persistent, unless you are using a field, like email, that requires a different format.\\n\\u200bStep 2: Disable JIT provisioning\\nBefore enabling SCIM, disable Just-in-time (JIT) provisioning to prevent conflicts between automatic and manual user provisioning.\\n\\u200bDisabling JIT for Cloud\\nUse the PATCH /orgs/current/info endpoint:\\nCopycurl -X PATCH $LANGCHAIN_ENDPOINT/orgs/current/info \\\\\\n  -H \"X-Api-Key: $LANGCHAIN_API_KEY\" \\\\\\n  -H \"Content-Type: application/json\" \\\\\\n  -d \\'{\"jit_provisioning_enabled\": false}\\'\\n\\n\\u200bDisabling JIT for Self-Hosted\\nAs of LangSmith chart version 0.11.14, you can disable JIT provisioning for your self-hosted organization using SSO. To disable, set the following values:\\nCopycommonEnv:\\n  - name: SELF_HOSTED_JIT_PROVISIONING_ENABLED\\n    value: \"false\"\\n\\n\\u200bStep 3: Generate SCIM bearer token\\nIn self-hosted environments, the full URL below may look like https://langsmith.yourdomain.com/api/v1/platform/orgs/current/scim/tokens (without a subdomain, note the /api/v1 path prefix) or https://langsmith.yourdomain.com/subdomain/api/v1/platform/orgs/current/scim/tokens (with a subdomain) - see the ingress docs for more details.\\nGenerate a SCIM Bearer Token for your organization. This token will be used by your IdP to authenticate SCIM API requests. Ensure env vars are set appropriately, for example:\\nCopycurl -X POST $LANGCHAIN_ENDPOINT/v1/platform/orgs/current/scim/tokens \\\\\\n  -H \"X-Api-Key: $LANGCHAIN_API_KEY\" \\\\\\n  -H \"X-Organization-Id: $LANGCHAIN_ORGANIZATION_ID\" \\\\\\n  -H \"Content-Type: application/json\" \\\\\\n  -d \\'{\"description\": \"Your description here\"}\\'\\n\\nNote that the SCIM Bearer Token value is not available outside of the response to this request. These additional endpoints are present:\\n\\nGET /v1/platform/orgs/current/scim/tokens\\nGET /v1/platform/orgs/current/scim/tokens/{scim_token_id}\\nPATCH /v1/platform/orgs/current/scim/tokens/{scim_token_id} (only the description field is supported)\\nDELETE /v1/platform/orgs/current/scim/tokens/{scim_token_id}\\n\\n\\u200bStep 4: Configure your Identity Provider\\nIf you use Azure Entra ID (formerly Azure AD) or Okta, there are specific instructions for identity provider setup (refer to Azure Entra ID, Okta). The requirements and steps above are applicable for all identity providers.\\n\\u200bAzure Entra ID configuration steps\\nFor additional information, see Microsoftâ€™s documentation.\\nIn self-hosted installations, the oid JWT claim is used as the sub.\\nSee this Microsoft Learn link\\nand the related configuration instructions for additional details.\\nStep 1: Configure SCIM in your Enterprise Application\\n\\nLog in to the Azure portal with a privileged role (e.g., Global Administrator).\\nNavigate to your existing LangSmith Enterprise Application.\\nIn the left-side navigation, select Manage > Provisioning.\\nClick Get started.\\n\\nStep 2: Configure Admin credentials\\n\\n\\nUnder Admin Credentials:\\n\\n\\nTenant URL:\\n\\nUS: https://api.smith.langchain.com/scim/v2\\nEU: https://eu.api.smith.langchain.com/scim/v2\\nSelf-hosted: <langsmith_url>/scim/v2\\n\\n\\n\\nSecret Token: Enter the SCIM Bearer Token generated in Step 3.\\n\\n\\n\\n\\nClick Test Connection to verify the configuration.\\n\\n\\nClick Save.\\n\\n\\nStep 3: Configure Attribute Mappings\\nConfigure the following attribute mappings under Mappings:\\nUser Attributes\\nSet Target Object Actions to Create and Update (start with Delete disabled for safety):\\nLangSmith App AttributeMicrosoft Entra ID AttributeMatching PrecedenceuserNameuserPrincipalNameactiveNot([IsSoftDeleted])emails[type eq \"work\"].valuemail1name.formatteddisplayName OR Join(\" \", [givenName], [surname])2externalIdobjectId31\\n\\nUserâ€™s email address must be present in Entra ID.\\nUse the Join expression if your displayName does not match the format of Firstname Lastname.\\nTo avoid inconsistency, this should match the SAML NameID assertion and the sub OAuth2.0 claim. For SAML SSO in cloud, the Unique User Identifier (Name ID) required claim should be user.objectID and the Name identifier format should be persistent.\\n\\nGroup Attributes\\nSet Target Object Actions to Create and Update only (start with Delete disabled for safety):\\nLangSmith App AttributeMicrosoft Entra ID AttributeMatching PrecedencedisplayNamedisplayName11externalIdobjectIdmembersmembers\\n\\nGroups must follow the naming convention described in the Group Naming Convention section.\\nIf your company has a group naming policy, you should instead map from the description Microsoft Entra ID Attribute and\\nset the description based on the Group Naming Convention section.\\n\\nStep 4: Assign Users and Groups\\n\\nUnder Applications > Applications, select your LangSmith Enterprise Application.\\nUnder the Assignments tab, click Assign then either Assign to People or Assign to Groups.\\nMake the desired selection(s), then Assign and Done.\\n\\nStep 5: Enable Provisioning\\n\\nSet Provisioning Status to On under Provisioning.\\nMonitor the initial sync to ensure users and groups are provisioned correctly.\\nOnce verified, enable Delete actions for both User and Group mappings.\\n\\nFor troubleshooting, refer to the SAML SSO FAQs. If you have issues setting up SCIM, reach out to the LangChain support team at support@langchain.dev.\\n\\u200bOkta configuration steps\\nYou must use the Okta Lifecycle Management product. This product tier is required to use SCIM on Okta.\\nSupported features\\n\\nCreate users\\nUpdate user attributes\\nDeactivate users\\nGroup push\\n\\nStep 1: Add application from Okta Integration Network\\nIf you have already configured SSO login via SAML (cloud) or OAuth2.0 with OIDC (self-hosted), skip this step.\\nSee SAML SSO setup for cloud or OAuth2.0 setup for self-hosted.\\nStep 2: Configure API Integration\\n\\nIn the Provisioning tab, select Configure API integration.\\nSelect Enable API integration.\\nFor Base URL (if present):\\n\\n\\nUS: https://api.smith.langchain.com/scim/v2\\nEU: https://eu.api.smith.langchain.com/scim/v2\\nSelf-hosted: <langsmith_url>/scim/v2 (note there is no /api/v1 path prefix) or if a subdomain is configured <langsmith_url>/subdomain/scim/v2\\n\\n\\nFor API Token, paste the SCIM token you generated above.\\nKeep Import Groups checked.\\nTo verify the configuration, select Test API Credentials.\\nSelect Save.\\nAfter saving the API integration details, new settings tabs appear on the left. Select To App.\\nSelect Edit.\\nSelect the Enable checkbox for Create Users, Update Users, and Deactivate Users.\\nSelect Save.\\nAssign users and/or groups in the Assignments tab. Assigned users are created and managed in your LangSmith group.\\n\\nStep 3: Configure User Provisioning Settings\\n\\nConfigure provisioning: under Provisioning > To App > Provisioning to App, click Edit, then check Create Users, Update User Attributes, and Deactivate Users.\\nUnder <application_name> Attribute Mappings, set the user attribute mappings as shown below, and delete the rest:\\n\\n\\nStep 4: Push Groups\\nOkta does not support group attributes besides the group name itself, so group name must follow the naming convention described in the Group Naming Convention section.\\nFollow Oktaâ€™s Enable Group Push instructions to configure groups to push by name or by rule.\\n\\u200bOther Identity Providers\\nOther identity providers have not been tested but may function depending on their SCIM implementation.Was this page helpful?YesNoSuggest editsSet up resource tagsFAQsâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_workspace', 'loc': 'https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_workspace', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Set up a workspace - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationSetupOverviewCreate an account and API keySet up a workspaceManage organizations using the APIManage billingSet up resource tagsUser managementAdditional resourcesFAQsCloud architecture and scalabilityRegions FAQAuthentication methodsData purging for complianceRelease versionsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationSetupSet up a workspaceGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageSet up an organizationCreate an organizationManage and navigate workspacesManage usersOrganization rolesSet up a workspaceCreate a workspaceManage usersConfigure workspace settingsDelete a workspaceDelete a workspace via the UISetupSet up a workspaceCopy pageCopy pageThis page describes setting up and managing your LangSmith organization and workspaces:\\n\\nSet up an organization: Create and manage organizations for team collaboration, including user management and role assignments.\\nSet up a workspace: Set up and configure workspaces to organize your LangSmith resources, manage workspace members, and configure settings for team collaboration.\\n\\nYou may find it helpful to refer to the overview on LangSmith resource hierarchy before you read this setup page.\\n\\u200bSet up an organization\\nIf youâ€™re interested in managing your organization and workspaces programmatically, see this how-to guide.\\n\\u200bCreate an organization\\nWhen you log in for the first time, LangSmith will create a personal organization for you automatically. If youâ€™d like to collaborate with others, you can create a separate organization and invite your team members to join.\\nTo do this, open the Organizations drawer by clicking your profile icon in the bottom left and click + New. Shared organizations require a credit card before they can be used. You will need to set up billing to proceed.\\n\\u200bManage and navigate workspaces\\nOnce youâ€™ve subscribed to a plan that allows for multiple users per organization, you can set up workspaces to collaborate more effectively and isolate LangSmith resources between different groups of users. To navigate between workspaces and access the resources within each workspace (trace projects, annotation queues, etc.), select the desired workspace from the picker in the top left:\\n\\n\\u200bManage users\\nManage membership in your shared organization in the Members and roles tabs on the Settings page. Here you can:\\n\\nInvite new users to your organization, selecting workspace membership and (if RBAC is enabled) workspace role.\\nEdit a userâ€™s organization role.\\nRemove users from your organization.\\n\\n\\nOrganizations on the Enterprise plan may set up custom workspace roles in the Roles tab. For more details, refer to the access control setup guide.\\n\\u200bOrganization roles\\nOrganization-scoped roles are used to determine access to organization settings. The role selected also impacts workspace membership:\\n\\nOrganization Admin grants full access to manage all organization configuration, users, billing, and workspaces. Any Organization Admin has Admin access to all workspaces in an organization.\\n\\n\\nOrganization User may read organization information, but cannot execute any write actions at the organization level. You can add an Organization User to a subset of workspaces and assigned workspace roles as usual (if RBAC is enabled), which specify permissions at the workspace level.\\n\\nThe Organization User role is only available in organizations on plans with multiple workspaces. In organizations limited to a single workspace, all users are Organization Admins. Custom organization-scoped roles are not available.\\nFor a full list of permissions associated with each role, refer to the Administration overview page.\\n\\u200bSet up a workspace\\nWhen you log in for the first time, a default workspace will be created for you in your personal organization. Workspaces are often used to separate resources between different teams or business units to establish clear trust boundaries between them. Within each workspace, Role-Based Access Control (RBAC) manages permissions and access levels, which ensures that users only have access to the resources and settings necessary for their role. Most LangSmith activity happens in the context of a workspace, each of which has its own settings and access controls.\\nTo organize resources within a workspace, you can use resource tags.\\n\\u200bCreate a workspace\\nTo create a new workspace, navigate to the Settings page Workspaces tab in your shared organization and click Add Workspace. Once you have created your workspace, you can manage its members and other configuration by selecting it on this page.\\n\\nDifferent plans have different limits placed on the number of workspaces that can be used in an organization. For more information, refer to the pricing page.\\n\\u200bManage users\\nOnly workspace Admins can manage workspace membership and, if RBAC is enabled, change a userâ€™s workspace role.\\nFor users that are already members of an organization, a workspace Admin may add them to a workspace in the Workspace members tab under Workspaces settings page. Users may also be invited directly to one or more workspaces when they are invited to an organization.\\n\\u200bConfigure workspace settings\\nWorkspace configuration exists in the Workspaces settings page tab. Select the workspace to configure and then the desired configuration sub-tab. The following example shows the API keys, and other configuration options including secrets, models, and shared URLs are available here as well.\\n\\n\\u200bDelete a workspace\\nDeleting a workspace will permanently delete the workspace and all associated data. This action cannot be undone.\\nYou can delete a workspace through the LangSmith UI or via API. You must be a workspace Admin in order to delete a workspace.\\n\\u200bDelete a workspace via the UI\\n\\nNavigate to Settings.\\nSelect the workspace you want to delete.\\nClick Delete in the top-right corner of the screen.\\n\\nWas this page helpful?YesNoSuggest editsCreate an account and API keyManage organizations using the APIâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/administration/how_to_guides/organization_management/update_business_info', 'loc': 'https://docs.smith.langchain.com/administration/how_to_guides/organization_management/update_business_info', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Manage billing in your account - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationSetupOverviewCreate an account and API keySet up a workspaceManage organizations using the APIManage billingSet up resource tagsUser managementAdditional resourcesFAQsCloud architecture and scalabilityRegions FAQAuthentication methodsData purging for complianceRelease versionsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationSetupManage billing in your accountGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageSet up billing for your accountDeveloper Plan: set up billing on your personal organizationPlus Plan: set up billing on a shared organizationSet up billing for accounts created before pricing introductionUpdate your informationInvoice emailBusiness information and tax IDOptimize your tracing spendUnderstand your current usageUsage graphInvoicesOptimization 1: manage data retentionChange org level retention defaults for new projectsChange project level retention defaultsApply extended data retention to a percentage of tracesSee results after 7 daysOptimization 2: limit usageSet a good total traces limitCut maximum spend with an extended data retention limitSet dev/staging limits and view total spent limit across workspacesSummarySetupManage billing in your accountCopy pageCopy pageThis page describes how to manage billing for your LangSmith organization:\\n\\nSet up billing for your account: Complete the billing setup process for Developer and Plus plans, including special instructions for legacy accounts.\\nUpdate your information: Modify invoice email addresses, business information, and tax IDs for your organization.\\nOptimize your tracing spend: Learn how to reduce costs through data retention management and usage limits.\\n\\n\\u200bSet up billing for your account\\nBefore using this guide, note the following:\\nIf you are interested in the Enterprise plan, please contact sales. This guide is only for our self-serve billing plans.\\nIf you created your LangSmith organization before pricing was introduced on April 2nd, 2024, please skip to the final section.\\n\\nTo set up billing for your LangSmith organization, navigate to the Usage and Billing page under Settings. Depending on your organizationâ€™s settings, there are different setup guides:\\n\\nDeveloper plan\\nPlus plan\\nSetup for accounts created before April 2, 2024 pricing introduction\\n\\n\\u200bDeveloper Plan: set up billing on your personal organization\\nPersonal organizations are limited to 5000 traces per month until a credit card is added. You can add a credit card on the Plans and Billing page as follows:\\n\\nClick Set up Billing\\n\\n\\n\\nAdd your credit card information. After this step, you will no longer be rate limited to 5000 traces, and you will be charged for any excess traces at rates specified on the pricing page.\\n\\n\\u200bPlus Plan: set up billing on a shared organization\\nIf you have not yet created an organization, you need to follow this guide before setting up billing. The following steps assume you are already in a new organization.\\nYou canâ€™t use a new organization until you enter credit card information. After you complete the following steps, you will gain complete access to LangSmith.\\n\\n\\nClick Subscribe on the Plus page.\\nIf you are a startup building with AI, instead click Apply Now on the Startup Plan. You may be eligible for discounted prices and a free, monthly trace allotment.\\n\\n\\n\\n\\nReview your existing members. Before subscribing, LangSmith lets you remove any added users that you do not want to be included in the bill.\\n\\n\\n\\nEnter your credit card information. Then, enter business information, invoice email, and tax ID. If this organization belongs to a business, check the This is a business checkbox and enter the information accordingly.\\n\\nFor more information, refer to the Update your information section.\\nOnce this step is complete, your organization will have access to the rest of LangSmith.\\n\\u200bSet up billing for accounts created before pricing introduction\\nIf you joined LangSmith before pricing was introduced on April 2, 2024, you have the option to upgrade your existing account to set up billing. If you did not set up billing by July 8, 2024, then your account is now rate limited to a maximum of 5,000 traces per month.\\n\\nNavigate to the Settings page.\\nClick Set up Billing.\\n\\n\\n\\nEnter your credit card information. If you are on a Personal organization, this will add you to the Developer plan. If you are on a shared organization, this will add you to the Plus plan. For more information, refer to the guides for the Developer or Plus plans respectively, starting at step 2.\\nClaim free credits as a thank you for being an early LangSmith user.\\n\\n\\u200bUpdate your information\\nTo update business information for your LangSmith organization, head to the Usage and Billing page under Settings and click on the Plans and Billing tab.\\nBusiness information, tax ID, and invoice email can only be updated for the Plus and Startup plans. Free and Developer plans cannot update this information.\\n\\u200bInvoice email\\n\\nTo update the email address for invoices, follow these steps:\\n\\nNavigate to the Plans and Billing tab.\\nLocate the section beneath the payment method, where the current invoice email is displayed.\\nEnter the new email address for invoices in the provided field.\\nThe new email address will be automatically saved.\\n\\nYou will receive all future invoices to the updated email address.\\n\\u200bBusiness information and tax ID\\nIn certain jurisdictions, LangSmith is required to collect sales tax. If you are a business, providing your tax ID may qualify you for a sales tax exemption.\\n\\nTo update your organizationâ€™s business information, follow these steps:\\n\\nNavigate to the Plans and Billing tab.\\nBelow the invoice email section, you will find a checkbox labeled Business.\\nCheck the Business checkbox if your organization belongs to a business.\\nA business information section will appear, allowing you to enter or update the following details:\\n\\nBusiness Name\\nAddress\\nTax ID for applicable jurisdictions\\n\\n\\nA Tax ID field will appear for applicable jurisdictions after you select a country.\\nAfter entering the necessary information, click the Save button to save your changes.\\n\\nThis ensures that your business information is up-to-date and accurate for billing and tax purposes.\\n\\u200bOptimize your tracing spend\\nYou may find it helpful to read the following pages, before continuing with this section on optimizing your tracing spend:\\nData Retention Conceptual Docs\\nUsage Limiting Conceptual Docs\\n\\nSome of the features mentioned in this guide are not currently available on Enterprise plan due to its custom nature of billing. If you are on the Enterprise plan and have questions about cost optimization, reach out to your sales rep or support@langchain.dev.\\nYou will learn how to optimize existing spend and prevent future overspend in LangSmith, which includes:\\n\\nReducing existing costs with data retention policies.\\nPreventing future overspend with usage limits.\\n\\nThis tutorial will use an existing LangSmith organization with high usage. You can transfer the concepts from this example to your own organization. The example organization has three workspaces, one for each deployment stage (Dev, Staging, and Prod):\\n\\n\\u200bUnderstand your current usage\\nThe first step of any optimization process is to understand current usage. LangSmith provides two ways to do this: Usage graph and Invoices.\\n\\u200bUsage graph\\nThe usage graph lets you examine how much of each usage-based pricing metric you have consumed. It does not directly show spend (which you will review later in the draft invoice).\\nYou can navigate to the usage graph under Settings -> Usage and Billing -> Usage Graph.\\n\\nThis graph shows that there are two usage metrics that LangSmith charges for:\\n\\nLangSmith Traces (Base Charge): tracks all traces that you send to LangSmith.\\nLangSmith Traces (Extended Data Retention Upgrades): tracks all traces that also have our Extended 400 Day Data Retention.\\n\\nFor more details, refer to the data retention conceptual docs. Notice that these graphs look identical, which you will review later in the tutorial.\\nLangSmith Traces usage is measured per workspace, because workspaces often represent development environments (as in the example), or teams within an organization. As a LangSmith administrator, you may want to understand spend granularly per each of these units. In this case where you just want to cut spend, you can focus on the environment responsible for the majority of costs first for the greatest savings.\\n\\u200bInvoices\\nYou understand what usage looks like in terms of traces, but you now need to translate that into spend. To do so, navigate to the Invoices tab. The first invoice that will appear on screen is a draft of your current monthâ€™s invoice, which shows your running spend thus far this month.\\n\\nLangSmithâ€™s Usage Graph and Invoice use the term tenant_id to refer to a workspace ID. They are interchangeable.\\nIn the GIF, youâ€™ll see that the charges for LangSmith Traces are broken up by â€œtenant_idâ€ (i.e., workspace ID), which means you can track tracing spend on each of the workspaces. In the first few days of June, the vast majority of the total spend of roughly $2,000 is in the production workspace. Further, the majority of spend in that workspace was on extended data retention trace upgrades.\\nThese upgrades occur for two reasons:\\n\\nYou use extended data retention tracing, which means by default your traces are retained for 400 days.\\nYou use base data retention tracing and use a feature that automatically extends the data retention of a trace. (Refer to the Auto-Upgrade conceptual docs.)\\n\\nGiven that the number of total traces per day is equal to the number of extended retention traces per day, itâ€™s most likely the case that this organization is using extended data retention tracing everywhere. As a result, start by optimizing the retention settings.\\n\\u200bOptimization 1: manage data retention\\nLangSmith charges differently based on a traceâ€™s data retention, where short-lived traces are an order of magnitude less expensive than ones that last for a long time. In this optimization, youâ€™ll learn how to get optimal settings for data retention without sacrificing historical observability, and see the effect it has on the bill.\\n\\u200bChange org level retention defaults for new projects\\nNavigate to the Usage configuration tab, and look at the organization level retention settings. Modifying this setting affects all new projects that are created going forward in all workspaces in the organizaton.\\nFor backwards compatibility, older organizations may have this defaulted to Extended. Organizations created after June 3rd, 2024 have this defaulted to Base.\\n\\n\\u200bChange project level retention defaults\\nData retention settings are adjustable per project on the tracing project page.\\nNavigate to Projects > Your project name > Select Retention and modify the default retention of the project to Base. This will only affect retention (and pricing) for traces going forward.\\n\\n\\u200bApply extended data retention to a percentage of traces\\nYou may not want all traces to expire after 14 days. You can automatically extend the retention of traces that match some criteria by creating an automation rule. You might want to apply extended data retention to specific types of traces, such as:\\n\\n10% of all traces: For general analysis or analyzing trends long term.\\nErrored traces: To investigate and debug issues thoroughly.\\nTraces with specific metadata: For long-term examination of particular features or user flows.\\n\\nTo configure this:\\n\\nNavigate to Projects > Your project name > Select + New > Select New Automation.\\nName your rule and optionally apply filters or a sample rate. For more information on configuring filters, refer to filtering techniques.\\n\\nWhen an automation rule matches any run within a trace, then all runs within the trace are upgraded to be retained for 400 days.\\nFor example, this is the expected configuration to keep 10% of all traces for extended data retention:\\n\\nIf you want to keep a subset of traces for longer than 400 days for data collection purposes, you can create another run rule that sends some runs to a dataset of your choosing. A dataset allows you to store the trace inputs and outputs (e.g., as a key-value dataset), and will persist indefinitely, even after the trace gets deleted.\\n\\u200bSee results after 7 days\\nWhile the total amount of traces per day stayed the same, the extended data retention traces was cut heavily. In the invoice, the spend reduced to roughly $900 in the last 7 days, as opposed to $2,000 in the previous 4. Thatâ€™s a cost reduction of nearly 75% per day.\\n\\n\\u200bOptimization 2: limit usage\\nIn the previous section, you managed data retention settings to optimize existing spend. In this section, you will use usage limits to prevent future overspend.\\nLangSmith has two usage limits: total traces and extended retention traces. These correspond to the two metrics tracked on the usage graph. You can use these in tandem to have granular control over spend.\\nTo set limits, navigate back to Settings -> Usage and Billing -> Usage configuration. There is a table at the bottom of the page that lets you set usage limits per workspace. For each workspace, the two limits appear, along with a cost estimate:\\n\\nStart by setting limits on production usage, since that is where the majority of spend comes from.\\n\\u200bSet a good total traces limit\\nPicking the right total traces limit depends on the expected load of traces that you will send to LangSmith. It is important to consider potential growth before setting a limit. For example:\\n\\nCurrent Load: The gen AI application is called between 1.2-1.5 times per second, and each API request has a trace associated with it, meaning it logs around 100,000-130,000 traces per day.\\nExpected Growth in Load: The expectation is that this will double in size in the near future.\\n\\nFrom these assumptions, you can calculate an approximate limit:\\nCopylimit = current_load_per_day * expected_growth * days/month\\n      = 130,000 * 2 * 30\\n      = 7,800,000 traces / month\\n\\nClick on the edit icon on the right side of the table for the Prod row to enter the limit.\\n\\nWhen set without the extended data retention traces limit, the maximum cost estimator assumes that all traces are using extended data retention.\\n\\u200bCut maximum spend with an extended data retention limit\\nFrom Optimization 1, you learned that the easiest way to cut cost was through managing data retention. The same is true for limits. If you only want to keep roughly 10% of traces to be around more than 14 days, you can set a limit on the maximum high retention traces you can keep. This would result in .10 * 7,800,000 = 780,000.\\n\\nThe maximum cost is cut from ~40k per month to ~7.5k per month, because you no longer allow as many expensive data retention upgrades. This ensures that new users on the platform will not accidentally cause cost to balloon.\\nThe extended data retention limit can cause features other than traces to stop working once reached. If you plan to use this feature, read more about its functionality and side effects.\\n\\u200bSet dev/staging limits and view total spent limit across workspaces\\nFollowing a similar logic for the dev and staging environments, you can set limits at 10% of the production limit on usage for each workspace.\\nWhile this works with this usage pattern, setting good dev and staging limits may vary depending on your use case with LangSmith. For example, if you run evals as part of CI/CD in dev or staging, you may want to be more flexible with your usage limits to avoid test failures.\\nWith the limits set, LangSmith shows a maximum spend estimate across all workspaces:\\n\\nYou can use the cost estimate to plan for your invoice total.\\n\\u200bSummary\\nIf you have questions about further optimizing your spend, please reach out to support@langchain.dev.Was this page helpful?YesNoSuggest editsManage organizations using the APISet up resource tagsâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials', 'loc': 'https://docs.smith.langchain.com/administration/tutorials', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Manage billing in your account - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationSetupOverviewCreate an account and API keySet up a workspaceManage organizations using the APIManage billingSet up resource tagsUser managementAdditional resourcesFAQsCloud architecture and scalabilityRegions FAQAuthentication methodsData purging for complianceRelease versionsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationSetupManage billing in your accountGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageSet up billing for your accountDeveloper Plan: set up billing on your personal organizationPlus Plan: set up billing on a shared organizationSet up billing for accounts created before pricing introductionUpdate your informationInvoice emailBusiness information and tax IDOptimize your tracing spendUnderstand your current usageUsage graphInvoicesOptimization 1: manage data retentionChange org level retention defaults for new projectsChange project level retention defaultsApply extended data retention to a percentage of tracesSee results after 7 daysOptimization 2: limit usageSet a good total traces limitCut maximum spend with an extended data retention limitSet dev/staging limits and view total spent limit across workspacesSummarySetupManage billing in your accountCopy pageCopy pageThis page describes how to manage billing for your LangSmith organization:\\n\\nSet up billing for your account: Complete the billing setup process for Developer and Plus plans, including special instructions for legacy accounts.\\nUpdate your information: Modify invoice email addresses, business information, and tax IDs for your organization.\\nOptimize your tracing spend: Learn how to reduce costs through data retention management and usage limits.\\n\\n\\u200bSet up billing for your account\\nBefore using this guide, note the following:\\nIf you are interested in the Enterprise plan, please contact sales. This guide is only for our self-serve billing plans.\\nIf you created your LangSmith organization before pricing was introduced on April 2nd, 2024, please skip to the final section.\\n\\nTo set up billing for your LangSmith organization, navigate to the Usage and Billing page under Settings. Depending on your organizationâ€™s settings, there are different setup guides:\\n\\nDeveloper plan\\nPlus plan\\nSetup for accounts created before April 2, 2024 pricing introduction\\n\\n\\u200bDeveloper Plan: set up billing on your personal organization\\nPersonal organizations are limited to 5000 traces per month until a credit card is added. You can add a credit card on the Plans and Billing page as follows:\\n\\nClick Set up Billing\\n\\n\\n\\nAdd your credit card information. After this step, you will no longer be rate limited to 5000 traces, and you will be charged for any excess traces at rates specified on the pricing page.\\n\\n\\u200bPlus Plan: set up billing on a shared organization\\nIf you have not yet created an organization, you need to follow this guide before setting up billing. The following steps assume you are already in a new organization.\\nYou canâ€™t use a new organization until you enter credit card information. After you complete the following steps, you will gain complete access to LangSmith.\\n\\n\\nClick Subscribe on the Plus page.\\nIf you are a startup building with AI, instead click Apply Now on the Startup Plan. You may be eligible for discounted prices and a free, monthly trace allotment.\\n\\n\\n\\n\\nReview your existing members. Before subscribing, LangSmith lets you remove any added users that you do not want to be included in the bill.\\n\\n\\n\\nEnter your credit card information. Then, enter business information, invoice email, and tax ID. If this organization belongs to a business, check the This is a business checkbox and enter the information accordingly.\\n\\nFor more information, refer to the Update your information section.\\nOnce this step is complete, your organization will have access to the rest of LangSmith.\\n\\u200bSet up billing for accounts created before pricing introduction\\nIf you joined LangSmith before pricing was introduced on April 2, 2024, you have the option to upgrade your existing account to set up billing. If you did not set up billing by July 8, 2024, then your account is now rate limited to a maximum of 5,000 traces per month.\\n\\nNavigate to the Settings page.\\nClick Set up Billing.\\n\\n\\n\\nEnter your credit card information. If you are on a Personal organization, this will add you to the Developer plan. If you are on a shared organization, this will add you to the Plus plan. For more information, refer to the guides for the Developer or Plus plans respectively, starting at step 2.\\nClaim free credits as a thank you for being an early LangSmith user.\\n\\n\\u200bUpdate your information\\nTo update business information for your LangSmith organization, head to the Usage and Billing page under Settings and click on the Plans and Billing tab.\\nBusiness information, tax ID, and invoice email can only be updated for the Plus and Startup plans. Free and Developer plans cannot update this information.\\n\\u200bInvoice email\\n\\nTo update the email address for invoices, follow these steps:\\n\\nNavigate to the Plans and Billing tab.\\nLocate the section beneath the payment method, where the current invoice email is displayed.\\nEnter the new email address for invoices in the provided field.\\nThe new email address will be automatically saved.\\n\\nYou will receive all future invoices to the updated email address.\\n\\u200bBusiness information and tax ID\\nIn certain jurisdictions, LangSmith is required to collect sales tax. If you are a business, providing your tax ID may qualify you for a sales tax exemption.\\n\\nTo update your organizationâ€™s business information, follow these steps:\\n\\nNavigate to the Plans and Billing tab.\\nBelow the invoice email section, you will find a checkbox labeled Business.\\nCheck the Business checkbox if your organization belongs to a business.\\nA business information section will appear, allowing you to enter or update the following details:\\n\\nBusiness Name\\nAddress\\nTax ID for applicable jurisdictions\\n\\n\\nA Tax ID field will appear for applicable jurisdictions after you select a country.\\nAfter entering the necessary information, click the Save button to save your changes.\\n\\nThis ensures that your business information is up-to-date and accurate for billing and tax purposes.\\n\\u200bOptimize your tracing spend\\nYou may find it helpful to read the following pages, before continuing with this section on optimizing your tracing spend:\\nData Retention Conceptual Docs\\nUsage Limiting Conceptual Docs\\n\\nSome of the features mentioned in this guide are not currently available on Enterprise plan due to its custom nature of billing. If you are on the Enterprise plan and have questions about cost optimization, reach out to your sales rep or support@langchain.dev.\\nYou will learn how to optimize existing spend and prevent future overspend in LangSmith, which includes:\\n\\nReducing existing costs with data retention policies.\\nPreventing future overspend with usage limits.\\n\\nThis tutorial will use an existing LangSmith organization with high usage. You can transfer the concepts from this example to your own organization. The example organization has three workspaces, one for each deployment stage (Dev, Staging, and Prod):\\n\\n\\u200bUnderstand your current usage\\nThe first step of any optimization process is to understand current usage. LangSmith provides two ways to do this: Usage graph and Invoices.\\n\\u200bUsage graph\\nThe usage graph lets you examine how much of each usage-based pricing metric you have consumed. It does not directly show spend (which you will review later in the draft invoice).\\nYou can navigate to the usage graph under Settings -> Usage and Billing -> Usage Graph.\\n\\nThis graph shows that there are two usage metrics that LangSmith charges for:\\n\\nLangSmith Traces (Base Charge): tracks all traces that you send to LangSmith.\\nLangSmith Traces (Extended Data Retention Upgrades): tracks all traces that also have our Extended 400 Day Data Retention.\\n\\nFor more details, refer to the data retention conceptual docs. Notice that these graphs look identical, which you will review later in the tutorial.\\nLangSmith Traces usage is measured per workspace, because workspaces often represent development environments (as in the example), or teams within an organization. As a LangSmith administrator, you may want to understand spend granularly per each of these units. In this case where you just want to cut spend, you can focus on the environment responsible for the majority of costs first for the greatest savings.\\n\\u200bInvoices\\nYou understand what usage looks like in terms of traces, but you now need to translate that into spend. To do so, navigate to the Invoices tab. The first invoice that will appear on screen is a draft of your current monthâ€™s invoice, which shows your running spend thus far this month.\\n\\nLangSmithâ€™s Usage Graph and Invoice use the term tenant_id to refer to a workspace ID. They are interchangeable.\\nIn the GIF, youâ€™ll see that the charges for LangSmith Traces are broken up by â€œtenant_idâ€ (i.e., workspace ID), which means you can track tracing spend on each of the workspaces. In the first few days of June, the vast majority of the total spend of roughly $2,000 is in the production workspace. Further, the majority of spend in that workspace was on extended data retention trace upgrades.\\nThese upgrades occur for two reasons:\\n\\nYou use extended data retention tracing, which means by default your traces are retained for 400 days.\\nYou use base data retention tracing and use a feature that automatically extends the data retention of a trace. (Refer to the Auto-Upgrade conceptual docs.)\\n\\nGiven that the number of total traces per day is equal to the number of extended retention traces per day, itâ€™s most likely the case that this organization is using extended data retention tracing everywhere. As a result, start by optimizing the retention settings.\\n\\u200bOptimization 1: manage data retention\\nLangSmith charges differently based on a traceâ€™s data retention, where short-lived traces are an order of magnitude less expensive than ones that last for a long time. In this optimization, youâ€™ll learn how to get optimal settings for data retention without sacrificing historical observability, and see the effect it has on the bill.\\n\\u200bChange org level retention defaults for new projects\\nNavigate to the Usage configuration tab, and look at the organization level retention settings. Modifying this setting affects all new projects that are created going forward in all workspaces in the organizaton.\\nFor backwards compatibility, older organizations may have this defaulted to Extended. Organizations created after June 3rd, 2024 have this defaulted to Base.\\n\\n\\u200bChange project level retention defaults\\nData retention settings are adjustable per project on the tracing project page.\\nNavigate to Projects > Your project name > Select Retention and modify the default retention of the project to Base. This will only affect retention (and pricing) for traces going forward.\\n\\n\\u200bApply extended data retention to a percentage of traces\\nYou may not want all traces to expire after 14 days. You can automatically extend the retention of traces that match some criteria by creating an automation rule. You might want to apply extended data retention to specific types of traces, such as:\\n\\n10% of all traces: For general analysis or analyzing trends long term.\\nErrored traces: To investigate and debug issues thoroughly.\\nTraces with specific metadata: For long-term examination of particular features or user flows.\\n\\nTo configure this:\\n\\nNavigate to Projects > Your project name > Select + New > Select New Automation.\\nName your rule and optionally apply filters or a sample rate. For more information on configuring filters, refer to filtering techniques.\\n\\nWhen an automation rule matches any run within a trace, then all runs within the trace are upgraded to be retained for 400 days.\\nFor example, this is the expected configuration to keep 10% of all traces for extended data retention:\\n\\nIf you want to keep a subset of traces for longer than 400 days for data collection purposes, you can create another run rule that sends some runs to a dataset of your choosing. A dataset allows you to store the trace inputs and outputs (e.g., as a key-value dataset), and will persist indefinitely, even after the trace gets deleted.\\n\\u200bSee results after 7 days\\nWhile the total amount of traces per day stayed the same, the extended data retention traces was cut heavily. In the invoice, the spend reduced to roughly $900 in the last 7 days, as opposed to $2,000 in the previous 4. Thatâ€™s a cost reduction of nearly 75% per day.\\n\\n\\u200bOptimization 2: limit usage\\nIn the previous section, you managed data retention settings to optimize existing spend. In this section, you will use usage limits to prevent future overspend.\\nLangSmith has two usage limits: total traces and extended retention traces. These correspond to the two metrics tracked on the usage graph. You can use these in tandem to have granular control over spend.\\nTo set limits, navigate back to Settings -> Usage and Billing -> Usage configuration. There is a table at the bottom of the page that lets you set usage limits per workspace. For each workspace, the two limits appear, along with a cost estimate:\\n\\nStart by setting limits on production usage, since that is where the majority of spend comes from.\\n\\u200bSet a good total traces limit\\nPicking the right total traces limit depends on the expected load of traces that you will send to LangSmith. It is important to consider potential growth before setting a limit. For example:\\n\\nCurrent Load: The gen AI application is called between 1.2-1.5 times per second, and each API request has a trace associated with it, meaning it logs around 100,000-130,000 traces per day.\\nExpected Growth in Load: The expectation is that this will double in size in the near future.\\n\\nFrom these assumptions, you can calculate an approximate limit:\\nCopylimit = current_load_per_day * expected_growth * days/month\\n      = 130,000 * 2 * 30\\n      = 7,800,000 traces / month\\n\\nClick on the edit icon on the right side of the table for the Prod row to enter the limit.\\n\\nWhen set without the extended data retention traces limit, the maximum cost estimator assumes that all traces are using extended data retention.\\n\\u200bCut maximum spend with an extended data retention limit\\nFrom Optimization 1, you learned that the easiest way to cut cost was through managing data retention. The same is true for limits. If you only want to keep roughly 10% of traces to be around more than 14 days, you can set a limit on the maximum high retention traces you can keep. This would result in .10 * 7,800,000 = 780,000.\\n\\nThe maximum cost is cut from ~40k per month to ~7.5k per month, because you no longer allow as many expensive data retention upgrades. This ensures that new users on the platform will not accidentally cause cost to balloon.\\nThe extended data retention limit can cause features other than traces to stop working once reached. If you plan to use this feature, read more about its functionality and side effects.\\n\\u200bSet dev/staging limits and view total spent limit across workspaces\\nFollowing a similar logic for the dev and staging environments, you can set limits at 10% of the production limit on usage for each workspace.\\nWhile this works with this usage pattern, setting good dev and staging limits may vary depending on your use case with LangSmith. For example, if you run evals as part of CI/CD in dev or staging, you may want to be more flexible with your usage limits to avoid test failures.\\nWith the limits set, LangSmith shows a maximum spend estimate across all workspaces:\\n\\nYou can use the cost estimate to plan for your invoice total.\\n\\u200bSummary\\nIf you have questions about further optimizing your spend, please reach out to support@langchain.dev.Was this page helpful?YesNoSuggest editsManage organizations using the APISet up resource tagsâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'loc': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Manage billing in your account - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationSetupOverviewCreate an account and API keySet up a workspaceManage organizations using the APIManage billingSet up resource tagsUser managementAdditional resourcesFAQsCloud architecture and scalabilityRegions FAQAuthentication methodsData purging for complianceRelease versionsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationSetupManage billing in your accountGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageSet up billing for your accountDeveloper Plan: set up billing on your personal organizationPlus Plan: set up billing on a shared organizationSet up billing for accounts created before pricing introductionUpdate your informationInvoice emailBusiness information and tax IDOptimize your tracing spendUnderstand your current usageUsage graphInvoicesOptimization 1: manage data retentionChange org level retention defaults for new projectsChange project level retention defaultsApply extended data retention to a percentage of tracesSee results after 7 daysOptimization 2: limit usageSet a good total traces limitCut maximum spend with an extended data retention limitSet dev/staging limits and view total spent limit across workspacesSummarySetupManage billing in your accountCopy pageCopy pageThis page describes how to manage billing for your LangSmith organization:\\n\\nSet up billing for your account: Complete the billing setup process for Developer and Plus plans, including special instructions for legacy accounts.\\nUpdate your information: Modify invoice email addresses, business information, and tax IDs for your organization.\\nOptimize your tracing spend: Learn how to reduce costs through data retention management and usage limits.\\n\\n\\u200bSet up billing for your account\\nBefore using this guide, note the following:\\nIf you are interested in the Enterprise plan, please contact sales. This guide is only for our self-serve billing plans.\\nIf you created your LangSmith organization before pricing was introduced on April 2nd, 2024, please skip to the final section.\\n\\nTo set up billing for your LangSmith organization, navigate to the Usage and Billing page under Settings. Depending on your organizationâ€™s settings, there are different setup guides:\\n\\nDeveloper plan\\nPlus plan\\nSetup for accounts created before April 2, 2024 pricing introduction\\n\\n\\u200bDeveloper Plan: set up billing on your personal organization\\nPersonal organizations are limited to 5000 traces per month until a credit card is added. You can add a credit card on the Plans and Billing page as follows:\\n\\nClick Set up Billing\\n\\n\\n\\nAdd your credit card information. After this step, you will no longer be rate limited to 5000 traces, and you will be charged for any excess traces at rates specified on the pricing page.\\n\\n\\u200bPlus Plan: set up billing on a shared organization\\nIf you have not yet created an organization, you need to follow this guide before setting up billing. The following steps assume you are already in a new organization.\\nYou canâ€™t use a new organization until you enter credit card information. After you complete the following steps, you will gain complete access to LangSmith.\\n\\n\\nClick Subscribe on the Plus page.\\nIf you are a startup building with AI, instead click Apply Now on the Startup Plan. You may be eligible for discounted prices and a free, monthly trace allotment.\\n\\n\\n\\n\\nReview your existing members. Before subscribing, LangSmith lets you remove any added users that you do not want to be included in the bill.\\n\\n\\n\\nEnter your credit card information. Then, enter business information, invoice email, and tax ID. If this organization belongs to a business, check the This is a business checkbox and enter the information accordingly.\\n\\nFor more information, refer to the Update your information section.\\nOnce this step is complete, your organization will have access to the rest of LangSmith.\\n\\u200bSet up billing for accounts created before pricing introduction\\nIf you joined LangSmith before pricing was introduced on April 2, 2024, you have the option to upgrade your existing account to set up billing. If you did not set up billing by July 8, 2024, then your account is now rate limited to a maximum of 5,000 traces per month.\\n\\nNavigate to the Settings page.\\nClick Set up Billing.\\n\\n\\n\\nEnter your credit card information. If you are on a Personal organization, this will add you to the Developer plan. If you are on a shared organization, this will add you to the Plus plan. For more information, refer to the guides for the Developer or Plus plans respectively, starting at step 2.\\nClaim free credits as a thank you for being an early LangSmith user.\\n\\n\\u200bUpdate your information\\nTo update business information for your LangSmith organization, head to the Usage and Billing page under Settings and click on the Plans and Billing tab.\\nBusiness information, tax ID, and invoice email can only be updated for the Plus and Startup plans. Free and Developer plans cannot update this information.\\n\\u200bInvoice email\\n\\nTo update the email address for invoices, follow these steps:\\n\\nNavigate to the Plans and Billing tab.\\nLocate the section beneath the payment method, where the current invoice email is displayed.\\nEnter the new email address for invoices in the provided field.\\nThe new email address will be automatically saved.\\n\\nYou will receive all future invoices to the updated email address.\\n\\u200bBusiness information and tax ID\\nIn certain jurisdictions, LangSmith is required to collect sales tax. If you are a business, providing your tax ID may qualify you for a sales tax exemption.\\n\\nTo update your organizationâ€™s business information, follow these steps:\\n\\nNavigate to the Plans and Billing tab.\\nBelow the invoice email section, you will find a checkbox labeled Business.\\nCheck the Business checkbox if your organization belongs to a business.\\nA business information section will appear, allowing you to enter or update the following details:\\n\\nBusiness Name\\nAddress\\nTax ID for applicable jurisdictions\\n\\n\\nA Tax ID field will appear for applicable jurisdictions after you select a country.\\nAfter entering the necessary information, click the Save button to save your changes.\\n\\nThis ensures that your business information is up-to-date and accurate for billing and tax purposes.\\n\\u200bOptimize your tracing spend\\nYou may find it helpful to read the following pages, before continuing with this section on optimizing your tracing spend:\\nData Retention Conceptual Docs\\nUsage Limiting Conceptual Docs\\n\\nSome of the features mentioned in this guide are not currently available on Enterprise plan due to its custom nature of billing. If you are on the Enterprise plan and have questions about cost optimization, reach out to your sales rep or support@langchain.dev.\\nYou will learn how to optimize existing spend and prevent future overspend in LangSmith, which includes:\\n\\nReducing existing costs with data retention policies.\\nPreventing future overspend with usage limits.\\n\\nThis tutorial will use an existing LangSmith organization with high usage. You can transfer the concepts from this example to your own organization. The example organization has three workspaces, one for each deployment stage (Dev, Staging, and Prod):\\n\\n\\u200bUnderstand your current usage\\nThe first step of any optimization process is to understand current usage. LangSmith provides two ways to do this: Usage graph and Invoices.\\n\\u200bUsage graph\\nThe usage graph lets you examine how much of each usage-based pricing metric you have consumed. It does not directly show spend (which you will review later in the draft invoice).\\nYou can navigate to the usage graph under Settings -> Usage and Billing -> Usage Graph.\\n\\nThis graph shows that there are two usage metrics that LangSmith charges for:\\n\\nLangSmith Traces (Base Charge): tracks all traces that you send to LangSmith.\\nLangSmith Traces (Extended Data Retention Upgrades): tracks all traces that also have our Extended 400 Day Data Retention.\\n\\nFor more details, refer to the data retention conceptual docs. Notice that these graphs look identical, which you will review later in the tutorial.\\nLangSmith Traces usage is measured per workspace, because workspaces often represent development environments (as in the example), or teams within an organization. As a LangSmith administrator, you may want to understand spend granularly per each of these units. In this case where you just want to cut spend, you can focus on the environment responsible for the majority of costs first for the greatest savings.\\n\\u200bInvoices\\nYou understand what usage looks like in terms of traces, but you now need to translate that into spend. To do so, navigate to the Invoices tab. The first invoice that will appear on screen is a draft of your current monthâ€™s invoice, which shows your running spend thus far this month.\\n\\nLangSmithâ€™s Usage Graph and Invoice use the term tenant_id to refer to a workspace ID. They are interchangeable.\\nIn the GIF, youâ€™ll see that the charges for LangSmith Traces are broken up by â€œtenant_idâ€ (i.e., workspace ID), which means you can track tracing spend on each of the workspaces. In the first few days of June, the vast majority of the total spend of roughly $2,000 is in the production workspace. Further, the majority of spend in that workspace was on extended data retention trace upgrades.\\nThese upgrades occur for two reasons:\\n\\nYou use extended data retention tracing, which means by default your traces are retained for 400 days.\\nYou use base data retention tracing and use a feature that automatically extends the data retention of a trace. (Refer to the Auto-Upgrade conceptual docs.)\\n\\nGiven that the number of total traces per day is equal to the number of extended retention traces per day, itâ€™s most likely the case that this organization is using extended data retention tracing everywhere. As a result, start by optimizing the retention settings.\\n\\u200bOptimization 1: manage data retention\\nLangSmith charges differently based on a traceâ€™s data retention, where short-lived traces are an order of magnitude less expensive than ones that last for a long time. In this optimization, youâ€™ll learn how to get optimal settings for data retention without sacrificing historical observability, and see the effect it has on the bill.\\n\\u200bChange org level retention defaults for new projects\\nNavigate to the Usage configuration tab, and look at the organization level retention settings. Modifying this setting affects all new projects that are created going forward in all workspaces in the organizaton.\\nFor backwards compatibility, older organizations may have this defaulted to Extended. Organizations created after June 3rd, 2024 have this defaulted to Base.\\n\\n\\u200bChange project level retention defaults\\nData retention settings are adjustable per project on the tracing project page.\\nNavigate to Projects > Your project name > Select Retention and modify the default retention of the project to Base. This will only affect retention (and pricing) for traces going forward.\\n\\n\\u200bApply extended data retention to a percentage of traces\\nYou may not want all traces to expire after 14 days. You can automatically extend the retention of traces that match some criteria by creating an automation rule. You might want to apply extended data retention to specific types of traces, such as:\\n\\n10% of all traces: For general analysis or analyzing trends long term.\\nErrored traces: To investigate and debug issues thoroughly.\\nTraces with specific metadata: For long-term examination of particular features or user flows.\\n\\nTo configure this:\\n\\nNavigate to Projects > Your project name > Select + New > Select New Automation.\\nName your rule and optionally apply filters or a sample rate. For more information on configuring filters, refer to filtering techniques.\\n\\nWhen an automation rule matches any run within a trace, then all runs within the trace are upgraded to be retained for 400 days.\\nFor example, this is the expected configuration to keep 10% of all traces for extended data retention:\\n\\nIf you want to keep a subset of traces for longer than 400 days for data collection purposes, you can create another run rule that sends some runs to a dataset of your choosing. A dataset allows you to store the trace inputs and outputs (e.g., as a key-value dataset), and will persist indefinitely, even after the trace gets deleted.\\n\\u200bSee results after 7 days\\nWhile the total amount of traces per day stayed the same, the extended data retention traces was cut heavily. In the invoice, the spend reduced to roughly $900 in the last 7 days, as opposed to $2,000 in the previous 4. Thatâ€™s a cost reduction of nearly 75% per day.\\n\\n\\u200bOptimization 2: limit usage\\nIn the previous section, you managed data retention settings to optimize existing spend. In this section, you will use usage limits to prevent future overspend.\\nLangSmith has two usage limits: total traces and extended retention traces. These correspond to the two metrics tracked on the usage graph. You can use these in tandem to have granular control over spend.\\nTo set limits, navigate back to Settings -> Usage and Billing -> Usage configuration. There is a table at the bottom of the page that lets you set usage limits per workspace. For each workspace, the two limits appear, along with a cost estimate:\\n\\nStart by setting limits on production usage, since that is where the majority of spend comes from.\\n\\u200bSet a good total traces limit\\nPicking the right total traces limit depends on the expected load of traces that you will send to LangSmith. It is important to consider potential growth before setting a limit. For example:\\n\\nCurrent Load: The gen AI application is called between 1.2-1.5 times per second, and each API request has a trace associated with it, meaning it logs around 100,000-130,000 traces per day.\\nExpected Growth in Load: The expectation is that this will double in size in the near future.\\n\\nFrom these assumptions, you can calculate an approximate limit:\\nCopylimit = current_load_per_day * expected_growth * days/month\\n      = 130,000 * 2 * 30\\n      = 7,800,000 traces / month\\n\\nClick on the edit icon on the right side of the table for the Prod row to enter the limit.\\n\\nWhen set without the extended data retention traces limit, the maximum cost estimator assumes that all traces are using extended data retention.\\n\\u200bCut maximum spend with an extended data retention limit\\nFrom Optimization 1, you learned that the easiest way to cut cost was through managing data retention. The same is true for limits. If you only want to keep roughly 10% of traces to be around more than 14 days, you can set a limit on the maximum high retention traces you can keep. This would result in .10 * 7,800,000 = 780,000.\\n\\nThe maximum cost is cut from ~40k per month to ~7.5k per month, because you no longer allow as many expensive data retention upgrades. This ensures that new users on the platform will not accidentally cause cost to balloon.\\nThe extended data retention limit can cause features other than traces to stop working once reached. If you plan to use this feature, read more about its functionality and side effects.\\n\\u200bSet dev/staging limits and view total spent limit across workspaces\\nFollowing a similar logic for the dev and staging environments, you can set limits at 10% of the production limit on usage for each workspace.\\nWhile this works with this usage pattern, setting good dev and staging limits may vary depending on your use case with LangSmith. For example, if you run evals as part of CI/CD in dev or staging, you may want to be more flexible with your usage limits to avoid test failures.\\nWith the limits set, LangSmith shows a maximum spend estimate across all workspaces:\\n\\nYou can use the cost estimate to plan for your invoice total.\\n\\u200bSummary\\nIf you have questions about further optimizing your spend, please reach out to support@langchain.dev.Was this page helpful?YesNoSuggest editsManage organizations using the APISet up resource tagsâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'loc': 'https://docs.smith.langchain.com/evaluation', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Evaluation Concepts - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationEvaluate your applicationEvaluation ConceptsGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageDatasetsExamplesDataset curationManually curated examplesHistorical tracesSynthetic dataSplitsVersionsEvaluatorsEvaluator inputsEvaluator outputsDefining evaluatorsEvaluation techniquesHumanHeuristicLLM-as-judgePairwiseExperimentExperiment configurationRepetitionsConcurrencyCachingAnnotation queuesOffline evaluationBenchmarkingUnit testsRegression testsBacktestingPairwise evaluationOnline evaluationTestingEvaluations vs testingUsing pytest and Vitest/JestEvaluate your applicationEvaluation ConceptsCopy pageCopy pageLangSmith makes building high-quality evaluations easy. This guide explains the key concepts of the LangSmith evaluation framework. The building blocks of the LangSmith framework are:\\n\\nDatasets: Collections of test inputs and reference outputs.\\nEvaluators: Functions for scoring outputs. These can be online evaluators that run on traces in real time or offline evaluators that run on a dataset.\\n\\n\\u200bDatasets\\nA dataset is a collection of examples used for evaluating an application. An example is a test input, reference output pair.\\n\\n\\u200bExamples\\nEach example consists of:\\n\\nInputs: a dictionary of input variables to pass to your application.\\nReference outputs (optional): a dictionary of reference outputs. These do not get passed to your application, they are only used in evaluators.\\nMetadata (optional): a dictionary of additional information that can be used to create filtered views of a dataset.\\n\\n\\n\\u200bDataset curation\\nThere are various ways to build datasets for evaluation, including:\\n\\u200bManually curated examples\\nThis is how we typically recommend people get started creating datasets. From building your application, you probably have some idea of what types of inputs you expect your application to be able to handle, and what â€œgoodâ€ responses may be. You probably want to cover a few different common edge cases or situations you can imagine. Even 10-20 high-quality, manually-curated examples can go a long way.\\n\\u200bHistorical traces\\nOnce you have an application in production, you start getting valuable information: how are users actually using it? These real-world runs make for great examples because theyâ€™re, well, the most realistic!\\nIf youâ€™re getting a lot of traffic, how can you determine which runs are valuable to add to a dataset? There are a few techniques you can use:\\n\\nUser feedback: If possible - try to collect end user feedback. You can then see which datapoints got negative feedback. That is super valuable! These are spots where your application did not perform well. You should add these to your dataset to test against in the future.\\nHeuristics: You can also use other heuristics to identify â€œinterestingâ€ datapoints. For example, runs that took a long time to complete could be interesting to look at and add to a dataset.\\nLLM feedback: You can use another LLM to detect noteworthy runs. For example, you could use an LLM to label chatbot conversations where the user had to rephrase their question or correct the model in some way, indicating the chatbot did not initially respond correctly.\\n\\n\\u200bSynthetic data\\nOnce you have a few examples, you can try to artificially generate some more. Itâ€™s generally advised to have a few good hand-crafted examples before this, as this synthetic data will often resemble them in some way. This can be a useful way to get a lot of datapoints, quickly.\\n\\u200bSplits\\nWhen setting up your evaluation, you may want to partition your dataset into different splits. For example, you might use a smaller split for many rapid and cheap iterations and a larger split for your final evaluation. In addition, splits can be important for the interpretability of your experiments. For example, if you have a RAG application, you may want your dataset splits to focus on different types of questions (e.g., factual, opinion, etc) and to evaluate your application on each split separately.\\nLearn how to create and manage dataset splits.\\n\\u200bVersions\\nDatasets are versioned such that every time you add, update, or delete examples in your dataset, a new version of the dataset is created. This makes it easy to inspect and revert changes to your dataset in case you make a mistake. You can also tag versions of your dataset to give them a more human-readable name. This can be useful for marking important milestones in your datasetâ€™s history.\\nYou can run evaluations on specific versions of a dataset. This can be useful when running evaluations in CI, to make sure that a dataset update doesnâ€™t accidentally break your CI pipelines.\\n\\u200bEvaluators\\nEvaluators are functions that score how well your application performs on a particular example.\\n\\u200bEvaluator inputs\\nEvaluators receive these inputs:\\n\\nExample: The example(s) from your Dataset. Contains inputs, (reference) outputs, and metadata.\\nRun: The actual outputs and intermediate steps (child runs) from passing the example inputs to the application.\\n\\n\\u200bEvaluator outputs\\nAn evaluator returns one or more metrics. These should be returned as a dictionary or list of dictionaries of the form:\\n\\nkey: The name of the metric.\\nscore | value: The value of the metric. Use score if itâ€™s a numerical metric and value if itâ€™s categorical.\\ncomment (optional): The reasoning or additional string information justifying the score.\\n\\n\\u200bDefining evaluators\\nThere are a number of ways to define and run evaluators:\\n\\nCustom code: Define custom evaluators as Python or TypeScript functions and run them client-side using the SDKs or server-side via the UI.\\nBuilt-in evaluators: LangSmith has a number of built-in evaluators that you can configure and run via the UI.\\n\\nYou can run evaluators using the LangSmith SDK (Python and TypeScript), via the Prompt Playground, or by configuring Rules to automatically run them on particular tracing projects or datasets.\\n\\u200bEvaluation techniques\\nThere are a few high-level approaches to LLM evaluation:\\n\\u200bHuman\\nHuman evaluation is often a great starting point for evaluation. LangSmith makes it easy to review your LLM application outputs as well as the traces (all intermediate steps).\\nLangSmithâ€™s annotation queues make it easy to get human feedback on your applicationâ€™s outputs.\\n\\u200bHeuristic\\nHeuristic evaluators are deterministic, rule-based functions. These are good for simple checks like making sure that a chatbotâ€™s response isnâ€™t empty, that a snippet of generated code can be compiled, or that a classification is exactly correct.\\n\\u200bLLM-as-judge\\nLLM-as-judge evaluators use LLMs to score the applicationâ€™s output. To use them, you typically encode the grading rules / criteria in the LLM prompt. They can be reference-free (e.g., check if system output contains offensive content or adheres to specific criteria). Or, they can compare task output to a reference output (e.g., check if the output is factually accurate relative to the reference).\\nWith LLM-as-judge evaluators, it is important to carefully review the resulting scores and tune the grader prompt if needed. Often it is helpful to write these as few-shot evaluators, where you provide examples of inputs, outputs, and expected grades as part of the grader prompt.\\nLearn about how to define an LLM-as-a-judge evaluator.\\n\\u200bPairwise\\nPairwise evaluators allow you to compare the outputs of two versions of an application. Think LMSYS Chatbot Arena - this is the same concept, but applied to AI applications more generally, not just models! This can use either a heuristic (â€œwhich response is longerâ€), an LLM (with a specific pairwise prompt), or human (asking them to manually annotate examples).\\nWhen should you use pairwise evaluation?\\nPairwise evaluation is helpful when it is difficult to directly score an LLM output, but easier to compare two outputs. This can be the case for tasks like summarization - it may be hard to give a summary an absolute score, but easy to choose which of two summaries is more informative.\\nLearn how run pairwise evaluations.\\n\\u200bExperiment\\nEach time we evaluate an application on a dataset, we are conducting an experiment. An experiment contains the results of running a specific version of your application on the dataset. To understand how to use the LangSmith experiment view, see how to analyze experiment results.\\n\\nTypically, we will run multiple experiments on a given dataset, testing different configurations of our application (e.g., different prompts or LLMs). In LangSmith, you can easily view all the experiments associated with your dataset. Additionally, you can compare multiple experiments in a comparison view.\\n\\n\\u200bExperiment configuration\\nLangSmith supports a number of experiment configurations which make it easier to run your evals in the manner you want.\\n\\u200bRepetitions\\nRunning an experiment multiple times can be helpful since LLM outputs are not deterministic and can differ from one repetition to the next. By running multiple repetitions, you can get a more accurate estimate of the performance of your system.\\nRepetitions can be configured by passing the num_repetitions argument to evaluate / aevaluate (Python, TypeScript). Repeating the experiment involves both re-running the target function to generate outputs and re-running the evaluators.\\nTo learn more about running repetitions on experiments, read the how-to-guide.\\n\\u200bConcurrency\\nBy passing the max_concurrency argument to evaluate / aevaluate, you can specify the concurrency of your experiment. The max_concurrency argument has slightly different semantics depending on whether you are using evaluate or aevaluate.\\nevaluate\\nThe max_concurrency argument to evaluate specifies the maximum number of concurrent threads to use when running the experiment. This is both for when running your target function as well as your evaluators.\\naevaluate\\nThe max_concurrency argument to aevaluate is fairly similar to evaluate, but instead uses a semaphore to limit the number of concurrent tasks that can run at once. aevaluate works by creating a task for each example in the dataset. Each task consists of running the target function as well as all of the evaluators on that specific example. The max_concurrency argument specifies the maximum number of concurrent tasks, or put another way - examples, to run at once.\\n\\u200bCaching\\nLastly, you can also cache the API calls made in your experiment by setting the LANGSMITH_TEST_CACHE to a valid folder on your device with write access. This will cause the API calls made in your experiment to be cached to disk, meaning future experiments that make the same API calls will be greatly sped up.\\n\\u200bAnnotation queues\\nHuman feedback is often the most valuable feedback you can gather on your application. With annotation queues you can flag runs of your application for annotation. Human annotators then have a streamlined view to review and provide feedback on the runs in a queue. Often (some subset of) these annotated runs are then transferred to a dataset for future evaluations. While you can always annotate runs inline, annotation queues provide another option to group runs together, specify annotation criteria, and configure permissions.\\nLearn more about annotation queues and human feedback.\\n\\u200bOffline evaluation\\nEvaluating an application on a dataset is what we call â€œofflineâ€ evaluation. It is offline because weâ€™re evaluating on a pre-compiled set of data. An online evaluation, on the other hand, is one in which we evaluate a deployed applicationâ€™s outputs on real traffic, in near realtime. Offline evaluations are used for testing a version(s) of your application pre-deployment.\\nYou can run offline evaluations client-side using the LangSmith SDK (Python and TypeScript). You can run them server-side via the Prompt Playground or by configuring automations to run certain evaluators on every new experiment against a specific dataset.\\n\\n\\u200bBenchmarking\\nPerhaps the most common type of offline evaluation is one in which we curate a dataset of representative inputs, define the key performance metrics, and benchmark multiple versions of our application to find the best one. Benchmarking can be laborious because for many use cases you have to curate a dataset with gold-standard reference outputs and design good metrics for comparing experimental outputs to them. For a RAG Q&A bot this might look like a dataset of questions and reference answers, and an LLM-as-judge evaluator that determines if the actual answer is semantically equivalent to the reference answer. For a ReACT agent this might look like a dataset of user requests and a reference set of all the tool calls the model is supposed to make, and a heuristic evaluator that checks if all of the reference tool calls were made.\\n\\u200bUnit tests\\nUnit tests are used in software development to verify the correctness of individual system components. Unit tests in the context of LLMs are often rule-based assertions on LLM inputs or outputs (e.g., checking that LLM-generated code can be compiled, JSON can be loaded, etc.) that validate basic functionality.\\nUnit tests are often written with the expectation that they should always pass. These types of tests are nice to run as part of CI. Note that when doing so it is useful to set up a cache to minimize LLM calls (because those can quickly rack up!).\\n\\u200bRegression tests\\nRegression tests are used to measure performance across versions of your application over time. They are used to, at the very least, ensure that a new app version does not regress on examples that your current version correctly handles, and ideally to measure how much better your new version is relative to the current. Often these are triggered when you are making app updates (e.g. updating models or architectures) that are expected to influence the user experience.\\nLangSmithâ€™s comparison view has native support for regression testing, allowing you to quickly see examples that have changed relative to the baseline. Regressions are highlighted red, improvements green.\\n\\n\\u200bBacktesting\\nBacktesting is an approach that combines dataset creation (discussed above) with evaluation. If you have a collection of production logs, you can turn them into a dataset. Then, you can re-run those production examples with newer application versions. This allows you to assess performance on past and realistic user inputs.\\nThis is commonly used to evaluate new model versions. Anthropic dropped a new model? No problem! Grab the 1000 most recent runs through your application and pass them through the new model. Then compare those results to what actually happened in production.\\n\\u200bPairwise evaluation\\nFor some tasks it is easier for a human or LLM grader to determine if â€œversion A is better than Bâ€ than to assign an absolute score to either A or B. Pairwise evaluations are just this â€”\\xa0a scoring of the outputs of two versions against each other as opposed to against some reference output or absolute criteria. Pairwise evaluations are often useful when using LLM-as-judge evaluators on more general tasks. For example, if you have a summarizer application, it may be easier for an LLM-as-judge to determine â€œWhich of these two summaries is more clear and concise?â€ than to give an absolute score like â€œGive this summary a score of 1-10 in terms of clarity and concision.â€\\nLearn how run pairwise evaluations.\\n\\u200bOnline evaluation\\nEvaluating a deployed applicationâ€™s outputs in (roughly) realtime is what we call â€œonlineâ€ evaluation. In this case there is no dataset involved and no possibility of reference outputs â€” weâ€™re running evaluators on real inputs and real outputs as theyâ€™re produced. This is useful for monitoring your application and flagging unintended behavior. Online evaluation can also work hand-in-hand with offline evaluation: for example, an online evaluator can be used to classify input questions into a set of categories that can later be used to curate a dataset for offline evaluation.\\nOnline evaluators are generally intended to be run server-side. LangSmith has built-in LLM-as-judge evaluators that you can configure, or you can define custom code evaluators that are also run within LangSmith.\\n\\n\\u200bTesting\\n\\u200bEvaluations vs testing\\nTesting and evaluation are very similar and overlapping concepts that often get confused.\\nAn evaluation measures performance according to a metric(s). Evaluation metrics can be fuzzy or subjective, and are more useful in relative terms than absolute ones. That is, theyâ€™re often used to compare two systems against each other rather than to assert something about an individual system.\\nTesting asserts correctness. A system can only be deployed if it passes all tests.\\nEvaluation metrics can be turned into tests. For example, you can write regression tests to assert that any new version of a system must outperform some baseline version of the system on the relevant evaluation metrics.\\nIt can also be more resource efficient to run tests and evaluations together if your system is expensive to run and you have overlapping datasets for your tests and evaluations.\\nYou can also choose to write evaluations using standard software testing tools like pytest or vitest/jest out of convenience.\\n\\u200bUsing pytest and Vitest/Jest\\nThe LangSmith SDKs come with integrations for pytest and Vitest/Jest. These make it easy to:\\n\\nTrack test results in LangSmith\\nWrite evaluations as tests\\n\\nTracking test results in LangSmith makes it easy to share results, compare systems, and debug failing tests.\\nWriting evaluations as tests can be useful when each example you want to evaluate on requires custom logic for running the application and/or evaluators. The standard evaluation flows assume that you can run your application and evaluators in the same way on every example in a dataset. But for more complex systems or comprehensive evals, you may want to evaluate specific subsets of your system with specific types of inputs and metrics. These types of heterogenous evals are much easier to write as a suite of distinct test cases that all get tracked together rather than using the standard evaluate flow.\\nUsing testing tools is also helpful when you want to both evaluate your systemâ€™s outputs and assert some basic things about them.Was this page helpful?YesNoSuggest editsOverviewEvaluation approachesâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'loc': 'https://docs.smith.langchain.com/evaluation/concepts', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Evaluation - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationEvaluationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumEvaluationCopy pageCopy pageWelcome to the LangSmith Evaluation documentation. The following sections help you create datasets, run evaluations, and analyze results:\\n\\n\\nDatasets: Create and manage datasets for evaluation, including creating datasets through the UI or SDK and managing existing datasets.\\n\\n\\nEvaluations: Run evaluations on your applications using various methods and techniques, including different evaluator types and evaluation techniques.\\n\\n\\nAnalyze experiment results: View and analyze your evaluation results, including comparing experiments, filtering results, and downloading data.\\n\\n\\nAnnotation & human feedback: Collect human feedback on your application outputs through annotation queues and inline annotation.\\n\\n\\nTutorials: Follow step-by-step tutorials to evaluate different types of applications, from chatbots to complex agents.\\n\\n\\nFor terminology definitions and core concepts, refer to the introduction on evaluation.Was this page helpful?YesNoSuggest editsConceptsâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides', 'loc': 'https://docs.smith.langchain.com/evaluation/how_to_guides', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='How to evaluate an LLM application - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationWith the SDKWith the UIUse prebuilt evaluatorsEvaluation typesFrameworks & integrationsEvaluation techniquesImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationRun an evaluationHow to evaluate an LLM applicationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageDefine an applicationCreate or select a datasetDefine an evaluatorRun the evaluationExplore the results\\u200bReference code\\u200bRelated\\u200bSet up evaluationsRun an evaluationHow to evaluate an LLM applicationCopy pageCopy pageThis guide shows you how to run an evaluation on an LLM application using the LangSmith SDK.\\nEvaluations | Evaluators | Datasets\\nIn this guide weâ€™ll go over how to evaluate an application using the evaluate() method in the LangSmith SDK.\\nFor larger evaluation jobs in Python we recommend using aevaluate(), the asynchronous version of evaluate(). It is still worthwhile to read this guide first, as the two have identical interfaces, before reading the how-to guide on running an evaluation asynchronously.In JS/TS evaluate() is already asynchronous so no separate method is needed.It is also important to configure the max_concurrency/maxConcurrency arg when running large jobs. This parallelizes evaluation by effectively splitting the dataset across threads.\\n\\u200bDefine an application\\nFirst we need an application to evaluate. Letâ€™s create a simple toxicity classifier for this example.\\nPythonTypeScriptCopyfrom langsmith import traceable, wrappers\\nfrom openai import OpenAI\\n\\n# Optionally wrap the OpenAI client to trace all model calls.\\noai_client = wrappers.wrap_openai(OpenAI())\\n\\n# Optionally add the \\'traceable\\' decorator to trace the inputs/outputs of this function.\\n@traceable\\ndef toxicity_classifier(inputs: dict) -> dict:\\n    instructions = (\\n      \"Please review the user query below and determine if it contains any form of toxic behavior, \"\\n      \"such as insults, threats, or highly negative comments. Respond with \\'Toxic\\' if it does \"\\n      \"and \\'Not toxic\\' if it doesn\\'t.\"\\n    )\\n    messages = [\\n        {\"role\": \"system\", \"content\": instructions},\\n        {\"role\": \"user\", \"content\": inputs[\"text\"]},\\n    ]\\n    result = oai_client.chat.completions.create(\\n        messages=messages, model=\"gpt-4o-mini\", temperature=0\\n    )\\n    return {\"class\": result.choices[0].message.content}\\n\\nWeâ€™ve optionally enabled tracing to capture the inputs and outputs of each step in the pipeline. To understand how to annotate your code for tracing, please refer to this guide.\\n\\u200bCreate or select a dataset\\nWe need a Dataset to evaluate our application on. Our dataset will contain labeled examples of toxic and non-toxic text.\\nRequires langsmith>=0.3.13\\nPythonTypeScriptCopyfrom langsmith import Client\\nls_client = Client()\\n\\nexamples = [\\n  {\\n    \"inputs\": {\"text\": \"Shut up, idiot\"},\\n    \"outputs\": {\"label\": \"Toxic\"},\\n  },\\n  {\\n    \"inputs\": {\"text\": \"You\\'re a wonderful person\"},\\n    \"outputs\": {\"label\": \"Not toxic\"},\\n  },\\n  {\\n    \"inputs\": {\"text\": \"This is the worst thing ever\"},\\n    \"outputs\": {\"label\": \"Toxic\"},\\n  },\\n  {\\n    \"inputs\": {\"text\": \"I had a great day today\"},\\n    \"outputs\": {\"label\": \"Not toxic\"},\\n  },\\n  {\\n    \"inputs\": {\"text\": \"Nobody likes you\"},\\n    \"outputs\": {\"label\": \"Toxic\"},\\n  },\\n  {\\n    \"inputs\": {\"text\": \"This is unacceptable. I want to speak to the manager.\"},\\n    \"outputs\": {\"label\": \"Not toxic\"},\\n  },\\n]\\n\\ndataset = ls_client.create_dataset(dataset_name=\"Toxic Queries\")\\nls_client.create_examples(\\n  dataset_id=dataset.id,\\n  examples=examples,\\n)\\n\\nFor more details on datasets, refer to the Manage datasets page.\\n\\u200bDefine an evaluator\\nYou can also check out LangChainâ€™s open source evaluation package openevals for common pre-built evaluators.\\nEvaluators are functions for scoring your applicationâ€™s outputs. They take in the example inputs, actual outputs, and, when present, the reference outputs. Since we have labels for this task, our evaluator can directly check if the actual outputs match the reference outputs.\\n\\nPython: Requires langsmith>=0.3.13\\nTypeScript: Requires langsmith>=0.2.9\\n\\nPythonTypeScriptCopydef correct(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\\n    return outputs[\"class\"] == reference_outputs[\"label\"]\\n\\n\\u200bRun the evaluation\\nWeâ€™ll use the evaluate() / aevaluate() methods to run the evaluation.\\nThe key arguments are:\\n\\na target function that takes an input dictionary and returns an output dictionary. The example.inputs field of each Example is what gets passed to the target function. In this case our toxicity_classifier is already set up to take in example inputs so we can use it directly.\\ndata - the name OR UUID of the LangSmith dataset to evaluate on, or an iterator of examples\\nevaluators - a list of evaluators to score the outputs of the function\\n\\nPython: Requires langsmith>=0.3.13\\nPythonTypeScriptCopy# Can equivalently use the \\'evaluate\\' function directly:\\n# from langsmith import evaluate; evaluate(...)\\nresults = ls_client.evaluate(\\n    toxicity_classifier,\\n    data=dataset.name,\\n    evaluators=[correct],\\n    experiment_prefix=\"gpt-4o-mini, baseline\",  # optional, experiment name prefix\\n    description=\"Testing the baseline system.\",  # optional, experiment description\\n    max_concurrency=4, # optional, add concurrency\\n)\\n\\n\\u200bExplore the results\\u200b\\nEach invocation of evaluate() creates an Experiment which can be viewed in the LangSmith UI or queried via the SDK. Evaluation scores are stored against each actual output as feedback.\\nIf youâ€™ve annotated your code for tracing, you can open the trace of each row in a side panel view.\\n\\n\\u200bReference code\\u200b\\nClick to see a consolidated code snippetPythonTypeScriptCopyfrom langsmith import Client, traceable, wrappers\\nfrom openai import OpenAI\\n\\n# Step 1. Define an application\\noai_client = wrappers.wrap_openai(OpenAI())\\n\\n@traceable\\ndef toxicity_classifier(inputs: dict) -> str:\\n    system = (\\n      \"Please review the user query below and determine if it contains any form of toxic behavior, \"\\n      \"such as insults, threats, or highly negative comments. Respond with \\'Toxic\\' if it does \"\\n      \"and \\'Not toxic\\' if it doesn\\'t.\"\\n    )\\n    messages = [\\n        {\"role\": \"system\", \"content\": system},\\n        {\"role\": \"user\", \"content\": inputs[\"text\"]},\\n    ]\\n    result = oai_client.chat.completions.create(\\n        messages=messages, model=\"gpt-4o-mini\", temperature=0\\n    )\\n    return result.choices[0].message.content\\n\\n# Step 2. Create a dataset\\nls_client = Client()\\ndataset = ls_client.create_dataset(dataset_name=\"Toxic Queries\")\\nexamples = [\\n  {\\n    \"inputs\": {\"text\": \"Shut up, idiot\"},\\n    \"outputs\": {\"label\": \"Toxic\"},\\n  },\\n  {\\n    \"inputs\": {\"text\": \"You\\'re a wonderful person\"},\\n    \"outputs\": {\"label\": \"Not toxic\"},\\n  },\\n  {\\n    \"inputs\": {\"text\": \"This is the worst thing ever\"},\\n    \"outputs\": {\"label\": \"Toxic\"},\\n  },\\n  {\\n    \"inputs\": {\"text\": \"I had a great day today\"},\\n    \"outputs\": {\"label\": \"Not toxic\"},\\n  },\\n  {\\n    \"inputs\": {\"text\": \"Nobody likes you\"},\\n    \"outputs\": {\"label\": \"Toxic\"},\\n  },\\n  {\\n    \"inputs\": {\"text\": \"This is unacceptable. I want to speak to the manager.\"},\\n    \"outputs\": {\"label\": \"Not toxic\"},\\n  },\\n]\\nls_client.create_examples(\\n  dataset_id=dataset.id,\\n  examples=examples,\\n)\\n\\n# Step 3. Define an evaluator\\ndef correct(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\\n    return outputs[\"output\"] == reference_outputs[\"label\"]\\n\\n# Step 4. Run the evaluation\\n# Client.evaluate() and evaluate() behave the same.\\nresults = ls_client.evaluate(\\n    toxicity_classifier,\\n    data=dataset.name,\\n    evaluators=[correct],\\n    experiment_prefix=\"gpt-4o-mini, simple\",  # optional, experiment name prefix\\n    description=\"Testing the baseline system.\",  # optional, experiment description\\n    max_concurrency=4,  # optional, add concurrency\\n)\\n\\n\\u200bRelated\\u200b\\n\\nRun an evaluation asynchronously\\nRun an evaluation via the REST API\\nRun an evaluation from the prompt playground\\nWas this page helpful?YesNoSuggest editsManage datasetsWith the UIâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment', 'loc': 'https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Analyze an experiment - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationAnalyze experiment resultsAnalyze an experimentGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageAnalyze a single experimentOpen the experiment viewView experiment resultsCustomize columnsSort and filterTable viewsView the tracesView evaluator runsGroup results by metadataRepetitionsCompare to another experimentDownload experiment results as a CSVRename an experimentAnalyze experiment resultsAnalyze an experimentCopy pageCopy pageThis page describes some of the essential tasks for working with experiments in LangSmith:\\n\\nAnalyze a single experiment: View and interpret experiment results, customize columns, filter data, and compare runs.\\nDownload experiment results as a CSV: Export your experiment data for external analysis and sharing.\\nRename an experiment: Update experiment names in both the Playground and Experiments view.\\n\\n\\u200bAnalyze a single experiment\\nAfter running an experiment, you can use LangSmithâ€™s experiment view to analyze the results and draw insights about your experimentâ€™s performance.\\n\\u200bOpen the experiment view\\nTo open the experiment view, select the relevant dataset from the Dataset & Experiments page and then select the experiment you want to view.\\n\\n\\u200bView experiment results\\n\\u200bCustomize columns\\nBy default, the experiment view shows the input, output, and reference output for each example in the dataset, feedback scores from evaluations and experiment metrics like cost, token counts, latency and status.\\nYou can customize the columns using the Display button to make it easier to interpret experiment results:\\n\\nBreak out fields from inputs, outputs, and reference outputs into their own columns. This is especially helpful if you have long inputs/outputs/reference outputs and want to surface important fields.\\nHide and reorder columns to create focused views for analysis.\\nControl decimal precision on feedback scores. By default, LangSmith surfaces numerical feedback scores with a decimal precision of 2, but you can customize this setting to be up to 6 decimals.\\nSet the Heat Map threshold to high, middle, and low for numeric feedback scores in your experiment, which affects the threshold at which score chips render as red or green:\\n\\n\\nYou can set default configurations for an entire dataset or temporarily save settings just for yourself.\\n\\u200bSort and filter\\nTo sort or filter feedback scores, you can use the actions in the column headers.\\n\\n\\u200bTable views\\nDepending on the view most useful for your analysis, you can change the formatting of the table by toggling between a compact view, a full, view, and a diff view.\\n\\nThe Compact view shows each run as a one-line row, for ease of comparing scores at a glance.\\nThe Full view shows the full output for each run for digging into the details of individual runs.\\nThe Diff view shows the text difference between the reference output and the output for each run.\\n\\n\\n\\u200bView the traces\\nHover over any of the output cells, and click on the trace icon to view the trace for that run. This will open up a trace in the side panel.\\nTo view the entire tracing project, click on the View Project button in the top right of the header.\\n\\n\\u200bView evaluator runs\\nFor evaluator scores, you can view the source run by hovering over the evaluator score cell and clicking on the arrow icon. This will open up a trace in the side panel. If youâ€™re running a LLM-as-a-judge evaluator, you can view the prompt used for the evaluator in this run. If your experiment has repetitions, you can click on the aggregate average score to find links to all of the individual runs.\\n\\n\\u200bGroup results by metadata\\nYou can add metadata to examples to categorize and organize them. For example, if youâ€™re evaluating factual accuracy on a question answering dataset, the metadata might include which subject area each question belongs to. Metadata can be added either via the UI or via the SDK.\\nTo analyze results by metadata, use the Group by dropdown in the top right corner of the experiment view and select your desired metadata key. This displays average feedback scores, latency, total tokens, and cost for each metadata group.\\nYou will only be able to group by example metadata on experiments created after February 20th, 2025. Any experiments before that date can still be grouped by metadata, but only if the metadata is on the experiment traces themselves.\\n\\u200bRepetitions\\nIf youâ€™ve run your experiment with repetitions, there will be arrows in the output results column so you can view outputs in the table. To view each run from the repetition, hover over the output cell and click the expanded view.\\nWhen you run an experiment with repetitions, LangSmith displays the average for each feedback score in the table. Click on the feedback score to view the feedback scores from individual runs, or to view the standard deviation across repetitions.\\n\\n\\u200bCompare to another experiment\\nIn the top right of the experiment view, you can select another experiment to compare to. This will open up a comparison view, where you can see how the two experiments compare. To learn more about the comparison view, see how to compare experiment results.\\n\\u200bDownload experiment results as a CSV\\nLangSmith lets you download experiment results as a CSV file, which allows you to analyze and share your results.\\nTo download as a CSV, click the download icon at the top of the experiment view. The icon is directly to the left of the Compact toggle.\\n\\n\\u200bRename an experiment\\nExperiment names must be unique per workspace.\\nYou can rename an experiment in the LangSmith UI in:\\n\\n\\nThe Playground. When running experiments in the Playground, a default name with the format pg::prompt-name::model::uuid (eg. pg::gpt-4o-mini::897ee630) is automatically assigned.\\nYou can rename an experiment immediately after running it by editing its name in the Playground table header.\\n\\n\\n\\nThe Experiments view. When viewing results in the experiments view, you can rename an experiment by using the pencil icon beside the experiment name.\\n\\n\\nWas this page helpful?YesNoSuggest editsRun backtests on a new version of an agentCompare experiment resultsâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline', 'loc': 'https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Annotate traces and runs inline - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationAnnotation & human feedbackAnnotate traces and runs inlineGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumAnnotation & human feedbackAnnotate traces and runs inlineCopy pageCopy pageLangSmith allows you to manually annotate traces with feedback within the application. This can be useful for adding context to a trace, such as a userâ€™s comment or a note about a specific issue.\\nYou can annotate a trace either inline or by sending the trace to an annotation queue, which allows you to closely inspect and log feedbacks to runs one at a time.\\nFeedback tags are associated with your workspace.\\nYou can attach user feedback to ANY intermediate run (span) of the trace, not just the root span.This is useful for critiquing specific parts of the LLM application, such as the retrieval step or generation step of the RAG pipeline.\\nTo annotate a trace inline, click on the Annotate in the upper right corner of trace view for any particular run that is part of the trace.\\n\\nThis will open up a pane that allows you to choose from feedback tags associated with your workspace and add a score for particular tags. You can also add a standalone comment. Follow this guide to set up feedback tags for your workspace.\\nYou can also set up new feedback criteria from within the pane itself.\\n\\nYou can use the labeled keyboard shortcuts to streamline the annotation process.Was this page helpful?YesNoSuggest editsSet up feedback criteriaAudit evaluator scoresâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues', 'loc': 'https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Use annotation queues - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationAnnotation & human feedbackUse annotation queuesGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageCreate an annotation queueBasic DetailsAnnotation RubricCollaborator SettingsAssign runs to an annotation queueReview runs in an annotation queueVideo guideAnnotation & human feedbackUse annotation queuesCopy pageCopy pageAnnotation queues are a powerful LangSmith feature that provide a streamlined, directed view for human annotators to attach feedback to specific runs. While you can always annotate traces inline, annotation queues provide another option to group runs together, then have annotators review and provide feedback on them.\\n\\u200bCreate an annotation queue\\n\\nTo create an annotation queue, navigate to the Annotation queues section through the homepage or left-hand navigation bar. Then click + New annotation queue in the top right corner.\\n\\n\\u200bBasic Details\\nFill in the form with the name and description of the queue. You can also assign a default dataset to queue, which will streamline the process of sending the inputs and outputs of certain runs to datasets in your LangSmith workspace.\\n\\u200bAnnotation Rubric\\nBegin by drafting some high-level instructions for your annotators, which will be shown in the sidebar on every run.\\nNext, click â€+ Desired Feedbackâ€ to add feedback keys to your annotation queue. Annotators will be presented with these feedback keys on each run. Add a description for each, as well as a short description of each category if the feedback is categorical.\\n\\nReviewers will see this:\\n\\n\\u200bCollaborator Settings\\nThere are a few settings related to multiple annotators:\\n\\n\\nNumber of reviewers per run: This determines the number of reviewers that must mark a run as â€œDoneâ€ for it to be removed from the queue. If you check â€œAll workspace members review each run,â€ then a run will remain in the queue until all workspace members have marked it â€œDoneâ€.\\n\\nReviewers cannot view the feedback left by other reviewers.\\nComments on runs are visible to all reviewers.\\n\\n\\n\\nEnable reservations on runs: We recommend enabling reservations. This will prevent multiple annotators from reviewing the same run at the same time.\\n\\n\\n\\nHow do reservations work?\\n\\nWhen a reviewer views a run, the run is reserved for that reviewer for the specified â€œreservation lengthâ€. If there are multiple reviewers per run as specified above, the run can be reserved by multiple reviewers (up to the number of reviewers per run) at the same time.\\n\\nWhat happens if time runs out?\\n\\nIf a reviewer has viewed a run and then leaves the run without marking it â€œDoneâ€, the reservation will expire after the specified â€œreservation lengthâ€. The run is then released back into the queue and can be reserved by another reviewer.\\nClicking â€œRequeue at endâ€ will only move the current run to the end of the current userâ€™s queue; it wonâ€™t affect the queue order of any other user. It will also release the reservation that the current user has on that run.\\nBecause of these settings, itâ€™s possible (and likely) that the number of runs visible to an individual in an annotation queue differs from the total number of runs in the queue as well as anyone elseâ€™s queue size.\\nYou can update these settings at any time by clicking on the pencil icon in the Annotation Queues section.\\n\\n\\u200bAssign runs to an annotation queue\\nTo assign runs to an annotation queue, either:\\n\\n\\nClick on Add to Annotation Queue in top right corner of any trace view. You can add ANY intermediate run (span) of the trace to an annotation queue, not just the root span. \\n\\n\\nSelect multiple runs in the runs table then click Add to Annotation Queue at the bottom of the page. \\n\\n\\nSet up an automation rule that automatically assigns runs which pass a certain filter and sampling condition to an annotation queue.\\n\\n\\nSelect one or multiple experiments from the dataset page and click Annotate. From the resulting popup, you may either create a new queue or add the runs to an existing one: \\n\\n\\nIt is often a very good idea to assign runs that have a certain user feedback score (eg thumbs up, thumbs down) from the application to an annotation queue. This way, you can identify and address issues that are causing user dissatisfaction. To learn more about how to capture user feedback from your LLM application, follow this guide.\\n\\u200bReview runs in an annotation queue\\nTo review runs in an annotation queue, navigate to the Annotation Queues section through the homepage or left-hand navigation bar. Then click on the queue you want to review. This will take you to a focused, cyclical view of the runs in the queue that require review.\\nYou can attach a comment, attach a score for a particular feedback criteria, add the run a dataset and/or mark the run as reviewed. You can also remove the run from the queue for all users, despite any current reservations or settings for the queue, by clicking the Trash icon next to â€œView runâ€.\\nThe keyboard shortcuts shown can help streamline the review process.\\n\\n\\u200bVideo guide\\nWas this page helpful?YesNoSuggest editsUpload experiments run outside of LangSmithSet up feedback criteriaâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides/async', 'loc': 'https://docs.smith.langchain.com/evaluation/how_to_guides/async', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='How to run an evaluation asynchronously - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsRun an evaluation asynchronouslyRun evaluations with pytestRun evals with Vitest/JestWith the APIEvaluation techniquesImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationFrameworks & integrationsHow to run an evaluation asynchronouslyGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageUse aevaluate()RelatedSet up evaluationsFrameworks & integrationsHow to run an evaluation asynchronouslyCopy pageCopy pageEvaluations | Evaluators | Datasets | Experiments\\nWe can run evaluations asynchronously via the SDK using aevaluate(), which accepts all of the same arguments as evaluate() but expects the application function to be asynchronous. You can learn more about how to use the evaluate() function here.\\nThis guide is only relevant when using the Python SDK. In JS/TS the evaluate() function is already async. You can see how to use it here.\\n\\u200bUse aevaluate()\\n\\nPython\\n\\nRequires langsmith>=0.3.13\\nCopyfrom langsmith import wrappers, Client\\nfrom openai import AsyncOpenAI\\n\\n# Optionally wrap the OpenAI client to trace all model calls.\\noai_client = wrappers.wrap_openai(AsyncOpenAI())\\n\\n# Optionally add the \\'traceable\\' decorator to trace the inputs/outputs of this function.\\n@traceable\\nasync def researcher_app(inputs: dict) -> str:\\n    instructions = \"\"\"You are an excellent researcher. Given a high-level research idea, \\\\\\nlist 5 concrete questions that should be investigated to determine if the idea is worth pursuing.\"\"\"\\n\\n    response = await oai_client.chat.completions.create(\\n        model=\"gpt-4o-mini\",\\n        messages=[\\n            {\"role\": \"system\", \"content\": instructions},\\n            {\"role\": \"user\", \"content\": inputs[\"idea\"]},\\n        ],\\n    )\\n    return response.choices[0].message.content\\n\\n# Evaluator functions can be sync or async\\ndef concise(inputs: dict, outputs: dict) -> bool:\\n    return len(outputs[\"output\"]) < 3 * len(inputs[\"idea\"])\\n\\nls_client = Client()\\nideas = [\\n    \"universal basic income\",\\n    \"nuclear fusion\",\\n    \"hyperloop\",\\n    \"nuclear powered rockets\",\\n]\\ndataset = ls_client.create_dataset(\"research ideas\")\\nls_client.create_examples(\\n    dataset_name=dataset.name,\\n    examples=[{\"inputs\": {\"idea\": i}} for i in ideas],\\n)\\n\\n# Can equivalently use the \\'aevaluate\\' function directly:\\n# from langsmith import aevaluate\\n# await aevaluate(...)\\nresults = await ls_client.aevaluate(\\n    researcher_app,\\n    data=dataset,\\n    evaluators=[concise],\\n    # Optional, add concurrency.\\n    max_concurrency=2,  # Optional, add concurrency.\\n    experiment_prefix=\"gpt-4o-mini-baseline\"  # Optional, random by default.\\n)\\n\\n\\u200bRelated\\n\\nRun an evaluation (synchronously)\\nHandle model rate limits\\nWas this page helpful?YesNoSuggest editsPairwise evaluationRun evaluations with pytestâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback', 'loc': 'https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Log user feedback using the SDK - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsManual instrumentationConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationFeedback & evaluationLog user feedback using the SDKGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageUse create_feedback() / createFeedback()Feedback & evaluationLog user feedback using the SDKCopy pageCopy pageKey concepts\\nConceptual guide on tracing and feedback\\nReference guide on feedback data format\\n\\nLangSmith makes it easy to attach feedback to traces.\\nThis feedback can come from users, annotators, automated evaluators, etc., and is crucial for monitoring and evaluating applications.\\n\\u200bUse create_feedback() / createFeedback()\\nHere weâ€™ll walk through how to log feedback using the SDK.\\nChild runs\\nYou can attach user feedback to ANY child run of a trace, not just the trace (root run) itself.\\nThis is useful for critiquing specific steps of the LLM application, such as the retrieval step or generation step of a RAG pipeline.\\nNon-blocking creation (Python only)\\nThe Python client will automatically background feedback creation if you pass trace_id= to create_feedback().\\nThis is essential for low-latency environments, where you want to make sure your application isnâ€™t blocked on feedback creation.\\nPythonTypeScriptCopyfrom langsmith import trace, traceable, Client\\n\\n    @traceable\\n    def foo(x):\\n        return {\"y\": x * 2}\\n\\n    @traceable\\n    def bar(y):\\n        return {\"z\": y - 1}\\n\\n    client = Client()\\n\\n    inputs = {\"x\": 1}\\n    with trace(name=\"foobar\", inputs=inputs) as root_run:\\n        result = foo(**inputs)\\n        result = bar(**result)\\n        root_run.outputs = result\\n        trace_id = root_run.id\\n        child_runs = root_run.child_runs\\n\\n    # Provide feedback for a trace (a.k.a. a root run)\\n    client.create_feedback(\\n        key=\"user_feedback\",\\n        score=1,\\n        trace_id=trace_id,\\n        comment=\"the user said that ...\"\\n    )\\n\\n# Provide feedback for a child run\\nfoo_run_id = [run for run in child_runs if run.name == \"foo\"][0].id\\nclient.create_feedback(\\n    key=\"correctness\",\\n    score=0,\\n    run_id=foo_run_id,\\n    # trace_id= is optional but recommended to enable batched and backgrounded\\n    # feedback ingestion.\\n    trace_id=trace_id,\\n)\\n\\nYou can even log feedback for in-progress runs using create_feedback() / createFeedback(). See this guide for how to get the run ID of an in-progress run.\\nTo learn more about how to filter traces based on various attributes, including user feedback, see this guide.Was this page helpful?YesNoSuggest editsConfigure webhook notifications for rulesSet up online evaluatorsâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores', 'loc': 'https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='How to audit evaluator scores - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationAnnotation & human feedbackHow to audit evaluator scoresGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageIn the comparison viewIn the runs tableIn the SDKAnnotation & human feedbackHow to audit evaluator scoresCopy pageCopy pageLLM-as-a-judge evaluators donâ€™t always get it right. Because of this, it is often useful for a human to manually audit the scores left by an evaluator and correct them where necessary. LangSmith allows you to make corrections on evaluator scores in the UI or SDK.\\n\\u200bIn the comparison view\\nIn the comparison view, you may click on any feedback tag to bring up the feedback details. From there, click the â€œeditâ€ icon on the right to bring up the corrections view. You may then type in your desired score in the text box under â€œMake correctionâ€. If you would like, you may also attach an explanation to your correction. This is useful if you are using a few-shot evaluator and will be automatically inserted into your few-shot examples in place of the few_shot_explanation prompt variable.\\n\\n\\u200bIn the runs table\\nIn the runs table, find the â€œFeedbackâ€ column and click on the feedback tag to bring up the feedback details. Again, click the â€œeditâ€ icon on the right to bring up the corrections view.\\n\\n\\u200bIn the SDK\\nCorrections can be made via the SDKâ€™s update_feedback function, with the correction dict. You must specify a score key which corresponds to a number for it to be rendered in the UI.\\nPythonTypeScriptCopyimport langsmith\\n\\nclient = langsmith.Client()\\n\\nclient.update_feedback(\\n    my_feedback_id,\\n    correction={\\n        \"score\": 1,\\n    },\\n)\\nWas this page helpful?YesNoSuggest editsAnnotate traces and runs inlineExample data formatâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset', 'loc': 'https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Automatically run evaluators on experiments - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesDefine a target function to evaluateEvaluate intermediate stepsReturn multiple scores in one evaluatorReturn categorical vs numerical metricsRun evaluators on experimentsEvaluate with repetitionsHandle model rate limitsRun an evaluation locally (Python)Evaluate a runnableEvaluate a graphEvaluate an existing experiment (Python)Run an evaluation with multimodal contentSimulate multi-turn interactionsImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationEvaluation techniquesAutomatically run evaluators on experimentsGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageConfiguring an evaluator on a datasetLLM-as-a-judge evaluatorsCustom code evaluatorsNext stepsSet up evaluationsEvaluation techniquesAutomatically run evaluators on experimentsCopy pageCopy pageLangSmith supports two ways to grade experiments created via the SDK:\\n\\nProgrammatically, by specifying evaluators in your code (see this guide for details)\\nBy binding evaluators to a dataset in the UI. This will automatically run the evaluators on any new experiments created, in addition to any evaluators youâ€™ve set up via the SDK. This is useful when youâ€™re iterating on your application (target function), and have a standard set of evaluators you want to run for all experiments.\\n\\n\\u200bConfiguring an evaluator on a dataset\\n\\nClick on the Datasets and Experiments tab in the sidebar.\\nSelect the dataset you want to configure the evaluator for.\\nClick on the + Evaluator button to add an evaluator to the dataset. This will open a pane you can use to configure the evaluator.\\n\\nWhen you configure an evaluator for a dataset, it will only affect the experiment runs that are created after the evaluator is configured. It will not affect the evaluation of experiment runs that were created before the evaluator was configured.\\n\\u200bLLM-as-a-judge evaluators\\nThe process for binding evaluators to a dataset is very similar to the process for configuring a LLM-as-a-judge evaluator in the Playground. View instructions for configuring an LLM-as-a-judge evaluator in the Playground.\\n\\u200bCustom code evaluators\\nThe process for binding a code evaluators to a dataset is very similar to the process for configuring a code evaluator in online evaluation. View instruction for configuring code evaluators.\\nThe only difference between configuring a code evaluator in online evaluation and binding a code evaluator to a dataset is that the custom code evaluator can reference outputs that are part of the datasetâ€™s Example.\\nFor custom code evaluators bound to a dataset, the evaluator function takes in two arguments:\\n\\nA Run (reference). This represents the new run in your experiment. For example, if you ran an experiment via SDK, this would contain the input/output from your chain or model you are testing.\\nAn Example (reference). This represents the reference example in your dataset that the chain or model you are testing uses. The inputs to the Run and Example should be the same. If your Example has a reference outputs, then you can use this to compare to the runâ€™s output for scoring.\\n\\nThe code below shows an example of a simple evaluator function that checks that the outputs exactly equal the reference outputs.\\nPythonJavaScriptCopyimport numpy as np\\n\\ndef perform_eval(run, example):\\n    # run is a Run object\\n    # example is an Example object\\n    output = run[\\'outputs\\'][\\'output\\']\\n    ref_output = example[\\'outputs\\'][\\'outputs\\']\\n    output_match = np.array_equal(output, ref_output)\\n\\n    return { \"exact_match\": output_match }\\n\\n\\u200bNext steps\\n\\nAnalyze your experiment results in the experiments tab\\nCompare your experiment results in the comparison view\\nWas this page helpful?YesNoSuggest editsReturn categorical vs numerical metricsEvaluate with repetitionsâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results', 'loc': 'https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='How to compare experiment results - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationAnalyze experiment resultsHow to compare experiment resultsGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageOpen the comparison viewAdjust the table displayView regressions and improvementsUpdate baseline experiment and metricOpen a traceExpand detailed viewView summary chartsUse experiment metadata as chart labelsAnalyze experiment resultsHow to compare experiment resultsCopy pageCopy pageWhen you are iterating on your LLM application (such as changing the model or the prompt), you will want to compare the results of different experiments.\\nLangSmith supports a comparison view that lets you hone in on key differences, regressions, and improvements between different experiments.\\n\\u200bOpen the comparison view\\n\\nTo access the experiment comaprison view, navigate to the Datasets & Experiments page.\\nSelect a dataset, which will open the Experiments tab.\\nSelect two or more experiments abd then click Compare.\\n\\n\\n\\u200bAdjust the table display\\nYou can toggle between different views by clicking Full or Compact at the top of the Comparing Experiments page.\\nToggling Full will show the full text of the input, output, and reference output for each run. If the reference output is too long to display in the table, you can click on Expand detailed view to view the full content.\\nYou can also select and hide individual feedback keys or individual metrics in the Display settings dropdown to isolate the information you need in the comparison view.\\n\\u200bView regressions and improvements\\nIn the comparison view, runs that regressed on your specified feedback key against your baseline experiment will be highlighted in red, while runs that improved will be highlighted in green. At the top of each column, you can find how many runs in that experiment did better and how many did worse than your baseline experiment.\\nClick on the regressions or improvements buttons on the top of each column to filter to the runs that regressed or improved in that specific experiment.\\n\\n\\u200bUpdate baseline experiment and metric\\nIn order to track regressions, you need to:\\n\\nIn the Baseline dropdown at the top of the comparison view, select a Baseline experiment against which to compare. By default, the newest experiment is selected as the baseline.\\nSelect a  Feedback key (evaluation metric) you want to focus compare against. One will be assigned by default, but you can adjust as needed.\\nConfigure whether a higher score is better for the selected feedback key. This preference will be stored.\\n\\n\\n\\u200bOpen a trace\\nIf the example youâ€™re evaluating is from an ingested run, you can hover over the output cell and click on the trace icon to open the trace view for that run. This will open up a trace in the side panel.\\n\\n\\u200bExpand detailed view\\nFrom any cell, you can click on the expand icon in the hover state to open up a detailed view of all experiment results on that particular example input, along with feedback keys and scores.\\n\\n\\u200bView summary charts\\nView summary charts by clicking on the Charts tab at the top of the page.\\n\\n\\u200bUse experiment metadata as chart labels\\nYou can configure the x-axis labels for the charts based on experiment metadata.\\nSelect a metadata key in the x-axis dropdown to change the chart labels.\\nWas this page helpful?YesNoSuggest editsAnalyze an experimentFilter experiments in the UIâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators', 'loc': 'https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='How to improve your evaluator with few-shot examples - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesImprove evaluatorsImprove LLM-as-judge evaluators using human feedbackImprove your evaluator with few-shot examplesDynamic few shot example selectionTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationImprove evaluatorsHow to improve your evaluator with few-shot examplesGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageHow few-shot examples workConfigure your evaluator1. Configure variable mapping2. Specify the number of few-shot examples to useMake correctionsView your corrections datasetSet up evaluationsImprove evaluatorsHow to improve your evaluator with few-shot examplesCopy pageCopy pageUsing LLM-as-a-judge evaluators can be very helpful when you canâ€™t evaluate your system programmatically. However, their effectiveness depends on their quality and how well they align with human reviewer feedback. LangSmith provides the ability to improve the alignment of LLM-as-a-judge evaluator to human preferences using few-shot examples.\\nHuman corrections are automatically inserted into your evaluator prompt using few-shot examples. Few-shot examples is a technique inspired by few-shot prompting that guides the models output with a few high-quality examples.\\nThis guide covers how to set up few-shot examples as part of your LLM-as-a-judge evaluator and apply corrections to feedback scores.\\n\\u200bHow few-shot examples work\\n\\nFew-shot examples are added to your evaluator prompt using the {{Few-shot examples}} variable\\nCreating an evaluator with few-shot examples, will automatically create a dataset for you, which will be auto-populated with few-shot examples once you start making corrections\\nAt runtime, these examples will inserted into the evaluator to serve as a guide for its outputs - this will help the evaluator to better align with human preferences\\n\\n\\u200bConfigure your evaluator\\nFew-shot examples are not currently supported in LLM-as-a-judge evaluators that use the prompt hub and are only compatible with prompts that use mustache formatting.\\nBefore enabling few-shot examples, set up your LLM-as-a-judge evaluator. If you havenâ€™t done this yet, follow the steps in the LLM-as-a-judge evaluator guide.\\n\\u200b1. Configure variable mapping\\nEach few-shot example is formatted according to the variable mapping specified in the configuration. The variable mapping for few-shot examples, should contain the same variables as your main prompt, plus a few_shot_explanation and a score variable which should have the same name as your feedback key.\\nFor example, if your main prompt has variables question and response, and your evaluator outputs a correctness score, then your few-shot prompt should have the vartiables question, response, few_shot_explanation, and correctness.\\n\\u200b2. Specify the number of few-shot examples to use\\nYou may also specify the number of few-shot examples to use. The default is 5. If your examples are very long, you may want to set this number lower to save tokens - whereas if your examples tend to be short, you can set a higher number in order to give your evaluator more examples to learn from. If you have more examples in your dataset than this number, we will randomly choose them for you.\\n\\u200bMake corrections\\nAudit evaluator scores\\nAs you start logging traces or running experiments, you will likely disagree with some of the scores that your evaluator has given. When you make corrections to these scores, you will begin seeing examples populated inside your corrections dataset. As you make corrections, make sure to attach explanations - these will get populated into your evaluator prompt in place of the few_shot_explanation variable.\\nThe inputs to the few-shot examples will be the relevant fields from the inputs, outputs, and reference (if this an offline evaluator) of your chain/dataset. The outputs will be the corrected evaluator score and the explanations that you created when you left the corrections. Feel free to edit these to your liking. Here is an example of a few-shot example in a corrections dataset:\\n\\nNote that the corrections may take a minute or two to be populated into your few-shot dataset. Once they are there, future runs of your evaluator will include them in the prompt!\\n\\u200bView your corrections dataset\\nIn order to view your corrections dataset:\\n\\nOnline evaluators: Select your run rule and click Edit Rule\\nOffline evaluators: Select your evaluator and click Edit Evaluator\\n\\n\\nHead to your dataset of corrections linked in the the Improve evaluator accuracy using few-shot examples section. You can view and update your few-shot examples in the dataset.\\nWas this page helpful?YesNoSuggest editsImprove LLM-as-judge evaluators using human feedbackDynamic few shot example selectionâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator', 'loc': 'https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='How to define a code evaluator - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesCode evaluatorLLM-as-a-judge evaluatorComposite evaluatorsSummary evaluatorPairwise evaluationFrameworks & integrationsEvaluation techniquesImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationEvaluation typesHow to define a code evaluatorGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageBasic exampleEvaluator argsEvaluator outputAdditional examplesRelatedSet up evaluationsEvaluation typesHow to define a code evaluatorCopy pageCopy page\\nEvaluators\\n\\nCode evaluators are just functions that take a dataset example and the resulting application output, and return one or more metrics. These functions can be passed directly into evaluate() / aevaluate().\\n\\u200bBasic example\\nPythonTypeScriptCopyfrom langsmith import evaluate\\n\\ndef correct(outputs: dict, reference_outputs: dict) -> bool:\\n    \"\"\"Check if the answer exactly matches the expected answer.\"\"\"\\n    return outputs[\"answer\"] == reference_outputs[\"answer\"]\\n\\ndef dummy_app(inputs: dict) -> dict:\\n    return {\"answer\": \"hmm i\\'m not sure\", \"reasoning\": \"i didn\\'t understand the question\"}\\n\\nresults = evaluate(\\n    dummy_app,\\n    data=\"dataset_name\",\\n    evaluators=[correct]\\n)\\n\\n\\u200bEvaluator args\\ncode evaluator functions must have specific argument names. They can take any subset of the following arguments:\\n\\nrun: Run: The full Run object generated by the application on the given example.\\nexample: Example: The full dataset Example, including the example inputs, outputs (if available), and metdata (if available).\\ninputs: dict: A dictionary of the inputs corresponding to a single example in a dataset.\\noutputs: dict: A dictionary of the outputs generated by the application on the given inputs.\\nreference_outputs/referenceOutputs: dict: A dictionary of the reference outputs associated with the example, if available.\\n\\nFor most use cases youâ€™ll only need inputs, outputs, and reference_outputs. run and example are useful only if you need some extra trace or example metadata outside of the actual inputs and outputs of the application.\\nWhen using JS/TS these should all be passed in as part of a single object argument.\\n\\u200bEvaluator output\\nCode evaluators are expected to return one of the following types:\\nPython and JS/TS\\n\\ndict: dicts of the form {\"score\" | \"value\": ..., \"key\": ...} allow you to customize the metric type (â€œscoreâ€ for numerical and â€œvalueâ€ for categorical) and metric name. This if useful if, for example, you want to log an integer as a categorical metric.\\n\\nPython only\\n\\nint | float | bool: this is interepreted as an continuous metric that can be averaged, sorted, etc. The function name is used as the name of the metric.\\nstr: this is intepreted as a categorical metric. The function name is used as the name of the metric.\\nlist[dict]: return multiple metrics using a single function.\\n\\n\\u200bAdditional examples\\nRequires langsmith>=0.2.0\\nPythonTypeScriptCopyfrom langsmith import evaluate, wrappers\\nfrom langsmith.schemas import Run, Example\\nfrom openai import AsyncOpenAI\\n# Assumes you\\'ve installed pydantic.\\nfrom pydantic import BaseModel\\n\\n# We can still pass in Run and Example objects if we\\'d like\\ndef correct_old_signature(run: Run, example: Example) -> dict:\\n    \"\"\"Check if the answer exactly matches the expected answer.\"\"\"\\n    return {\"key\": \"correct\", \"score\": run.outputs[\"answer\"] == example.outputs[\"answer\"]}\\n\\n# Just evaluate actual outputs\\ndef concision(outputs: dict) -> int:\\n    \"\"\"Score how concise the answer is. 1 is the most concise, 5 is the least concise.\"\"\"\\n    return min(len(outputs[\"answer\"]) // 1000, 4) + 1\\n\\n# Use an LLM-as-a-judge\\noai_client = wrappers.wrap_openai(AsyncOpenAI())\\n\\nasync def valid_reasoning(inputs: dict, outputs: dict) -> bool:\\n    \"\"\"Use an LLM to judge if the reasoning and the answer are consistent.\"\"\"\\n    instructions = \"\"\"\\\\\\nGiven the following question, answer, and reasoning, determine if the reasoning for the \\\\\\nanswer is logically valid and consistent with question and the answer.\"\"\"\\n\\n    class Response(BaseModel):\\n        reasoning_is_valid: bool\\n\\n    msg = f\"Question: {inputs[\\'question\\']}\\\\nAnswer: {outputs[\\'answer\\']}\\\\nReasoning: {outputs[\\'reasoning\\']}\"\\n    response = await oai_client.beta.chat.completions.parse(\\n        model=\"gpt-4o-mini\",\\n        messages=[{\"role\": \"system\", \"content\": instructions,}, {\"role\": \"user\", \"content\": msg}],\\n        response_format=Response\\n    )\\n    return response.choices[0].message.parsed.reasoning_is_valid\\n\\ndef dummy_app(inputs: dict) -> dict:\\n    return {\"answer\": \"hmm i\\'m not sure\", \"reasoning\": \"i didn\\'t understand the question\"}\\n\\nresults = evaluate(\\n    dummy_app,\\n    data=\"dataset_name\",\\n    evaluators=[correct_old_signature, concision, valid_reasoning]\\n)\\n\\n\\u200bRelated\\n\\nEvaluate aggregate experiment results: Define summary evaluators, which compute metrics for an entire experiment.\\nRun an evaluation comparing two experiments: Define pairwise evaluators, which compute metrics by comparing two (or more) experiments against each other.\\nWas this page helpful?YesNoSuggest editsUse prebuilt evaluatorsLLM-as-a-judge evaluatorâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset', 'loc': 'https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Manage datasets - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationDatasetsManage datasetsGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageVersion a datasetCreate a new version of a datasetTag a versionEvaluate on a specific dataset versionUse list_examplesEvaluate on a split / filtered view of a datasetEvaluate on a filtered view of a datasetEvaluate on a dataset splitShare a datasetShare a dataset publiclyUnshare a datasetExport a datasetExport filtered traces from experiment to datasetView experiment tracesDatasetsManage datasetsCopy pageCopy pageLangSmith provides tools for managing and working with your datasets. This page describes dataset operations including:\\n\\nVersioning datasets to track changes over time.\\nFiltering and splitting datasets for evaluation.\\nSharing datasets publicly.\\nExporting datasets in various formats.\\n\\nYouâ€™ll also learn how to export filtered traces from experiments back to datasets for further analysis and iteration.\\n\\u200bVersion a dataset\\nIn LangSmith, datasets are versioned. This means that every time you add, update, or delete examples in your dataset, a new version of the dataset is created.\\n\\u200bCreate a new version of a dataset\\nAny time you add, update, or delete examples in your dataset, a new version of your dataset is created. This allows you to track changes to your dataset over time and understand how your dataset has evolved.\\nBy default, the version is defined by the timestamp of the change. When you click on a particular version of a dataset (by timestamp) in the Examples tab, you will find the state of the dataset at that point in time.\\n\\nNote that examples are read-only when viewing a past version of the dataset. You will also see the operations that were between this version of the dataset and the latest version of the dataset.\\nBy default, the latest version of the dataset is shown in the Examples tab and experiments from all versions are shown in the Tests tab.\\nIn the Tests tab, you will find the results of tests run on the dataset at different versions.\\n\\n\\u200bTag a version\\nYou can also tag versions of your dataset to give them a more human-readable name, which can be useful for marking important milestones in your datasetâ€™s history.\\nFor example, you might tag a version of your dataset as â€œprodâ€ and use it to run tests against your LLM pipeline.\\nYou can tag a version of your dataset in the UI by clicking on + Tag this version in the Examples tab.\\n\\nYou can also tag versions of your dataset using the SDK. Hereâ€™s an example of how to tag a version of a dataset using the Python SDK:\\nCopyfrom langsmith import Client\\nfrom datetime import datetime\\n\\nclient = Client()\\ninitial_time = datetime(2024, 1, 1, 0, 0, 0) # The timestamp of the version you want to tag\\n\\n# You can tag a specific dataset version with a semantic name, like \"prod\"\\nclient.update_dataset_tag(\\n    dataset_name=toxic_dataset_name, as_of=initial_time, tag=\"prod\"\\n)\\n\\nTo run an evaluation on a particular tagged version of a dataset, refer to the Evaluate on a specific dataset version section.\\n\\u200bEvaluate on a specific dataset version\\nYou may find it helpful to refer to the following content before you read this section:\\nVersion a dataset.\\nFetching examples.\\n\\n\\u200bUse list_examples\\nYou can use evaluate / aevaluate to pass in an iterable of examples to evaluate on a particular version of a dataset. Use list_examples / listExamples to fetch examples from a particular version tag using as_of / asOf and pass that into the data argument.\\nPythonTypeScriptCopyfrom langsmith import Client\\n\\nls_client = Client()\\n\\n# Assumes actual outputs have a \\'class\\' key.\\n# Assumes example outputs have a \\'label\\' key.\\ndef correct(outputs: dict, reference_outputs: dict) -> bool:\\n  return outputs[\"class\"] == reference_outputs[\"label\"]\\n\\nresults = ls_client.evaluate(\\n    lambda inputs: {\"class\": \"Not toxic\"},\\n    # Pass in filtered data here:\\n    data=ls_client.list_examples(\\n      dataset_name=\"Toxic Queries\",\\n      as_of=\"latest\",  # specify version here\\n    ),\\n    evaluators=[correct],\\n)\\n\\nLearn more about how to fetch views of a dataset on the Create and manage datasets programmatically page.\\n\\u200bEvaluate on a split / filtered view of a dataset\\nYou may find it helpful to refer to the following content before you read this section:\\nFetching examples.\\nCreating and managing dataset splits.\\n\\n\\u200bEvaluate on a filtered view of a dataset\\nYou can use the list_examples / listExamples method to fetch a subset of examples from a dataset to evaluate on.\\nOne common workflow is to fetch examples that have a certain metadata key-value pair.\\nPythonTypeScriptCopyfrom langsmith import evaluate\\n\\nresults = evaluate(\\n    lambda inputs: label_text(inputs[\"text\"]),\\n    data=client.list_examples(dataset_name=dataset_name, metadata={\"desired_key\": \"desired_value\"}),\\n    evaluators=[correct_label],\\n    experiment_prefix=\"Toxic Queries\",\\n)\\n\\nFor more filtering capabilities, refer to this how-to guide.\\n\\u200bEvaluate on a dataset split\\nYou can use the list_examples / listExamples method to evaluate on one or multiple splits of your dataset. The splits parameter takes a list of the splits you would like to evaluate.\\nPythonTypeScriptCopyfrom langsmith import evaluate\\n\\nresults = evaluate(\\n    lambda inputs: label_text(inputs[\"text\"]),\\n    data=client.list_examples(dataset_name=dataset_name, splits=[\"test\", \"training\"]),\\n    evaluators=[correct_label],\\n    experiment_prefix=\"Toxic Queries\",\\n)\\n\\nFor more details on fetching views of a dataset, refer to the guide on fetching datasets.\\n\\u200bShare a dataset\\n\\u200bShare a dataset publicly\\nSharing a dataset publicly will make the dataset examples, experiments and associated runs, and feedback on this dataset accessible to anyone with the link, even if they donâ€™t have a LangSmith account. Make sure youâ€™re not sharing sensitive information.This feature is only available in the cloud-hosted version of LangSmith.\\nFrom the Dataset & Experiments tab, select a dataset, click â‹® (top right of the page), click Share Dataset. This will open a dialog where you can copy the link to the dataset.\\n\\n\\u200bUnshare a dataset\\n\\n\\nClick on Unshare by clicking on Public in the upper right hand corner of any publicly shared dataset, then Unshare in the dialog. \\n\\n\\nNavigate to your organizationâ€™s list of publicly shared datasets, by clicking on Settings -> Shared URLs or this link, then click on Unshare next to the dataset you want to unshare.\\n\\n\\n\\n\\u200bExport a dataset\\nYou can export your LangSmith dataset to a CSV, JSONL, or OpenAIâ€™s fine tuning format from the LangSmith UI.\\nFrom the Dataset & Experiments tab, select a dataset, click â‹® (top right of the page), click Download Dataset.\\n\\n\\u200bExport filtered traces from experiment to dataset\\nAfter running an offline evaluation in LangSmith, you may want to export traces that met some evaluation criteria to a dataset.\\n\\u200bView experiment traces\\n\\nTo do so, first click on the arrow next to your experiment name. This will direct you to a project that contains the traces generated from your experiment.\\n\\nFrom there, you can filter the traces based on your evaluation criteria. In this example, weâ€™re filtering for all traces that received an accuracy score greater than 0.5.\\n\\nAfter applying the filter on the project, we can multi-select runs to add to the dataset, and click Add to Dataset.\\nWas this page helpful?YesNoSuggest editsWith the SDKWith the SDKâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version', 'loc': 'https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Manage datasets - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationDatasetsManage datasetsGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageVersion a datasetCreate a new version of a datasetTag a versionEvaluate on a specific dataset versionUse list_examplesEvaluate on a split / filtered view of a datasetEvaluate on a filtered view of a datasetEvaluate on a dataset splitShare a datasetShare a dataset publiclyUnshare a datasetExport a datasetExport filtered traces from experiment to datasetView experiment tracesDatasetsManage datasetsCopy pageCopy pageLangSmith provides tools for managing and working with your datasets. This page describes dataset operations including:\\n\\nVersioning datasets to track changes over time.\\nFiltering and splitting datasets for evaluation.\\nSharing datasets publicly.\\nExporting datasets in various formats.\\n\\nYouâ€™ll also learn how to export filtered traces from experiments back to datasets for further analysis and iteration.\\n\\u200bVersion a dataset\\nIn LangSmith, datasets are versioned. This means that every time you add, update, or delete examples in your dataset, a new version of the dataset is created.\\n\\u200bCreate a new version of a dataset\\nAny time you add, update, or delete examples in your dataset, a new version of your dataset is created. This allows you to track changes to your dataset over time and understand how your dataset has evolved.\\nBy default, the version is defined by the timestamp of the change. When you click on a particular version of a dataset (by timestamp) in the Examples tab, you will find the state of the dataset at that point in time.\\n\\nNote that examples are read-only when viewing a past version of the dataset. You will also see the operations that were between this version of the dataset and the latest version of the dataset.\\nBy default, the latest version of the dataset is shown in the Examples tab and experiments from all versions are shown in the Tests tab.\\nIn the Tests tab, you will find the results of tests run on the dataset at different versions.\\n\\n\\u200bTag a version\\nYou can also tag versions of your dataset to give them a more human-readable name, which can be useful for marking important milestones in your datasetâ€™s history.\\nFor example, you might tag a version of your dataset as â€œprodâ€ and use it to run tests against your LLM pipeline.\\nYou can tag a version of your dataset in the UI by clicking on + Tag this version in the Examples tab.\\n\\nYou can also tag versions of your dataset using the SDK. Hereâ€™s an example of how to tag a version of a dataset using the Python SDK:\\nCopyfrom langsmith import Client\\nfrom datetime import datetime\\n\\nclient = Client()\\ninitial_time = datetime(2024, 1, 1, 0, 0, 0) # The timestamp of the version you want to tag\\n\\n# You can tag a specific dataset version with a semantic name, like \"prod\"\\nclient.update_dataset_tag(\\n    dataset_name=toxic_dataset_name, as_of=initial_time, tag=\"prod\"\\n)\\n\\nTo run an evaluation on a particular tagged version of a dataset, refer to the Evaluate on a specific dataset version section.\\n\\u200bEvaluate on a specific dataset version\\nYou may find it helpful to refer to the following content before you read this section:\\nVersion a dataset.\\nFetching examples.\\n\\n\\u200bUse list_examples\\nYou can use evaluate / aevaluate to pass in an iterable of examples to evaluate on a particular version of a dataset. Use list_examples / listExamples to fetch examples from a particular version tag using as_of / asOf and pass that into the data argument.\\nPythonTypeScriptCopyfrom langsmith import Client\\n\\nls_client = Client()\\n\\n# Assumes actual outputs have a \\'class\\' key.\\n# Assumes example outputs have a \\'label\\' key.\\ndef correct(outputs: dict, reference_outputs: dict) -> bool:\\n  return outputs[\"class\"] == reference_outputs[\"label\"]\\n\\nresults = ls_client.evaluate(\\n    lambda inputs: {\"class\": \"Not toxic\"},\\n    # Pass in filtered data here:\\n    data=ls_client.list_examples(\\n      dataset_name=\"Toxic Queries\",\\n      as_of=\"latest\",  # specify version here\\n    ),\\n    evaluators=[correct],\\n)\\n\\nLearn more about how to fetch views of a dataset on the Create and manage datasets programmatically page.\\n\\u200bEvaluate on a split / filtered view of a dataset\\nYou may find it helpful to refer to the following content before you read this section:\\nFetching examples.\\nCreating and managing dataset splits.\\n\\n\\u200bEvaluate on a filtered view of a dataset\\nYou can use the list_examples / listExamples method to fetch a subset of examples from a dataset to evaluate on.\\nOne common workflow is to fetch examples that have a certain metadata key-value pair.\\nPythonTypeScriptCopyfrom langsmith import evaluate\\n\\nresults = evaluate(\\n    lambda inputs: label_text(inputs[\"text\"]),\\n    data=client.list_examples(dataset_name=dataset_name, metadata={\"desired_key\": \"desired_value\"}),\\n    evaluators=[correct_label],\\n    experiment_prefix=\"Toxic Queries\",\\n)\\n\\nFor more filtering capabilities, refer to this how-to guide.\\n\\u200bEvaluate on a dataset split\\nYou can use the list_examples / listExamples method to evaluate on one or multiple splits of your dataset. The splits parameter takes a list of the splits you would like to evaluate.\\nPythonTypeScriptCopyfrom langsmith import evaluate\\n\\nresults = evaluate(\\n    lambda inputs: label_text(inputs[\"text\"]),\\n    data=client.list_examples(dataset_name=dataset_name, splits=[\"test\", \"training\"]),\\n    evaluators=[correct_label],\\n    experiment_prefix=\"Toxic Queries\",\\n)\\n\\nFor more details on fetching views of a dataset, refer to the guide on fetching datasets.\\n\\u200bShare a dataset\\n\\u200bShare a dataset publicly\\nSharing a dataset publicly will make the dataset examples, experiments and associated runs, and feedback on this dataset accessible to anyone with the link, even if they donâ€™t have a LangSmith account. Make sure youâ€™re not sharing sensitive information.This feature is only available in the cloud-hosted version of LangSmith.\\nFrom the Dataset & Experiments tab, select a dataset, click â‹® (top right of the page), click Share Dataset. This will open a dialog where you can copy the link to the dataset.\\n\\n\\u200bUnshare a dataset\\n\\n\\nClick on Unshare by clicking on Public in the upper right hand corner of any publicly shared dataset, then Unshare in the dialog. \\n\\n\\nNavigate to your organizationâ€™s list of publicly shared datasets, by clicking on Settings -> Shared URLs or this link, then click on Unshare next to the dataset you want to unshare.\\n\\n\\n\\n\\u200bExport a dataset\\nYou can export your LangSmith dataset to a CSV, JSONL, or OpenAIâ€™s fine tuning format from the LangSmith UI.\\nFrom the Dataset & Experiments tab, select a dataset, click â‹® (top right of the page), click Download Dataset.\\n\\n\\u200bExport filtered traces from experiment to dataset\\nAfter running an offline evaluation in LangSmith, you may want to export traces that met some evaluation criteria to a dataset.\\n\\u200bView experiment traces\\n\\nTo do so, first click on the arrow next to your experiment name. This will direct you to a project that contains the traces generated from your experiment.\\n\\nFrom there, you can filter the traces based on your evaluation criteria. In this example, weâ€™re filtering for all traces that received an accuracy score greater than 0.5.\\n\\nAfter applying the filter on the project, we can multi-select runs to add to the dataset, and click Add to Dataset.\\nWas this page helpful?YesNoSuggest editsWith the SDKWith the SDKâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides/define_target', 'loc': 'https://docs.smith.langchain.com/evaluation/how_to_guides/define_target', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='How to define a target function to evaluate - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesDefine a target function to evaluateEvaluate intermediate stepsReturn multiple scores in one evaluatorReturn categorical vs numerical metricsRun evaluators on experimentsEvaluate with repetitionsHandle model rate limitsRun an evaluation locally (Python)Evaluate a runnableEvaluate a graphEvaluate an existing experiment (Python)Run an evaluation with multimodal contentSimulate multi-turn interactionsImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationEvaluation techniquesHow to define a target function to evaluateGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageTarget function signatureExample: Single LLM callExample: Non-LLM componentExample: Application or agentSet up evaluationsEvaluation techniquesHow to define a target function to evaluateCopy pageCopy pageThere are three main pieces need to run an evaluation:\\n\\nA dataset of test inputs and expected outputs.\\nA target function which is what youâ€™re evaluating.\\nEvaluators that score your target functionâ€™s outputs.\\n\\nThis guide shows you how to define the target function depending on the part of your application you are evaluating. See here for how to create a dataset and how to define evaluators, and here for an end-to-end example of running an evaluation.\\n\\u200bTarget function signature\\nIn order to evaluate an application in code, we need a way to run the application. When using evaluate() (Python/TypeScript)weâ€™ll do this by passing in a target function argument. This is a function that takes in a dataset Exampleâ€™s inputs and returns the application output as a dict. Within this function we can call our application however weâ€™d like. We can also format the output however weâ€™d like. The key is that any evaluator functions we define should work with the output format we return in our target function.\\nCopyfrom langsmith import Client\\n\\n# \\'inputs\\' will come from your dataset.\\ndef dummy_target(inputs: dict) -> dict:\\n    return {\"foo\": 1, \"bar\": \"two\"}\\n\\n# \\'inputs\\' will come from your dataset.\\n# \\'outputs\\' will come from your target function.\\ndef evaluator_one(inputs: dict, outputs: dict) -> bool:\\n    return outputs[\"foo\"] == 2\\n\\ndef evaluator_two(inputs: dict, outputs: dict) -> bool:\\n    return len(outputs[\"bar\"]) < 3\\n\\nclient = Client()\\nresults = client.evaluate(\\n    dummy_target,  # <-- target function\\n    data=\"your-dataset-name\",\\n    evaluators=[evaluator_one, evaluator_two],\\n    ...\\n)\\n\\nevaluate() will automatically trace your target function. This means that if you run any traceable code within your target function, this will also be traced as child runs of the target trace.\\n\\u200bExample: Single LLM call\\nPythonTypeScriptPython (LangChain)TypeScript (LangChain)Copyfrom langsmith import wrappers\\nfrom openai import OpenAI\\n\\n# Optionally wrap the OpenAI client to automatically\\n# trace all model calls.\\noai_client = wrappers.wrap_openai(OpenAI())\\n\\ndef target(inputs: dict) -> dict:\\n  # This assumes your dataset has inputs with a \\'messages\\' key.\\n  # You can update to match your dataset schema.\\n  messages = inputs[\"messages\"]\\n  response = oai_client.chat.completions.create(\\n      messages=messages,\\n      model=\"gpt-4o-mini\",\\n  )\\n  return {\"answer\": response.choices[0].message.content}\\n\\n\\u200bExample: Non-LLM component\\nPythonTypeScriptCopyfrom langsmith import traceable\\n\\n# Optionally decorate with \\'@traceable\\' to trace all invocations of this function.\\n@traceable\\ndef calculator_tool(operation: str, number1: float, number2: float) -> str:\\n  if operation == \"add\":\\n      return str(number1 + number2)\\n  elif operation == \"subtract\":\\n      return str(number1 - number2)\\n  elif operation == \"multiply\":\\n      return str(number1 * number2)\\n  elif operation == \"divide\":\\n      return str(number1 / number2)\\n  else:\\n      raise ValueError(f\"Unrecognized operation: {operation}.\")\\n\\n# This is the function you will evaluate.\\ndef target(inputs: dict) -> dict:\\n  # This assumes your dataset has inputs with `operation`, `num1`, and `num2` keys.\\n  operation = inputs[\"operation\"]\\n  number1 = inputs[\"num1\"]\\n  number2 = inputs[\"num2\"]\\n  result = calculator_tool(operation, number1, number2)\\n  return {\"result\": result}\\n\\n\\u200bExample: Application or agent\\nPythonTypeScriptCopyfrom my_agent import agent\\n\\n      # This is the function you will evaluate.\\ndef target(inputs: dict) -> dict:\\n  # This assumes your dataset has inputs with a `messages` key\\n  messages = inputs[\"messages\"]\\n  # Replace `invoke` with whatever you use to call your agent\\n  response = agent.invoke({\"messages\": messages})\\n  # This assumes your agent output is in the right format\\n  return response\\n\\nIf you have a LangGraph/LangChain agent that accepts the inputs defined in your dataset and that returns the output format you want to use in your evaluators, you can pass that object in as the target directly:Copyfrom my_agent import agent\\nfrom langsmith import Client\\nclient = Client()\\nclient.evaluate(agent, ...)\\nWas this page helpful?YesNoSuggest editsWith the APIEvaluate intermediate stepsâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv', 'loc': 'https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Analyze an experiment - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationAnalyze experiment resultsAnalyze an experimentGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageAnalyze a single experimentOpen the experiment viewView experiment resultsCustomize columnsSort and filterTable viewsView the tracesView evaluator runsGroup results by metadataRepetitionsCompare to another experimentDownload experiment results as a CSVRename an experimentAnalyze experiment resultsAnalyze an experimentCopy pageCopy pageThis page describes some of the essential tasks for working with experiments in LangSmith:\\n\\nAnalyze a single experiment: View and interpret experiment results, customize columns, filter data, and compare runs.\\nDownload experiment results as a CSV: Export your experiment data for external analysis and sharing.\\nRename an experiment: Update experiment names in both the Playground and Experiments view.\\n\\n\\u200bAnalyze a single experiment\\nAfter running an experiment, you can use LangSmithâ€™s experiment view to analyze the results and draw insights about your experimentâ€™s performance.\\n\\u200bOpen the experiment view\\nTo open the experiment view, select the relevant dataset from the Dataset & Experiments page and then select the experiment you want to view.\\n\\n\\u200bView experiment results\\n\\u200bCustomize columns\\nBy default, the experiment view shows the input, output, and reference output for each example in the dataset, feedback scores from evaluations and experiment metrics like cost, token counts, latency and status.\\nYou can customize the columns using the Display button to make it easier to interpret experiment results:\\n\\nBreak out fields from inputs, outputs, and reference outputs into their own columns. This is especially helpful if you have long inputs/outputs/reference outputs and want to surface important fields.\\nHide and reorder columns to create focused views for analysis.\\nControl decimal precision on feedback scores. By default, LangSmith surfaces numerical feedback scores with a decimal precision of 2, but you can customize this setting to be up to 6 decimals.\\nSet the Heat Map threshold to high, middle, and low for numeric feedback scores in your experiment, which affects the threshold at which score chips render as red or green:\\n\\n\\nYou can set default configurations for an entire dataset or temporarily save settings just for yourself.\\n\\u200bSort and filter\\nTo sort or filter feedback scores, you can use the actions in the column headers.\\n\\n\\u200bTable views\\nDepending on the view most useful for your analysis, you can change the formatting of the table by toggling between a compact view, a full, view, and a diff view.\\n\\nThe Compact view shows each run as a one-line row, for ease of comparing scores at a glance.\\nThe Full view shows the full output for each run for digging into the details of individual runs.\\nThe Diff view shows the text difference between the reference output and the output for each run.\\n\\n\\n\\u200bView the traces\\nHover over any of the output cells, and click on the trace icon to view the trace for that run. This will open up a trace in the side panel.\\nTo view the entire tracing project, click on the View Project button in the top right of the header.\\n\\n\\u200bView evaluator runs\\nFor evaluator scores, you can view the source run by hovering over the evaluator score cell and clicking on the arrow icon. This will open up a trace in the side panel. If youâ€™re running a LLM-as-a-judge evaluator, you can view the prompt used for the evaluator in this run. If your experiment has repetitions, you can click on the aggregate average score to find links to all of the individual runs.\\n\\n\\u200bGroup results by metadata\\nYou can add metadata to examples to categorize and organize them. For example, if youâ€™re evaluating factual accuracy on a question answering dataset, the metadata might include which subject area each question belongs to. Metadata can be added either via the UI or via the SDK.\\nTo analyze results by metadata, use the Group by dropdown in the top right corner of the experiment view and select your desired metadata key. This displays average feedback scores, latency, total tokens, and cost for each metadata group.\\nYou will only be able to group by example metadata on experiments created after February 20th, 2025. Any experiments before that date can still be grouped by metadata, but only if the metadata is on the experiment traces themselves.\\n\\u200bRepetitions\\nIf youâ€™ve run your experiment with repetitions, there will be arrows in the output results column so you can view outputs in the table. To view each run from the repetition, hover over the output cell and click the expanded view.\\nWhen you run an experiment with repetitions, LangSmith displays the average for each feedback score in the table. Click on the feedback score to view the feedback scores from individual runs, or to view the standard deviation across repetitions.\\n\\n\\u200bCompare to another experiment\\nIn the top right of the experiment view, you can select another experiment to compare to. This will open up a comparison view, where you can see how the two experiments compare. To learn more about the comparison view, see how to compare experiment results.\\n\\u200bDownload experiment results as a CSV\\nLangSmith lets you download experiment results as a CSV file, which allows you to analyze and share your results.\\nTo download as a CSV, click the download icon at the top of the experiment view. The icon is directly to the left of the Compact toggle.\\n\\n\\u200bRename an experiment\\nExperiment names must be unique per workspace.\\nYou can rename an experiment in the LangSmith UI in:\\n\\n\\nThe Playground. When running experiments in the Playground, a default name with the format pg::prompt-name::model::uuid (eg. pg::gpt-4o-mini::897ee630) is automatically assigned.\\nYou can rename an experiment immediately after running it by editing its name in the Playground table header.\\n\\n\\n\\nThe Experiments view. When viewing results in the experiments view, you can rename an experiment by using the pencil icon beside the experiment name.\\n\\n\\nWas this page helpful?YesNoSuggest editsRun backtests on a new version of an agentCompare experiment resultsâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment', 'loc': 'https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='How to evaluate an existing experiment (Python only) - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesDefine a target function to evaluateEvaluate intermediate stepsReturn multiple scores in one evaluatorReturn categorical vs numerical metricsRun evaluators on experimentsEvaluate with repetitionsHandle model rate limitsRun an evaluation locally (Python)Evaluate a runnableEvaluate a graphEvaluate an existing experiment (Python)Run an evaluation with multimodal contentSimulate multi-turn interactionsImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationEvaluation techniquesHow to evaluate an existing experiment (Python only)Get startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumSet up evaluationsEvaluation techniquesHow to evaluate an existing experiment (Python only)Copy pageCopy pageEvaluation of existing experiments is currently only supported in the Python SDK.\\nIf you have already run an experiment and want to add additional evaluation metrics, you can apply any evaluators to the experiment using the evaluate() / aevaluate() methods as before. Just pass in the experiment name / ID instead of a target function:\\nCopyfrom langsmith import evaluate\\n\\ndef always_half(inputs: dict, outputs: dict) -> float:\\n    return 0.5\\n\\nexperiment_name = \"my-experiment:abc\"  # Replace with an actual experiment name or ID\\n\\nevaluate(experiment_name, evaluators=[always_half])\\nWas this page helpful?YesNoSuggest editsEvaluate a graphRun an evaluation with multimodal contentâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application', 'loc': 'https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='How to evaluate an LLM application - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationWith the SDKWith the UIUse prebuilt evaluatorsEvaluation typesFrameworks & integrationsEvaluation techniquesImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationRun an evaluationHow to evaluate an LLM applicationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageDefine an applicationCreate or select a datasetDefine an evaluatorRun the evaluationExplore the results\\u200bReference code\\u200bRelated\\u200bSet up evaluationsRun an evaluationHow to evaluate an LLM applicationCopy pageCopy pageThis guide shows you how to run an evaluation on an LLM application using the LangSmith SDK.\\nEvaluations | Evaluators | Datasets\\nIn this guide weâ€™ll go over how to evaluate an application using the evaluate() method in the LangSmith SDK.\\nFor larger evaluation jobs in Python we recommend using aevaluate(), the asynchronous version of evaluate(). It is still worthwhile to read this guide first, as the two have identical interfaces, before reading the how-to guide on running an evaluation asynchronously.In JS/TS evaluate() is already asynchronous so no separate method is needed.It is also important to configure the max_concurrency/maxConcurrency arg when running large jobs. This parallelizes evaluation by effectively splitting the dataset across threads.\\n\\u200bDefine an application\\nFirst we need an application to evaluate. Letâ€™s create a simple toxicity classifier for this example.\\nPythonTypeScriptCopyfrom langsmith import traceable, wrappers\\nfrom openai import OpenAI\\n\\n# Optionally wrap the OpenAI client to trace all model calls.\\noai_client = wrappers.wrap_openai(OpenAI())\\n\\n# Optionally add the \\'traceable\\' decorator to trace the inputs/outputs of this function.\\n@traceable\\ndef toxicity_classifier(inputs: dict) -> dict:\\n    instructions = (\\n      \"Please review the user query below and determine if it contains any form of toxic behavior, \"\\n      \"such as insults, threats, or highly negative comments. Respond with \\'Toxic\\' if it does \"\\n      \"and \\'Not toxic\\' if it doesn\\'t.\"\\n    )\\n    messages = [\\n        {\"role\": \"system\", \"content\": instructions},\\n        {\"role\": \"user\", \"content\": inputs[\"text\"]},\\n    ]\\n    result = oai_client.chat.completions.create(\\n        messages=messages, model=\"gpt-4o-mini\", temperature=0\\n    )\\n    return {\"class\": result.choices[0].message.content}\\n\\nWeâ€™ve optionally enabled tracing to capture the inputs and outputs of each step in the pipeline. To understand how to annotate your code for tracing, please refer to this guide.\\n\\u200bCreate or select a dataset\\nWe need a Dataset to evaluate our application on. Our dataset will contain labeled examples of toxic and non-toxic text.\\nRequires langsmith>=0.3.13\\nPythonTypeScriptCopyfrom langsmith import Client\\nls_client = Client()\\n\\nexamples = [\\n  {\\n    \"inputs\": {\"text\": \"Shut up, idiot\"},\\n    \"outputs\": {\"label\": \"Toxic\"},\\n  },\\n  {\\n    \"inputs\": {\"text\": \"You\\'re a wonderful person\"},\\n    \"outputs\": {\"label\": \"Not toxic\"},\\n  },\\n  {\\n    \"inputs\": {\"text\": \"This is the worst thing ever\"},\\n    \"outputs\": {\"label\": \"Toxic\"},\\n  },\\n  {\\n    \"inputs\": {\"text\": \"I had a great day today\"},\\n    \"outputs\": {\"label\": \"Not toxic\"},\\n  },\\n  {\\n    \"inputs\": {\"text\": \"Nobody likes you\"},\\n    \"outputs\": {\"label\": \"Toxic\"},\\n  },\\n  {\\n    \"inputs\": {\"text\": \"This is unacceptable. I want to speak to the manager.\"},\\n    \"outputs\": {\"label\": \"Not toxic\"},\\n  },\\n]\\n\\ndataset = ls_client.create_dataset(dataset_name=\"Toxic Queries\")\\nls_client.create_examples(\\n  dataset_id=dataset.id,\\n  examples=examples,\\n)\\n\\nFor more details on datasets, refer to the Manage datasets page.\\n\\u200bDefine an evaluator\\nYou can also check out LangChainâ€™s open source evaluation package openevals for common pre-built evaluators.\\nEvaluators are functions for scoring your applicationâ€™s outputs. They take in the example inputs, actual outputs, and, when present, the reference outputs. Since we have labels for this task, our evaluator can directly check if the actual outputs match the reference outputs.\\n\\nPython: Requires langsmith>=0.3.13\\nTypeScript: Requires langsmith>=0.2.9\\n\\nPythonTypeScriptCopydef correct(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\\n    return outputs[\"class\"] == reference_outputs[\"label\"]\\n\\n\\u200bRun the evaluation\\nWeâ€™ll use the evaluate() / aevaluate() methods to run the evaluation.\\nThe key arguments are:\\n\\na target function that takes an input dictionary and returns an output dictionary. The example.inputs field of each Example is what gets passed to the target function. In this case our toxicity_classifier is already set up to take in example inputs so we can use it directly.\\ndata - the name OR UUID of the LangSmith dataset to evaluate on, or an iterator of examples\\nevaluators - a list of evaluators to score the outputs of the function\\n\\nPython: Requires langsmith>=0.3.13\\nPythonTypeScriptCopy# Can equivalently use the \\'evaluate\\' function directly:\\n# from langsmith import evaluate; evaluate(...)\\nresults = ls_client.evaluate(\\n    toxicity_classifier,\\n    data=dataset.name,\\n    evaluators=[correct],\\n    experiment_prefix=\"gpt-4o-mini, baseline\",  # optional, experiment name prefix\\n    description=\"Testing the baseline system.\",  # optional, experiment description\\n    max_concurrency=4, # optional, add concurrency\\n)\\n\\n\\u200bExplore the results\\u200b\\nEach invocation of evaluate() creates an Experiment which can be viewed in the LangSmith UI or queried via the SDK. Evaluation scores are stored against each actual output as feedback.\\nIf youâ€™ve annotated your code for tracing, you can open the trace of each row in a side panel view.\\n\\n\\u200bReference code\\u200b\\nClick to see a consolidated code snippetPythonTypeScriptCopyfrom langsmith import Client, traceable, wrappers\\nfrom openai import OpenAI\\n\\n# Step 1. Define an application\\noai_client = wrappers.wrap_openai(OpenAI())\\n\\n@traceable\\ndef toxicity_classifier(inputs: dict) -> str:\\n    system = (\\n      \"Please review the user query below and determine if it contains any form of toxic behavior, \"\\n      \"such as insults, threats, or highly negative comments. Respond with \\'Toxic\\' if it does \"\\n      \"and \\'Not toxic\\' if it doesn\\'t.\"\\n    )\\n    messages = [\\n        {\"role\": \"system\", \"content\": system},\\n        {\"role\": \"user\", \"content\": inputs[\"text\"]},\\n    ]\\n    result = oai_client.chat.completions.create(\\n        messages=messages, model=\"gpt-4o-mini\", temperature=0\\n    )\\n    return result.choices[0].message.content\\n\\n# Step 2. Create a dataset\\nls_client = Client()\\ndataset = ls_client.create_dataset(dataset_name=\"Toxic Queries\")\\nexamples = [\\n  {\\n    \"inputs\": {\"text\": \"Shut up, idiot\"},\\n    \"outputs\": {\"label\": \"Toxic\"},\\n  },\\n  {\\n    \"inputs\": {\"text\": \"You\\'re a wonderful person\"},\\n    \"outputs\": {\"label\": \"Not toxic\"},\\n  },\\n  {\\n    \"inputs\": {\"text\": \"This is the worst thing ever\"},\\n    \"outputs\": {\"label\": \"Toxic\"},\\n  },\\n  {\\n    \"inputs\": {\"text\": \"I had a great day today\"},\\n    \"outputs\": {\"label\": \"Not toxic\"},\\n  },\\n  {\\n    \"inputs\": {\"text\": \"Nobody likes you\"},\\n    \"outputs\": {\"label\": \"Toxic\"},\\n  },\\n  {\\n    \"inputs\": {\"text\": \"This is unacceptable. I want to speak to the manager.\"},\\n    \"outputs\": {\"label\": \"Not toxic\"},\\n  },\\n]\\nls_client.create_examples(\\n  dataset_id=dataset.id,\\n  examples=examples,\\n)\\n\\n# Step 3. Define an evaluator\\ndef correct(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\\n    return outputs[\"output\"] == reference_outputs[\"label\"]\\n\\n# Step 4. Run the evaluation\\n# Client.evaluate() and evaluate() behave the same.\\nresults = ls_client.evaluate(\\n    toxicity_classifier,\\n    data=dataset.name,\\n    evaluators=[correct],\\n    experiment_prefix=\"gpt-4o-mini, simple\",  # optional, experiment name prefix\\n    description=\"Testing the baseline system.\",  # optional, experiment description\\n    max_concurrency=4,  # optional, add concurrency\\n)\\n\\n\\u200bRelated\\u200b\\n\\nRun an evaluation asynchronously\\nRun an evaluation via the REST API\\nRun an evaluation from the prompt playground\\nWas this page helpful?YesNoSuggest editsManage datasetsWith the UIâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps', 'loc': 'https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='How to evaluate an application\\'s intermediate steps - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesDefine a target function to evaluateEvaluate intermediate stepsReturn multiple scores in one evaluatorReturn categorical vs numerical metricsRun evaluators on experimentsEvaluate with repetitionsHandle model rate limitsRun an evaluation locally (Python)Evaluate a runnableEvaluate a graphEvaluate an existing experiment (Python)Run an evaluation with multimodal contentSimulate multi-turn interactionsImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationEvaluation techniquesHow to evaluate an application\\'s intermediate stepsGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this page1. Define your LLM pipeline2. Create a dataset and examples to evaluate the pipeline3. Define your custom evaluators4. Evaluate the pipelineRelatedSet up evaluationsEvaluation techniquesHow to evaluate an application\\'s intermediate stepsCopy pageCopy pageWhile, in many scenarios, it is sufficient to evaluate the final output of your task, in some cases you might want to evaluate the intermediate steps of your pipeline.\\nFor example, for retrieval-augmented generation (RAG), you might want to\\n\\nEvaluate the retrieval step to ensure that the correct documents are retrieved w.r.t the input query.\\nEvaluate the generation step to ensure that the correct answer is generated w.r.t the retrieved documents.\\n\\nIn this guide, we will use a simple, fully-custom evaluator for evaluating criteria 1 and an LLM-based evaluator for evaluating criteria 2 to highlight both scenarios.\\nIn order to evaluate the intermediate steps of your pipeline, your evaluator function should traverse and process the run/rootRun argument, which is a Run object that contains the intermediate steps of your pipeline.\\n\\u200b1. Define your LLM pipeline\\nThe below RAG pipeline consists of 1) generating a Wikipedia query given the input question, 2) retrieving relevant documents from Wikipedia, and 3) generating an answer given the retrieved documents.\\nPythonTypeScriptCopypip install -U langsmith langchain[openai] wikipedia\\n\\nRequires langsmith>=0.3.13\\nPythonTypeScriptCopyimport wikipedia as wp\\nfrom openai import OpenAI\\nfrom langsmith import traceable, wrappers\\n\\noai_client = wrappers.wrap_openai(OpenAI())\\n\\n@traceable\\ndef generate_wiki_search(question: str) -> str:\\n    \"\"\"Generate the query to search in wikipedia.\"\"\"\\n    instructions = (\\n        \"Generate a search query to pass into wikipedia to answer the user\\'s question. \"\\n        \"Return only the search query and nothing more. \"\\n        \"This will passed in directly to the wikipedia search engine.\"\\n    )\\n    messages = [\\n        {\"role\": \"system\", \"content\": instructions},\\n        {\"role\": \"user\", \"content\": question}\\n    ]\\n    result = oai_client.chat.completions.create(\\n        messages=messages,\\n        model=\"gpt-4o-mini\",\\n        temperature=0,\\n    )\\n    return result.choices[0].message.content\\n\\n@traceable(run_type=\"retriever\")\\ndef retrieve(query: str) -> list:\\n    \"\"\"Get up to two search wikipedia results.\"\"\"\\n    results = []\\n    for term in wp.search(query, results = 10):\\n        try:\\n            page = wp.page(term, auto_suggest=False)\\n            results.append({\\n                \"page_content\": page.summary,\\n                \"type\": \"Document\",\\n                \"metadata\": {\"url\": page.url}\\n            })\\n        except wp.DisambiguationError:\\n            pass\\n        if len(results) >= 2:\\n            return results\\n\\n@traceable\\ndef generate_answer(question: str, context: str) -> str:\\n    \"\"\"Answer the question based on the retrieved information.\"\"\"\\n    instructions = f\"Answer the user\\'s question based ONLY on the content below:\\\\n\\\\n{context}\"\\n    messages = [\\n        {\"role\": \"system\", \"content\": instructions},\\n        {\"role\": \"user\", \"content\": question}\\n    ]\\n    result = oai_client.chat.completions.create(\\n        messages=messages,\\n        model=\"gpt-4o-mini\",\\n        temperature=0\\n    )\\n    return result.choices[0].message.content\\n\\n@traceable\\ndef qa_pipeline(question: str) -> str:\\n    \"\"\"The full pipeline.\"\"\"\\n    query = generate_wiki_search(question)\\n    context = \"\\\\n\\\\n\".join([doc[\"page_content\"] for doc in retrieve(query)])\\n    return generate_answer(question, context)\\n\\nThis pipeline will produce a trace that looks something like: \\n\\u200b2. Create a dataset and examples to evaluate the pipeline\\nWe are building a very simple dataset with a couple of examples to evaluate the pipeline.\\nRequires langsmith>=0.3.13\\nPythonTypeScriptCopyfrom langsmith import Client\\n\\nls_client = Client()\\ndataset_name = \"Wikipedia RAG\"\\n\\nif not ls_client.has_dataset(dataset_name=dataset_name):\\n    dataset = ls_client.create_dataset(dataset_name=dataset_name)\\n    examples = [\\n      {\"inputs\": {\"question\": \"What is LangChain?\"}},\\n      {\"inputs\": {\"question\": \"What is LangSmith?\"}},\\n    ]\\n    ls_client.create_examples(\\n      dataset_id=dataset.id,\\n      examples=examples,\\n    )\\n\\n\\u200b3. Define your custom evaluators\\nAs mentioned above, we will define two evaluators: one that evaluates the relevance of the retrieved documents w.r.t the input query and another that evaluates the hallucination of the generated answer w.r.t the retrieved documents. We will be using LangChain LLM wrappers, along with with_structured_output to define the evaluator for hallucination.\\nThe key here is that the evaluator function should traverse the run / rootRun argument to access the intermediate steps of the pipeline. The evaluator can then process the inputs and outputs of the intermediate steps to evaluate according to the desired criteria.\\nExample uses langchain for convenience, this is not required.\\nPythonTypeScriptCopyfrom langchain.chat_models import init_chat_model\\nfrom langsmith.schemas import Run\\nfrom pydantic import BaseModel, Field\\n\\ndef document_relevance(run: Run) -> bool:\\n    \"\"\"Checks if retriever input exists in the retrieved docs.\"\"\"\\n    qa_pipeline_run = next(\\n        r for run in run.child_runs if r.name == \"qa_pipeline\"\\n    )\\n    retrieve_run = next(\\n        r for run in qa_pipeline_run.child_runs if r.name == \"retrieve\"\\n    )\\n    page_contents = \"\\\\n\\\\n\".join(\\n        doc[\"page_content\"] for doc in retrieve_run.outputs[\"output\"]\\n    )\\n    return retrieve_run.inputs[\"query\"] in page_contents\\n\\n# Data model\\nclass GradeHallucinations(BaseModel):\\n    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\\n    is_grounded: bool = Field(..., description=\"True if the answer is grounded in the facts, False otherwise.\")\\n\\n# LLM with structured outputs for grading hallucinations\\n# For more see: https://python.langchain.com/docs/how_to/structured_output/\\ngrader_llm= init_chat_model(\"gpt-4o-mini\", temperature=0).with_structured_output(\\n    GradeHallucinations,\\n    method=\"json_schema\",\\n    strict=True,\\n)\\n\\ndef no_hallucination(run: Run) -> bool:\\n    \"\"\"Check if the answer is grounded in the documents.\\n    Return True if there is no hallucination, False otherwise.\\n    \"\"\"\\n    # Get documents and answer\\n    qa_pipeline_run = next(\\n        r for r in run.child_runs if r.name == \"qa_pipeline\"\\n    )\\n    retrieve_run = next(\\n        r for r in qa_pipeline_run.child_runs if r.name == \"retrieve\"\\n    )\\n    retrieved_content = \"\\\\n\\\\n\".join(\\n        doc[\"page_content\"] for doc in retrieve_run.outputs[\"output\"]\\n    )\\n\\n    # Construct prompt\\n    instructions = (\\n        \"You are a grader assessing whether an LLM generation is grounded in / \"\\n        \"supported by a set of retrieved facts. Give a binary score 1 or 0, \"\\n        \"where 1 means that the answer is grounded in / supported by the set of facts.\"\\n    )\\n    messages = [\\n        {\"role\": \"system\", \"content\": instructions},\\n        {\"role\": \"user\", \"content\": f\"Set of facts:\\\\n{retrieved_content}\\\\n\\\\nLLM generation: {run.outputs[\\'answer\\']}\"},\\n    ]\\n    grade = grader_llm.invoke(messages)\\n    return grade.is_grounded\\n\\n\\u200b4. Evaluate the pipeline\\nFinally, weâ€™ll run evaluate with the custom evaluators defined above.\\nPythonTypeScriptCopydef qa_wrapper(inputs: dict) -> dict:\\n  \"\"\"Wrap the qa_pipeline so it can accept the Example.inputs dict as input.\"\"\"\\n  return {\"answer\": qa_pipeline(inputs[\"question\"])}\\n\\nexperiment_results = ls_client.evaluate(\\n    qa_wrapper,\\n    data=dataset_name,\\n    evaluators=[document_relevance, no_hallucination],\\n    experiment_prefix=\"rag-wiki-oai\"\\n)\\n\\nThe experiment will contain the results of the evaluation, including the scores and comments from the evaluators: \\n\\u200bRelated\\n\\nEvaluate a langgraph graph\\nWas this page helpful?YesNoSuggest editsDefine a target function to evaluateReturn multiple scores in one evaluatorâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise', 'loc': 'https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='How to run a pairwise evaluation - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesCode evaluatorLLM-as-a-judge evaluatorComposite evaluatorsSummary evaluatorPairwise evaluationFrameworks & integrationsEvaluation techniquesImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationEvaluation typesHow to run a pairwise evaluationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pagePrerequisitesevaluate() comparative argsDefine a pairwise evaluatorEvaluator argsEvaluator outputRun a pairwise evaluationView pairwise experimentsSet up evaluationsEvaluation typesHow to run a pairwise evaluationCopy pageCopy pageConcept: Pairwise evaluations\\nLangSmith supports evaluating existing experiments in a comparative manner. Instead of evaluating one output at a time, you can score the output from multiple experiments against each other. In this guide, youâ€™ll use evaluate() with two existing experiments to define an evaluator and run a pairwise evaluation. Finally, youâ€™ll use the LangSmith UI to view the pairwise experiments.\\n\\u200bPrerequisites\\n\\nIf you havenâ€™t already created experiments to compare, check out the quick start or the how-to guide to get started with evaluations.\\nThis guide requires langsmith Python version >=0.2.0 or JS version >=0.2.9.\\n\\nYou can also use evaluate_comparative() with more than two existing experiments.\\n\\u200bevaluate() comparative args\\nAt its simplest, evaluate / aevaluate function takes the following arguments:\\nArgumentDescriptiontargetA list of the two existing experiments you would like to evaluate against each other. These can be uuids or experiment names.evaluatorsA list of the pairwise evaluators that you would like to attach to this evaluation. See the section below for how to define these.\\nAlong with these, you can also pass in the following optional args:\\nArgumentDescriptionrandomize_order / randomizeOrderAn optional boolean indicating whether the order of the outputs should be randomized for each evaluation. This is a strategy for minimizing positional bias in your prompt: often, the LLM will be biased towards one of the responses based on the order. This should mainly be addressed via prompt engineering, but this is another optional mitigation. Defaults to False.experiment_prefix / experimentPrefixA prefix to be attached to the beginning of the pairwise experiment name. Defaults to None.descriptionA description of the pairwise experiment. Defaults to None.max_concurrency / maxConcurrencyThe maximum number of concurrent evaluations to run. Defaults to 5.clientThe LangSmith client to use. Defaults to None.metadataMetadata to attach to your pairwise experiment. Defaults to None.load_nested / loadNestedWhether to load all child runs for the experiment. When False, only the root trace will be passed to your evaluator. Defaults to False.\\n\\u200bDefine a pairwise evaluator\\nPairwise evaluators are just functions with an expected signature.\\n\\u200bEvaluator args\\nCustom evaluator functions must have specific argument names. They can take any subset of the following arguments:\\n\\ninputs: dict: A dictionary of the inputs corresponding to a single example in a dataset.\\noutputs: list[dict]: A two-item list of the dict outputs produced by each experiment on the given inputs.\\nreference_outputs / referenceOutputs: dict: A dictionary of the reference outputs associated with the example, if available.\\nruns: list[Run]: A two-item list of the full Run objects generated by the two experiments on the given example. Use this if you need access to intermediate steps or metadata about each run.\\nexample: Example: The full dataset Example, including the example inputs, outputs (if available), and metadata (if available).\\n\\nFor most use cases youâ€™ll only need inputs, outputs, and reference_outputs / referenceOutputs. runs and example are useful only if you need some extra trace or example metadata outside of the actual inputs and outputs of the application.\\n\\u200bEvaluator output\\nCustom evaluators are expected to return one of the following types:\\nPython and JS/TS\\n\\n\\ndict: dictionary with keys:\\n\\nkey, which represents the feedback key that will be logged\\nscores, which is a mapping from run ID to score for that run.\\ncomment, which is a string. Most commonly used for model reasoning.\\n\\n\\n\\nCurrently Python only\\n\\nlist[int | float | bool]: a two-item list of scores. The list is assumed to have the same order as the runs / outputs evaluator args. The evaluator function name is used for the feedback key.\\n\\nNote that you should choose a feedback key that is distinct from standard feedbacks on your run. We recommend prefixing pairwise feedback keys with pairwise_ or ranked_.\\n\\u200bRun a pairwise evaluation\\nThe following example uses a prompt which asks the LLM to decide which is better between two AI assistant responses. It uses structured output to parse the AIâ€™s response: 0, 1, or 2.\\nIn the Python example below, we are pulling this structured prompt from the LangChain Hub and using it with a LangChain chat model wrapper.Usage of LangChain is totally optional. To illustrate this point, the TypeScript example uses the OpenAI SDK directly.\\n\\nPython: Requires langsmith>=0.2.0\\nTypeScript: Requires langsmith>=0.2.9\\n\\nPythonTypeScriptCopyfrom langchain import hub\\nfrom langchain.chat_models import init_chat_model\\nfrom langsmith import evaluate\\n\\n# See the prompt: https://smith.langchain.com/hub/langchain-ai/pairwise-evaluation-2\\nprompt = hub.pull(\"langchain-ai/pairwise-evaluation-2\")\\nmodel = init_chat_model(\"gpt-4o\")\\nchain = prompt | model\\n\\ndef ranked_preference(inputs: dict, outputs: list[dict]) -> list:\\n    # Assumes example inputs have a \\'question\\' key and experiment\\n    # outputs have an \\'answer\\' key.\\n    response = chain.invoke({\\n        \"question\": inputs[\"question\"],\\n        \"answer_a\": outputs[0].get(\"answer\", \"N/A\"),\\n        \"answer_b\": outputs[1].get(\"answer\", \"N/A\"),\\n    })\\n    if response[\"Preference\"] == 1:\\n        scores = [1, 0]\\n    elif response[\"Preference\"] == 2:\\n        scores = [0, 1]\\n    else:\\n        scores = [0, 0]\\n    return scores\\n\\nevaluate(\\n    (\"experiment-1\", \"experiment-2\"),  # Replace with the names/IDs of your experiments\\n    evaluators=[ranked_preference],\\n    randomize_order=True,\\n    max_concurrency=4,\\n)\\n\\n\\u200bView pairwise experiments\\nNavigate to the â€œPairwise Experimentsâ€ tab from the dataset page:\\n\\nClick on a pairwise experiment that you would like to inspect, and you will be brought to the Comparison View:\\n\\nYou may filter to runs where the first experiment was better or vice versa by clicking the thumbs up/thumbs down buttons in the table header:\\nWas this page helpful?YesNoSuggest editsSummary evaluatorRun an evaluation asynchronouslyâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments', 'loc': 'https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Run an evaluation with multimodal content - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesDefine a target function to evaluateEvaluate intermediate stepsReturn multiple scores in one evaluatorReturn categorical vs numerical metricsRun evaluators on experimentsEvaluate with repetitionsHandle model rate limitsRun an evaluation locally (Python)Evaluate a runnableEvaluate a graphEvaluate an existing experiment (Python)Run an evaluation with multimodal contentSimulate multi-turn interactionsImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationEvaluation techniquesRun an evaluation with multimodal contentGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageSDK1. Create examples with attachmentsPythonTypeScript2. Run evaluationsDefine a target functionPythonTypeSciptDefine custom evaluatorsUpdate examples with attachmentsUI1. Create examples with attachmentsFrom existing runsFrom scratch2. Create a multimodal promptDefine custom evaluatorsUpdate examples with attachmentsSet up evaluationsEvaluation techniquesRun an evaluation with multimodal contentCopy pageCopy pageLangSmith lets you create dataset examples with file attachmentsâ€”like images, audio files, or documentsâ€”so you can reference them when evaluating an application that uses multimodal inputs or outputs.\\nWhile you can include multimodal data in your examples by base64 encoding it, this approach is inefficient - the encoded data takes up more space than the original binary files, resulting in slower transfers to and from LangSmith. Using attachments instead provides two key benefits:\\n\\nFaster upload and download speeds due to more efficient binary file transfers\\nEnhanced visualization of different file types in the LangSmith UI\\n\\n\\u200bSDK\\n\\u200b1. Create examples with attachments\\nTo upload examples with attachments using the SDK, use the create_examples / update_examples Python methods or the uploadExamplesMultipart / updateExamplesMultipart TypeScript methods.\\n\\u200bPython\\nRequires langsmith>=0.3.13\\nCopyimport requests\\nimport uuid\\nfrom pathlib import Path\\nfrom langsmith import Client\\n\\n# Publicly available test files\\npdf_url = \"https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf\"\\nwav_url = \"https://openaiassets.blob.core.windows.net/$web/API/docs/audio/alloy.wav\"\\nimg_url = \"https://www.w3.org/Graphics/PNG/nurbcup2si.png\"\\n\\n# Fetch the files as bytes\\npdf_bytes = requests.get(pdf_url).content\\nwav_bytes = requests.get(wav_url).content\\nimg_bytes = requests.get(img_url).content\\n\\n# Create the dataset\\nls_client = Client()\\ndataset_name = \"attachment-test-dataset\"\\ndataset = ls_client.create_dataset(\\n  dataset_name=dataset_name,\\n  description=\"Test dataset for evals with publicly available attachments\",\\n)\\n\\ninputs = {\\n  \"audio_question\": \"What is in this audio clip?\",\\n  \"image_question\": \"What is in this image?\",\\n}\\n\\noutputs = {\\n  \"audio_answer\": \"The sun rises in the east and sets in the west. This simple fact has been observed by humans for thousands of years.\",\\n  \"image_answer\": \"A mug with a blanket over it.\",\\n}\\n\\n# Define an example with attachments\\nexample_id = uuid.uuid4()\\nexample = {\\n  \"id\": example_id,\\n  \"inputs\": inputs,\\n  \"outputs\": outputs,\\n  \"attachments\": {\\n      \"my_pdf\": {\"mime_type\": \"application/pdf\", \"data\": pdf_bytes},\\n      \"my_wav\": {\"mime_type\": \"audio/wav\", \"data\": wav_bytes},\\n      \"my_img\": {\"mime_type\": \"image/png\", \"data\": img_bytes},\\n      # Example of an attachment specified via a local file path:\\n      # \"my_local_img\": {\"mime_type\": \"image/png\", \"data\": Path(__file__).parent / \"my_local_img.png\"},\\n  },\\n}\\n\\n# Create the example\\nls_client.create_examples(\\n  dataset_id=dataset.id,\\n  examples=[example],\\n  # Uncomment this flag if you\\'d like to upload attachments from local files:\\n  # dangerously_allow_filesystem=True\\n)\\n\\n\\u200bTypeScript\\nRequires version >= 0.2.13\\nYou can use the uploadExamplesMultipart method to upload examples with attachments.\\nNote that this is a different method from the standard createExamples method, which currently does not support attachments. Each attachment requires either a Uint8Array or an ArrayBuffer as the data type.\\n\\nUint8Array: Useful for handling binary data directly.\\nArrayBuffer: Represents fixed-length binary data, which can be converted to Uint8Array as needed.\\n\\nNote that you cannot directly pass in a file path in the TypeScript SDK, as accessing local files is not supported in all runtime environments.\\nCopyimport { Client } from \"langsmith\";\\nimport { v4 as uuid4 } from \"uuid\";\\n\\n// Publicly available test files\\nconst pdfUrl = \"https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf\";\\nconst wavUrl = \"https://openaiassets.blob.core.windows.net/$web/API/docs/audio/alloy.wav\";\\nconst pngUrl = \"https://www.w3.org/Graphics/PNG/nurbcup2si.png\";\\n\\n// Helper function to fetch file as ArrayBuffer\\nasync function fetchArrayBuffer(url: string): Promise<ArrayBuffer> {\\n  const response = await fetch(url);\\n  if (!response.ok) {\\n    throw new Error(`Failed to fetch ${url}: ${response.statusText}`);\\n  }\\n  return response.arrayBuffer();\\n}\\n\\n// Fetch files as ArrayBuffer\\nconst pdfArrayBuffer = await fetchArrayBuffer(pdfUrl);\\nconst wavArrayBuffer = await fetchArrayBuffer(wavUrl);\\nconst pngArrayBuffer = await fetchArrayBuffer(pngUrl);\\n\\n// Create the LangSmith client (Ensure LANGSMITH_API_KEY is set in env)\\nconst langsmithClient = new Client();\\n\\n// Create a unique dataset name\\nconst datasetName = \"attachment-test-dataset:\" + uuid4().substring(0, 8);\\n\\n// Create the dataset\\nconst dataset = await langsmithClient.createDataset(datasetName, {\\n  description: \"Test dataset for evals with publicly available attachments\",\\n});\\n\\n// Define the example with attachments\\nconst exampleId = uuid4();\\nconst example = {\\n  id: exampleId,\\n  inputs: {\\n      audio_question: \"What is in this audio clip?\",\\n      image_question: \"What is in this image?\",\\n  },\\n  outputs: {\\n      audio_answer: \"The sun rises in the east and sets in the west. This simple fact has been observed by humans for thousands of years.\",\\n      image_answer: \"A mug with a blanket over it.\",\\n  },\\n  attachments: {\\n    my_pdf: {\\n      mimeType: \"application/pdf\",\\n      data: pdfArrayBuffer\\n    },\\n    my_wav: {\\n      mimeType: \"audio/wav\",\\n      data: wavArrayBuffer\\n    },\\n    my_img: {\\n      mimeType: \"image/png\",\\n      data: pngArrayBuffer\\n    },\\n  },\\n};\\n\\n// Upload the example with attachments to the dataset\\nawait langsmithClient.uploadExamplesMultipart(dataset.id, [example]);\\n\\nAlong with being passed in as bytes, attachments can be specified as paths to local files. To do so pass in a path for the attachment data value and specify arg dangerously_allow_filesystem=True:Copyclient.create_examples(..., dangerously_allow_filesystem=True)\\n\\n\\u200b2. Run evaluations\\n\\u200bDefine a target function\\nNow that we have a dataset that includes examples with attachments, we can define a target function to run over these examples. The following example simply uses OpenAIâ€™s GPT-4o model to answer questions about an image and an audio clip.\\n\\u200bPython\\nThe target function you are evaluating must have two positional arguments in order to consume the attachments associated with the example, the first must be called inputs and the second must be called attachments.\\n\\nThe inputs argument is a dictionary that contains the input data for the example, excluding the attachments.\\nThe attachments argument is a dictionary that maps the attachment name to a dictionary containing a presigned url, mime_type, and a reader of the bytes content of the file. You can use either the presigned url or the reader to get the file contents. Each value in the attachments dictionary is a dictionary with the following structure:\\n\\nCopy{\\n    \"presigned_url\": str,\\n    \"mime_type\": str,\\n    \"reader\": BinaryIO\\n}\\n\\nCopyfrom langsmith.wrappers import wrap_openai\\nimport base64\\nfrom openai import OpenAI\\n\\nclient = wrap_openai(OpenAI())\\n\\n# Define target function that uses attachments\\ndef file_qa(inputs, attachments):\\n    # Read the audio bytes from the reader and encode them in base64\\n    audio_reader = attachments[\"my_wav\"][\"reader\"]\\n    audio_b64 = base64.b64encode(audio_reader.read()).decode(\\'utf-8\\')\\n\\n    audio_completion = client.chat.completions.create(\\n        model=\"gpt-4o-audio-preview\",\\n        messages=[\\n            {\\n                \"role\": \"user\",\\n                \"content\": [\\n                    {\\n                        \"type\": \"text\",\\n                        \"text\": inputs[\"audio_question\"]\\n                    },\\n                    {\\n                        \"type\": \"input_audio\",\\n                        \"input_audio\": {\\n                            \"data\": audio_b64,\\n                            \"format\": \"wav\"\\n                        }\\n                    }\\n                ]\\n            }\\n        ]\\n    )\\n\\n    # Most models support taking in an image URL directly in addition to base64 encoded images\\n    # You can pipe the image pre-signed URL directly to the model\\n    image_url = attachments[\"my_img\"][\"presigned_url\"]\\n    image_completion = client.chat.completions.create(\\n        model=\"gpt-4o-mini\",\\n        messages=[\\n          {\\n            \"role\": \"user\",\\n            \"content\": [\\n              {\"type\": \"text\", \"text\": inputs[\"image_question\"]},\\n              {\\n                \"type\": \"image_url\",\\n                \"image_url\": {\\n                  \"url\": image_url,\\n                },\\n              },\\n            ],\\n          }\\n        ],\\n    )\\n\\n    return {\\n        \"audio_answer\": audio_completion.choices[0].message.content,\\n        \"image_answer\": image_completion.choices[0].message.content,\\n    }\\n\\n\\u200bTypeScipt\\nIn the TypeScript SDK, the config argument is used to pass in the attachments to the target function if includeAttachments is set to true.\\nThe config will contain attachments which is an object mapping the attachment name to an object of the form:\\nCopy{\\n  presigned_url: string,\\n  mime_type: string,\\n}\\n\\nCopyimport OpenAI from \"openai\";\\nimport { wrapOpenAI } from \"langsmith/wrappers\";\\n\\nconst client: any = wrapOpenAI(new OpenAI());\\n\\nasync function fileQA(inputs: Record<string, any>, config?: Record<string, any>) {\\n  const presignedUrl = config?.attachments?.[\"my_wav\"]?.presigned_url;\\n  if (!presignedUrl) {\\n    throw new Error(\"No presigned URL provided for audio.\");\\n  }\\n\\n  const response = await fetch(presignedUrl);\\n  if (!response.ok) {\\n    throw new Error(`Failed to fetch audio: ${response.statusText}`);\\n  }\\n\\n  const arrayBuffer = await response.arrayBuffer();\\n  const uint8Array = new Uint8Array(arrayBuffer);\\n  const audioB64 = Buffer.from(uint8Array).toString(\"base64\");\\n\\n  const audioCompletion = await client.chat.completions.create({\\n    model: \"gpt-4o-audio-preview\",\\n    messages: [\\n      {\\n        role: \"user\",\\n        content: [\\n          { type: \"text\", text: inputs[\"audio_question\"] },\\n          {\\n            type: \"input_audio\",\\n            input_audio: {\\n              data: audioB64,\\n              format: \"wav\",\\n            },\\n          },\\n        ],\\n      },\\n    ],\\n  });\\n\\n  const imageUrl = config?.attachments?.[\"my_img\"]?.presigned_url\\n  const imageCompletion = await client.chat.completions.create({\\n    model: \"gpt-4o-mini\",\\n    messages: [\\n      {\\n        role: \"user\",\\n        content: [\\n          { type: \"text\", text: inputs[\"image_question\"] },\\n          {\\n            type: \"image_url\",\\n            image_url: {\\n              url: imageUrl,\\n            },\\n          },\\n        ],\\n      },\\n    ],\\n  });\\n\\n  return {\\n    audio_answer: audioCompletion.choices[0].message.content,\\n    image_answer: imageCompletion.choices[0].message.content,\\n  };\\n}\\n\\n\\u200bDefine custom evaluators\\nThe exact same rules apply as above to determine whether the evaluator should receive attachments.\\nThe evaluator below uses an LLM to judge if the reasoning and the answer are consistent. To learn more about how to define llm-based evaluators, please see this guide.\\nPythonTypeScriptCopy# Assumes you\\'ve installed pydantic\\nfrom pydantic import BaseModel\\n\\ndef valid_image_description(outputs: dict, attachments: dict) -> bool:\\n  \"\"\"Use an LLM to judge if the image description and images are consistent.\"\"\"\\n  instructions = \"\"\"\\n  Does the description of the following image make sense?\\n  Please carefully review the image and the description to determine if the description is valid.\\n  \"\"\"\\n\\n  class Response(BaseModel):\\n      description_is_valid: bool\\n\\n  image_url = attachments[\"my_img\"][\"presigned_url\"]\\n  response = client.beta.chat.completions.parse(\\n      model=\"gpt-4o\",\\n      messages=[\\n          {\\n              \"role\": \"system\",\\n              \"content\": instructions\\n          },\\n          {\\n              \"role\": \"user\",\\n              \"content\": [\\n                  {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\\n                  {\"type\": \"text\", \"text\": outputs[\"image_answer\"]}\\n              ]\\n          }\\n      ],\\n      response_format=Response\\n  )\\n  return response.choices[0].message.parsed.description_is_valid\\n\\nls_client.evaluate(\\n  file_qa,\\n  data=dataset_name,\\n  evaluators=[valid_image_description],\\n)\\n\\n\\u200bUpdate examples with attachments\\nIn the code above, we showed how to add examples with attachments to a dataset. It is also possible to update these same examples using the SDK.\\nAs with existing examples, datasets are versioned when you update them with attachments. Therefore, you can navigate to the dataset version history to see the changes made to each example. To learn more, please see this guide.\\nWhen updating an example with attachments, you can update attachments in a few different ways:\\n\\nPass in new attachments\\nRename existing attachments\\nDelete existing attachments\\n\\nNote that:\\n\\nAny existing attachments that are not explicitly renamed or retained will be deleted.\\nAn error will be raised if you pass in a non-existent attachment name to retain or rename.\\nNew attachments take precedence over existing attachments in case the same attachment name appears in the attachments and attachment_operations fields.\\n\\nPythonTypeScriptCopyexample_update = {\\n  \"id\": example_id,\\n  \"attachments\": {\\n      # These are net new attachments\\n      \"my_new_file\": (\"text/plain\", b\"foo bar\"),\\n  },\\n  \"inputs\": inputs,\\n  \"outputs\": outputs,\\n  # Any attachments not in rename/retain will be deleted.\\n  # In this case, that would be \"my_img\" if we uploaded it.\\n  \"attachments_operations\": {\\n      # Retained attachments will stay exactly the same\\n      \"retain\": [\"my_pdf\"],\\n      # Renaming attachments preserves the original data\\n      \"rename\": {\\n          \"my_wav\": \"my_new_wav\",\\n      }\\n  },\\n}\\n\\nls_client.update_examples(dataset_id=dataset.id, updates=[example_update])\\n\\n\\u200bUI\\n\\u200b1. Create examples with attachments\\nYou can add examples with attachments to a dataset in a few different ways.\\n\\u200bFrom existing runs\\nWhen adding runs to a LangSmith dataset, attachments can be selectively propagated from the source run to the destination example. To learn more, please see this guide.\\n\\n\\u200bFrom scratch\\nYou can create examples with attachments directly from the LangSmith UI. Click the + Example button in the Examples tab of the dataset UI. Then upload attachments using the â€œUpload Filesâ€ button:\\n\\nOnce uploaded, you can view examples with attachments in the LangSmith UI. Each attachment will be rendered with a preview for easy inspection. \\n\\u200b2. Create a multimodal prompt\\nThe LangSmith UI allows you to include attachments in your prompts when evaluating multimodal models:\\nFirst, click the file icon in the message where you want to add multimodal content. Next, add a template variable for the attachment(s) you want to include for each example.\\n\\nFor a single attachment type: Use the suggested variable name. Note: all examples must have an attachment with this name.\\nFor multiple attachments or if your attachments have varying names from one example to another: Use the All attachments variable to include all available attachments for each example.\\n\\n\\n\\u200bDefine custom evaluators\\nThe LangSmith playground does not currently support pulling multimodal content into evaluators. If this would be helpful for your use case, please let us know in the LangChain Forum (sign up here if youâ€™re not already a member)!\\nYou can evaluate a modelâ€™s text output by adding an evaluator that takes in the exampleâ€™s inputs and outputs. Even without multimodal support in your evaluators, you can still run text-only evaluations. For example:\\n\\nOCR â†’ text correction: Use a vision model to extract text from a document, then evaluate the accuracy of the extracted output.\\nSpeech-to-text â†’ transcription quality: Use a voice model to transcribe audio to text, then evaluate the transcription against your reference.\\n\\nFor more information on defining custom evaluators, see the LLM as Judge guide.\\n\\u200bUpdate examples with attachments\\nAttachments are limited to 20MB in size in the UI.\\nWhen editing an example in the UI, you can:\\n\\nUpload new attachments\\nRename and delete attachments\\nReset attachments to their previous state using the quick reset button\\n\\nChanges are not saved until you click submit.\\nWas this page helpful?YesNoSuggest editsEvaluate an existing experiment (Python)Simulate multi-turn interactionsâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset', 'loc': 'https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Manage datasets - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationDatasetsManage datasetsGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageVersion a datasetCreate a new version of a datasetTag a versionEvaluate on a specific dataset versionUse list_examplesEvaluate on a split / filtered view of a datasetEvaluate on a filtered view of a datasetEvaluate on a dataset splitShare a datasetShare a dataset publiclyUnshare a datasetExport a datasetExport filtered traces from experiment to datasetView experiment tracesDatasetsManage datasetsCopy pageCopy pageLangSmith provides tools for managing and working with your datasets. This page describes dataset operations including:\\n\\nVersioning datasets to track changes over time.\\nFiltering and splitting datasets for evaluation.\\nSharing datasets publicly.\\nExporting datasets in various formats.\\n\\nYouâ€™ll also learn how to export filtered traces from experiments back to datasets for further analysis and iteration.\\n\\u200bVersion a dataset\\nIn LangSmith, datasets are versioned. This means that every time you add, update, or delete examples in your dataset, a new version of the dataset is created.\\n\\u200bCreate a new version of a dataset\\nAny time you add, update, or delete examples in your dataset, a new version of your dataset is created. This allows you to track changes to your dataset over time and understand how your dataset has evolved.\\nBy default, the version is defined by the timestamp of the change. When you click on a particular version of a dataset (by timestamp) in the Examples tab, you will find the state of the dataset at that point in time.\\n\\nNote that examples are read-only when viewing a past version of the dataset. You will also see the operations that were between this version of the dataset and the latest version of the dataset.\\nBy default, the latest version of the dataset is shown in the Examples tab and experiments from all versions are shown in the Tests tab.\\nIn the Tests tab, you will find the results of tests run on the dataset at different versions.\\n\\n\\u200bTag a version\\nYou can also tag versions of your dataset to give them a more human-readable name, which can be useful for marking important milestones in your datasetâ€™s history.\\nFor example, you might tag a version of your dataset as â€œprodâ€ and use it to run tests against your LLM pipeline.\\nYou can tag a version of your dataset in the UI by clicking on + Tag this version in the Examples tab.\\n\\nYou can also tag versions of your dataset using the SDK. Hereâ€™s an example of how to tag a version of a dataset using the Python SDK:\\nCopyfrom langsmith import Client\\nfrom datetime import datetime\\n\\nclient = Client()\\ninitial_time = datetime(2024, 1, 1, 0, 0, 0) # The timestamp of the version you want to tag\\n\\n# You can tag a specific dataset version with a semantic name, like \"prod\"\\nclient.update_dataset_tag(\\n    dataset_name=toxic_dataset_name, as_of=initial_time, tag=\"prod\"\\n)\\n\\nTo run an evaluation on a particular tagged version of a dataset, refer to the Evaluate on a specific dataset version section.\\n\\u200bEvaluate on a specific dataset version\\nYou may find it helpful to refer to the following content before you read this section:\\nVersion a dataset.\\nFetching examples.\\n\\n\\u200bUse list_examples\\nYou can use evaluate / aevaluate to pass in an iterable of examples to evaluate on a particular version of a dataset. Use list_examples / listExamples to fetch examples from a particular version tag using as_of / asOf and pass that into the data argument.\\nPythonTypeScriptCopyfrom langsmith import Client\\n\\nls_client = Client()\\n\\n# Assumes actual outputs have a \\'class\\' key.\\n# Assumes example outputs have a \\'label\\' key.\\ndef correct(outputs: dict, reference_outputs: dict) -> bool:\\n  return outputs[\"class\"] == reference_outputs[\"label\"]\\n\\nresults = ls_client.evaluate(\\n    lambda inputs: {\"class\": \"Not toxic\"},\\n    # Pass in filtered data here:\\n    data=ls_client.list_examples(\\n      dataset_name=\"Toxic Queries\",\\n      as_of=\"latest\",  # specify version here\\n    ),\\n    evaluators=[correct],\\n)\\n\\nLearn more about how to fetch views of a dataset on the Create and manage datasets programmatically page.\\n\\u200bEvaluate on a split / filtered view of a dataset\\nYou may find it helpful to refer to the following content before you read this section:\\nFetching examples.\\nCreating and managing dataset splits.\\n\\n\\u200bEvaluate on a filtered view of a dataset\\nYou can use the list_examples / listExamples method to fetch a subset of examples from a dataset to evaluate on.\\nOne common workflow is to fetch examples that have a certain metadata key-value pair.\\nPythonTypeScriptCopyfrom langsmith import evaluate\\n\\nresults = evaluate(\\n    lambda inputs: label_text(inputs[\"text\"]),\\n    data=client.list_examples(dataset_name=dataset_name, metadata={\"desired_key\": \"desired_value\"}),\\n    evaluators=[correct_label],\\n    experiment_prefix=\"Toxic Queries\",\\n)\\n\\nFor more filtering capabilities, refer to this how-to guide.\\n\\u200bEvaluate on a dataset split\\nYou can use the list_examples / listExamples method to evaluate on one or multiple splits of your dataset. The splits parameter takes a list of the splits you would like to evaluate.\\nPythonTypeScriptCopyfrom langsmith import evaluate\\n\\nresults = evaluate(\\n    lambda inputs: label_text(inputs[\"text\"]),\\n    data=client.list_examples(dataset_name=dataset_name, splits=[\"test\", \"training\"]),\\n    evaluators=[correct_label],\\n    experiment_prefix=\"Toxic Queries\",\\n)\\n\\nFor more details on fetching views of a dataset, refer to the guide on fetching datasets.\\n\\u200bShare a dataset\\n\\u200bShare a dataset publicly\\nSharing a dataset publicly will make the dataset examples, experiments and associated runs, and feedback on this dataset accessible to anyone with the link, even if they donâ€™t have a LangSmith account. Make sure youâ€™re not sharing sensitive information.This feature is only available in the cloud-hosted version of LangSmith.\\nFrom the Dataset & Experiments tab, select a dataset, click â‹® (top right of the page), click Share Dataset. This will open a dialog where you can copy the link to the dataset.\\n\\n\\u200bUnshare a dataset\\n\\n\\nClick on Unshare by clicking on Public in the upper right hand corner of any publicly shared dataset, then Unshare in the dialog. \\n\\n\\nNavigate to your organizationâ€™s list of publicly shared datasets, by clicking on Settings -> Shared URLs or this link, then click on Unshare next to the dataset you want to unshare.\\n\\n\\n\\n\\u200bExport a dataset\\nYou can export your LangSmith dataset to a CSV, JSONL, or OpenAIâ€™s fine tuning format from the LangSmith UI.\\nFrom the Dataset & Experiments tab, select a dataset, click â‹® (top right of the page), click Download Dataset.\\n\\n\\u200bExport filtered traces from experiment to dataset\\nAfter running an offline evaluation in LangSmith, you may want to export traces that met some evaluation criteria to a dataset.\\n\\u200bView experiment traces\\n\\nTo do so, first click on the arrow next to your experiment name. This will direct you to a project that contains the traces generated from your experiment.\\n\\nFrom there, you can filter the traces based on your evaluation criteria. In this example, weâ€™re filtering for all traces that received an accuracy score greater than 0.5.\\n\\nAfter applying the filter on the project, we can multi-select runs to add to the dataset, and click Add to Dataset.\\nWas this page helpful?YesNoSuggest editsWith the SDKWith the SDKâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment', 'loc': 'https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='How to fetch performance metrics for an experiment - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationAnalyze experiment resultsHow to fetch performance metrics for an experimentGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumAnalyze experiment resultsHow to fetch performance metrics for an experimentCopy pageCopy pageTracing projects and experiments use the same underlying data structure in our backend, which is called a â€œsession.â€You might see these terms interchangeably in our documentation, but they all refer to the same underlying data structure.We are working on unifying the terminology across our documentation and APIs.\\nWhen you run an experiment using evaluate with the Python or TypeScript SDK, you can fetch the performance metrics for the experiment using the read_project/readProject methods.\\nThe payload for experiment details includes the following values:\\nCopy{\\n  \"start_time\": \"2024-06-06T01:02:51.299960\",\\n  \"end_time\": \"2024-06-06T01:03:04.557530+00:00\",\\n  \"extra\": {\\n    \"metadata\": {\\n      \"git\": {\\n        \"tags\": null,\\n        \"dirty\": true,\\n        \"branch\": \"ankush/agent-eval\",\\n        \"commit\": \"...\",\\n        \"repo_name\": \"...\",\\n        \"remote_url\": \"...\",\\n        \"author_name\": \"Ankush Gola\",\\n        \"commit_time\": \"...\",\\n        \"author_email\": \"...\"\\n      },\\n      \"revision_id\": null,\\n      \"dataset_splits\": [\"base\"],\\n      \"dataset_version\": \"2024-06-05T04:57:01.535578+00:00\",\\n      \"num_repetitions\": 3\\n    }\\n  },\\n  \"name\": \"SQL Database Agent-ae9ad229\",\\n  \"description\": null,\\n  \"default_dataset_id\": null,\\n  \"reference_dataset_id\": \"...\",\\n  \"id\": \"...\",\\n  \"run_count\": 9,\\n  \"latency_p50\": 7.896,\\n  \"latency_p99\": 13.09332,\\n  \"first_token_p50\": null,\\n  \"first_token_p99\": null,\\n  \"total_tokens\": 35573,\\n  \"prompt_tokens\": 32711,\\n  \"completion_tokens\": 2862,\\n  \"total_cost\": 0.206485,\\n  \"prompt_cost\": 0.163555,\\n  \"completion_cost\": 0.04293,\\n  \"tenant_id\": \"...\",\\n  \"last_run_start_time\": \"2024-06-06T01:02:51.366397\",\\n  \"last_run_start_time_live\": null,\\n  \"feedback_stats\": {\\n    \"cot contextual accuracy\": {\\n      \"n\": 9,\\n      \"avg\": 0.6666666666666666,\\n      \"values\": {\\n        \"CORRECT\": 6,\\n        \"INCORRECT\": 3\\n      }\\n    }\\n  },\\n  \"session_feedback_stats\": {},\\n  \"run_facets\": [],\\n  \"error_rate\": 0,\\n  \"streaming_rate\": 0,\\n  \"test_run_number\": 11\\n}\\n\\nFrom here, you can extract performance metrics such as:\\n\\nlatency_p50: The 50th percentile latency in seconds.\\nlatency_p99: The 99th percentile latency in seconds.\\ntotal_tokens: The total number of tokens used.\\nprompt_tokens: The number of prompt tokens used.\\ncompletion_tokens: The number of completion tokens used.\\ntotal_cost: The total cost of the experiment.\\nprompt_cost: The cost of the prompt tokens.\\ncompletion_cost: The cost of the completion tokens.\\nfeedback_stats: The feedback statistics for the experiment.\\nerror_rate: The error rate for the experiment.\\nfirst_token_p50: The 50th percentile latency for the time to generate the first token (if using streaming).\\nfirst_token_p99: The 99th percentile latency for the time to generate the first token (if using streaming).\\n\\nHere is an example of how you can fetch the performance metrics for an experiment using the Python and TypeScript SDKs.\\nFirst, as a prerequisite, we will create a trivial dataset. Here, we only demonstrate this in Python, but you can do the same in TypeScript. Please view the how-to guide on evaluation for more details.\\nCopyfrom langsmith import Client\\n\\nclient = Client()\\n\\n# Create a dataset\\ndataset_name = \"HelloDataset\"\\ndataset = client.create_dataset(dataset_name=dataset_name)\\n\\nexamples = [\\n    {\\n        \"inputs\": {\"input\": \"Harrison\"},\\n        \"outputs\": {\"expected\": \"Hello Harrison\"},\\n    },\\n    {\\n        \"inputs\": {\"input\": \"Ankush\"},\\n        \"outputs\": {\"expected\": \"Hello Ankush\"},\\n    },\\n]\\n\\nclient.create_examples(dataset_id=dataset.id, examples=examples)\\n\\nNext, we will create an experiment, retrieve the experiment name from the result of evaluate, then fetch the performance metrics for the experiment.\\nPythonTypeScriptCopyfrom langsmith.schemas import Example, Run\\ndataset_name = \"HelloDataset\"\\n\\ndef foo_label(root_run: Run, example: Example) -> dict:\\n    return {\"score\": 1, \"key\": \"foo\"}\\n\\nfrom langsmith import evaluate\\n\\nresults = evaluate(\\n    lambda inputs: \"Hello \" + inputs[\"input\"],\\n    data=dataset_name,\\n    evaluators=[foo_label],\\n    experiment_prefix=\"Hello\",\\n)\\n\\nresp = client.read_project(project_name=results.experiment_name, include_stats=True)\\nprint(resp.json(indent=2))\\nWas this page helpful?YesNoSuggest editsFilter experiments in the UIUpload experiments run outside of LangSmithâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui', 'loc': 'https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='How to filter experiments in the UI - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationAnalyze experiment resultsHow to filter experiments in the UIGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageBackground: add metadata to your experimentsFilter experiments in the UIAnalyze experiment resultsHow to filter experiments in the UICopy pageCopy pageLangSmith lets you filter your previous experiments by feedback scores and metadata to make it easy to find only the experiments you care about.\\n\\u200bBackground: add metadata to your experiments\\nWhen you run an experiment in the SDK, you can attach metadata to make it easier to filter in UI. This is helpful if you know what axes you want to drill down into when running experiments.\\nIn our example, we are going to attach metadata to our experiment around the model used, the model provider, and a known ID of the prompt:\\nCopymodels = {\\n    \"openai-gpt-4o\": ChatOpenAI(model=\"gpt-4o\", temperature=0),\\n    \"openai-gpt-4o-mini\": ChatOpenAI(model=\"gpt-4o-mini\", temperature=0),\\n    \"anthropic-claude-3-sonnet-20240229\": ChatAnthropic(temperature=0, model_name=\"claude-3-sonnet-20240229\")\\n}\\n\\nprompts = {\\n    \"singleminded\": \"always answer questions with the word banana.\",\\n    \"fruitminded\": \"always discuss fruit in your answers.\",\\n    \"basic\": \"you are a chatbot.\"\\n}\\n\\ndef answer_evaluator(run, example) -> dict:\\n    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\\n    answer_grader = hub.pull(\"langchain-ai/rag-answer-vs-reference\") | llm\\n    score = answer_grader.invoke(\\n        {\\n            \"question\": example.inputs[\"question\"],\\n            \"correct_answer\": example.outputs[\"answer\"],\\n            \"student_answer\": run.outputs,\\n        }\\n    )\\n    return {\"key\": \"correctness\", \"score\": score[\"Score\"]}\\n\\ndataset_name = \"Filterable Dataset\"\\n\\nfor model_type, model in models.items():\\n    for prompt_type, prompt in prompts.items():\\n        def predict(example):\\n            return model.invoke(\\n                [(\"system\", prompt), (\"user\", example[\"question\"])]\\n            )\\n\\n        model_provider = model_type.split(\"-\")[0]\\n        model_name = model_type[len(model_provider) + 1:]\\n\\n        evaluate(\\n            predict,\\n            data=dataset_name,\\n            evaluators=[answer_evaluator],\\n            # ADD IN METADATA HERE!!\\n            metadata={\\n                \"model_provider\": model_provider,\\n                \"model_name\": model_name,\\n                \"prompt_id\": prompt_type\\n            }\\n        )\\n\\n\\u200bFilter experiments in the UI\\nIn the UI, we see all experiments that have been run by default.\\n\\nIf we, say, have a preference for openai models, we can easily filter down and see scores within just openai models first:\\n\\nWe can stack filters, allowing us to filter out low scores on correctness to make sure we only compare relevant experiments:\\n\\nFinally, we can clear and reset filters. For example, if we see there is clear thereâ€™s a winner with the singleminded prompt, we can change filtering settings to see if any other model providersâ€™ models work as well with it:\\nWas this page helpful?YesNoSuggest editsCompare experiment resultsFetch performance metrics for an experimentâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection', 'loc': 'https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Dynamic few shot example selection - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesImprove evaluatorsImprove LLM-as-judge evaluators using human feedbackImprove your evaluator with few-shot examplesDynamic few shot example selectionTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationImprove evaluatorsDynamic few shot example selectionGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pagePre-conditionsIndex your dataset for few shot searchTest search quality in the few shot playgroundAdding few shot search to your applicationCode snippetsSet up evaluationsImprove evaluatorsDynamic few shot example selectionCopy pageCopy pageThis feature is in open beta. It is only available to paid team plans. Please reach out to support@langchain.dev if you have questions about enablement.\\nConfigure your datasets so that you can search for few shot examples based on an incoming request.\\n\\u200bPre-conditions\\n\\nYour dataset must use the KV store data type (we do not currently support chat model or LLM type datasets)\\nYou must have an input schema defined for your dataset. See our docs on setting up schema validation in our UI for details.\\nYou must be on a paid team plan (e.g. Plus plan)\\nYou must be on LangSmith cloud\\n\\n\\u200bIndex your dataset for few shot search\\nNavigate to the datasets UI, and click the new Few-Shot search tab. Hit the Start sync button, which will create a new index on your dataset to make it searchable.\\n\\nBy default, we sync to the latest version of your dataset. That means when new examples are added to your dataset, they will automatically be added to your index. This process runs every few minutes, so there should be a very short delay for indexing new examples. You can see whether your index is up to date under Few-shot index on the lefthand side of the screen in the next section.\\n\\u200bTest search quality in the few shot playground\\nNow that you have turned on indexing for your dataset, you will see the new few shot playground.\\n\\nYou can type in a sample input, and check which results would be returned by our search API.\\n\\nEach result will have a score and a link to the example in the dataset. The scoring system works such that 0 is a completely random result, and higher scores are better. Results will be sorted in descending order according to score.\\nSearch uses a BM25-like algorithm for keyword based similarity scores. The actual score is subject to change as we improve the search algorithm, so we recommend not relying on the scores themselves, as their meaning may evolve over time. They are simply used for convenience in vibe-testing outputs in the playground.\\n\\u200bAdding few shot search to your application\\nClick the Get Code Snippet button in the previous diagram, youâ€™ll be taken to a screen that has code snippets from our LangSmith SDK in different languages.\\n\\nFor code samples on using few shot search in LangChain python applications, please see our how-to guide in the LangChain docs.\\n\\u200bCode snippets\\nPlease ensure you are using the python SDK with version >= 1.101 or the typescript SDK with version >= 1.43\\nFor copy and paste convenience, you can find the similar code snippets to the ones shown in the screenshot above here:\\nPython (Async)PythonTypeScriptCopyimport langsmith as ls\\n# Copy this value from LangSmith UI\\ndataset_id = \"1c5e9c95-dfd4-4dc5-a4b8-df7ea921c913\"\\nasync with ls.AsyncClient() as client:\\n  examples = await client.similar_examples(\\n      {\"question\": \"knock knock\"}, dataset_id=dataset_id, limit=1\\n  )\\n  print(examples[0].outputs)  # {\"output\": \"Few shots\\'ll do the trick.\"}\\nWas this page helpful?YesNoSuggest editsImprove your evaluator with few-shot examplesEvaluate a chatbotâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable', 'loc': 'https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='How to evaluate a runnable - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesDefine a target function to evaluateEvaluate intermediate stepsReturn multiple scores in one evaluatorReturn categorical vs numerical metricsRun evaluators on experimentsEvaluate with repetitionsHandle model rate limitsRun an evaluation locally (Python)Evaluate a runnableEvaluate a graphEvaluate an existing experiment (Python)Run an evaluation with multimodal contentSimulate multi-turn interactionsImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationEvaluation techniquesHow to evaluate a runnableGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageSetupEvaluateRelatedSet up evaluationsEvaluation techniquesHow to evaluate a runnableCopy pageCopy page\\nlangchain: Python and JS/TS\\nRunnable: Python and JS/TS\\n\\nlangchain Runnable objects (such as chat models, retrievers, chains, etc.) can be passed directly into evaluate() / aevaluate().\\n\\u200bSetup\\nLetâ€™s define a simple chain to evaluate. First, install all the required packages:\\nPythonTypeScriptCopypip install -U langsmith langchain[openai]\\n\\nNow define a chain:\\nPythonTypeScriptCopyfrom langchain.chat_models import init_chat_model\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\ninstructions = (\\n    \"Please review the user query below and determine if it contains any form \"\\n    \"of toxic behavior, such as insults, threats, or highly negative comments. \"\\n    \"Respond with \\'Toxic\\' if it does, and \\'Not toxic\\' if it doesn\\'t.\"\\n)\\n\\nprompt = ChatPromptTemplate(\\n    [(\"system\", instructions), (\"user\", \"{text}\")],\\n)\\n\\nllm = init_chat_model(\"gpt-4o\")\\nchain = prompt | llm | StrOutputParser()\\n\\n\\u200bEvaluate\\nTo evaluate our chain we can pass it directly to the evaluate() / aevaluate() method. Note that the input variables of the chain must match the keys of the example inputs. In this case, the example inputs should have the form {\"text\": \"...\"}.\\nPythonTypeScriptCopyfrom langsmith import aevaluate, Client\\n\\nclient = Client()\\n\\n# Clone a dataset of texts with toxicity labels.\\n# Each example input has a \"text\" key and each output has a \"label\" key.\\ndataset = client.clone_public_dataset(\\n    \"https://smith.langchain.com/public/3d6831e6-1680-4c88-94df-618c8e01fc55/d\"\\n)\\n\\ndef correct(outputs: dict, reference_outputs: dict) -> bool:\\n    # Since our chain outputs a string not a dict, this string\\n    # gets stored under the default \"output\" key in the outputs dict:\\n    actual = outputs[\"output\"]\\n    expected = reference_outputs[\"label\"]\\n    return actual == expected\\n\\nresults = await aevaluate(\\n    chain,\\n    data=dataset,\\n    evaluators=[correct],\\n    experiment_prefix=\"gpt-4o, baseline\",\\n)\\n\\nThe runnable is traced appropriately for each output.\\n\\n\\u200bRelated\\n\\nHow to evaluate a langgraph graph\\nWas this page helpful?YesNoSuggest editsRun an evaluation locally (Python)Evaluate a graphâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph', 'loc': 'https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='How to evaluate a graph - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesDefine a target function to evaluateEvaluate intermediate stepsReturn multiple scores in one evaluatorReturn categorical vs numerical metricsRun evaluators on experimentsEvaluate with repetitionsHandle model rate limitsRun an evaluation locally (Python)Evaluate a runnableEvaluate a graphEvaluate an existing experiment (Python)Run an evaluation with multimodal contentSimulate multi-turn interactionsImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationEvaluation techniquesHow to evaluate a graphGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageEnd-to-end evaluationsDefine a graphCreate a datasetCreate an evaluatorRun evaluationsEvaluating intermediate stepsRunning and evaluating individual nodesRelatedReference codeSet up evaluationsEvaluation techniquesHow to evaluate a graphCopy pageCopy pagelanggraph\\nlanggraph is a library for building stateful, multi-actor applications with LLMs, used to create agent and multi-agent workflows. Evaluating langgraph graphs can be challenging because a single invocation can involve many LLM calls, and which LLM calls are made may depend on the outputs of preceding calls. In this guide we will focus on the mechanics of how to pass graphs and graph nodes to evaluate() / aevaluate(). For evaluation techniques and best practices when building agents head to the langgraph docs.\\n\\u200bEnd-to-end evaluations\\nThe most common type of evaluation is an end-to-end one, where we want to evaluate the final graph output for each example input.\\n\\u200bDefine a graph\\nLets construct a simple ReACT agent to start:\\nCopyfrom typing import Annotated, Literal, TypedDict\\nfrom langchain.chat_models import init_chat_model\\nfrom langchain_core.tools import tool\\nfrom langgraph.graph import END, START, StateGraph\\nfrom langgraph.prebuilt import ToolNode\\nfrom langgraph.graph.message import add_messages\\n\\nclass State(TypedDict):\\n    # Messages have the type \"list\". The \\'add_messages\\' function\\n    # in the annotation defines how this state key should be updated\\n    # (in this case, it appends messages to the list, rather than overwriting them)\\n    messages: Annotated[list, add_messages]\\n\\n# Define the tools for the agent to use\\n@tool\\ndef search(query: str) -> str:\\n    \"\"\"Call to surf the web.\"\"\"\\n    # This is a placeholder, but don\\'t tell the LLM that...\\n    if \"sf\" in query.lower() or \"san francisco\" in query.lower():\\n        return \"It\\'s 60 degrees and foggy.\"\\n    return \"It\\'s 90 degrees and sunny.\"\\n\\ntools = [search]\\ntool_node = ToolNode(tools)\\nmodel = init_chat_model(\"claude-3-5-sonnet-latest\").bind_tools(tools)\\n\\n# Define the function that determines whether to continue or not\\ndef should_continue(state: State) -> Literal[\"tools\", END]:\\n    messages = state[\\'messages\\']\\n    last_message = messages[-1]\\n    # If the LLM makes a tool call, then we route to the \"tools\" node\\n    if last_message.tool_calls:\\n        return \"tools\"\\n    # Otherwise, we stop (reply to the user)\\n    return END\\n\\n# Define the function that calls the model\\ndef call_model(state: State):\\n    messages = state[\\'messages\\']\\n    response = model.invoke(messages)\\n    # We return a list, because this will get added to the existing list\\n    return {\"messages\": [response]}\\n\\n# Define a new graph\\nworkflow = StateGraph(State)\\n\\n# Define the two nodes we will cycle between\\nworkflow.add_node(\"agent\", call_model)\\nworkflow.add_node(\"tools\", tool_node)\\n\\n# Set the entrypoint as \\'agent\\'\\n# This means that this node is the first one called\\nworkflow.add_edge(START, \"agent\")\\n\\n# We now add a conditional edge\\nworkflow.add_conditional_edges(\\n    # First, we define the start node. We use \\'agent\\'.\\n    # This means these are the edges taken after the \\'agent\\' node is called.\\n    \"agent\",\\n    # Next, we pass in the function that will determine which node is called next.\\n    should_continue,\\n)\\n\\n# We now add a normal edge from \\'tools\\' to \\'agent\\'.\\n# This means that after \\'tools\\' is called, \\'agent\\' node is called next.\\nworkflow.add_edge(\"tools\", \\'agent\\')\\n\\n# Finally, we compile it!\\n# This compiles it into a LangChain Runnable,\\n# meaning you can use it as you would any other runnable.\\n# Note that we\\'re (optionally) passing the memory when compiling the graph\\napp = workflow.compile()\\n\\n\\u200bCreate a dataset\\nLetâ€™s create a simple dataset of questions and expected responses:\\nCopyfrom langsmith import Client\\n\\nquestions = [\\n    \"what\\'s the weather in sf\",\\n    \"whats the weather in san fran\",\\n    \"whats the weather in tangier\"\\n]\\n\\nanswers = [\\n    \"It\\'s 60 degrees and foggy.\",\\n    \"It\\'s 60 degrees and foggy.\",\\n    \"It\\'s 90 degrees and sunny.\",\\n]\\n\\nls_client = Client()\\ndataset = ls_client.create_dataset(\\n    \"weather agent\",\\n    inputs=[{\"question\": q} for q in questions],\\n    outputs=[{\"answers\": a} for a in answers],\\n)\\n\\n\\u200bCreate an evaluator\\nAnd a simple evaluator:\\nRequires langsmith>=0.2.0\\nCopyjudge_llm = init_chat_model(\"gpt-4o\")\\n\\nasync def correct(outputs: dict, reference_outputs: dict) -> bool:\\n    instructions = (\\n        \"Given an actual answer and an expected answer, determine whether\"\\n        \" the actual answer contains all of the information in the\"\\n        \" expected answer. Respond with \\'CORRECT\\' if the actual answer\"\\n        \" does contain all of the expected information and \\'INCORRECT\\'\"\\n        \" otherwise. Do not include anything else in your response.\"\\n    )\\n    # Our graph outputs a State dictionary, which in this case means\\n    # we\\'ll have a \\'messages\\' key and the final message should\\n    # be our actual answer.\\n    actual_answer = outputs[\"messages\"][-1].content\\n    expected_answer = reference_outputs[\"answer\"]\\n    user_msg = (\\n        f\"ACTUAL ANSWER: {actual_answer}\"\\n        f\"\\\\n\\\\nEXPECTED ANSWER: {expected_answer}\"\\n    )\\n    response = await judge_llm.ainvoke(\\n        [\\n            {\"role\": \"system\", \"content\": instructions},\\n            {\"role\": \"user\", \"content\": user_msg}\\n        ]\\n    )\\n    return response.content.upper() == \"CORRECT\"\\n\\n\\u200bRun evaluations\\nNow we can run our evaluations and explore the results. Weâ€™ll just need to wrap our graph function so that it can take inputs in the format theyâ€™re stored on our example:\\nIf all of your graph nodes are defined as sync functions then you can use evaluate or aevaluate. If any of you nodes are defined as async, youâ€™ll need to use aevaluate\\nRequires langsmith>=0.2.0\\nCopyfrom langsmith import aevaluate\\n\\ndef example_to_state(inputs: dict) -> dict:\\n  return {\"messages\": [{\"role\": \"user\", \"content\": inputs[\\'question\\']}]}\\n\\n# We use LCEL declarative syntax here.\\n# Remember that langgraph graphs are also langchain runnables.\\ntarget = example_to_state | app\\n\\nexperiment_results = await aevaluate(\\n    target,\\n    data=\"weather agent\",\\n    evaluators=[correct],\\n    max_concurrency=4,  # optional\\n    experiment_prefix=\"claude-3.5-baseline\",  # optional\\n)\\n\\n\\u200bEvaluating intermediate steps\\nOften it is valuable to evaluate not only the final output of an agent but also the intermediate steps it has taken. Whatâ€™s nice about langgraph is that the output of a graph is a state object that often already carries information about the intermediate steps taken. Usually we can evaluate whatever weâ€™re interested in just by looking at the messages in our state. For example, we can look at the messages to assert that the model invoked the â€˜searchâ€™ tool upon as a first step.\\nRequires langsmith>=0.2.0\\nCopydef right_tool(outputs: dict) -> bool:\\n    tool_calls = outputs[\"messages\"][1].tool_calls\\n    return bool(tool_calls and tool_calls[0][\"name\"] == \"search\")\\n\\nexperiment_results = await aevaluate(\\n    target,\\n    data=\"weather agent\",\\n    evaluators=[correct, right_tool],\\n    max_concurrency=4,  # optional\\n    experiment_prefix=\"claude-3.5-baseline\",  # optional\\n)\\n\\nIf we need access to information about intermediate steps that isnâ€™t in state, we can look at the Run object. This contains the full traces for all node inputs and outputs:\\nSee more about what arguments you can pass to custom evaluators in this how-to guide.\\nCopyfrom langsmith.schemas import Run, Example\\n\\ndef right_tool_from_run(run: Run, example: Example) -> dict:\\n    # Get documents and answer\\n    first_model_run = next(run for run in root_run.child_runs if run.name == \"agent\")\\n    tool_calls = first_model_run.outputs[\"messages\"][-1].tool_calls\\n    right_tool = bool(tool_calls and tool_calls[0][\"name\"] == \"search\")\\n    return {\"key\": \"right_tool\", \"value\": right_tool}\\n\\nexperiment_results = await aevaluate(\\n    target,\\n    data=\"weather agent\",\\n    evaluators=[correct, right_tool_from_run],\\n    max_concurrency=4,  # optional\\n    experiment_prefix=\"claude-3.5-baseline\",  # optional\\n)\\n\\n\\u200bRunning and evaluating individual nodes\\nSometimes you want to evaluate a single node directly to save time and costs. langgraph makes it easy to do this. In this case we can even continue using the evaluators weâ€™ve been using.\\nCopynode_target = example_to_state | app.nodes[\"agent\"]\\n\\nnode_experiment_results = await aevaluate(\\n    node_target,\\n    data=\"weather agent\",\\n    evaluators=[right_tool_from_run],\\n    max_concurrency=4,  # optional\\n    experiment_prefix=\"claude-3.5-model-node\",  # optional\\n)\\n\\n\\u200bRelated\\n\\nlanggraph evaluation docs\\n\\n\\u200bReference code\\nClick to see a consolidated code snippetCopyfrom typing import Annotated, Literal, TypedDict\\nfrom langchain.chat_models import init_chat_model\\nfrom langchain_core.tools import tool\\nfrom langgraph.graph import END, START, StateGraph\\nfrom langgraph.prebuilt import ToolNode\\nfrom langgraph.graph.message import add_messages\\nfrom langsmith import Client, aevaluate\\n\\n# Define a graph\\nclass State(TypedDict):\\n    # Messages have the type \"list\". The \\'add_messages\\' function\\n    # in the annotation defines how this state key should be updated\\n    # (in this case, it appends messages to the list, rather than overwriting them)\\n    messages: Annotated[list, add_messages]\\n\\n# Define the tools for the agent to use\\n@tool\\ndef search(query: str) -> str:\\n    \"\"\"Call to surf the web.\"\"\"\\n    # This is a placeholder, but don\\'t tell the LLM that...\\n    if \"sf\" in query.lower() or \"san francisco\" in query.lower():\\n        return \"It\\'s 60 degrees and foggy.\"\\n    return \"It\\'s 90 degrees and sunny.\"\\n\\ntools = [search]\\ntool_node = ToolNode(tools)\\nmodel = init_chat_model(\"claude-3-5-sonnet-latest\").bind_tools(tools)\\n\\n# Define the function that determines whether to continue or not\\ndef should_continue(state: State) -> Literal[\"tools\", END]:\\n    messages = state[\\'messages\\']\\n    last_message = messages[-1]\\n    # If the LLM makes a tool call, then we route to the \"tools\" node\\n    if last_message.tool_calls:\\n        return \"tools\"\\n    # Otherwise, we stop (reply to the user)\\n    return END\\n\\n# Define the function that calls the model\\ndef call_model(state: State):\\n    messages = state[\\'messages\\']\\n    response = model.invoke(messages)\\n    # We return a list, because this will get added to the existing list\\n    return {\"messages\": [response]}\\n\\n# Define a new graph\\nworkflow = StateGraph(State)\\n\\n# Define the two nodes we will cycle between\\nworkflow.add_node(\"agent\", call_model)\\nworkflow.add_node(\"tools\", tool_node)\\n\\n# Set the entrypoint as \\'agent\\'\\n# This means that this node is the first one called\\nworkflow.add_edge(START, \"agent\")\\n\\n# We now add a conditional edge\\nworkflow.add_conditional_edges(\\n    # First, we define the start node. We use \\'agent\\'.\\n    # This means these are the edges taken after the \\'agent\\' node is called.\\n    \"agent\",\\n    # Next, we pass in the function that will determine which node is called next.\\n    should_continue,\\n)\\n\\n# We now add a normal edge from \\'tools\\' to \\'agent\\'.\\n# This means that after \\'tools\\' is called, \\'agent\\' node is called next.\\nworkflow.add_edge(\"tools\", \\'agent\\')\\n\\n# Finally, we compile it!\\n# This compiles it into a LangChain Runnable,\\n# meaning you can use it as you would any other runnable.\\n# Note that we\\'re (optionally) passing the memory when compiling the graph\\napp = workflow.compile()\\n\\nquestions = [\\n    \"what\\'s the weather in sf\",\\n    \"whats the weather in san fran\",\\n    \"whats the weather in tangier\"\\n]\\n\\nanswers = [\\n    \"It\\'s 60 degrees and foggy.\",\\n    \"It\\'s 60 degrees and foggy.\",\\n    \"It\\'s 90 degrees and sunny.\",\\n]\\n\\n# Create a dataset\\nls_client = Client()\\ndataset = ls_client.create_dataset(\\n    \"weather agent\",\\n    inputs=[{\"question\": q} for q in questions],\\n    outputs=[{\"answers\": a} for a in answers],\\n)\\n\\n# Define evaluators\\nasync def correct(outputs: dict, reference_outputs: dict) -> bool:\\n    instructions = (\\n        \"Given an actual answer and an expected answer, determine whether\"\\n        \" the actual answer contains all of the information in the\"\\n        \" expected answer. Respond with \\'CORRECT\\' if the actual answer\"\\n        \" does contain all of the expected information and \\'INCORRECT\\'\"\\n        \" otherwise. Do not include anything else in your response.\"\\n    )\\n    # Our graph outputs a State dictionary, which in this case means\\n    # we\\'ll have a \\'messages\\' key and the final message should\\n    # be our actual answer.\\n    actual_answer = outputs[\"messages\"][-1].content\\n    expected_answer = reference_outputs[\"answer\"]\\n    user_msg = (\\n        f\"ACTUAL ANSWER: {actual_answer}\"\\n        f\"\\\\n\\\\nEXPECTED ANSWER: {expected_answer}\"\\n    )\\n    response = await judge_llm.ainvoke(\\n        [\\n            {\"role\": \"system\", \"content\": instructions},\\n            {\"role\": \"user\", \"content\": user_msg}\\n        ]\\n    )\\n    return response.content.upper() == \"CORRECT\"\\n\\ndef right_tool(outputs: dict) -> bool:\\n    tool_calls = outputs[\"messages\"][1].tool_calls\\n    return bool(tool_calls and tool_calls[0][\"name\"] == \"search\")\\n\\n# Run evaluation\\nexperiment_results = await aevaluate(\\n    target,\\n    data=\"weather agent\",\\n    evaluators=[correct, right_tool],\\n    max_concurrency=4,  # optional\\n    experiment_prefix=\"claude-3.5-baseline\",  # optional\\n)\\nWas this page helpful?YesNoSuggest editsEvaluate a runnableEvaluate an existing experiment (Python)âŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge', 'loc': 'https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='How to define an LLM-as-a-judge evaluator - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesCode evaluatorLLM-as-a-judge evaluatorComposite evaluatorsSummary evaluatorPairwise evaluationFrameworks & integrationsEvaluation techniquesImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationEvaluation typesHow to define an LLM-as-a-judge evaluatorGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageSDKPre-built evaluatorsCreate your own LLM-as-a-judge evaluatorUIPre-built evaluatorsCustomize your LLM-as-a-judge evaluatorSelect/create the evaluatorConfigure the evaluatorPromptModelMapping variablesPreviewImprove your evaluator with few-shot examplesFeedback configurationSave the evaluatorSet up evaluationsEvaluation typesHow to define an LLM-as-a-judge evaluatorCopy pageCopy page\\nLLM-as-a-judge evaluator\\n\\nLLM applications can be challenging to evaluate since they often generate conversational text with no single correct answer.\\nThis guide shows you how to define an LLM-as-a-judge evaluator for offline evaluation using either the LangSmith SDK or the UI. Note: To run evaluations in real-time on your production traces, refer to setting up online evaluations.\\n\\u200bSDK\\n\\u200bPre-built evaluators\\nPre-built evaluators are a useful starting point for setting up evaluations. Refer to pre-built evaluators for how to use pre-built evaluators with LangSmith.\\n\\u200bCreate your own LLM-as-a-judge evaluator\\nFor complete control of evaluator logic, create your own LLM-as-a-judge evaluator and run it using the LangSmith SDK (Python / TypeScript).\\nRequires langsmith>=0.2.0\\nCopyfrom langsmith import evaluate, traceable, wrappers, Client\\nfrom openai import OpenAI\\n# Assumes you\\'ve installed pydantic\\nfrom pydantic import BaseModel\\n\\n# Optionally wrap the OpenAI client to trace all model calls.\\noai_client = wrappers.wrap_openai(OpenAI())\\n\\ndef valid_reasoning(inputs: dict, outputs: dict) -> bool:\\n    \"\"\"Use an LLM to judge if the reasoning and the answer are consistent.\"\"\"\\n    instructions = \"\"\"\\nGiven the following question, answer, and reasoning, determine if the reasoning\\nfor the answer is logically valid and consistent with the question and the answer.\"\"\"\\n\\n    class Response(BaseModel):\\n        reasoning_is_valid: bool\\n\\n    msg = f\"Question: {inputs[\\'question\\']}\\\\nAnswer: {outputs[\\'answer\\']}\\\\nReasoning: {outputs[\\'reasoning\\']}\"\\n    response = oai_client.beta.chat.completions.parse(\\n        model=\"gpt-4o\",\\n        messages=[{\"role\": \"system\", \"content\": instructions,}, {\"role\": \"user\", \"content\": msg}],\\n        response_format=Response\\n    )\\n    return response.choices[0].message.parsed.reasoning_is_valid\\n\\n# Optionally add the \\'traceable\\' decorator to trace the inputs/outputs of this function.\\n@traceable\\ndef dummy_app(inputs: dict) -> dict:\\n    return {\"answer\": \"hmm i\\'m not sure\", \"reasoning\": \"i didn\\'t understand the question\"}\\n\\nls_client = Client()\\ndataset = ls_client.create_dataset(\"big questions\")\\nexamples = [\\n    {\"inputs\": {\"question\": \"how will the universe end\"}},\\n    {\"inputs\": {\"question\": \"are we alone\"}},\\n]\\nls_client.create_examples(dataset_id=dataset.id, examples=examples)\\n\\nresults = evaluate(\\n    dummy_app,\\n    data=dataset,\\n    evaluators=[valid_reasoning]\\n)\\n\\nSee here for more on how to write a custom evaluator.\\n\\u200bUI\\n\\u200bPre-built evaluators\\nPre-built evaluators are a useful starting point when setting up evaluations. The LangSmith UI supports the following pre-built evaluators:\\n\\nHallucination: Detect factually incorrect outputs. Requires a reference output.\\nCorrectness: Check semantic similarity to a reference.\\nConciseness: Evaluate whether an answer is a concise response to a question.\\nCode checker: Verify correctness of code answers.\\n\\nYou can configure these evaluators::\\n\\nWhen running an evaluation using the playground\\nAs part of a dataset to automatically run evaluations on experiments\\nWhen running an online evaluation\\n\\n\\u200bCustomize your LLM-as-a-judge evaluator\\nAdd specific instructions for your LLM-as-a-judge evalutor prompt and configure which parts of the input/output/reference output should be passed to the evaluator.\\n\\u200bSelect/create the evaluator\\n\\nIn the playground or from a dataset: Select the +Evaluator button\\nFrom a tracing project: Select Add rules, configure your rule and select Apply evaluator\\n\\nSelect the Create your own evaluator option. Alternatively, you may start by selecting a pre-built evaluator and editing it.\\n\\u200bConfigure the evaluator\\n\\u200bPrompt\\nCreate a new prompt, or choose an existing prompt from the prompt hub.\\n\\n\\nCreate your own prompt: Create a custom prompt inline.\\n\\n\\nPull a prompt from the prompt hub: Use the Select a prompt dropdown to select from an existing prompt. You canâ€™t edit these prompts directly within the prompt editor, but you can view the prompt and the schema it uses. To make changes, edit the prompt in the playground and commit the version, and then pull in your new prompt in the evaluator.\\n\\n\\n\\u200bModel\\nSelect the desired model from the provided options.\\n\\u200bMapping variables\\nUse variable mapping to indicate the variables that are passed into your evaluator prompt from your run or example. To aid with variable mapping, an example (or run) is provided for reference. Click on the the variables in your prompt and use the dropdown to map them to the relevant parts of the input, output, or reference output.\\nTo add prompt variables type the variable with double curly brackets {{prompt_var}} if using mustache formatting (the default) or single curly brackets {prompt_var} if using f-string formatting.\\nYou may remove variables as needed. For example if you are evaluating a metric such as conciseness, you typically donâ€™t need a reference output so you may remove that variable.\\n\\u200bPreview\\nPreviewing the prompt will show you of what the formatted prompt will look like using the reference run and dataset example shown on the right.\\n\\u200bImprove your evaluator with few-shot examples\\nTo better align the LLM-as-a-judge evaluator to human preferences, LangSmith allows you to collect human corrections on evaluator scores. With this selection enabled, corrections are then inserted automatically as few-shot examples into your prompt.\\nLearn how to set up few-shot examples and make corrections.\\n\\u200bFeedback configuration\\nFeedback configuration is the scoring criteria that your LLM-as-a-judge evaluator will use. Think of this as the rubric that your evaluator will grade based on. Scores will be added as feedback to a run or example. Defining feedback for your evaluator:\\n\\n\\nName the feedback key: This is the name that will appear when viewing evaluation results. Names should be unique across experiments.\\n\\n\\nAdd a description: Describe what the feedback represents.\\n\\n\\nChoose a feedback type:\\n\\n\\n\\nBoolean: True/false feedback.\\nCategorical: Select from predefined categories.\\nContinuous: Numerical scoring within a specified range.\\n\\nBehind the scenes, feedback configuration is added as structured output to the LLM-as-a-judge prompt. If youâ€™re using an existing prompt from the hub, you must add an output schema to the prompt before configuring an evaluator to use it. Each top-level key in the output schema will be treated as a separate piece of feedback.\\n\\u200bSave the evaluator\\nOnce your are finished configuring, save your changes.Was this page helpful?YesNoSuggest editsCode evaluatorComposite evaluatorsâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides/local', 'loc': 'https://docs.smith.langchain.com/evaluation/how_to_guides/local', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='How to run an evaluation locally (Python only) - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesDefine a target function to evaluateEvaluate intermediate stepsReturn multiple scores in one evaluatorReturn categorical vs numerical metricsRun evaluators on experimentsEvaluate with repetitionsHandle model rate limitsRun an evaluation locally (Python)Evaluate a runnableEvaluate a graphEvaluate an existing experiment (Python)Run an evaluation with multimodal contentSimulate multi-turn interactionsImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationEvaluation techniquesHow to run an evaluation locally (Python only)Get startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageExampleSet up evaluationsEvaluation techniquesHow to run an evaluation locally (Python only)Copy pageCopy pageSometimes it is helpful to run an evaluation locally without uploading any results to LangSmith. For example, if youâ€™re quickly iterating on a prompt and want to smoke test it on a few examples, or if youâ€™re validating that your target and evaluator functions are defined correctly, you may not want to record these evaluations.\\nYou can do this by using the LangSmith Python SDK and passing upload_results=False to evaluate() / aevaluate().\\nThis will run you application and evaluators exactly as it always does and return the same output, but nothing will be recorded to LangSmith. This includes not just the experiment results but also the application and evaluator traces.\\n\\u200bExample\\nLetâ€™s take a look at an example:\\nRequires langsmith>=0.2.0. Example also uses pandas.\\nCopyfrom langsmith import Client\\n\\n# 1. Create and/or select your dataset\\nls_client = Client()\\ndataset = ls_client.clone_public_dataset(\\n    \"https://smith.langchain.com/public/a63525f9-bdf2-4512-83e3-077dc9417f96/d\"\\n)\\n\\n# 2. Define an evaluator\\ndef is_concise(outputs: dict, reference_outputs: dict) -> bool:\\n    return len(outputs[\"answer\"]) < (3 * len(reference_outputs[\"answer\"]))\\n\\n# 3. Define the interface to your app\\ndef chatbot(inputs: dict) -> dict:\\n    return {\"answer\": inputs[\"question\"] + \" is a good question. I don\\'t know the answer.\"}\\n\\n# 4. Run an evaluation\\nexperiment = ls_client.evaluate(\\n    chatbot,\\n    data=dataset,\\n    evaluators=[is_concise],\\n    experiment_prefix=\"my-first-experiment\",\\n    # \\'upload_results\\' is the relevant arg.\\n    upload_results=False\\n)\\n\\n# 5. Analyze results locally\\nresults = list(experiment)\\n\\n# Check if \\'is_concise\\' returned False.\\nfailed = [r for r in results if not r[\"evaluation_results\"][\"results\"][0].score]\\n\\n# Explore the failed inputs and outputs.\\nfor r in failed:\\n    print(r[\"example\"].inputs)\\n    print(r[\"run\"].outputs)\\n\\n# Explore the results as a Pandas DataFrame.\\n# Must have \\'pandas\\' installed.\\ndf = experiment.to_pandas()\\ndf[[\"inputs.question\", \"outputs.answer\", \"reference.answer\", \"feedback.is_concise\"]]\\n\\nCopy{\\'question\\': \\'What is the largest mammal?\\'}\\n{\\'answer\\': \"What is the largest mammal? is a good question. I don\\'t know the answer.\"}\\n{\\'question\\': \\'What do mammals and birds have in common?\\'}\\n{\\'answer\\': \"What do mammals and birds have in common? is a good question. I don\\'t know the answer.\"}\\n\\ninputs.questionoutputs.answerreference.answerfeedback.is_concise0What is the largest mammal?What is the largest mammal? is a good question. I donâ€™t know the answer.The blue whaleFalse1What do mammals and birds have in common?What do mammals and birds have in common? is a good question. I donâ€™t know the answer.They are both warm-bloodedFalseWas this page helpful?YesNoSuggest editsHandle model rate limitsEvaluate a runnableâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application', 'loc': 'https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Create and manage datasets in the UI - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetWith the UIWith the SDKManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationCreate a datasetCreate and manage datasets in the UIGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageCreate a dataset and add examplesManually from a tracing projectAutomatically from a tracing projectFrom examples in an Annotation QueueFrom the Prompt PlaygroundImport a dataset from a CSV or JSONL fileCreate a new dataset from the Datasets & Experiments pageAdd synthetic examples created by an LLMManage a datasetCreate a dataset schemaCreate and manage dataset splitsEdit example metadataFilter examplesDatasetsCreate a datasetCreate and manage datasets in the UICopy pageCopy pageDatasets enable you to perform repeatable evaluations over time using consistent data. Datasets are made up of examples, which store inputs, outputs, and optionally, reference outputs.\\nThis page outlines the various methods for creating and managing datasets in the LangSmith UI.\\n\\u200bCreate a dataset and add examples\\nThe following sections explain the different ways you can create a dataset in LangSmith and add examples to it. Depending on your workflow, you can manually curate examples, automatically capture them from tracing, import files, or even generate synthetic data:\\n\\nManually from a tracing project\\nAutomatically from a tracing project\\nFrom examples in an Annotation Queue\\nFrom the Prompt Playground\\nImport a dataset from a CSV or JSONL file\\nCreate a new dataset from the dataset page\\nAdd synthetic examples created by an LLM via the Datasets UI\\n\\n\\u200bManually from a tracing project\\nA common pattern for constructing datasets is to convert notable traces from your application into dataset examples. This approach requires that you have configured tracing to LangSmith.\\nA technique to build datasets is to filter the most interesting traces, such as traces that were tagged with poor user feedback, and add them to a dataset. For tips on how to filter traces, refer to Filter traces guide.\\nThere are two ways to add data manually from a tracing project to datasets. Navigate to Tracing Projects and select a project.\\n\\n\\nMulti-select runs from the runs table. On the Runs tab, multi-select runs. At the bottom of the page, click  Add to Dataset.\\n\\n\\n\\nOn the Runs tab, select a run from the table. On the individual run details page, select  Add to -> Dataset in the top right corner.\\n\\nWhen you select a dataset from the run details page, a modal will pop up letting you know if any transformations were applied or if schema validation failed. For example, the screenshot below shows a dataset that is using transformations to optimize for collecting LLM runs.\\n\\nYou can then optionally edit the run before adding it to the dataset.\\n\\n\\n\\u200bAutomatically from a tracing project\\nYou can use run rules to automatically add traces to a dataset based on certain conditions. For example, you could add all traces that are tagged with a specific use case or have a low feedback score.\\n\\u200bFrom examples in an Annotation Queue\\nIf you rely on subject matter experts to build meaningful datasets, use annotation queues to provide a streamlined view for reviewers. Human reviewers can optionally modify the inputs/outputs/reference outputs from a trace before it is added to the dataset.\\nAnnotation queues can be optionally configured with a default dataset, though you can add runs to any dataset by using the dataset switcher on the bottom of the screen. Once you select the right dataset, click Add to Dataset or hit the hot key D to add the run to it.\\nAny modifications you make to the run in your annotation queue will carry over to the dataset, and all metadata associated with the run will also be copied.\\n\\nNote you can also set up rules to add runs that meet specific criteria to an annotation queue using automation rules.\\n\\u200bFrom the Prompt Playground\\nOn the Prompt Playground page, select Set up Evaluation, click +New if youâ€™re starting a new dataset or select from an existing dataset.\\nCreating datasets inline in the playground is not supported for datasets that have nested keys. In order to add/edit examples with nested keys, you must edit from the datasets page.\\nTo edit the examples:\\n\\nUse +Row to add a new example to the dataset\\nDelete an example using the â‹® dropdown on the right hand side of the table\\nIf youâ€™re creating a reference-free dataset remove the â€œReference Outputâ€ column using the x button in the column. Note: this action is not reversible.\\n\\n\\n\\u200bImport a dataset from a CSV or JSONL file\\nOn the Datasets & Experiments page, click +New Dataset, then Import an existing dataset from CSV or JSONL file.\\n\\u200bCreate a new dataset from the Datasets & Experiments page\\n\\nNavigate to the Datasets & Experiments page from the left-hand menu.\\nClick + New Dataset.\\nOn the New Dataset page, select the Create from scratch tab.\\nAdd a name and description for the dataset.\\n(Optional) Create a dataset schema to validate your dataset.\\nClick Create, which will create an empty dataset.\\nTo add examples inline, on the datasetâ€™s page, go to the Examples tab. Click + Example.\\nDefine examples in JSON and click Submit. For more details on dataset splits, refer to Create and manage dataset splits.\\n\\n\\u200bAdd synthetic examples created by an LLM\\nIf you have existing examples and a schema defined on your dataset, when you click + Example there is an option to  Add AI-Generated Examples. This will use an LLM to create synthetic examples.\\nIn Generate examples, do the following:\\n\\n\\nClick API Key in the top right of the pane to set your OpenAI API key as a workspace secret. If your workspace already has an OpenAI API key set, you can skip this step.\\n\\n\\nSelect few-shot examples: Toggle Automatic or Manual reference examples. You can select these examples manually from your dataset or use the automatic selection option.\\n\\n\\nEnter the number of synthetic examples you want to generate.\\n\\n\\nClick Generate.\\n\\n\\n\\nThe examples will appear on the Select generated examples page. Choose which examples to add to your dataset, with the option to edit them before finalizing. Click Save Examples.\\n\\n\\nEach example will be validated against your specified dataset schema and tagged as synthetic in the source metadata.\\n\\n\\n\\n\\u200bManage a dataset\\n\\u200bCreate a dataset schema\\nLangSmith datasets store arbitrary JSON objects. We recommend (but do not require) that you define a schema for your dataset to ensure that they conform to a specific JSON schema. Dataset schemas are defined with standard JSON schema, with the addition of a few prebuilt types that make it easier to type common primitives like messages and tools.\\nCertain fields in your schema have a + Transformations option. Transformations are preprocessing steps that, if enabled, update your examples when you add them to the dataset. For example the convert to OpenAI messages transformation will convert message-like objects, like LangChain messages, to OpenAI message format.\\nFor the full list of available transformations, see our reference.\\nIf you plan to collect production traces in your dataset from LangChain ChatModels or from OpenAI calls using the LangSmith OpenAI wrapper, we offer a prebuilt Chat Model schema that converts messages and tools into industry standard openai formats that can be used downstream with any model for testing. You can also customize the template settings to match your use case.Please see the dataset transformations reference for more information.\\n\\u200bCreate and manage dataset splits\\nDataset splits are divisions of your dataset that you can use to segment your data. For example, it is common in machine learning workflows to split datasets into training, validation, and test sets. This can be useful to prevent overfitting - where a model performs well on the training data but poorly on unseen data. In evaluation workflows, it can be useful to do this when you have a dataset with multiple categories that you may want to evaluate separately; or if you are testing a new use case that you may want to include in your dataset in the future, but want to keep separate for now. Note that the same effect can be achieved manually via metadata - but we expect splits to be used for higher level organization of your dataset to split it into separate groups for evaluation, whereas metadata would be used more for storing information on your examples like tags and information about its origin.\\nIn machine learning, it is best practice to keep your splits separate (each example belongs to exactly one split). However, we allow you to select multiple splits for the same example in LangSmith because it can make sense for some evaluation workflows - for example, if an example falls into multiple categories on which you may want to evaluate your application.\\nIn order to create and manage splits in the app, you can select some examples in your dataset and click â€œAdd to Splitâ€. From the resulting popup menu, you can select and unselect splits for the selected examples, or create a new split.\\n\\n\\u200bEdit example metadata\\nYou can add metadata to your examples by clicking on an example and then clicking â€œEditâ€ on the top righthand side of the popover. From this page, you can update/delete existing metadata, or add new metadata. You may use this to store information about your examples, such as tags or version info, which you can then group by when analyzing experiment results or filter by when you call list_examples in the SDK.\\n\\n\\u200bFilter examples\\nYou can filter examples by split, metadata key/value or perform full-text search over examples. These filtering options are available to the top left of the examples table.\\n\\nFilter by split: Select split > Select a split to filter by\\nFilter by metadata: Filters > Select â€œMetadataâ€ from the dropdown > Select the metadata key and value to filter on\\nFull-text search: Filters > Select â€œFull Textâ€ from the dropdown > Enter your search criteria\\n\\nYou may add multiple filters, and only examples that satisfy all of the filters will be displayed in the table.\\nWas this page helpful?YesNoSuggest editsEvaluation approachesWith the SDKâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically', 'loc': 'https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='How to create and manage datasets programmatically - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetWith the UIWith the SDKManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationCreate a datasetHow to create and manage datasets programmaticallyGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageCreate a datasetCreate a dataset from list of valuesCreate a dataset from tracesCreate a dataset from a CSV fileCreate a dataset from pandas DataFrame (Python only)Fetch datasetsQuery all datasetsList datasets by nameList datasets by typeFetch examplesList all examples for a datasetList examples by idList examples by metadataList examples by structured filterUpdate examplesUpdate single exampleBulk update examplesDatasetsCreate a datasetHow to create and manage datasets programmaticallyCopy pageCopy pageYou can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.\\n\\u200bCreate a dataset\\n\\u200bCreate a dataset from list of values\\nThe most flexible way to make a dataset using the client is by creating examples from a list of inputs and optional outputs. Below is an example.\\nNote that you can add arbitrary metadata to each example, such as a note or a source. The metadata is stored as a dictionary.\\nIf you have many examples to create, consider using the create_examples/createExamples method to create multiple examples in a single request. If creating a single example, you can use the create_example/createExample method.\\nPythonTypeScriptCopyfrom langsmith import Client\\n\\nexamples = [\\n  {\\n    \"inputs\": {\"question\": \"What is the largest mammal?\"},\\n    \"outputs\": {\"answer\": \"The blue whale\"},\\n    \"metadata\": {\"source\": \"Wikipedia\"},\\n  },\\n  {\\n    \"inputs\": {\"question\": \"What do mammals and birds have in common?\"},\\n    \"outputs\": {\"answer\": \"They are both warm-blooded\"},\\n    \"metadata\": {\"source\": \"Wikipedia\"},\\n  },\\n  {\\n    \"inputs\": {\"question\": \"What are reptiles known for?\"},\\n    \"outputs\": {\"answer\": \"Having scales\"},\\n    \"metadata\": {\"source\": \"Wikipedia\"},\\n  },\\n  {\\n    \"inputs\": {\"question\": \"What\\'s the main characteristic of amphibians?\"},\\n    \"outputs\": {\"answer\": \"They live both in water and on land\"},\\n    \"metadata\": {\"source\": \"Wikipedia\"},\\n  },\\n]\\n\\nclient = Client()\\ndataset_name = \"Elementary Animal Questions\"\\n\\n# Storing inputs in a dataset lets us\\n# run chains and LLMs over a shared set of examples.\\ndataset = client.create_dataset(\\n  dataset_name=dataset_name, description=\"Questions and answers about animal phylogenetics.\",\\n)\\n\\n# Prepare inputs, outputs, and metadata for bulk creation\\nclient.create_examples(\\n  dataset_id=dataset.id,\\n  examples=examples\\n)\\n\\n\\u200bCreate a dataset from traces\\nTo create datasets from the runs (spans) of your traces, you can use the same approach. For many more examples of how to fetch and filter runs, see the export traces guide. Below is an example:\\nPythonTypeScriptCopyfrom langsmith import Client\\n\\nclient = Client()\\ndataset_name = \"Example Dataset\"\\n\\n# Filter runs to add to the dataset\\nruns = client.list_runs(\\n  project_name=\"my_project\",\\n  is_root=True,\\n  error=False,\\n)\\n\\ndataset = client.create_dataset(dataset_name, description=\"An example dataset\")\\n\\n# Prepare inputs and outputs for bulk creation\\nexamples = [{\"inputs\": run.inputs, \"outputs\": run.outputs} for run in runs]\\n\\n# Use the bulk create_examples method\\nclient.create_examples(\\n  dataset_id=dataset.id,\\n  examples=examples\\n)\\n\\n\\u200bCreate a dataset from a CSV file\\nIn this section, we will demonstrate how you can create a dataset by uploading a CSV file.\\nFirst, ensure your CSV file is properly formatted with columns that represent your input and output keys. These keys will be utilized to map your data properly during the upload. You can specify an optional name and description for your dataset. Otherwise, the file name will be used as the dataset name and no description will be provided.\\nPythonTypeScriptCopyfrom langsmith import Client\\nimport os\\n\\nclient = Client()\\ncsv_file = \\'path/to/your/csvfile.csv\\'\\ninput_keys = [\\'column1\\', \\'column2\\'] # replace with your input column names\\noutput_keys = [\\'output1\\', \\'output2\\'] # replace with your output column names\\n\\ndataset = client.upload_csv(\\n  csv_file=csv_file,\\n  input_keys=input_keys,\\n  output_keys=output_keys,\\n  name=\"My CSV Dataset\",\\n  description=\"Dataset created from a CSV file\",\\n  data_type=\"kv\"\\n)\\n\\n\\u200bCreate a dataset from pandas DataFrame (Python only)\\nThe python client offers an additional convenience method to upload a dataset from a pandas dataframe.\\nCopyfrom langsmith import Client\\nimport os\\nimport pandas as pd\\n\\nclient = Client()\\ndf = pd.read_parquet(\\'path/to/your/myfile.parquet\\')\\ninput_keys = [\\'column1\\', \\'column2\\'] # replace with your input column names\\noutput_keys = [\\'output1\\', \\'output2\\'] # replace with your output column names\\n\\ndataset = client.upload_dataframe(\\n    df=df,\\n    input_keys=input_keys,\\n    output_keys=output_keys,\\n    name=\"My Parquet Dataset\",\\n    description=\"Dataset created from a parquet file\",\\n    data_type=\"kv\" # The default\\n)\\n\\n\\u200bFetch datasets\\nYou can programmatically fetch datasets from LangSmith using the list_datasets/listDatasets method in the Python and TypeScript SDKs. Below are some common calls.\\nInitialize the client before running the below code snippets.\\nPythonTypeScriptCopyfrom langsmith import Client\\n\\nclient = Client()\\n\\n\\u200bQuery all datasets\\nPythonTypeScriptCopydatasets = client.list_datasets()\\n\\n\\u200bList datasets by name\\nIf you want to search by the exact name, you can do the following:\\nPythonTypeScriptCopydatasets = client.list_datasets(dataset_name=\"My Test Dataset 1\")\\n\\nIf you want to do a case-invariant substring search, try the following:\\nPythonTypeScriptCopydatasets = client.list_datasets(dataset_name_contains=\"some substring\")\\n\\n\\u200bList datasets by type\\nYou can filter datasets by type. Below is an example querying for chat datasets.\\nPythonTypeScriptCopydatasets = client.list_datasets(data_type=\"chat\")\\n\\n\\u200bFetch examples\\nYou can programmatically fetch examples from LangSmith using the list_examples/listExamples method in the Python and TypeScript SDKs. Below are some common calls.\\nInitialize the client before running the below code snippets.\\nPythonTypeScriptCopyfrom langsmith import Client\\n\\nclient = Client()\\n\\n\\u200bList all examples for a dataset\\nYou can filter by dataset ID:\\nPythonTypeScriptCopyexamples = client.list_examples(dataset_id=\"c9ace0d8-a82c-4b6c-13d2-83401d68e9ab\")\\n\\nOr you can filter by dataset name (this must exactly match the dataset name you want to query)\\nPythonTypeScriptCopyexamples = client.list_examples(dataset_name=\"My Test Dataset\")\\n\\n\\u200bList examples by id\\nYou can also list multiple examples all by ID.\\nPythonTypeScriptCopyexample_ids = [\\n  \\'734fc6a0-c187-4266-9721-90b7a025751a\\',\\n  \\'d6b4c1b9-6160-4d63-9b61-b034c585074f\\',\\n  \\'4d31df4e-f9c3-4a6e-8b6c-65701c2fed13\\',\\n]\\n\\nexamples = client.list_examples(example_ids=example_ids)\\n\\n\\u200bList examples by metadata\\nYou can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair. Under the hood, we check to see if the exampleâ€™s metadata contains the key-value pair(s) you specify.\\nFor example, if you have an example with metadata {\"foo\": \"bar\", \"baz\": \"qux\"}, both {foo: bar} and {baz: qux} would match, as would {foo: bar, baz: qux}.\\nPythonTypeScriptCopyexamples = client.list_examples(dataset_name=dataset_name, metadata={\"foo\": \"bar\"})\\n\\n\\u200bList examples by structured filter\\nSimilar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples.\\nThis is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for metadata fields.\\nYou can use the has operator to fetch examples with metadata fields that contain specific key/value pairs and the exists operator to fetch examples with metadata fields that contain a specific key. Additionally, you can also chain multiple filters together using the and operator and negate a filter using the not operator.\\nPythonTypeScriptCopyexamples = client.list_examples(\\n  dataset_name=dataset_name,\\n  filter=\\'and(not(has(metadata, \\\\\\'{\"foo\": \"bar\"}\\\\\\')), exists(metadata, \"tenant_id\"))\\'\\n)\\n\\n\\u200bUpdate examples\\n\\u200bUpdate single example\\nYou can programmatically update examples from LangSmith using the update_example/updateExample method in the Python and TypeScript SDKs. Below is an example.\\nPythonTypeScriptCopyclient.update_example(\\n  example_id=example.id,\\n  inputs={\"input\": \"updated input\"},\\n  outputs={\"output\": \"updated output\"},\\n  metadata={\"foo\": \"bar\"},\\n  split=\"train\"\\n)\\n\\n\\u200bBulk update examples\\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.\\nPythonTypeScriptCopyclient.update_examples(\\n  example_ids=[example.id, example_2.id],\\n  inputs=[{\"input\": \"updated input 1\"}, {\"input\": \"updated input 2\"}],\\n  outputs=[\\n      {\"output\": \"updated output 1\"},\\n      {\"output\": \"updated output 2\"},\\n  ],\\n  metadata=[{\"foo\": \"baz\"}, {\"foo\": \"qux\"}],\\n  splits=[[\"training\", \"foo\"], \"training\"] # Splits can be arrays or standalone strings\\n)\\nWas this page helpful?YesNoSuggest editsWith the UIManage datasetsâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type', 'loc': 'https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='How to return categorical vs numerical metrics - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesDefine a target function to evaluateEvaluate intermediate stepsReturn multiple scores in one evaluatorReturn categorical vs numerical metricsRun evaluators on experimentsEvaluate with repetitionsHandle model rate limitsRun an evaluation locally (Python)Evaluate a runnableEvaluate a graphEvaluate an existing experiment (Python)Run an evaluation with multimodal contentSimulate multi-turn interactionsImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationEvaluation techniquesHow to return categorical vs numerical metricsGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageRelatedSet up evaluationsEvaluation techniquesHow to return categorical vs numerical metricsCopy pageCopy pageLangSmith supports both categorical and numerical metrics, and you can return either when writing a custom evaluator.\\nFor an evaluator result to be logged as a numerical metric, it must returned as:\\n\\n(Python only) an int, float, or bool\\na dict of the form {\"key\": \"metric_name\", \"score\": int | float | bool}\\n\\nFor an evaluator result to be logged as a categorical metric, it must be returned as:\\n\\n(Python only) a str\\na dict of the form {\"key\": \"metric_name\", \"value\": str | int | float | bool}\\n\\nHere are some examples:\\n\\nPython: Requires langsmith>=0.2.0\\nTypeScript: Support for multiple scores is available in langsmith@0.1.32 and higher\\n\\nPythonTypeScriptCopydef numerical_metric(inputs: dict, outputs: dict, reference_outputs: dict) -> float:\\n    # Evaluation logic...\\n    return 0.8\\n    # Equivalently\\n    # return {\"score\": 0.8}\\n    # Or\\n    # return {\"key\": \"numerical_metric\", \"score\": 0.8}\\n\\ndef categorical_metric(inputs: dict, outputs: dict, reference_outputs: dict) -> str:\\n    # Evaluation logic...\\n    return \"english\"\\n    # Equivalently\\n    # return {\"key\": \"categorical_metric\", \"score\": \"english\"}\\n    # Or\\n    # return {\"score\": \"english\"}\\n\\n\\u200bRelated\\n\\nReturn multiple metrics in one evaluator\\nWas this page helpful?YesNoSuggest editsReturn multiple scores in one evaluatorRun evaluators on experimentsâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides/multi_turn_simulation', 'loc': 'https://docs.smith.langchain.com/evaluation/how_to_guides/multi_turn_simulation', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='How to simulate multi-turn interactions - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesDefine a target function to evaluateEvaluate intermediate stepsReturn multiple scores in one evaluatorReturn categorical vs numerical metricsRun evaluators on experimentsEvaluate with repetitionsHandle model rate limitsRun an evaluation locally (Python)Evaluate a runnableEvaluate a graphEvaluate an existing experiment (Python)Run an evaluation with multimodal contentSimulate multi-turn interactionsImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationEvaluation techniquesHow to simulate multi-turn interactionsGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageSetupRunning a simulationRunning in LangSmith experimentsUsing pytest or Vitest/JestUsing evaluateModifying the simulated user personaNext StepsSet up evaluationsEvaluation techniquesHow to simulate multi-turn interactionsCopy pageCopy page\\nMulti-turn interactions\\nEvaluators\\nLLM-as-judge\\nOpenEvals\\n\\nAI applications with conversational interfaces, like chatbots, operate over multiple interactions with a user, also called conversation turns. When evaluating the performance of such applications, core concepts such as building a dataset and defining evaluators and metrics to judge your app outputs remain useful. However, you may also find it useful to run a simulation between your app and a user, then evaluate this dynamically created trajectory.\\nSome advantages of doing this are:\\n\\nEase of getting started vs. an evaluation over a full dataset of pre-existing trajectories\\nEnd-to-end coverage from an initial query until a successful or unsuccessful resolution\\nThe ability to detect repetitive behavior or context loss over several iterations of your app\\n\\nThe downside is that because you are broadening your evaluation surface area to contain multiple turns, there is less consistency than evaluating a single output from your app given a static input from a dataset.\\n\\nThis guide will show you how to simulate multi-turn interactions and evaluate them using the open-source openevals package, which contains prebuilt evaluators and other convenient resources for evaluating your AI apps. It will also use OpenAI models, though you can use other providers as well.\\n\\u200bSetup\\nFirst, ensure you have the required dependencies installed:\\nPythonTypeScriptCopypip install -U langsmith openevals\\n\\nIf you are using yarn as your package manager, you will also need to manually install @langchain/core as a peer dependency of openevals. This is not required for LangSmith evals in general.\\nAnd set up your environment variables:\\nCopyexport LANGSMITH_TRACING=\"true\"\\nexport LANGSMITH_API_KEY=\"<Your LangSmith API key>\"\\nexport OPENAI_API_KEY=\"<Your OpenAI API key>\"\\n\\n\\u200bRunning a simulation\\nThere are two primary components youâ€™ll need to get started:\\n\\napp: Your application, or a function wrapping it. Must accept a single chat message (dict with â€œroleâ€ and â€œcontentâ€ keys) as an input arg and a thread_id as a kwarg. Should accept other kwargs as more may be added in future releases. Returns a chat message as output with at least role and content keys.\\nuser: The simulated user. In this guide, we will use an imported prebuilt function named create_llm_simulated_user which uses an LLM to generate user responses, though you can create your own too.\\n\\nThe simulator in openevals passes a single chat message to your app from the user for each turn. Therefore you should statefully track the current history internally based on thread_id if needed.\\nHereâ€™s an example that simulates a multi-turn customer support interaction. This guide uses a simple chat app that wraps a single call to the OpenAI chat completions API, however this is where you would call your application or agent. In this example, our simulated user is playing the role of a particularly aggressive customer:\\nPythonTypeScriptCopyfrom openevals.simulators import run_multiturn_simulation, create_llm_simulated_user\\nfrom openevals.types import ChatCompletionMessage\\nfrom langsmith.wrappers import wrap_openai\\nfrom openai import OpenAI\\n\\n# Wrap OpenAI client for tracing\\nclient = wrap_openai(OpenAI())\\nhistory = {}\\n\\n# Your application logic\\ndef app(inputs: ChatCompletionMessage, *, thread_id: str, **kwargs):\\n    if thread_id not in history:\\n        history[thread_id] = []\\n    history[thread_id].append(inputs)\\n    # inputs is a message object with role and content\\n    res = client.chat.completions.create(\\n        model=\"gpt-4.1-mini\",\\n        messages=[\\n            {\\n                \"role\": \"system\",\\n                \"content\": \"You are a patient and understanding customer service agent.\",\\n            },\\n        ] + history[thread_id],\\n    )\\n    response_message = res.choices[0].message\\n    history[thread_id].append(response_message)\\n    return response_message\\n\\nuser = create_llm_simulated_user(\\n    system=\"You are an aggressive and hostile customer who wants a refund for their car.\",\\n    model=\"openai:gpt-4.1-mini\",\\n)\\n\\n# Run the simulation directly with the new function\\nsimulator_result = run_multiturn_simulation(\\n    app=app,\\n    user=user,\\n    max_turns=5,\\n)\\nprint(simulator_result)\\n\\nThe response looks like this:\\nCopy{\\n  \"trajectory\": [\\n    {\\n      \"role\": \"user\",\\n      \"content\": \"This piece of junk car is a complete disaster! I demand a full refund immediately. How dare you sell me such a worthless vehicle!\",\\n      \"id\": \"chatcmpl-BUpXa07LaM7wXbyaNnng1Gtn5Dsbh\"\\n    },\\n    {\\n      \"role\": \"assistant\",\\n      \"content\": \"I\\'m really sorry to hear about your experience and understand how frustrating this must be. I\\'d like to help resolve this issue as smoothly as possible. Could you please provide some details about the problem with the vehicle? Once I have more information, I\\'ll do my best to assist you with a solution, whether it\\'s a refund or other options. Thank you for your patience.\",\\n      \"refusal\": null,\\n      \"annotations\": [],\\n      \"id\": \"d7520f6a-7cf8-46f8-abe4-7df04f134482\"\\n    },\\n    \"...\",\\n    {\\n      \"role\": \"assistant\",\\n      \"content\": \"I truly understand your frustration and sincerely apologize for the inconvenience you\\'ve experienced.\\\\n\\\\nPlease allow me a moment to review your case, and I will do everything I can to expedite your refund. Your patience is greatly appreciated, and I am committed to resolving this matter to your satisfaction.\",\\n      \"refusal\": null,\\n      \"annotations\": [],\\n      \"id\": \"a0536d4f-9353-4cfa-84df-51c8d29e076d\"\\n    }\\n  ]\\n}\\n\\nThe simulation first generates an initial query from the simulated user, then passes response chat messages back and forth until it reaches max_turns (you can alternatively pass a stopping_condition that takes the current trajectory and returns True or False - see the OpenEvals README for more information). The return value is the final list of chat messages that make up the converationâ€™s trajectory.\\nThere are several ways to configure the simulated user, such as having it return fixed responses for the first turns of your simulation, as well as the simulation as a whole. For full details, check out the OpenEvals README.\\nThe final trace will look something like this with responses from your app and user interleaved:\\n\\nCongrats! You just ran your first multi-turn simulation. Next, weâ€™ll cover how to run it in a LangSmith experiment.\\n\\u200bRunning in LangSmith experiments\\nYou can use the results of multi-turn simulations as part of a LangSmith experiment to track performance and progress over time. For these sections, it helps to be familiar with at least one of LangSmithâ€™s pytest (Python-only), Vitest/Jest (JS only), or evaluate runners.\\n\\u200bUsing pytest or Vitest/Jest\\nSee the following guides to learn how to set up evals using LangSmithâ€™s integrations with test frameworks:\\npytest\\nVitest or Jest\\n\\nIf you are using one of the LangSmith test framework integrations, you can pass in an array of OpenEvals evaluators as a trajectory_evaluators param when running the simulation. These evaluators will run at the end of the simulation, taking the final list of chat messages as an outputs kwarg. Your passed trajectory_evaluator must therefore accept this kwarg.\\n\\nHereâ€™s an example:\\nPythonTypeScriptCopyfrom openevals.simulators import run_multiturn_simulation, create_llm_simulated_user\\nfrom openevals.llm import create_llm_as_judge\\nfrom openevals.types import ChatCompletionMessage\\nfrom langsmith import testing as t\\nfrom langsmith.wrappers import wrap_openai\\nfrom openai import OpenAI\\nimport pytest\\n\\n@pytest.mark.langsmith\\ndef test_multiturn_message_with_openai():\\n    inputs = {\"role\": \"user\", \"content\": \"I want a refund for my car!\"}\\n    t.log_inputs(inputs)\\n    # Wrap OpenAI client for tracing\\n    client = wrap_openai(OpenAI())\\n    history = {}\\n\\n    def app(inputs: ChatCompletionMessage, *, thread_id: str):\\n        if thread_id not in history:\\n            history[thread_id] = []\\n        history[thread_id] = history[thread_id] + [inputs]\\n        res = client.chat.completions.create(\\n            model=\"gpt-4.1-nano\",\\n            messages=[\\n                {\\n                    \"role\": \"system\",\\n                    \"content\": \"You are a patient and understanding customer service agent.\",\\n                }\\n            ]\\n            + history[thread_id],\\n        )\\n        response = res.choices[0].message\\n        history[thread_id].append(response)\\n        return response\\n\\n    user = create_llm_simulated_user(\\n        system=\"You are a nice customer who wants a refund for their car.\",\\n        model=\"openai:gpt-4.1-nano\",\\n        fixed_responses=[\\n            inputs,\\n        ],\\n    )\\n    trajectory_evaluator = create_llm_as_judge(\\n        model=\"openai:o3-mini\",\\n        prompt=\"Based on the below conversation, was the user satisfied?\\\\n{outputs}\",\\n        feedback_key=\"satisfaction\",\\n    )\\n    res = run_multiturn_simulation(\\n        app=app,\\n        user=user,\\n        trajectory_evaluators=[trajectory_evaluator],\\n        max_turns=5,\\n    )\\n    t.log_outputs(res)\\n    # Optionally, assert that the evaluator scored the interaction as satisfactory.\\n    # This will cause the overall test case to fail if \"score\" is False.\\n    assert res[\"evaluator_results\"][0][\"score\"]\\n\\nLangSmith will automatically detect and log the feedback returned from the passed trajectory_evaluators, adding it to the experiment. Note also that the test case uses the fixed_responses param on the simulated user to start the conversation with a specific input, which you can log and make part of your stored dataset.\\nYou may also find it convenient to have the simulated userâ€™s system prompt to be part of your logged dataset as well.\\n\\u200bUsing evaluate\\nYou can also use the evaluate runner to evaluate simulated multi-turn interactions. This will be a little bit different from the pytest/Vitest/Jest example in the following ways:\\n\\nThe simulation should be part of your target function, and your target function should return the final trajectory.\\n\\nThis will make the trajectory the outputs that LangSmith will pass to your evaluators.\\n\\n\\nInstead of using the trajectory_evaluators param, you should pass your evaluators as a param into the evaluate() method.\\nYou will need an existing dataset of inputs and (optionally) reference trajectories.\\n\\nHereâ€™s an example:\\nPythonTypeScriptCopyfrom openevals.simulators import run_multiturn_simulation, create_llm_simulated_user\\nfrom openevals.llm import create_llm_as_judge\\nfrom openevals.types import ChatCompletionMessage\\nfrom langsmith.wrappers import wrap_openai\\nfrom langsmith import Client\\nfrom openai import OpenAI\\n\\nls_client = Client()\\nexamples = [\\n    {\\n        \"inputs\": {\\n            \"messages\": [{ \"role\": \"user\", \"content\": \"I want a refund for my car!\" }]\\n        },\\n    },\\n]\\ndataset = ls_client.create_dataset(dataset_name=\"multiturn-starter\")\\nls_client.create_examples(\\n    dataset_id=dataset.id,\\n    examples=examples,\\n)\\ntrajectory_evaluator = create_llm_as_judge(\\n    model=\"openai:o3-mini\",\\n    prompt=\"Based on the below conversation, was the user satisfied?\\\\n{outputs}\",\\n    feedback_key=\"satisfaction\",\\n)\\n\\ndef target(inputs: dict):\\n    # Wrap OpenAI client for tracing\\n    client = wrap_openai(OpenAI())\\n    history = {}\\n\\n    def app(next_message: ChatCompletionMessage, *, thread_id: str):\\n        if thread_id not in history:\\n            history[thread_id] = []\\n        history[thread_id] = history[thread_id] + [next_message]\\n        res = client.chat.completions.create(\\n            model=\"gpt-4.1-nano\",\\n            messages=[\\n                {\\n                    \"role\": \"system\",\\n                    \"content\": \"You are a patient and understanding customer service agent.\",\\n                }\\n            ]\\n            + history[thread_id],\\n        )\\n        response = res.choices[0].message\\n        history[thread_id].append(response)\\n        return response\\n\\n    user = create_llm_simulated_user(\\n        system=\"You are a nice customer who wants a refund for their car.\",\\n        model=\"openai:gpt-4.1-nano\",\\n        fixed_responses=inputs[\"messages\"],\\n    )\\n    res = run_multiturn_simulation(\\n        app=app,\\n        user=user,\\n        max_turns=5,\\n    )\\n    return res[\"trajectory\"]\\n\\nresults = ls_client.evaluate(\\n    target,\\n    data=dataset.name,\\n    evaluators=[trajectory_evaluator],\\n)\\n\\n\\u200bModifying the simulated user persona\\nThe above examples run using the same simulated user persona for all input examples, defined by the system parameter passed into create_llm_simulated_user. If you would like to use a different persona for specific items in your dataset, you can update your dataset examples to also contain an extra field with the desired system prompt, then pass that field in when creating your simulated user like this:\\nPythonTypeScriptCopyfrom openevals.simulators import run_multiturn_simulation, create_llm_simulated_user\\nfrom openevals.llm import create_llm_as_judge\\nfrom openevals.types import ChatCompletionMessage\\nfrom langsmith.wrappers import wrap_openai\\nfrom langsmith import Client\\nfrom openai import OpenAI\\n\\nls_client = Client()\\nexamples = [\\n    {\\n        \"inputs\": {\\n            \"messages\": [{ \"role\": \"user\", \"content\": \"I want a refund for my car!\" }],\\n            \"simulated_user_prompt\": \"You are an angry and belligerent customer who wants a refund for their car.\"\\n        },\\n    },\\n    {\\n        \"inputs\": {\\n            \"messages\": [{ \"role\": \"user\", \"content\": \"Please give me a refund for my car.\" }],\\n            \"simulated_user_prompt\": \"You are a nice customer who wants a refund for their car.\",\\n        },\\n    }\\n]\\ndataset = ls_client.create_dataset(dataset_name=\"multiturn-with-personas\")\\nls_client.create_examples(\\n    dataset_id=dataset.id,\\n    examples=examples,\\n)\\ntrajectory_evaluator = create_llm_as_judge(\\n    model=\"openai:o3-mini\",\\n    prompt=\"Based on the below conversation, was the user satisfied?\\\\n{outputs}\",\\n    feedback_key=\"satisfaction\",\\n)\\n\\ndef target(inputs: dict):\\n    # Wrap OpenAI client for tracing\\n    client = wrap_openai(OpenAI())\\n    history = {}\\n\\n    def app(next_message: ChatCompletionMessage, *, thread_id: str):\\n        if thread_id not in history:\\n            history[thread_id] = []\\n        history[thread_id] = history[thread_id] + [next_message]\\n        res = client.chat.completions.create(\\n            model=\"gpt-4.1-nano\",\\n            messages=[\\n                {\\n                    \"role\": \"system\",\\n                    \"content\": \"You are a patient and understanding customer service agent.\",\\n                }\\n            ]\\n            + history[thread_id],\\n        )\\n        response = res.choices[0].message\\n        history[thread_id].append(response)\\n        return response\\n\\n    user = create_llm_simulated_user(\\n        system=inputs[\"simulated_user_prompt\"],\\n        model=\"openai:gpt-4.1-nano\",\\n        fixed_responses=inputs[\"messages\"],\\n    )\\n    res = run_multiturn_simulation(\\n        app=app,\\n        user=user,\\n        max_turns=5,\\n    )\\n    return res[\"trajectory\"]\\n\\nresults = ls_client.evaluate(\\n    target,\\n    data=dataset.name,\\n    evaluators=[trajectory_evaluator],\\n)\\n\\n\\u200bNext Steps\\nYouâ€™ve just seen some techniques for simulating multi-turn interactions and running them in LangSmith evals.\\nHere are some topics you might want to explore next:\\n\\nTrace multiturn conversations across different traces\\nUse multiple messages in the playground UI\\nReturn multiple metrics in one evaluator\\n\\nYou can also explore the OpenEvals readme for more on prebuilt evaluators.Was this page helpful?YesNoSuggest editsRun an evaluation with multimodal contentImprove LLM-as-judge evaluators using human feedbackâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores', 'loc': 'https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='How to return multiple scores in one evaluator - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesDefine a target function to evaluateEvaluate intermediate stepsReturn multiple scores in one evaluatorReturn categorical vs numerical metricsRun evaluators on experimentsEvaluate with repetitionsHandle model rate limitsRun an evaluation locally (Python)Evaluate a runnableEvaluate a graphEvaluate an existing experiment (Python)Run an evaluation with multimodal contentSimulate multi-turn interactionsImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationEvaluation techniquesHow to return multiple scores in one evaluatorGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageRelatedSet up evaluationsEvaluation techniquesHow to return multiple scores in one evaluatorCopy pageCopy pageSometimes it is useful for a custom evaluator or summary evaluator to return multiple metrics. For example, if you have multiple metrics being generated by an LLM judge, you can save time and money by making a single LLM call that generates multiple metrics instead of making multiple LLM calls.\\nTo return multiple scores using the Python SDK, simply return a list of dictionaries/objects of the following form:\\nCopy[\\n    # \\'key\\' is the metric name\\n    # \\'score\\' is the value of a numerical metric\\n    {\"key\": string, \"score\": number},\\n    # \\'value\\' is the value of a categorical metric\\n    {\"key\": string, \"value\": string},\\n    ... # You may log as many as you wish\\n]\\n\\nTo do so with the JS/TS SDK, return an object with a â€˜resultsâ€™ key and then a list of the above form\\nCopy{results: [{ key: string, score: number }, ...]};\\n\\nEach of these dictionaries can contain any or all of the feedback fields; check out the linked document for more information.\\nExample:\\n\\nPython: Requires langsmith>=0.2.0\\nTypeScript: Support for multiple scores is available in langsmith@0.1.32 and higher\\n\\nPythonTypeScriptCopydef multiple_scores(outputs: dict, reference_outputs: dict) -> list[dict]:\\n    # Replace with real evaluation logic.\\n    precision = 0.8\\n    recall = 0.9\\n    f1 = 0.85\\n    return [\\n        {\"key\": \"precision\", \"score\": precision},\\n        {\"key\": \"recall\", \"score\": recall},\\n        {\"key\": \"f1\", \"score\": f1},\\n    ]\\n\\nRows from the resulting experiment will display each of the scores.\\n\\n\\u200bRelated\\n\\nReturn categorical vs numerical metrics\\nWas this page helpful?YesNoSuggest editsEvaluate intermediate stepsReturn categorical vs numerical metricsâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators', 'loc': 'https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='How to use prebuilt evaluators - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationWith the SDKWith the UIUse prebuilt evaluatorsEvaluation typesFrameworks & integrationsEvaluation techniquesImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationRun an evaluationHow to use prebuilt evaluatorsGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageSetupRunning an evaluatorSet up evaluationsRun an evaluationHow to use prebuilt evaluatorsCopy pageCopy pageLangSmith integrates with the open-source openevals package to provide a suite of prebuilt evaluators that you can use as starting points for evaluation.\\nThis how-to guide will demonstrate how to set up and run one type of evaluator (LLM-as-a-judge). For a complete list of prebuilt evaluators with usage examples, refer to the openevals and agentevals repos.\\n\\u200bSetup\\nYouâ€™ll need to install the openevals package to use the pre-built LLM-as-a-judge evaluator.\\nPythonTypeScriptCopypip install -U openevals\\n\\nYouâ€™ll also need to set your OpenAI API key as an environment variable, though you can choose different providers too:\\nCopyexport OPENAI_API_KEY=\"your_openai_api_key\"\\n\\nWeâ€™ll also use LangSmithâ€™s pytest integration for Python and Vitest/Jest for TypeScript to run our evals. openevals also integrates seamlessly with the evaluate method as well. See the appropriate guides for setup instructions.\\n\\u200bRunning an evaluator\\nThe general flow is simple: import the evaluator or factory function from openevals, then run it within your test file with inputs, outputs, and reference outputs. LangSmith will automatically log the evaluatorâ€™s results as feedback.\\nNote that not all evaluators will require each parameter (the exact match evaluator only requires outputs and reference outputs, for example). Additionally, if your LLM-as-a-judge prompt requires additional variables, passing them in as kwargs will format them into the prompt.\\nSet up your test file like this:\\nPythonTypeScriptCopyimport pytest\\nfrom langsmith import testing as t\\nfrom openevals.llm import create_llm_as_judge\\nfrom openevals.prompts import CORRECTNESS_PROMPT\\n\\ncorrectness_evaluator = create_llm_as_judge(\\n    prompt=CORRECTNESS_PROMPT,\\n    feedback_key=\"correctness\",\\n    model=\"openai:o3-mini\",\\n)\\n\\n# Mock standin for your application\\ndef my_llm_app(inputs: dict) -> str:\\n    return \"Doodads have increased in price by 10% in the past year.\"\\n\\n@pytest.mark.langsmith\\ndef test_correctness():\\n    inputs = \"How much has the price of doodads changed in the past year?\"\\n    reference_outputs = \"The price of doodads has decreased by 50% in the past year.\"\\n    outputs = my_llm_app(inputs)\\n\\n    t.log_inputs({\"question\": inputs})\\n    t.log_outputs({\"answer\": outputs})\\n    t.log_reference_outputs({\"answer\": reference_outputs})\\n\\n    correctness_evaluator(\\n        inputs=inputs,\\n        outputs=outputs,\\n        reference_outputs=reference_outputs\\n    )\\n\\nThe feedback_key/feedbackKey parameter will be used as the name of the feedback in your experiment.\\nRunning the eval in your terminal will result in something like the following:\\n\\nYou can also pass prebuilt evaluators directly into the evaluate method if you have already created a dataset in LangSmith. If using Python, this requires langsmith>=0.3.11:\\nPythonTypeScriptCopyfrom langsmith import Client\\nfrom openevals.llm import create_llm_as_judge\\nfrom openevals.prompts import CONCISENESS_PROMPT\\n\\nclient = Client()\\nconciseness_evaluator = create_llm_as_judge(\\n    prompt=CONCISENESS_PROMPT,\\n    feedback_key=\"conciseness\",\\n    model=\"openai:o3-mini\",\\n)\\n\\nexperiment_results = client.evaluate(\\n    # This is a dummy target function, replace with your actual LLM-based system\\n    lambda inputs: \"What color is the sky?\",\\n    data=\"Sample dataset\",\\n    evaluators=[\\n        conciseness_evaluator\\n    ]\\n)\\n\\nFor a complete list of available evaluators, see the openevals and agentevals repos.Was this page helpful?YesNoSuggest editsWith the UICode evaluatorâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides/pytest', 'loc': 'https://docs.smith.langchain.com/evaluation/how_to_guides/pytest', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='How to run evaluations with pytest (beta) - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsRun an evaluation asynchronouslyRun evaluations with pytestRun evals with Vitest/JestWith the APIEvaluation techniquesImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationFrameworks & integrationsHow to run evaluations with pytest (beta)Get startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageInstallationDefine and run testsLog inputs, outputs, and reference outputsLog feedbackTrace intermediate callsGrouping tests into a test suiteNaming experimentsCachingpytest featuresParametrize with pytest.mark.parametrizeParallelize with pytest-xdistAsync tests with pytest-asyncioWatch mode with pytest-watchRich outputsDry-run modeExpectationsLegacy@test / @unit decoratorSet up evaluationsFrameworks & integrationsHow to run evaluations with pytest (beta)Copy pageCopy pageThe LangSmith pytest plugin lets Python developers define their datasets and evaluations as pytest test cases. Compared to the standard evaluation flow, this is useful when:\\n\\nEach example requires different evaluation logic\\nYou want to assert binary expectations, and both track these assertions in LangSmith and raise assertion errors locally (e.g. in CI pipelines)\\nYou want pytest-like terminal outputs\\nYou already use pytest to test your app and want to add LangSmith tracking\\n\\nThe pytest integration is in beta and is subject to change in upcoming releases.\\nThe JS/TS SDK has an analogous Vitest/Jest integration.\\n\\u200bInstallation\\nThis functionality requires Python SDK version langsmith>=0.3.4.\\nFor extra features like rich terminal outputs and test caching install:\\npipuvCopypip install -U \"langsmith[pytest]\"\\n\\n\\u200bDefine and run tests\\nThe pytest integration lets you define datasets and evaluators as test cases.\\nTo track a test in LangSmith add the @pytest.mark.langsmith decorator. Every decorated test case will be synced to a dataset example. When you run the test suite, the dataset will be updated and a new experiment will be created with one result for each test case.\\nPythonCopy###################### my_app/main.py ######################\\nimport openai\\nfrom langsmith import traceable, wrappers\\n\\noai_client = wrappers.wrap_openai(openai.OpenAI())\\n\\n@traceable\\ndef generate_sql(user_query: str) -> str:\\n    result = oai_client.chat.completions.create(\\n        model=\"gpt-4o-mini\",\\n        messages=[\\n            {\"role\": \"system\", \"content\": \"Convert the user query to a SQL query.\"},\\n            {\"role\": \"user\", \"content\": user_query},\\n        ],\\n    )\\n    return result.choices[0].message.content\\n\\n###################### tests/test_my_app.py ######################\\nimport pytest\\nfrom langsmith import testing as t\\n\\ndef is_valid_sql(query: str) -> bool:\\n    \"\"\"Return True if the query is valid SQL.\"\"\"\\n    return True  # Dummy implementation\\n\\n@pytest.mark.langsmith  # <-- Mark as a LangSmith test case\\ndef test_sql_generation_select_all() -> None:\\n    user_query = \"Get all users from the customers table\"\\n    t.log_inputs({\"user_query\": user_query})  # <-- Log example inputs, optional\\n    expected = \"SELECT * FROM customers;\"\\n    t.log_reference_outputs({\"sql\": expected})  # <-- Log example reference outputs, optional\\n\\n    sql = generate_sql(user_query)\\n    t.log_outputs({\"sql\": sql})  # <-- Log run outputs, optional\\n\\n    t.log_feedback(key=\"valid_sql\", score=is_valid_sql(sql))  # <-- Log feedback, optional\\n    assert sql == expected  # <-- Test pass/fail status automatically logged to LangSmith under \\'pass\\' feedback key\\n\\nWhen you run this test it will have a default pass boolean feedback key based on the test case passing / failing. It will also track any inputs, outputs, and reference (expected) outputs that you log.\\nUse pytest as you normally would to run the tests:\\nCopypytest tests/\\n\\nIn most cases we recommend setting a test suite name:\\nCopyLANGSMITH_TEST_SUITE=\\'SQL app tests\\' pytest tests/\\n\\nEach time you run this test suite, LangSmith:\\n\\ncreates a dataset for each test file. If a dataset for this test file already exists it will be updated\\ncreates an experiment in each created/updated dataset\\ncreates an experiment row for each test case, with the inputs, outputs, reference outputs and feedback youâ€™ve logged\\ncollects the pass/fail rate under the pass feedback key for each test case\\n\\nHereâ€™s what a test suite dataset looks like:\\n\\nAnd what an experiment against that test suite looks like:\\n\\n\\u200bLog inputs, outputs, and reference outputs\\nEvery time we run a test weâ€™re syncing it to a dataset example and tracing it as a run. Thereâ€™s a few different ways that we can trace the example inputs and reference outputs and the run outputs. The simplest is to use the log_inputs, log_outputs, and log_reference_outputs methods. You can run these any time in a test to update the example and run for that test:\\nCopyimport pytest\\nfrom langsmith import testing as t\\n\\n@pytest.mark.langsmith\\ndef test_foo() -> None:\\n    t.log_inputs({\"a\": 1, \"b\": 2})\\n    t.log_reference_outputs({\"foo\": \"bar\"})\\n    t.log_outputs({\"foo\": \"baz\"})\\n    assert True\\n\\nRunning this test will create/update an example with name â€œtest_fooâ€, inputs {\"a\": 1, \"b\": 2}, reference outputs {\"foo\": \"bar\"} and trace a run with outputs {\"foo\": \"baz\"}.\\nNOTE: If you run log_inputs, log_outputs, or log_reference_outputs twice, the previous values will be overwritten.\\nAnother way to define example inputs and reference outputs is via pytest fixtures/parametrizations. By default any arguments to your test function will be logged as inputs on the corresponding example. If certain arguments are meant to represet reference outputs, you can specify that they should be logged as such using @pytest.mark.langsmith(output_keys=[\"name_of_ref_output_arg\"]):\\nCopyimport pytest\\n\\n@pytest.fixture\\ndef c() -> int:\\n    return 5\\n\\n@pytest.fixture\\ndef d() -> int:\\n    return 6\\n\\n@pytest.mark.langsmith(output_keys=[\"d\"])\\ndef test_cd(c: int, d: int) -> None:\\n    result = 2 * c\\n    t.log_outputs({\"d\": result})  # Log run outputs\\n    assert result == d\\n\\nThis will create/sync an example with name â€œtest_cdâ€, inputs {\"c\": 5} and reference outputs {\"d\": 6}, and run output {\"d\": 10}.\\n\\u200bLog feedback\\nBy default LangSmith collects the pass/fail rate under the pass feedback key for each test case. You can add additional feedback with log_feedback.\\nCopyimport openai\\nimport pytest\\nfrom langsmith import wrappers\\nfrom langsmith import testing as t\\n\\noai_client = wrappers.wrap_openai(openai.OpenAI())\\n\\n@pytest.mark.langsmith\\ndef test_offtopic_input() -> None:\\n    user_query = \"whats up\"\\n    t.log_inputs({\"user_query\": user_query})\\n\\n    sql = generate_sql(user_query)\\n    t.log_outputs({\"sql\": sql})\\n\\n    expected = \"Sorry that is not a valid query.\"\\n    t.log_reference_outputs({\"sql\": expected})\\n\\n    # Use this context manager to trace any steps used for generating evaluation\\n    # feedback separately from the main application logic\\n    with t.trace_feedback():\\n        instructions = (\\n            \"Return 1 if the ACTUAL and EXPECTED answers are semantically equivalent, \"\\n            \"otherwise return 0. Return only 0 or 1 and nothing else.\"\\n        )\\n\\n        grade = oai_client.chat.completions.create(\\n            model=\"gpt-4o-mini\",\\n            messages=[\\n                {\"role\": \"system\", \"content\": instructions},\\n                {\"role\": \"user\", \"content\": f\"ACTUAL: {sql}\\\\nEXPECTED: {expected}\"},\\n            ],\\n        )\\n        score = float(grade.choices[0].message.content)\\n        t.log_feedback(key=\"correct\", score=score)\\n\\n    assert score\\n\\nNote the use of the trace_feedback() context manager. This makes it so that the LLM-as-judge call is traced separately from the rest of the test case. Instead of showing up in the main test case run it will instead show up in the trace for the correct feedback key.\\nNOTE: Make sure that the log_feedback call associated with the feedback trace occurs inside the trace_feedback context. This way weâ€™ll be able to associate the feedback with the trace, and when seeing the feedback in the UI youâ€™ll be able to click on it to see the trace that generated it.\\n\\u200bTrace intermediate calls\\nLangSmith will automatically trace any traceable intermediate calls that happen in the course of test case execution.\\n\\u200bGrouping tests into a test suite\\nBy default, all tests within a given file will be grouped as a single â€œtest suiteâ€ with a corresponding dataset. You can configure which test suite a test belongs to by passing the test_suite_name parameter to @pytest.mark.langsmith for case-by-case grouping, or you can set the LANGSMITH_TEST_SUITE env var to group all tests from an execution into a single test suite:\\nCopyLANGSMITH_TEST_SUITE=\"SQL app tests\" pytest tests/\\n\\nWe generally recommend setting LANGSMITH_TEST_SUITE to get a consolidated view of all of your results.\\n\\u200bNaming experiments\\nYou can name an experiment using the LANGSMITH_EXPERIMENT env var:\\nCopyLANGSMITH_TEST_SUITE=\"SQL app tests\" LANGSMITH_EXPERIMENT=\"baseline\" pytest tests/\\n\\n\\u200bCaching\\nLLMs on every commit in CI can get expensive. To save time and resources, LangSmith lets you cache HTTP requests to disk. To enable caching, install with langsmith[pytest] and set an env var: LANGSMITH_TEST_CACHE=/my/cache/path:\\npipuvCopypip install -U \"langsmith[pytest]\"\\nLANGSMITH_TEST_CACHE=tests/cassettes pytest tests/my_llm_tests\\n\\nAll requests will be cached to tests/cassettes and loaded from there on subsequent runs. If you check this in to your repository, your CI will be able to use the cache as well.\\nIn langsmith>=0.4.10, you may selectively enable caching for requests to individual URLs or hostnames like this:\\nCopy@pytest.mark.langsmith(cached_hosts=[\"api.openai.com\", \"https://api.anthropic.com\"])\\ndef my_test():\\n    ...\\n\\n\\u200bpytest features\\n@pytest.mark.langsmith is designed to stay out of your way and works well with familiar pytest features.\\n\\u200bParametrize with pytest.mark.parametrize\\nYou can use the parametrize decorator as before. This will create a new test case for each parametrized instance of the test.\\nCopy@pytest.mark.langsmith(output_keys=[\"expected_sql\"])\\n@pytest.mark.parametrize(\\n    \"user_query, expected_sql\",\\n    [\\n        (\"Get all users from the customers table\", \"SELECT * FROM customers\"),\\n        (\"Get all users from the orders table\", \"SELECT * FROM orders\"),\\n    ],\\n)\\ndef test_sql_generation_parametrized(user_query, expected_sql):\\n    sql = generate_sql(user_query)\\n    assert sql == expected_sql\\n\\nNote: as the parametrized list grows, you may consider using evaluate() instead. This parallelizes the evaluation and makes it easier to control individual experiments and the corresponding dataset.\\n\\u200bParallelize with pytest-xdist\\nYou can use pytest-xdist as you normally would to parallelize test execution:\\npipuvCopypip install -U pytest-xdist\\npytest -n auto tests\\n\\n\\u200bAsync tests with pytest-asyncio\\n@pytest.mark.langsmith works with sync or async tests, so you can run async tests exactly as before.\\n\\u200bWatch mode with pytest-watch\\nUse watch mode to quickly iterate on your tests. We highly recommend ony using this with test caching (see below) enabled to avoid unnecessary LLM calls:\\npipuvCopypip install pytest-watch\\nLANGSMITH_TEST_CACHE=tests/cassettes ptw tests/my_llm_tests\\n\\n\\u200bRich outputs\\nIf youâ€™d like to see a rich display of the LangSmith results of your test run you can specify --langsmith-output:\\nCopypytest --langsmith-output tests\\n\\nNote: This flag used to be --output=langsmith in langsmith<=0.3.3 but was updated to avoid collisions with other pytest plugins.\\nYouâ€™ll get a nice table per test suite that updates live as the results are uploaded to LangSmith:\\n\\nSome important notes for using this feature:\\n\\nMake sure youâ€™ve installed pip install -U \"langsmith[pytest]\"\\nRich outputs do not currently work with pytest-xdist\\n\\nNOTE: The custom output removes all the standard pytest outputs. If youâ€™re trying to debug some unexpected behavior itâ€™s often better to show the regular pytest outputs so to get full error traces.\\n\\u200bDry-run mode\\nIf you want to run the tests without syncing the results to LangSmith, you can set LANGSMITH_TEST_TRACKING=false in your environment.\\nCopyLANGSMITH_TEST_TRACKING=false pytest tests/\\n\\nThe tests will run as normal, but the experiment logs will not be sent to LangSmith.\\n\\u200bExpectations\\nLangSmith provides an expect utility to help define expectations about your LLM output. For example:\\nCopyfrom langsmith import expect\\n\\n@pytest.mark.langsmith\\ndef test_sql_generation_select_all():\\n    user_query = \"Get all users from the customers table\"\\n    sql = generate_sql(user_query)\\n    expect(sql).to_contain(\"customers\")\\n\\nThis will log the binary â€œexpectationâ€ score to the experiment results, additionally asserting that the expectation is met possibly triggering a test failure.\\nexpect also provides â€œfuzzy matchâ€ methods. For example:\\nCopy@pytest.mark.langsmith(output_keys=[\"expectation\"])\\n@pytest.mark.parametrize(\\n    \"query, expectation\",\\n    [\\n       (\"what\\'s the capital of France?\", \"Paris\"),\\n    ],\\n)\\ndef test_embedding_similarity(query, expectation):\\n    prediction = my_chatbot(query)\\n    expect.embedding_distance(\\n        # This step logs the distance as feedback for this run\\n        prediction=prediction, expectation=expectation\\n        # Adding a matcher (in this case, \\'to_be_*\"), logs \\'expectation\\' feedback\\n    ).to_be_less_than(0.5) # Optional predicate to assert against\\n\\n    expect.edit_distance(\\n        # This computes the normalized Damerau-Levenshtein distance between the two strings\\n        prediction=prediction, expectation=expectation\\n        # If no predicate is provided below, \\'assert\\' isn\\'t called, but the score is still logged\\n    )\\n\\nThis test case will be assigned 4 scores:\\n\\nThe embedding_distance between the prediction and the expectation\\nThe binary expectation score (1 if cosine distance is less than 0.5, 0 if not)\\nThe edit_distance between the prediction and the expectation\\nThe overall test pass/fail score (binary)\\n\\nThe expect utility is modeled off of Jestâ€™s expect API, with some off-the-shelf functionality to make it easier to grade your LLMs.\\n\\u200bLegacy\\n\\u200b@test / @unit decorator\\nThe legacy method for marking test cases is using the @test or @unit decorators:\\nCopyfrom langsmith import test\\n\\n@test\\ndef test_foo() -> None:\\n    pass\\nWas this page helpful?YesNoSuggest editsRun an evaluation asynchronouslyRun evals with Vitest/JestâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting', 'loc': 'https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='How to handle model rate limits - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesDefine a target function to evaluateEvaluate intermediate stepsReturn multiple scores in one evaluatorReturn categorical vs numerical metricsRun evaluators on experimentsEvaluate with repetitionsHandle model rate limitsRun an evaluation locally (Python)Evaluate a runnableEvaluate a graphEvaluate an existing experiment (Python)Run an evaluation with multimodal contentSimulate multi-turn interactionsImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationEvaluation techniquesHow to handle model rate limitsGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageUsing langchain RateLimiters (Python only)Retrying with exponential backoffWith langchainWithout langchainLimiting max_concurrencySet up evaluationsEvaluation techniquesHow to handle model rate limitsCopy pageCopy pageA common issue when running large evaluation jobs is running into third-party API rate limits, usually from model providers. There are a few ways to deal with rate limits.\\n\\u200bUsing langchain RateLimiters (Python only)\\nIf youâ€™re using langchain Python ChatModels in your application or evaluators, you can add rate limiters to your model(s) that will add client-side control of the frequency with which requests are sent to the model provider API to avoid rate limit errors.\\nCopyfrom langchain.chat_models import init_chat_model\\nfrom langchain_core.rate_limiters import InMemoryRateLimiter\\n\\nrate_limiter = InMemoryRateLimiter(\\n    requests_per_second=0.1,  # <-- Super slow! We can only make a request once every 10 seconds!!\\n    check_every_n_seconds=0.1,  # Wake up every 100 ms to check whether allowed to make a request,\\n    max_bucket_size=10,  # Controls the maximum burst size.\\n)\\n\\nllm = init_chat_model(\"gpt-4o\", rate_limiter=rate_limiter)\\n\\ndef app(inputs: dict) -> dict:\\n    response = llm.invoke(...)\\n    ...\\n\\ndef evaluator(inputs: dict, outputs: dict, reference_outputs: dict) -> dict:\\n    response = llm.invoke(...)\\n    ...\\n\\nSee the langchain documentation for more on how to configure rate limiters.\\n\\u200bRetrying with exponential backoff\\nA very common way to deal with rate limit errors is retrying with exponential backoff. Retrying with exponential backoff means repeatedly retrying failed requests with an (exponentially) increasing wait time between each retry. This continues until either the request succeeds or a maximum number of requests is made.\\n\\u200bWith langchain\\nIf youâ€™re using langchain components you can add retries to all model calls with the .with_retry(...) / .withRetry() method:\\nPythonTypeScriptCopyfrom langchain import init_chat_model\\n\\nllm_with_retry = init_chat_model(\"gpt-4o-mini\").with_retry(stop_after_attempt=6)\\n\\nSee the langchain Python and JS API references for more.\\n\\u200bWithout langchain\\nIf youâ€™re not using langchain you can use other libraries like tenacity (Python) or backoff (Python) to implement retries with exponential backoff, or you can implement it from scratch. See some examples of how to do this in the OpenAI docs.\\n\\u200bLimiting max_concurrency\\nLimiting the number of concurrent calls youâ€™re making to your application and evaluators is another way to decrease the frequency of model calls youâ€™re making, and in that way avoid rate limit errors. max_concurrency can be set directly on the evaluate() / aevaluate() functions. This parallelizes evaluation by effectively splitting the dataset across threads.\\nPythonTypeScriptCopyfrom langsmith import aevaluate\\n\\nresults = await aevaluate(\\n    ...\\n    max_concurrency=4,\\n)\\nWas this page helpful?YesNoSuggest editsEvaluate with repetitionsRun an evaluation locally (Python)âŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment', 'loc': 'https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment', 'changefreq': 'weekly', 'priority': '0.5'}, page_content=\"Page Not FoundSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsCreate and update promptsCreate a promptManage promptsManage prompts programmaticallyConfigure prompt settingsUse tools in a promptInclude multimodal content in a promptWrite your prompt with AIConnect to modelsTutorialsOptimize a classifierSync prompts with GitHubTest multi-turn conversationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationPage Not FoundGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForum404Page Not FoundWe couldn't find the page you were looking for. Maybe you were looking for?How to upload experiments run outside of LangSmith with the REST APIGet started with LangSmithAnalyze an experimentAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides/repetition', 'loc': 'https://docs.smith.langchain.com/evaluation/how_to_guides/repetition', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='How to evaluate with repetitions - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesDefine a target function to evaluateEvaluate intermediate stepsReturn multiple scores in one evaluatorReturn categorical vs numerical metricsRun evaluators on experimentsEvaluate with repetitionsHandle model rate limitsRun an evaluation locally (Python)Evaluate a runnableEvaluate a graphEvaluate an existing experiment (Python)Run an evaluation with multimodal contentSimulate multi-turn interactionsImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationEvaluation techniquesHow to evaluate with repetitionsGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageConfiguring repetitions on an experimentViewing results of experiments run with repetitionsSet up evaluationsEvaluation techniquesHow to evaluate with repetitionsCopy pageCopy pageRunning multiple repetitions can give a more accurate estimate of the performance of your system since LLM outputs are not deterministic. Outputs can differ from one repetition to the next. Repetitions are a way to reduce noise in systems prone to high variability, such as agents.\\n\\u200bConfiguring repetitions on an experiment\\nAdd the optional num_repetitions param to the evaluate / aevaluate function (Python, TypeScript) to specify how many times to evaluate over each example in your dataset. For instance, if you have 5 examples in the dataset and set num_repetitions=5, each example will be run 5 times, for a total of 25 runs.\\nPythonTypeScriptCopyfrom langsmith import evaluate\\n\\nresults = evaluate(\\n    lambda inputs: label_text(inputs[\"text\"]),\\n    data=dataset_name,\\n    evaluators=[correct_label],\\n    experiment_prefix=\"Toxic Queries\",\\n    num_repetitions=3,\\n)\\n\\n\\u200bViewing results of experiments run with repetitions\\nIf youâ€™ve run your experiment with repetitions, there will be arrows in the output results column so you can view outputs in the table. To view each run from the repetition, hover over the output cell and click the expanded view. When you run an experiment with repetitions, LangSmith displays the average for each feedback score in the table. Click on the feedback score to view the feedback scores from individual runs, or to view the standard deviation across repetitions.\\nWas this page helpful?YesNoSuggest editsRun evaluators on experimentsHandle model rate limitsâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only', 'loc': 'https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='How to use the REST API - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsRun an evaluation asynchronouslyRun evaluations with pytestRun evals with Vitest/JestWith the APIEvaluation techniquesImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationFrameworks & integrationsHow to use the REST APIGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this page1. Create a target function endpoint2. Start the evaluationRun a pairwise experiment\\u200bSet up evaluationsFrameworks & integrationsHow to use the REST APICopy pageCopy pageBefore diving into this content, it might be helpful to read the following:\\nEvaluate LLM applications\\nLangSmith API Reference\\n\\nIt is highly recommended to run evals with either the Python or TypeScript SDKs. The SDKs have many optimizations and features that enhance the performance and reliability of your evals. However, if you are unable to use the SDKs, either because you are using a different language or because you are running in a restricted environment, you can use the REST API directly.\\nThis guide will show you how to run evals using the REST API, using the requests library in Python as an example. However, the same principles apply to any language.\\n\\u200b1. Create a target function endpoint\\nHere, we are using the python SDK for convenience. You can also use the API directly use the UI, see this guide for more information.\\n\\nAccept POST requests\\nAccept JSON input with the example inputs\\nReturn JSON output with the results\\n\\nHereâ€™s an example using FastAPI:\\nCopyfrom fastapi import FastAPI\\nfrom pydantic import BaseModel\\nfrom typing import Dict, Any\\n\\napp = FastAPI()\\n\\nclass EvaluationInput(BaseModel):\\n    inputs: Dict[str, Any]\\n\\nclass EvaluationOutput(BaseModel):\\n    outputs: Dict[str, Any]\\n\\n@app.post(\"/evaluate\", response_model=EvaluationOutput)\\nasync def evaluate(inputs: EvaluationInput):\\n    # Your evaluation logic here\\n    # This is just an example\\n    result = {\"output\": f\"Processed: {inputs.inputs}\"}\\n    return EvaluationOutput(outputs=result)\\n\\n\\u200b2. Start the evaluation\\nFirst, pull all of the examples youâ€™d want to use in your experiment.\\nCopyimport requests\\nimport json\\n\\n# Your API key\\napi_key = \"your-api-key\"\\n\\n# The dataset ID\\ndataset_id = \"your-dataset-id\"\\n\\n# The URL of your target function\\ntarget_url = \"https://your-function-url.com/evaluate\"\\n\\n# Start the evaluation\\nresponse = requests.post(\\n    \"https://api.smith.langchain.com/evaluations\",\\n    headers={\\n        \"Authorization\": f\"Bearer {api_key}\",\\n        \"Content-Type\": \"application/json\"\\n    },\\n    json={\\n        \"dataset_id\": dataset_id,\\n        \"target_url\": target_url,\\n        \"evaluators\": [\"your-evaluator-name\"],\\n        \"experiment_prefix\": \"api-evaluation\"\\n    }\\n)\\n\\nprint(response.json())\\n\\nWe are going to run completions on all examples using two models: gpt-3.5-turbo and gpt-4o-mini.\\nYou can check the status of your evaluation:\\nCopyevaluation_id = response.json()[\"id\"]\\n\\nstatus_response = requests.get(\\n    f\"https://api.smith.langchain.com/evaluations/{evaluation_id}\",\\n    headers={\"Authorization\": f\"Bearer {api_key}\"}\\n)\\n\\nprint(status_response.json())\\n\\n\\u200bRun a pairwise experiment\\u200b\\nNext, weâ€™ll demonstrate how to run a pairwise experiment. In a pairwise experiment, you compare two examples against each other. For more information, check out this guide.\\nCopyresults_response = requests.get(\\n    f\"https://api.smith.langchain.com/evaluations/{evaluation_id}/results\",\\n    headers={\"Authorization\": f\"Bearer {api_key}\"}\\n)\\n\\nprint(results_response.json())\\nWas this page helpful?YesNoSuggest editsRun evals with Vitest/JestDefine a target function to evaluateâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground', 'loc': 'https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Run an evaluation from the prompt playground - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationWith the SDKWith the UIUse prebuilt evaluatorsEvaluation typesFrameworks & integrationsEvaluation techniquesImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationRun an evaluationRun an evaluation from the prompt playgroundGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageCreate an experiment in the prompt playground\\u200bAdd evaluation scores to the experiment\\u200bSet up evaluationsRun an evaluationRun an evaluation from the prompt playgroundCopy pageCopy pageLangSmith allows you to run evaluations directly in the UI. The Prompt Playground allows you to test your prompt or model configuration over a series of inputs to see how well it scores across different contexts or scenarios, without having to write any code.\\nBefore you run an evaluation, you need to have an existing dataset. Learn how to create a dataset from the UI.\\nIf you prefer to run experiments in code, visit run an evaluation using the SDK.\\n\\n\\u200bCreate an experiment in the prompt playground\\u200b\\n\\nNavigate to the playground by clicking Playground in the sidebar.\\nAdd a prompt by selecting an existing saved a prompt or creating a new one.\\nSelect a dataset from the Test over dataset dropdown\\n\\n\\nNote that the keys in the dataset input must match the input variables of the prompt. For example, in the above video the selected dataset has inputs with the key â€œblogâ€, which correctly match the input variable of the prompt.\\nThere is a maximum of 15 input variables allowed in the prompt playground.\\n\\n\\nStart the experiment by clicking on the Start or CMD+Enter. This will run the prompt over all the examples in the dataset and create an entry for the experiment in the dataset details page. We recommend committing the prompt to the prompt hub before starting the experiment so that it can be easily referenced later when reviewing your experiment.\\nView the full results by clicking View full experiment. This will take you to the experiment details page where you can see the results of the experiment.\\n\\n\\u200bAdd evaluation scores to the experiment\\u200b\\nEvaluate your experiment over specific critera by adding evaluators. Add LLM-as-a-judge or custom code evaluators in the playground using the +Evaluator button.\\nTo learn more about adding evaluators in via UI, visit how to define an LLM-as-a-judge evaluator.Was this page helpful?YesNoSuggest editsWith the SDKUse prebuilt evaluatorsâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria', 'loc': 'https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Set up feedback criteria - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationAnnotation & human feedbackSet up feedback criteriaGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageContinuous feedbackCategorical feedbackAnnotation & human feedbackSet up feedback criteriaCopy pageCopy pageRecommended ReadingBefore diving into this content, it might be helpful to read the following:\\nConceptual guide on tracing and feedback\\nReference guide on feedback data format\\n\\nFeedback criteria are represented in the application as feedback tags. For human feedback, you can set up new feedback criteria as continuous feedback or categorical feedback.\\nTo set up a new feedback criteria, follow this link to view all existing tags for your workspace, then click New Tag.\\n\\u200bContinuous feedback\\nFor continuous feedback, you can enter a feedback tag name, then select a minimum and maximum value. Every value, including floating-point numbers, within this range will be accepted as feedback scores.\\n\\n\\u200bCategorical feedback\\nFor categorical feedback, you can enter a feedback tag name, then add a list of categories, each category mapping to a score. When you provide feedback, you can select one of these categories as the feedback score.\\nBoth the category label and the score will be logged as feedback in value and score fields, respectively.\\nWas this page helpful?YesNoSuggest editsUse annotation queuesAnnotate traces and runs inlineâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset', 'loc': 'https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Manage datasets - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationDatasetsManage datasetsGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageVersion a datasetCreate a new version of a datasetTag a versionEvaluate on a specific dataset versionUse list_examplesEvaluate on a split / filtered view of a datasetEvaluate on a filtered view of a datasetEvaluate on a dataset splitShare a datasetShare a dataset publiclyUnshare a datasetExport a datasetExport filtered traces from experiment to datasetView experiment tracesDatasetsManage datasetsCopy pageCopy pageLangSmith provides tools for managing and working with your datasets. This page describes dataset operations including:\\n\\nVersioning datasets to track changes over time.\\nFiltering and splitting datasets for evaluation.\\nSharing datasets publicly.\\nExporting datasets in various formats.\\n\\nYouâ€™ll also learn how to export filtered traces from experiments back to datasets for further analysis and iteration.\\n\\u200bVersion a dataset\\nIn LangSmith, datasets are versioned. This means that every time you add, update, or delete examples in your dataset, a new version of the dataset is created.\\n\\u200bCreate a new version of a dataset\\nAny time you add, update, or delete examples in your dataset, a new version of your dataset is created. This allows you to track changes to your dataset over time and understand how your dataset has evolved.\\nBy default, the version is defined by the timestamp of the change. When you click on a particular version of a dataset (by timestamp) in the Examples tab, you will find the state of the dataset at that point in time.\\n\\nNote that examples are read-only when viewing a past version of the dataset. You will also see the operations that were between this version of the dataset and the latest version of the dataset.\\nBy default, the latest version of the dataset is shown in the Examples tab and experiments from all versions are shown in the Tests tab.\\nIn the Tests tab, you will find the results of tests run on the dataset at different versions.\\n\\n\\u200bTag a version\\nYou can also tag versions of your dataset to give them a more human-readable name, which can be useful for marking important milestones in your datasetâ€™s history.\\nFor example, you might tag a version of your dataset as â€œprodâ€ and use it to run tests against your LLM pipeline.\\nYou can tag a version of your dataset in the UI by clicking on + Tag this version in the Examples tab.\\n\\nYou can also tag versions of your dataset using the SDK. Hereâ€™s an example of how to tag a version of a dataset using the Python SDK:\\nCopyfrom langsmith import Client\\nfrom datetime import datetime\\n\\nclient = Client()\\ninitial_time = datetime(2024, 1, 1, 0, 0, 0) # The timestamp of the version you want to tag\\n\\n# You can tag a specific dataset version with a semantic name, like \"prod\"\\nclient.update_dataset_tag(\\n    dataset_name=toxic_dataset_name, as_of=initial_time, tag=\"prod\"\\n)\\n\\nTo run an evaluation on a particular tagged version of a dataset, refer to the Evaluate on a specific dataset version section.\\n\\u200bEvaluate on a specific dataset version\\nYou may find it helpful to refer to the following content before you read this section:\\nVersion a dataset.\\nFetching examples.\\n\\n\\u200bUse list_examples\\nYou can use evaluate / aevaluate to pass in an iterable of examples to evaluate on a particular version of a dataset. Use list_examples / listExamples to fetch examples from a particular version tag using as_of / asOf and pass that into the data argument.\\nPythonTypeScriptCopyfrom langsmith import Client\\n\\nls_client = Client()\\n\\n# Assumes actual outputs have a \\'class\\' key.\\n# Assumes example outputs have a \\'label\\' key.\\ndef correct(outputs: dict, reference_outputs: dict) -> bool:\\n  return outputs[\"class\"] == reference_outputs[\"label\"]\\n\\nresults = ls_client.evaluate(\\n    lambda inputs: {\"class\": \"Not toxic\"},\\n    # Pass in filtered data here:\\n    data=ls_client.list_examples(\\n      dataset_name=\"Toxic Queries\",\\n      as_of=\"latest\",  # specify version here\\n    ),\\n    evaluators=[correct],\\n)\\n\\nLearn more about how to fetch views of a dataset on the Create and manage datasets programmatically page.\\n\\u200bEvaluate on a split / filtered view of a dataset\\nYou may find it helpful to refer to the following content before you read this section:\\nFetching examples.\\nCreating and managing dataset splits.\\n\\n\\u200bEvaluate on a filtered view of a dataset\\nYou can use the list_examples / listExamples method to fetch a subset of examples from a dataset to evaluate on.\\nOne common workflow is to fetch examples that have a certain metadata key-value pair.\\nPythonTypeScriptCopyfrom langsmith import evaluate\\n\\nresults = evaluate(\\n    lambda inputs: label_text(inputs[\"text\"]),\\n    data=client.list_examples(dataset_name=dataset_name, metadata={\"desired_key\": \"desired_value\"}),\\n    evaluators=[correct_label],\\n    experiment_prefix=\"Toxic Queries\",\\n)\\n\\nFor more filtering capabilities, refer to this how-to guide.\\n\\u200bEvaluate on a dataset split\\nYou can use the list_examples / listExamples method to evaluate on one or multiple splits of your dataset. The splits parameter takes a list of the splits you would like to evaluate.\\nPythonTypeScriptCopyfrom langsmith import evaluate\\n\\nresults = evaluate(\\n    lambda inputs: label_text(inputs[\"text\"]),\\n    data=client.list_examples(dataset_name=dataset_name, splits=[\"test\", \"training\"]),\\n    evaluators=[correct_label],\\n    experiment_prefix=\"Toxic Queries\",\\n)\\n\\nFor more details on fetching views of a dataset, refer to the guide on fetching datasets.\\n\\u200bShare a dataset\\n\\u200bShare a dataset publicly\\nSharing a dataset publicly will make the dataset examples, experiments and associated runs, and feedback on this dataset accessible to anyone with the link, even if they donâ€™t have a LangSmith account. Make sure youâ€™re not sharing sensitive information.This feature is only available in the cloud-hosted version of LangSmith.\\nFrom the Dataset & Experiments tab, select a dataset, click â‹® (top right of the page), click Share Dataset. This will open a dialog where you can copy the link to the dataset.\\n\\n\\u200bUnshare a dataset\\n\\n\\nClick on Unshare by clicking on Public in the upper right hand corner of any publicly shared dataset, then Unshare in the dialog. \\n\\n\\nNavigate to your organizationâ€™s list of publicly shared datasets, by clicking on Settings -> Shared URLs or this link, then click on Unshare next to the dataset you want to unshare.\\n\\n\\n\\n\\u200bExport a dataset\\nYou can export your LangSmith dataset to a CSV, JSONL, or OpenAIâ€™s fine tuning format from the LangSmith UI.\\nFrom the Dataset & Experiments tab, select a dataset, click â‹® (top right of the page), click Download Dataset.\\n\\n\\u200bExport filtered traces from experiment to dataset\\nAfter running an offline evaluation in LangSmith, you may want to export traces that met some evaluation criteria to a dataset.\\n\\u200bView experiment traces\\n\\nTo do so, first click on the arrow next to your experiment name. This will direct you to a project that contains the traces generated from your experiment.\\n\\nFrom there, you can filter the traces based on your evaluation criteria. In this example, weâ€™re filtering for all traces that received an accuracy score greater than 0.5.\\n\\nAfter applying the filter on the project, we can multi-select runs to add to the dataset, and click Add to Dataset.\\nWas this page helpful?YesNoSuggest editsWith the SDKWith the SDKâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides/summary', 'loc': 'https://docs.smith.langchain.com/evaluation/how_to_guides/summary', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='How to define a summary evaluator - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesCode evaluatorLLM-as-a-judge evaluatorComposite evaluatorsSummary evaluatorPairwise evaluationFrameworks & integrationsEvaluation techniquesImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationEvaluation typesHow to define a summary evaluatorGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageBasic exampleSummary evaluator argsSummary evaluator outputSet up evaluationsEvaluation typesHow to define a summary evaluatorCopy pageCopy pageSome metrics can only be defined on the entire experiment level as opposed to the individual runs of the experiment. For example, you may want to compute the overall pass rate or f1 score of your evaluation target across all examples in the dataset. These are called summary evaluators.\\n\\u200bBasic example\\nHere, weâ€™ll compute the f1-score, which is a combination of precision and recall.\\nThis sort of metric can only be computed over all of the examples in our experiment, so our evaluator takes in a list of outputs, and a list of reference_outputs.\\nPythonTypeScriptCopydef f1_score_summary_evaluator(outputs: list[dict], reference_outputs: list[dict]) -> dict:\\n    true_positives = 0\\n    false_positives = 0\\n    false_negatives = 0\\n\\n    for output_dict, reference_output_dict in zip(outputs, reference_outputs):\\n        output = output_dict[\"class\"]\\n        reference_output = reference_output_dict[\"class\"]\\n\\n        if output == \"Toxic\" and reference_output == \"Toxic\":\\n            true_positives += 1\\n        elif output == \"Toxic\" and reference_output == \"Not toxic\":\\n            false_positives += 1\\n        elif output == \"Not toxic\" and reference_output == \"Toxic\":\\n            false_negatives += 1\\n\\n    if true_positives == 0:\\n        return {\"key\": \"f1_score\", \"score\": 0.0}\\n\\n    precision = true_positives / (true_positives + false_positives)\\n    recall = true_positives / (true_positives + false_negatives)\\n    f1_score = 2 * (precision * recall) / (precision + recall)\\n\\n    return {\"key\": \"f1_score\", \"score\": f1_score}\\n\\nYou can then pass this evaluator to the evaluate method as follows:\\nPythonTypeScriptCopyfrom langsmith import Client\\n\\nls_client = Client()\\ndataset = ls_client.clone_public_dataset(\\n    \"https://smith.langchain.com/public/3d6831e6-1680-4c88-94df-618c8e01fc55/d\"\\n)\\n\\ndef bad_classifier(inputs: dict) -> dict:\\n    return {\"class\": \"Not toxic\"}\\n\\ndef correct(outputs: dict, reference_outputs: dict) -> bool:\\n    \"\"\"Row-level correctness evaluator.\"\"\"\\n    return outputs[\"class\"] == reference_outputs[\"label\"]\\n\\nresults = ls_client.evaluate(\\n    bad_classified,\\n    data=dataset,\\n    evaluators=[correct],\\n    summary_evaluators=[pass_50],\\n)\\n\\nIn the LangSmith UI, youâ€™ll the summary evaluatorâ€™s score displayed with the corresponding key.\\n\\n\\u200bSummary evaluator args\\nSummary evaluator functions must have specific argument names. They can take any subset of the following arguments:\\n\\ninputs: list[dict]: A list of the inputs corresponding to a single example in a dataset.\\noutputs: list[dict]: A list of the dict outputs produced by each experiment on the given inputs.\\nreference_outputs/referenceOutputs: list[dict]: A list of the reference outputs associated with the example, if available.\\nruns: list[Run]: A list of the full Run objects generated by the two experiments on the given example. Use this if you need access to intermediate steps or metadata about each run.\\nexamples: list[Example]: All of the dataset Example objects, including the example inputs, outputs (if available), and metdata (if available).\\n\\n\\u200bSummary evaluator output\\nSummary evaluators are expected to return one of the following types:\\nPython and JS/TS\\n\\ndict: dicts of the form {\"score\": ..., \"name\": ...} allow you to pass a numeric or boolean score and metric name.\\n\\nCurrently Python only\\n\\nint | float | bool: this is interepreted as an continuous metric that can be averaged, sorted, etc. The function name is used as the name of the metric.\\nWas this page helpful?YesNoSuggest editsComposite evaluatorsPairwise evaluationâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments', 'loc': 'https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='How to upload experiments run outside of LangSmith with the REST API - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationAnalyze experiment resultsHow to upload experiments run outside of LangSmith with the REST APIGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageRequest body schemaConsiderationsExample requestView the experiment in the UIAnalyze experiment resultsHow to upload experiments run outside of LangSmith with the REST APICopy pageCopy pageSome users prefer to manage their datasets and run their experiments outside of LangSmith, but want to use the LangSmith UI to view the results. This is supported via our endpoint.\\nThis guide will show you how to upload evals using the REST API, using the requests library in Python as an example. However, the same principles apply to any language.\\n\\u200bRequest body schema\\nUploading an experiment requires specifying the relevant high-level information for your experiment and dataset, along with the individual data for your examples and runs within the experiment. Each object in the results represents a â€œrowâ€ in the experiment - a single dataset example, along with an associated run. Note that dataset_id and dataset_name refer to your dataset identifier in your external system and will be used to group external experiments together in a single dataset. They should not refer to an existing dataset in LangSmith (unless that dataset was created via this endpoint).\\nYou may use the following schema to upload experiments to the /datasets/upload-experiment endpoint:\\nCopy{\\n  \"experiment_name\": \"string (required)\",\\n  \"experiment_description\": \"string (optional)\",\\n  \"experiment_start_time\": \"datetime (required)\",\\n  \"experiment_end_time\": \"datetime (required)\",\\n  \"dataset_id\": \"uuid (optional - an external dataset id, used to group experiments together)\",\\n  \"dataset_name\": \"string (optional - must provide either dataset_id or dataset_name)\",\\n  \"dataset_description\": \"string (optional)\",\\n  \"experiment_metadata\": { // Object (any shape - optional)\\n    \"key\": \"value\"\\n  },\\n  \"summary_experiment_scores\": [ // List of summary feedback objects (optional)\\n    {\\n      \"key\": \"string (required)\",\\n      \"score\": \"number (optional)\",\\n      \"value\": \"string (optional)\",\\n      \"comment\": \"string (optional)\",\\n      \"feedback_source\": { // Object (optional)\\n        \"type\": \"string (required)\"\\n      },\\n      \"feedback_config\": { // Object (optional)\\n        \"type\": \"string enum: continuous, categorical, or freeform\",\\n        \"min\": \"number (optional)\",\\n        \"max\": \"number (optional)\",\\n        \"categories\": [ // List of feedback category objects (optional)\\n          {\\n            \"value\": \"number (required)\",\\n            \"label\": \"string (optional)\"\\n          }\\n        ]\\n      },\\n      \"created_at\": \"datetime (optional - defaults to now)\",\\n      \"modified_at\": \"datetime (optional - defaults to now)\",\\n      \"correction\": \"Object or string (optional)\"\\n    }\\n  ],\\n  \"results\": [ // List of experiment row objects (required)\\n    {\\n      \"row_id\": \"uuid (required)\",\\n      \"inputs\": { // Object (required - any shape). This will\\n        \"key\": \"val\" // be the input to both the run and the dataset example.\\n      },\\n      \"expected_outputs\": { // Object (optional - any shape).\\n        \"key\": \"val\" // These will be the outputs of the dataset examples.\\n      },\\n      \"actual_outputs\": { // Object (optional - any shape).\\n        \"key\": \"val\" // These will be the outputs of the runs.\\n      },\\n      \"evaluation_scores\": [ // List of feedback objects for the run (optional)\\n        {\\n          \"key\": \"string (required)\",\\n          \"score\": \"number (optional)\",\\n          \"value\": \"string (optional)\",\\n          \"comment\": \"string (optional)\",\\n          \"feedback_source\": { // Object (optional)\\n            \"type\": \"string (required)\"\\n          },\\n          \"feedback_config\": { // Object (optional)\\n            \"type\": \"string enum: continuous, categorical, or freeform\",\\n            \"min\": \"number (optional)\",\\n            \"max\": \"number (optional)\",\\n            \"categories\": [ // List of feedback category objects (optional)\\n              {\\n                \"value\": \"number (required)\",\\n                \"label\": \"string (optional)\"\\n              }\\n            ]\\n          },\\n          \"created_at\": \"datetime (optional - defaults to now)\",\\n          \"modified_at\": \"datetime (optional - defaults to now)\",\\n          \"correction\": \"Object or string (optional)\"\\n        }\\n      ],\\n      \"start_time\": \"datetime (required)\", // The start/end times for the runs will be used to\\n      \"end_time\": \"datetime (required)\", // calculate latency. They must all fall between the\\n      \"run_name\": \"string (optional)\", // start and end times for the experiment.\\n      \"error\": \"string (optional)\",\\n      \"run_metadata\": { // Object (any shape - optional)\\n        \"key\": \"value\"\\n      }\\n    }\\n  ]\\n}\\n\\nThe response JSON will be a dict with keys experiment and dataset, each of which is an object that contains relevant information about the experiment and dataset that was created.\\n\\u200bConsiderations\\nYou may upload multiple experiments to the same dataset by providing the same dataset_id or dataset_name between multiple calls. Your experiments will be grouped together under a single dataset, and you will be able to use the comparison view to compare results between experiments.\\nEnsure that the start and end times of your individual rows are all between the start and end time of your experiment.\\nYou must provide either a dataset_id or a dataset_name. If you only provide an ID and the dataset does not yet exist, we will generate a name for you, and vice versa if you only provide a name.\\nYou may not upload experiments to a dataset that was not created via this endpoint. Uploading experiments is only supported for externally-managed datasets.\\n\\u200bExample request\\nBelow is an example of a simple call to the /datasets/upload-experiment. This is a basic example that just uses the most important fields as an illustration.\\nCopyimport os\\nimport requests\\n\\nbody = {\\n    \"experiment_name\": \"My external experiment\",\\n    \"experiment_description\": \"An experiment uploaded to LangSmith\",\\n    \"dataset_name\": \"my-external-dataset\",\\n    \"summary_experiment_scores\": [\\n        {\\n            \"key\": \"summary_accuracy\",\\n            \"score\": 0.9,\\n            \"comment\": \"Great job!\"\\n        }\\n    ],\\n    \"results\": [\\n        {\\n            \"row_id\": \"<<uuid>>\",\\n            \"inputs\": {\\n                \"input\": \"Hello, what is the weather in San Francisco today?\"\\n            },\\n            \"expected_outputs\": {\\n                \"output\": \"Sorry, I am unable to provide information about the current weather.\"\\n            },\\n            \"actual_outputs\": {\\n                \"output\": \"The weather is partly cloudy with a high of 65.\"\\n            },\\n            \"evaluation_scores\": [\\n                {\\n                    \"key\": \"hallucination\",\\n                    \"score\": 1,\\n                    \"comment\": \"The chatbot made up the weather instead of identifying that \"\\n                               \"they don\\'t have enough info to answer the question. This is \"\\n                               \"a hallucination.\"\\n                }\\n            ],\\n            \"start_time\": \"2024-08-03T00:12:39\",\\n            \"end_time\": \"2024-08-03T00:12:41\",\\n            \"run_name\": \"Chatbot\"\\n        },\\n        {\\n            \"row_id\": \"<<uuid>>\",\\n            \"inputs\": {\\n                \"input\": \"Hello, what is the square root of 49?\"\\n            },\\n            \"expected_outputs\": {\\n                \"output\": \"The square root of 49 is 7.\"\\n            },\\n            \"actual_outputs\": {\\n                \"output\": \"7.\"\\n            },\\n            \"evaluation_scores\": [\\n                {\\n                    \"key\": \"hallucination\",\\n                    \"score\": 0,\\n                    \"comment\": \"The chatbot correctly identified the answer. This is not a \"\\n                               \"hallucination.\"\\n                }\\n            ],\\n            \"start_time\": \"2024-08-03T00:12:40\",\\n            \"end_time\": \"2024-08-03T00:12:42\",\\n            \"run_name\": \"Chatbot\"\\n        }\\n    ],\\n    \"experiment_start_time\": \"2024-08-03T00:12:38\",\\n    \"experiment_end_time\": \"2024-08-03T00:12:43\"\\n}\\n\\nresp = requests.post(\\n    \"https://api.smith.langchain.com/api/v1/datasets/upload-experiment\", # Update appropriately for self-hosted installations or the EU region\\n    json=body,\\n    headers={\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]}\\n)\\n\\nprint(resp.json())\\n\\nBelow is the response received:\\nCopy{\\n  \"dataset\": {\\n    \"name\": \"my-external-dataset\",\\n    \"description\": null,\\n    \"created_at\": \"2024-08-03T00:36:23.289730+00:00\",\\n    \"data_type\": \"kv\",\\n    \"inputs_schema_definition\": null,\\n    \"outputs_schema_definition\": null,\\n    \"externally_managed\": true,\\n    \"id\": \"<<uuid>>\",\\n    \"tenant_id\": \"<<uuid>>\",\\n    \"example_count\": 0,\\n    \"session_count\": 0,\\n    \"modified_at\": \"2024-08-03T00:36:23.289730+00:00\",\\n    \"last_session_start_time\": null\\n  },\\n  \"experiment\": {\\n    \"start_time\": \"2024-08-03T00:12:38\",\\n    \"end_time\": \"2024-08-03T00:12:43+00:00\",\\n    \"extra\": null,\\n    \"name\": \"My external experiment\",\\n    \"description\": \"An experiment uploaded to LangSmith\",\\n    \"default_dataset_id\": null,\\n    \"reference_dataset_id\": \"<<uuid>>\",\\n    \"trace_tier\": \"longlived\",\\n    \"id\": \"<<uuid>>\",\\n    \"run_count\": null,\\n    \"latency_p50\": null,\\n    \"latency_p99\": null,\\n    \"first_token_p50\": null,\\n    \"first_token_p99\": null,\\n    \"total_tokens\": null,\\n    \"prompt_tokens\": null,\\n    \"completion_tokens\": null,\\n    \"total_cost\": null,\\n    \"prompt_cost\": null,\\n    \"completion_cost\": null,\\n    \"tenant_id\": \"<<uuid>>\",\\n    \"last_run_start_time\": null,\\n    \"last_run_start_time_live\": null,\\n    \"feedback_stats\": null,\\n    \"session_feedback_stats\": null,\\n    \"run_facets\": null,\\n    \"error_rate\": null,\\n    \"streaming_rate\": null,\\n    \"test_run_number\": 1\\n  }\\n}\\n\\nNote that the latency and feedback stats in the experiment results are null because the runs havenâ€™t had a chance to be persisted yet, which may take a few seconds. If you save the experiment id and query again in a few seconds, you will see all the stats (although tokens/cost will still be null, because we donâ€™t ask for this information in the request body).\\n\\u200bView the experiment in the UI\\nNow, login to the UI and click on your newly-created dataset! You should see a single experiment: \\nYour examples will have been uploaded: \\nClicking on your experiment will bring you to the comparison view: \\nAs you upload more experiments to your dataset, you will be able to compare the results and easily identify regressions in the comparison view.Was this page helpful?YesNoSuggest editsFetch performance metrics for an experimentUse annotation queuesâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old', 'loc': 'https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='How to use prebuilt evaluators - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationWith the SDKWith the UIUse prebuilt evaluatorsEvaluation typesFrameworks & integrationsEvaluation techniquesImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationRun an evaluationHow to use prebuilt evaluatorsGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageSetupRunning an evaluatorSet up evaluationsRun an evaluationHow to use prebuilt evaluatorsCopy pageCopy pageLangSmith integrates with the open-source openevals package to provide a suite of prebuilt evaluators that you can use as starting points for evaluation.\\nThis how-to guide will demonstrate how to set up and run one type of evaluator (LLM-as-a-judge). For a complete list of prebuilt evaluators with usage examples, refer to the openevals and agentevals repos.\\n\\u200bSetup\\nYouâ€™ll need to install the openevals package to use the pre-built LLM-as-a-judge evaluator.\\nPythonTypeScriptCopypip install -U openevals\\n\\nYouâ€™ll also need to set your OpenAI API key as an environment variable, though you can choose different providers too:\\nCopyexport OPENAI_API_KEY=\"your_openai_api_key\"\\n\\nWeâ€™ll also use LangSmithâ€™s pytest integration for Python and Vitest/Jest for TypeScript to run our evals. openevals also integrates seamlessly with the evaluate method as well. See the appropriate guides for setup instructions.\\n\\u200bRunning an evaluator\\nThe general flow is simple: import the evaluator or factory function from openevals, then run it within your test file with inputs, outputs, and reference outputs. LangSmith will automatically log the evaluatorâ€™s results as feedback.\\nNote that not all evaluators will require each parameter (the exact match evaluator only requires outputs and reference outputs, for example). Additionally, if your LLM-as-a-judge prompt requires additional variables, passing them in as kwargs will format them into the prompt.\\nSet up your test file like this:\\nPythonTypeScriptCopyimport pytest\\nfrom langsmith import testing as t\\nfrom openevals.llm import create_llm_as_judge\\nfrom openevals.prompts import CORRECTNESS_PROMPT\\n\\ncorrectness_evaluator = create_llm_as_judge(\\n    prompt=CORRECTNESS_PROMPT,\\n    feedback_key=\"correctness\",\\n    model=\"openai:o3-mini\",\\n)\\n\\n# Mock standin for your application\\ndef my_llm_app(inputs: dict) -> str:\\n    return \"Doodads have increased in price by 10% in the past year.\"\\n\\n@pytest.mark.langsmith\\ndef test_correctness():\\n    inputs = \"How much has the price of doodads changed in the past year?\"\\n    reference_outputs = \"The price of doodads has decreased by 50% in the past year.\"\\n    outputs = my_llm_app(inputs)\\n\\n    t.log_inputs({\"question\": inputs})\\n    t.log_outputs({\"answer\": outputs})\\n    t.log_reference_outputs({\"answer\": reference_outputs})\\n\\n    correctness_evaluator(\\n        inputs=inputs,\\n        outputs=outputs,\\n        reference_outputs=reference_outputs\\n    )\\n\\nThe feedback_key/feedbackKey parameter will be used as the name of the feedback in your experiment.\\nRunning the eval in your terminal will result in something like the following:\\n\\nYou can also pass prebuilt evaluators directly into the evaluate method if you have already created a dataset in LangSmith. If using Python, this requires langsmith>=0.3.11:\\nPythonTypeScriptCopyfrom langsmith import Client\\nfrom openevals.llm import create_llm_as_judge\\nfrom openevals.prompts import CONCISENESS_PROMPT\\n\\nclient = Client()\\nconciseness_evaluator = create_llm_as_judge(\\n    prompt=CONCISENESS_PROMPT,\\n    feedback_key=\"conciseness\",\\n    model=\"openai:o3-mini\",\\n)\\n\\nexperiment_results = client.evaluate(\\n    # This is a dummy target function, replace with your actual LLM-based system\\n    lambda inputs: \"What color is the sky?\",\\n    data=\"Sample dataset\",\\n    evaluators=[\\n        conciseness_evaluator\\n    ]\\n)\\n\\nFor a complete list of available evaluators, see the openevals and agentevals repos.Was this page helpful?YesNoSuggest editsWith the UICode evaluatorâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets', 'loc': 'https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Manage datasets - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationDatasetsManage datasetsGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageVersion a datasetCreate a new version of a datasetTag a versionEvaluate on a specific dataset versionUse list_examplesEvaluate on a split / filtered view of a datasetEvaluate on a filtered view of a datasetEvaluate on a dataset splitShare a datasetShare a dataset publiclyUnshare a datasetExport a datasetExport filtered traces from experiment to datasetView experiment tracesDatasetsManage datasetsCopy pageCopy pageLangSmith provides tools for managing and working with your datasets. This page describes dataset operations including:\\n\\nVersioning datasets to track changes over time.\\nFiltering and splitting datasets for evaluation.\\nSharing datasets publicly.\\nExporting datasets in various formats.\\n\\nYouâ€™ll also learn how to export filtered traces from experiments back to datasets for further analysis and iteration.\\n\\u200bVersion a dataset\\nIn LangSmith, datasets are versioned. This means that every time you add, update, or delete examples in your dataset, a new version of the dataset is created.\\n\\u200bCreate a new version of a dataset\\nAny time you add, update, or delete examples in your dataset, a new version of your dataset is created. This allows you to track changes to your dataset over time and understand how your dataset has evolved.\\nBy default, the version is defined by the timestamp of the change. When you click on a particular version of a dataset (by timestamp) in the Examples tab, you will find the state of the dataset at that point in time.\\n\\nNote that examples are read-only when viewing a past version of the dataset. You will also see the operations that were between this version of the dataset and the latest version of the dataset.\\nBy default, the latest version of the dataset is shown in the Examples tab and experiments from all versions are shown in the Tests tab.\\nIn the Tests tab, you will find the results of tests run on the dataset at different versions.\\n\\n\\u200bTag a version\\nYou can also tag versions of your dataset to give them a more human-readable name, which can be useful for marking important milestones in your datasetâ€™s history.\\nFor example, you might tag a version of your dataset as â€œprodâ€ and use it to run tests against your LLM pipeline.\\nYou can tag a version of your dataset in the UI by clicking on + Tag this version in the Examples tab.\\n\\nYou can also tag versions of your dataset using the SDK. Hereâ€™s an example of how to tag a version of a dataset using the Python SDK:\\nCopyfrom langsmith import Client\\nfrom datetime import datetime\\n\\nclient = Client()\\ninitial_time = datetime(2024, 1, 1, 0, 0, 0) # The timestamp of the version you want to tag\\n\\n# You can tag a specific dataset version with a semantic name, like \"prod\"\\nclient.update_dataset_tag(\\n    dataset_name=toxic_dataset_name, as_of=initial_time, tag=\"prod\"\\n)\\n\\nTo run an evaluation on a particular tagged version of a dataset, refer to the Evaluate on a specific dataset version section.\\n\\u200bEvaluate on a specific dataset version\\nYou may find it helpful to refer to the following content before you read this section:\\nVersion a dataset.\\nFetching examples.\\n\\n\\u200bUse list_examples\\nYou can use evaluate / aevaluate to pass in an iterable of examples to evaluate on a particular version of a dataset. Use list_examples / listExamples to fetch examples from a particular version tag using as_of / asOf and pass that into the data argument.\\nPythonTypeScriptCopyfrom langsmith import Client\\n\\nls_client = Client()\\n\\n# Assumes actual outputs have a \\'class\\' key.\\n# Assumes example outputs have a \\'label\\' key.\\ndef correct(outputs: dict, reference_outputs: dict) -> bool:\\n  return outputs[\"class\"] == reference_outputs[\"label\"]\\n\\nresults = ls_client.evaluate(\\n    lambda inputs: {\"class\": \"Not toxic\"},\\n    # Pass in filtered data here:\\n    data=ls_client.list_examples(\\n      dataset_name=\"Toxic Queries\",\\n      as_of=\"latest\",  # specify version here\\n    ),\\n    evaluators=[correct],\\n)\\n\\nLearn more about how to fetch views of a dataset on the Create and manage datasets programmatically page.\\n\\u200bEvaluate on a split / filtered view of a dataset\\nYou may find it helpful to refer to the following content before you read this section:\\nFetching examples.\\nCreating and managing dataset splits.\\n\\n\\u200bEvaluate on a filtered view of a dataset\\nYou can use the list_examples / listExamples method to fetch a subset of examples from a dataset to evaluate on.\\nOne common workflow is to fetch examples that have a certain metadata key-value pair.\\nPythonTypeScriptCopyfrom langsmith import evaluate\\n\\nresults = evaluate(\\n    lambda inputs: label_text(inputs[\"text\"]),\\n    data=client.list_examples(dataset_name=dataset_name, metadata={\"desired_key\": \"desired_value\"}),\\n    evaluators=[correct_label],\\n    experiment_prefix=\"Toxic Queries\",\\n)\\n\\nFor more filtering capabilities, refer to this how-to guide.\\n\\u200bEvaluate on a dataset split\\nYou can use the list_examples / listExamples method to evaluate on one or multiple splits of your dataset. The splits parameter takes a list of the splits you would like to evaluate.\\nPythonTypeScriptCopyfrom langsmith import evaluate\\n\\nresults = evaluate(\\n    lambda inputs: label_text(inputs[\"text\"]),\\n    data=client.list_examples(dataset_name=dataset_name, splits=[\"test\", \"training\"]),\\n    evaluators=[correct_label],\\n    experiment_prefix=\"Toxic Queries\",\\n)\\n\\nFor more details on fetching views of a dataset, refer to the guide on fetching datasets.\\n\\u200bShare a dataset\\n\\u200bShare a dataset publicly\\nSharing a dataset publicly will make the dataset examples, experiments and associated runs, and feedback on this dataset accessible to anyone with the link, even if they donâ€™t have a LangSmith account. Make sure youâ€™re not sharing sensitive information.This feature is only available in the cloud-hosted version of LangSmith.\\nFrom the Dataset & Experiments tab, select a dataset, click â‹® (top right of the page), click Share Dataset. This will open a dialog where you can copy the link to the dataset.\\n\\n\\u200bUnshare a dataset\\n\\n\\nClick on Unshare by clicking on Public in the upper right hand corner of any publicly shared dataset, then Unshare in the dialog. \\n\\n\\nNavigate to your organizationâ€™s list of publicly shared datasets, by clicking on Settings -> Shared URLs or this link, then click on Unshare next to the dataset you want to unshare.\\n\\n\\n\\n\\u200bExport a dataset\\nYou can export your LangSmith dataset to a CSV, JSONL, or OpenAIâ€™s fine tuning format from the LangSmith UI.\\nFrom the Dataset & Experiments tab, select a dataset, click â‹® (top right of the page), click Download Dataset.\\n\\n\\u200bExport filtered traces from experiment to dataset\\nAfter running an offline evaluation in LangSmith, you may want to export traces that met some evaluation criteria to a dataset.\\n\\u200bView experiment traces\\n\\nTo do so, first click on the arrow next to your experiment name. This will direct you to a project that contains the traces generated from your experiment.\\n\\nFrom there, you can filter the traces based on your evaluation criteria. In this example, weâ€™re filtering for all traces that received an accuracy score greater than 0.5.\\n\\nAfter applying the filter on the project, we can multi-select runs to add to the dataset, and click Add to Dataset.\\nWas this page helpful?YesNoSuggest editsWith the SDKWith the SDKâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest', 'loc': 'https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='How to run evaluations with Vitest/Jest (beta) - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsRun an evaluation asynchronouslyRun evaluations with pytestRun evals with Vitest/JestWith the APIEvaluation techniquesImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationFrameworks & integrationsHow to run evaluations with Vitest/Jest (beta)Get startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageSetupVitestJestDefine and run evalsTrace feedbackRunning multiple examples against a test caseLog outputsTrace intermediate callsFocusing or skipping testsConfiguring test suitesDry-run modeSet up evaluationsFrameworks & integrationsHow to run evaluations with Vitest/Jest (beta)Copy pageCopy pageLangSmith provides integrations with Vitest and Jest that allow JavaScript and TypeScript developers define their datasets and evaluate using familiar syntax.\\n\\nCompared to the evaluate() evaluation flow, this is useful when:\\n\\nEach example requires different evaluation logic\\nYou want to assert binary expectations, and both track these assertions in LangSmith and raise assertion errors locally (e.g. in CI pipelines)\\nYou want to take advantage of mocks, watch mode, local results, or other features of the Vitest/Jest ecosystems\\n\\nRequires JS/TS SDK version langsmith>=0.3.1.\\nThe Vitest/Jest integrations are in beta and are subject to change in upcoming releases.\\nThe Python SDK has an analogous pytest integration.\\n\\u200bSetup\\nSet up the integrations as follows. Note that while you can add LangSmith evals alongside your other unit tests (as standard *.test.ts files) using your existing test config files, the below examples will also set up a separate test config file and command to run your evals. It will assume you end your test files with .eval.ts.\\nThis ensures that the custom test reporter and other LangSmith touchpoints do not modify your existing test outputs.\\n\\u200bVitest\\nInstall the required development dependencies if you have not already:\\nyarnnpmpnpmCopyyarn add -D vitest dotenv\\n\\nThe examples below also require openai (and of course langsmith!) as a dependency:\\nyarnnpmpnpmCopyyarn add langsmith openai\\n\\nThen create a separate ls.vitest.config.ts file with the following base config:\\nCopyimport { defineConfig } from \"vitest/config\";\\n\\nexport default defineConfig({\\n  test: {\\n    include: [\"**/*.eval.?(c|m)[jt]s\"],\\n    reporters: [\"langsmith/vitest/reporter\"],\\n    setupFiles: [\"dotenv/config\"],\\n  },\\n});\\n\\n\\ninclude ensures that only files ending with some variation of eval.ts in your project are run\\nreporters is responsible for nicely formatting your output as shown above\\nsetupFiles runs dotenv to load environment variables before running your evals\\n\\nJSDom environments are not supported at this time. You should either omit the \"environment\" field from your config or set it to \"node\".\\nFinally, add the following to the scripts field in your package.json to run Vitest with the config you just created:\\nCopy{\\n  \"name\": \"YOUR_PROJECT_NAME\",\\n  \"scripts\": {\\n    \"eval\": \"vitest run --config ls.vitest.config.ts\"\\n  },\\n  \"dependencies\": {\\n    ...\\n  },\\n  \"devDependencies\": {\\n    ...\\n  }\\n}\\n\\nNote that the above script disables Vitestâ€™s default watch mode for running evals since many evaluators may include longer running LLM calls.\\n\\u200bJest\\nInstall the required development dependencies if you have not already:\\nyarnnpmpnpmCopyyarn add -D jest dotenv\\n\\nThe examples below also require openai (and of course langsmith!) as a dependency:\\nyarnnpmpnpmCopyyarn add langsmith openai\\n\\nThe setup instructions below are for basic JS files and CJS. To add support for TypeScript and ESM, see Jestâ€™s official docs or use Vitest.\\nThen create a separate config file named ls.jest.config.cjs:\\nCopymodule.exports = {\\n  testMatch: [\"**/*.eval.?(c|m)[jt]s\"],\\n  reporters: [\"langsmith/jest/reporter\"],\\n  setupFiles: [\"dotenv/config\"],\\n};\\n\\n\\ntestMatch ensures that only files ending with some variation of eval.js in your project are run\\nreporters is responsible for nicely formatting your output as shown above\\nsetupFiles runs dotenv to load environment variables before running your evals\\n\\nJSDom environments are not supported at this time. You should either omit the \"testEnvironment\" field from your config or set it to \"node\".\\nFinally, add the following to the scripts field in your package.json to run Jest with the config you just created:\\nCopy{\\n  \"name\": \"YOUR_PROJECT_NAME\",\\n  \"scripts\": {\\n    \"eval\": \"jest --config ls.jest.config.cjs\"\\n  },\\n  \"dependencies\": {\\n    ...\\n  },\\n  \"devDependencies\": {\\n    ...\\n  }\\n}\\n\\n\\u200bDefine and run evals\\nYou can now define evals as tests using familiar Vitest/Jest syntax, with a few caveats:\\n\\nYou should import describe and test from the langsmith/jest or langsmith/vitest entrypoint\\nYou must wrap your test cases in a describe block\\nWhen declaring tests, the signature is slightly different - there is an extra argument containing example inputs and expected outputs\\n\\nTry it out by creating a file named sql.eval.ts (or sql.eval.js if you are using Jest without TypeScript) and pasting the below contents into it:\\nCopyimport * as ls from \"langsmith/vitest\";\\nimport { expect } from \"vitest\";\\n// import * as ls from \"langsmith/jest\";\\n// import { expect } from \"@jest/globals\";\\nimport OpenAI from \"openai\";\\nimport { traceable } from \"langsmith/traceable\";\\nimport { wrapOpenAI } from \"langsmith/wrappers/openai\";\\n\\n// Add \"openai\" as a dependency and set OPENAI_API_KEY as an environment variable\\nconst tracedClient = wrapOpenAI(new OpenAI());\\n\\nconst generateSql = traceable(\\n  async (userQuery: string) => {\\n    const result = await tracedClient.chat.completions.create({\\n      model: \"gpt-4o-mini\",\\n      messages: [\\n        {\\n          role: \"system\",\\n          content:\\n            \"Convert the user query to a SQL query. Do not wrap in any markdown tags.\",\\n        },\\n        {\\n          role: \"user\",\\n          content: userQuery,\\n        },\\n      ],\\n    });\\n    return result.choices[0].message.content;\\n  },\\n  { name: \"generate_sql\" }\\n);\\n\\nls.describe(\"generate sql demo\", () => {\\n  ls.test(\\n    \"generates select all\",\\n    {\\n      inputs: { userQuery: \"Get all users from the customers table\" },\\n      referenceOutputs: { sql: \"SELECT * FROM customers;\" },\\n    },\\n    async ({ inputs, referenceOutputs }) => {\\n      const sql = await generateSql(inputs.userQuery);\\n      ls.logOutputs({ sql }); // <-- Log run outputs, optional\\n      expect(sql).toEqual(referenceOutputs?.sql); // <-- Assertion result logged under \\'pass\\' feedback key\\n    }\\n  );\\n});\\n\\nYou can think of each ls.test() case as corresponding to a dataset example, and ls.describe() as defining a LangSmith dataset. If you have LangSmith tracing environment variables set when you run the test suite, the SDK does the following:\\n\\ncreates a dataset with the same name as the name passed to ls.describe() in LangSmith if it does not exist\\ncreates an example in the dataset for each input and expected output passed into a test case if a matching one does not already exist\\ncreates a new experiment with one result for each test case\\ncollects the pass/fail rate under the pass feedback key for each test case\\n\\nWhen you run this test it will have a default pass boolean feedback key based on the test case passing / failing. It will also track any outputs that you log with the ls.logOutputs() or return from the test function as â€œactualâ€ result values from your app for the experiment.\\nCreate a .env file with your OPENAI_API_KEY and LangSmith credentials if you donâ€™t already have one:\\nCopyOPENAI_API_KEY=\"YOUR_KEY_HERE\"\\nLANGSMITH_API_KEY=\"YOUR_LANGSMITH_KEY\"\\nLANGSMITH_TRACING=\"true\"\\n\\nNow use the eval script we set up in the previous step to run the test:\\nyarnnpmpnpmCopyyarn run eval\\n\\nAnd your declared test should run!\\nOnce it finishes, if youâ€™ve set your LangSmith environment variables, you should see a link directing you to an experiment created in LangSmith alongside the test results.\\nHereâ€™s what an experiment against that test suite looks like:\\n\\n\\u200bTrace feedback\\nBy default LangSmith collects the pass/fail rate under the pass feedback key for each test case. You can add additional feedback with either ls.logFeedback() or wrapEvaluator(). To do so, try the following as your sql.eval.ts file (or sql.eval.js if you are using Jest without TypeScript):\\nCopyimport * as ls from \"langsmith/vitest\";\\n// import * as ls from \"langsmith/jest\";\\nimport OpenAI from \"openai\";\\nimport { traceable } from \"langsmith/traceable\";\\nimport { wrapOpenAI } from \"langsmith/wrappers/openai\";\\n\\n// Add \"openai\" as a dependency and set OPENAI_API_KEY as an environment variable\\nconst tracedClient = wrapOpenAI(new OpenAI());\\n\\nconst generateSql = traceable(\\n  async (userQuery: string) => {\\n    const result = await tracedClient.chat.completions.create({\\n      model: \"gpt-4o-mini\",\\n      messages: [\\n        {\\n          role: \"system\",\\n          content:\\n            \"Convert the user query to a SQL query. Do not wrap in any markdown tags.\",\\n        },\\n        {\\n          role: \"user\",\\n          content: userQuery,\\n        },\\n      ],\\n    });\\n    return result.choices[0].message.content ?? \"\";\\n  },\\n  { name: \"generate_sql\" }\\n);\\n\\nconst myEvaluator = async (params: {\\n  outputs: { sql: string };\\n  referenceOutputs: { sql: string };\\n}) => {\\n  const { outputs, referenceOutputs } = params;\\n  const instructions = [\\n    \"Return 1 if the ACTUAL and EXPECTED answers are semantically equivalent, \",\\n    \"otherwise return 0. Return only 0 or 1 and nothing else.\",\\n  ].join(\"\\\\n\");\\n  const grade = await tracedClient.chat.completions.create({\\n    model: \"gpt-4o-mini\",\\n    messages: [\\n      {\\n        role: \"system\",\\n        content: instructions,\\n      },\\n      {\\n        role: \"user\",\\n        content: `ACTUAL: ${outputs.sql}\\\\nEXPECTED: ${referenceOutputs?.sql}`,\\n      },\\n    ],\\n  });\\n  const score = parseInt(grade.choices[0].message.content ?? \"\");\\n  return { key: \"correctness\", score };\\n};\\n\\nls.describe(\"generate sql demo\", () => {\\n  ls.test(\\n    \"generates select all\",\\n    {\\n      inputs: { userQuery: \"Get all users from the customers table\" },\\n      referenceOutputs: { sql: \"SELECT * FROM customers;\" },\\n    },\\n    async ({ inputs, referenceOutputs }) => {\\n      const sql = await generateSql(inputs.userQuery);\\n      ls.logOutputs({ sql });\\n      const wrappedEvaluator = ls.wrapEvaluator(myEvaluator);\\n      // Will automatically log \"correctness\" as feedback\\n      await wrappedEvaluator({\\n        outputs: { sql },\\n        referenceOutputs,\\n      });\\n      // You can also manually log feedback with `ls.logFeedback()`\\n      ls.logFeedback({\\n        key: \"harmfulness\",\\n        score: 0.2,\\n      });\\n    }\\n  );\\n  ls.test(\\n    \"offtopic input\",\\n    {\\n      inputs: { userQuery: \"whats up\" },\\n      referenceOutputs: { sql: \"sorry that is not a valid query\" },\\n    },\\n    async ({ inputs, referenceOutputs }) => {\\n      const sql = await generateSql(inputs.userQuery);\\n      ls.logOutputs({ sql });\\n      const wrappedEvaluator = ls.wrapEvaluator(myEvaluator);\\n      // Will automatically log \"correctness\" as feedback\\n      await wrappedEvaluator({\\n        outputs: { sql },\\n        referenceOutputs,\\n      });\\n      // You can also manually log feedback with `ls.logFeedback()`\\n      ls.logFeedback({\\n        key: \"harmfulness\",\\n        score: 0.2,\\n      });\\n    }\\n  );\\n});\\n\\nNote the use of ls.wrapEvaluator() around the myEvaluator function. This makes it so that the LLM-as-judge call is traced separately from the rest of the test case to avoid clutter, and conveniently creates feedback if the return value from the wrapped function matches { key: string; score: number | boolean }. In this case, instead of showing up in the main test case run, the evaluator trace will instead show up in a trace associated with the correctness feedback key.\\nYou can see the evaluator runs in LangSmith by clicking their corresponding feedback chips in the UI.\\n\\u200bRunning multiple examples against a test case\\nYou can run the same test case over multiple examples and parameterize your tests using ls.test.each(). This is useful when you want to evaluate your app the same way against different inputs:\\nCopyimport * as ls from \"langsmith/vitest\";\\n// import * as ls from \"langsmith/jest\";\\n\\nconst DATASET = [\\n  {\\n    inputs: { userQuery: \"whats up\" },\\n    referenceOutputs: { sql: \"sorry that is not a valid query\" }\\n  },\\n  {\\n    inputs: { userQuery: \"what color is the sky?\" },\\n    referenceOutputs: { sql: \"sorry that is not a valid query\" }\\n  },\\n  {\\n    inputs: { userQuery: \"how are you today?\" },\\n    referenceOutputs: { sql: \"sorry that is not a valid query\" }\\n  }\\n];\\n\\nls.describe(\"generate sql demo\", () => {\\n  ls.test.each(DATASET)(\\n    \"offtopic inputs\",\\n    async ({ inputs, referenceOutputs }) => {\\n      ...\\n    },\\n  );\\n});\\n\\nIf you have tracking enabled, each example in the local dataset will be synced to the one created in LangSmith.\\n\\u200bLog outputs\\nEvery time we run a test weâ€™re syncing it to a dataset example and tracing it as a run. To trace final outputs for the run, you can use ls.logOutputs() like this:\\nCopyimport * as ls from \"langsmith/vitest\";\\n// import * as ls from \"langsmith/jest\";\\n\\nls.describe(\"generate sql demo\", () => {\\n  ls.test(\\n    \"offtopic input\",\\n    {\\n      inputs: { userQuery: \"...\" },\\n      referenceOutputs: { sql: \"...\" }\\n    },\\n    async ({ inputs, referenceOutputs }) => {\\n      ls.logOutputs({ sql: \"SELECT * FROM users;\" })\\n    },\\n  );\\n});\\n\\nThe logged outputs will appear in your reporter summary and in LangSmith.\\nYou can also directly return a value from your test function:\\nCopyimport * as ls from \"langsmith/vitest\";\\n// import * as ls from \"langsmith/jest\";\\n\\nls.describe(\"generate sql demo\", () => {\\n  ls.test(\\n    \"offtopic input\",\\n    {\\n      inputs: { userQuery: \"...\" },\\n      referenceOutputs: { sql: \"...\" }\\n    },\\n    async ({ inputs, referenceOutputs }) => {\\n      return { sql: \"SELECT * FROM users;\" }\\n    },\\n  );\\n});\\n\\nHowever keep in mind if you do this that if your test fails to complete due to a failed assertion or other error, your output will not appear.\\n\\u200bTrace intermediate calls\\nLangSmith will automatically trace any traceable intermediate calls that happen in the course of test case execution.\\n\\u200bFocusing or skipping tests\\nYou can chain the Vitest/Jest .skip and .only methods on ls.test() and ls.describe():\\nCopyimport * as ls from \"langsmith/vitest\";\\n// import * as ls from \"langsmith/jest\";\\n\\nls.describe(\"generate sql demo\", () => {\\n  ls.test.skip(\\n    \"offtopic input\",\\n    {\\n      inputs: { userQuery: \"...\" },\\n      referenceOutputs: { sql: \"...\" }\\n    },\\n    async ({ inputs, referenceOutputs }) => {\\n      return { sql: \"SELECT * FROM users;\" }\\n    },\\n  );\\n  ls.test.only(\\n    \"other\",\\n    {\\n      inputs: { userQuery: \"...\" },\\n      referenceOutputs: { sql: \"...\" }\\n    },\\n    async ({ inputs, referenceOutputs }) => {\\n      return { sql: \"SELECT * FROM users;\" }\\n    },\\n  );\\n});\\n\\n\\u200bConfiguring test suites\\nYou can configure test suites with values like metadata or a custom client by passing an extra argument to ls.describe() for the full suite or by passing a config field into ls.test() for individual tests:\\nCopyls.describe(\"test suite name\", () => {\\n  ls.test(\\n    \"test name\",\\n    {\\n      inputs: { ... },\\n      referenceOutputs: { ... },\\n      // Extra config for the test run\\n      config: { tags: [...], metadata: { ... } }\\n    },\\n    {\\n      name: \"test name\",\\n      tags: [\"tag1\", \"tag2\"],\\n      skip: true,\\n      only: true,\\n    }\\n  );\\n}, {\\n  testSuiteName: \"overridden value\",\\n  metadata: { ... },\\n  // Custom client\\n  client: new Client(),\\n});\\n\\nThe test suite will also automatically extract environment variables from process.env.ENVIRONMENT, process.env.NODE_ENV and process.env.LANGSMITH_ENVIRONMENT and set them as metadata on created experiments. You can then filter experiments by metadata in LangSmithâ€™s UI.\\nSee the API refs for a full list of configuration options.\\n\\u200bDry-run mode\\nIf you want to run the tests without syncing the results to LangSmith, you can set omit your LangSmith tracing environment variables or set LANGSMITH_TEST_TRACKING=false in your environment.\\nThe tests will run as normal, but the experiment logs will not be sent to LangSmith.Was this page helpful?YesNoSuggest editsRun evaluations with pytestWith the APIâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials', 'loc': 'https://docs.smith.langchain.com/evaluation/tutorials', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Evaluation quickstart - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewQuickstartsTrace an applicationEvaluate an applicationTest promptsAPI & SDKsAPI referencePython SDKJS/TS SDKPricingPlansPricing FAQOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationQuickstartsEvaluation quickstartGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageLangSmith SDK1. Install dependencies2. Create a LangSmith API key3. Set up environment variables4. Create a dataset5. Define what youâ€™re evaluating6. Define evaluator7. Run and view resultsNext stepsLangSmith UI1. Navigate to the playground2. Create a prompt3. Create a dataset4. Add an evaluator5. Run your evaluationNext stepsVideo guideQuickstartsEvaluation quickstartCopy pageCopy pageEvaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs donâ€™t always behave predictably â€” small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\\nEvaluations are made up of three components:\\n\\nA dataset with test inputs and optionally expected outputs.\\nA target function that defines what youâ€™re evaluating. For example, this may be one LLM call that includes the new prompt you are testing, a part of your application or your end to end application.\\nEvaluators that score your target functionâ€™s outputs.\\n\\nThis quick start guides you through running a simple evaluation to test the correctness of LLM responses with the LangSmith SDK or UI.\\nThis guide uses prebuilt LLM-as-judge evaluators from the open-source openevals package. OpenEvals includes a set of commonly used evaluators and is a great starting point if youâ€™re new to evaluations. If you want greater flexibility in how you evaluate your apps, you can also define completely custom evaluators using your own code.\\n\\u200bLangSmith SDK\\n\\u200b1. Install dependencies\\nPythonTypeScriptCopypip install -U langsmith openevals openai\\n\\nIf you are using yarn as your package manager, you will also need to manually install @langchain/core as a peer dependency of openevals. This is not required for LangSmith evals in general - you may define evaluators using arbitrary custom code.\\n\\u200b2. Create a LangSmith API key\\nTo create an API key, head to the Settings page. Then click + API Key.\\n\\u200b3. Set up environment variables\\nThis guide uses OpenAI, but you can adapt it to use any LLM provider.\\nIf youâ€™re using Anthropic, use the Anthropic wrapper to trace your calls. For other providers, use the traceable wrapper.\\nCopyexport LANGSMITH_TRACING=true\\nexport LANGSMITH_API_KEY=\"<your-langchain-api-key>\"\\nexport OPENAI_API_KEY=\"<your-openai-api-key>\"\\n\\n\\u200b4. Create a dataset\\nNext, define example input and reference output pairs that youâ€™ll use to evaluate your app:\\nPythonTypeScriptCopyfrom langsmith import Client\\nclient = Client()\\n\\n# Programmatically create a dataset in LangSmith\\n# For other dataset creation methods, see:\\n# https://docs.smith.langchain.com/langsmith/manage-datasets-programmatically\\n# https://docs.smith.langchain.com/langsmith/manage-datasets-in-application\\ndataset = client.create_dataset(\\n    dataset_name=\"Sample dataset\", description=\"A sample dataset in LangSmith.\"\\n)\\n\\n# Create examples\\nexamples = [\\n    {\\n        \"inputs\": {\"question\": \"Which country is Mount Kilimanjaro located in?\"},\\n        \"outputs\": {\"answer\": \"Mount Kilimanjaro is located in Tanzania.\"},\\n    },\\n    {\\n        \"inputs\": {\"question\": \"What is Earth\\'s lowest point?\"},\\n        \"outputs\": {\"answer\": \"Earth\\'s lowest point is The Dead Sea.\"},\\n    },\\n]\\n\\n# Add examples to the dataset\\nclient.create_examples(dataset_id=dataset.id, examples=examples)\\n\\n\\u200b5. Define what youâ€™re evaluating\\nNow, define a target function that contains what youâ€™re evaluating. In this guide, weâ€™ll define a target function that contains a single LLM call to answer a question.\\nPythonTypeScriptCopyfrom langsmith import wrappers\\nfrom openai import OpenAI\\n\\n# Wrap the OpenAI client for LangSmith tracing\\nopenai_client = wrappers.wrap_openai(OpenAI())\\n\\n# Define the application logic you want to evaluate inside a target function\\n# The SDK will automatically send the inputs from the dataset to your target function\\ndef target(inputs: dict) -> dict:\\n    response = openai_client.chat.completions.create(\\n        model=\"gpt-4o-mini\",\\n        messages=[\\n            {\"role\": \"system\", \"content\": \"Answer the following question accurately\"},\\n            {\"role\": \"user\", \"content\": inputs[\"question\"]},\\n        ],\\n    )\\n    return { \"answer\": response.choices[0].message.content.strip() }\\n\\n\\u200b6. Define evaluator\\nImport a prebuilt prompt from openevals and create an evaluator. outputs are the result of your target function. reference_outputs / referenceOutputs are from the example pairs you defined in step 4 above.\\nCORRECTNESS_PROMPT is just an f-string with variables for \"inputs\", \"outputs\", and \"reference_outputs\". See here for more information on customizing OpenEvals prompts.\\nPythonTypeScriptCopyfrom openevals.llm import create_llm_as_judge\\nfrom openevals.prompts import CORRECTNESS_PROMPT\\n\\ndef correctness_evaluator(inputs: dict, outputs: dict, reference_outputs: dict):\\n    evaluator = create_llm_as_judge(\\n        prompt=CORRECTNESS_PROMPT,\\n        model=\"openai:o3-mini\",\\n        feedback_key=\"correctness\",\\n    )\\n    eval_result = evaluator(\\n        inputs=inputs,\\n        outputs=outputs,\\n        reference_outputs=reference_outputs\\n    )\\n    return eval_result\\n\\n\\u200b7. Run and view results\\nFinally, run the experiment!\\nPythonTypeScriptCopy# After running the evaluation, a link will be provided to view the results in langsmith\\nexperiment_results = client.evaluate(\\n    target,\\n    data=\"Sample dataset\",\\n    evaluators=[\\n        correctness_evaluator,\\n        # can add multiple evaluators here\\n    ],\\n    experiment_prefix=\"first-eval-in-langsmith\",\\n    max_concurrency=2,\\n)\\n\\nClick the link printed out by your evaluation run to access the LangSmith Experiments UI, and explore the results of the experiment.\\n\\n\\u200bNext steps\\nTo learn more about running experiments in LangSmith, read the evaluation conceptual guide.\\n\\nFor more details on evaluations, refer to the Evaluation documentation.\\nCheck out the OpenEvals README to see all available prebuilt evaluators and how to customize them.\\nLearn how to define custom evaluators that contain arbitrary code.\\nFor comprehensive descriptions of every class and function see the Python or Typescript SDK references.\\n\\nOr, if you prefer video tutorials, check out the Datasets, Evaluators, and Experiments videos from the Introduction to LangSmith Course.\\n\\u200bLangSmith UI\\n\\u200b1. Navigate to the playground\\nLangSmithâ€™s prompt playground makes it possible to run evaluations over different prompts, new models or test different model configurations. Go to LangSmithâ€™s Playground in the UI.\\n\\u200b2. Create a prompt\\nModify the system prompt to:\\nCopyAnswer the following question accurately:\\n\\n\\u200b3. Create a dataset\\nClick Set up Evaluation, then use the + New button in the dropdown to create a new dataset.\\nAdd the following examples to the dataset:\\nInputsReference Outputsquestion: Which country is Mount Kilimanjaro located in?output: Mount Kilimanjaro is located in Tanzania.question: What is Earthâ€™s lowest point?output: Earthâ€™s lowest point is The Dead Sea.\\nPress Save to save your newly created dataset.\\n\\u200b4. Add an evaluator\\nClick +Evaluator. Select Correctness from the pre-built evaluator options. Press Save.\\n\\u200b5. Run your evaluation\\nPress Start on the top right to run your evaluation. Running this evaluation will create an experiment that you can view in full by clicking the experiment name.\\n\\n\\u200bNext steps\\nTo learn more about running experiments in LangSmith, read the evaluation conceptual guide.\\n\\n\\nFor more details on evaluations, refer to the Evaluation documentation.\\n\\n\\nLearn how to create and manage datasets in the UI\\n\\n\\nLearn how to run an evaluation from the prompt playground\\n\\n\\nIf you prefer video tutorials, check out the Playground videos from the Introduction to LangSmith Course.\\n\\u200bVideo guide\\nWas this page helpful?YesNoSuggest editsTrace an applicationTest promptsâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/agents', 'loc': 'https://docs.smith.langchain.com/evaluation/tutorials/agents', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Evaluate a complex agent - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesImprove evaluatorsTutorialsEvaluate a chatbotEvaluate a RAG applicationTest a ReAct agent with Pytest/Vitest and LangSmithEvaluate a complex agentRun backtests on a new version of an agentAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationTutorialsEvaluate a complex agentGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageSetupConfigure the environmentDownload the databaseDefine the customer support agentRefund agentLookup agentParent agentTry it outEvaluationsFinal response evaluatorTrajectory evaluatorSingle step evaluatorsReference codeSet up evaluationsTutorialsEvaluate a complex agentCopy pageCopy pageAgent evaluation | Evaluators | LLM-as-judge evaluators\\nIn this tutorial, weâ€™ll build a customer support bot that helps users navigate a digital music store. Then, weâ€™ll go through the three most effective types of evaluations to run on chat bots:\\n\\nFinal response: Evaluate the agentâ€™s final response.\\nTrajectory: Evaluate whether the agent took the expected path (e.g., of tool calls) to arrive at the final answer.\\nSingle step: Evaluate any agent step in isolation (e.g., whether it selects the appropriate first tool for a given step).\\n\\nWeâ€™ll build our agent using LangGraph, but the techniques and LangSmith functionality shown here are framework-agnostic.\\n\\u200bSetup\\n\\u200bConfigure the environment\\nLetâ€™s install the required dependencies:\\npipuvCopypip install -U langgraph langchain[openai]\\n\\nLetâ€™s set up environment variables for OpenAI and LangSmith:\\nCopyimport getpass\\nimport os\\n\\ndef _set_env(var: str) -> None:\\n    if not os.environ.get(var):\\n        os.environ[var] = getpass.getpass(f\"Set {var}: \")\\n\\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\\n_set_env(\"LANGSMITH_API_KEY\")\\n_set_env(\"OPENAI_API_KEY\")\\n\\n\\u200bDownload the database\\nWe will create a SQLite database for this tutorial. SQLite is a lightweight database that is easy to set up and use. We will load the chinook database, which is a sample database that represents a digital media store. Find more information about the database here.\\nFor convenience, we have hosted the database in a public GCS bucket:\\nCopyimport requests\\n\\nurl = \"https://storage.googleapis.com/benchmarks-artifacts/chinook/Chinook.db\"\\nresponse = requests.get(url)\\n\\nif response.status_code == 200:\\n    # Open a local file in binary write mode\\n    with open(\"chinook.db\", \"wb\") as file:\\n        # Write the content of the response (the file) to the local file\\n        file.write(response.content)\\n    print(\"File downloaded and saved as Chinook.db\")\\nelse:\\n    print(f\"Failed to download the file. Status code: {response.status_code}\")\\n\\nHereâ€™s a sample of the data in the db:\\nCopyimport sqlite3\\n# ... database connection and query code\\n\\nCopy[(1, \\'AC/DC\\'), (2, \\'Accept\\'), (3, \\'Aerosmith\\'), (4, \\'Alanis Morissette\\'), (5, \\'Alice In Chains\\'), (6, \\'AntÃ´nio Carlos Jobim\\'), (7, \\'Apocalyptica\\'), (8, \\'Audioslave\\'), (9, \\'BackBeat\\'), (10, \\'Billy Cobham\\')]\\n\\nAnd hereâ€™s the database schema (image from https://github.com/lerocha/chinook-database):\\n\\n\\u200bDefine the customer support agent\\nWeâ€™ll create a LangGraph agent with limited access to our database. For demo purposes, our agent will support two basic types of requests:\\n\\nLookup: The customer can look up song titles, artist names, and albums based on other identifying information. For example: â€œWhat songs do you have by Jimi Hendrix?â€\\nRefund: The customer can request a refund on their past purchases. For example: â€œMy name is Claude Shannon and Iâ€™d like a refund on a purchase I made last week, could you help me?â€\\n\\nFor simplicity in this demo, weâ€™ll implement refunds by deleting the corresponding database records. Weâ€™ll skip implementing user authentication and other production security measures.\\nThe agentâ€™s logic will be structured as two separate subgraphs (one for lookups and one for refunds), with a parent graph that routes requests to the appropriate subgraph.\\n\\u200bRefund agent\\nLetâ€™s build the refund processing agent. This agent needs to:\\n\\nFind the customerâ€™s purchase records in the database\\nDelete the relevant Invoice and InvoiceLine records to process the refund\\n\\nWeâ€™ll create two SQL helper functions:\\n\\nA function to execute the refund by deleting records\\nA function to look up a customerâ€™s purchase history\\n\\nTo make testing easier, weâ€™ll add a â€œmockâ€ mode to these functions. When mock mode is enabled, the functions will simulate database operations without actually modifying any data.\\nCopyimport sqlite3\\n\\ndef _refund(invoice_id: int | None, invoice_line_ids: list[int] | None, mock: bool = False) -> float:\\n    ...\\n\\ndef _lookup( ...\\n\\nNow letâ€™s define our graph. Weâ€™ll use a simple architecture with three main paths:\\n\\n\\nExtract customer and purchase information from the conversation\\n\\n\\nRoute the request to one of three paths:\\n\\nRefund path: If we have sufficient purchase details (Invoice ID or Invoice Line IDs) to process a refund\\nLookup path: If we have enough customer information (name and phone) to search their purchase history\\nResponse path: If we need more information, respond to the user requesting the specific details needed\\n\\n\\n\\nThe graphâ€™s state will track:\\n\\nThe conversation history (messages between user and agent)\\nAll customer and purchase information extracted from the conversation\\nThe next message to send to the user (followup text)\\n\\nCopyfrom typing import Literal\\nimport json\\n\\nfrom langchain.chat_models import init_chat_model\\nfrom langchain_core.runnables import RunnableConfig\\nfrom langgraph.graph import END, StateGraph\\nfrom langgraph.graph.message import AnyMessage, add_messages\\nfrom langgraph.types import Command, interrupt\\nfrom tabulate import tabulate\\nfrom typing_extensions import Annotated, TypedDict\\n\\n# Graph state.\\nclass State(TypedDict):\\n    \"\"\"Agent state.\"\"\"\\n    messages: Annotated[list[AnyMessage], add_messages]\\n    followup: str | None\\n\\n    invoice_id: int | None\\n    invoice_line_ids: list[int] | None\\n    customer_first_name: str | None\\n    customer_last_name: str | None\\n    customer_phone: str | None\\n    track_name: str | None\\n    album_title: str | None\\n    artist_name: str | None\\n    purchase_date_iso_8601: str | None\\n\\n# Instructions for extracting the user/purchase info from the conversation.\\ngather_info_instructions = \"\"\"You are managing an online music store that sells song tracks. \\\\\\nCustomers can buy multiple tracks at a time and these purchases are recorded in a database as \\\\\\nan Invoice per purchase and an associated set of Invoice Lines for each purchased track.\\n\\nYour task is to help customers who would like a refund for one or more of the tracks they\\'ve \\\\\\npurchased. In order for you to be able refund them, the customer must specify the Invoice ID \\\\\\nto get a refund on all the tracks they bought in a single transaction, or one or more Invoice \\\\\\nLine IDs if they would like refunds on individual tracks.\\n\\nOften a user will not know the specific Invoice ID(s) or Invoice Line ID(s) for which they \\\\\\nwould like a refund. In this case you can help them look up their invoices by asking them to \\\\\\nspecify:\\n- Required: Their first name, last name, and phone number.\\n- Optionally: The track name, artist name, album name, or purchase date.\\n\\nIf the customer has not specified the required information (either Invoice/Invoice Line IDs \\\\\\nor first name, last name, phone) then please ask them to specify it.\"\"\"\\n\\n# Extraction schema, mirrors the graph state.\\nclass PurchaseInformation(TypedDict):\\n    \"\"\"All of the known information about the invoice / invoice lines the customer would like refunded. Do not make up values, leave fields as null if you don\\'t know their value.\"\"\"\\n\\n    invoice_id: int | None\\n    invoice_line_ids: list[int] | None\\n    customer_first_name: str | None\\n    customer_last_name: str | None\\n    customer_phone: str | None\\n    track_name: str | None\\n    album_title: str | None\\n    artist_name: str | None\\n    purchase_date_iso_8601: str | None\\n    followup: Annotated[\\n        str | None,\\n        ...,\\n        \"If the user hasn\\'t enough identifying information, please tell them what the required information is and ask them to specify it.\",\\n    ]\\n\\n# Model for performing extraction.\\ninfo_llm = init_chat_model(\"gpt-4o-mini\").with_structured_output(\\n    PurchaseInformation, method=\"json_schema\", include_raw=True\\n)\\n\\n# Graph node for extracting user info and routing to lookup/refund/END.\\nasync def gather_info(state: State) -> Command[Literal[\"lookup\", \"refund\", END]]:\\n    info = await info_llm.ainvoke(\\n        [\\n            {\"role\": \"system\", \"content\": gather_info_instructions},\\n            *state[\"messages\"],\\n        ]\\n    )\\n    parsed = info[\"parsed\"]\\n    if any(parsed[k] for k in (\"invoice_id\", \"invoice_line_ids\")):\\n        goto = \"refund\"\\n    elif all(\\n        parsed[k]\\n        for k in (\"customer_first_name\", \"customer_last_name\", \"customer_phone\")\\n    ):\\n        goto = \"lookup\"\\n    else:\\n        goto = END\\n    update = {\"messages\": [info[\"raw\"]], **parsed}\\n    return Command(update=update, goto=goto)\\n\\n# Graph node for executing the refund.\\n# Note that here we inspect the runtime config for an \"env\" variable.\\n# If \"env\" is set to \"test\", then we don\\'t actually delete any rows from our database.\\n# This will become important when we\\'re running our evaluations.\\ndef refund(state: State, config: RunnableConfig) -> dict:\\n    # Whether to mock the deletion. True if the configurable var \\'env\\' is set to \\'test\\'.\\n    mock = config.get(\"configurable\", {}).get(\"env\", \"prod\") == \"test\"\\n    refunded = _refund(\\n        invoice_id=state[\"invoice_id\"], invoice_line_ids=state[\"invoice_line_ids\"], mock=mock\\n    )\\n    response = f\"You have been refunded a total of: ${refunded:.2f}. Is there anything else I can help with?\"\\n    return {\\n        \"messages\": [{\"role\": \"assistant\", \"content\": response}],\\n        \"followup\": response,\\n    }\\n\\n# Graph node for looking up the users purchases\\ndef lookup(state: State) -> dict:\\n    args = (\\n        state[k]\\n        for k in (\\n            \"customer_first_name\",\\n            \"customer_last_name\",\\n            \"customer_phone\",\\n            \"track_name\",\\n            \"album_title\",\\n            \"artist_name\",\\n            \"purchase_date_iso_8601\",\\n        )\\n    )\\n    results = _lookup(*args)\\n    if not results:\\n        response = \"We did not find any purchases associated with the information you\\'ve provided. Are you sure you\\'ve entered all of your information correctly?\"\\n        followup = response\\n    else:\\n        response = f\"Which of the following purchases would you like to be refunded for?\\\\n\\\\n```json{json.dumps(results, indent=2)}\\\\n```\"\\n        followup = f\"Which of the following purchases would you like to be refunded for?\\\\n\\\\n{tabulate(results, headers=\\'keys\\')}\"\\n    return {\\n        \"messages\": [{\"role\": \"assistant\", \"content\": response}],\\n        \"followup\": followup,\\n        \"invoice_line_ids\": [res[\"invoice_line_id\"] for res in results],\\n    }\\n\\n# Building our graph\\ngraph_builder = StateGraph(State)\\n\\ngraph_builder.add_node(gather_info)\\ngraph_builder.add_node(refund)\\ngraph_builder.add_node(lookup)\\n\\ngraph_builder.set_entry_point(\"gather_info\")\\ngraph_builder.add_edge(\"lookup\", END)\\ngraph_builder.add_edge(\"refund\", END)\\n\\nrefund_graph = graph_builder.compile()\\n\\nWe can visualize our refund graph:\\nCopy# Assumes you\\'re in an interactive Python environmentfrom IPython.display import Image, display ...\\n\\n\\n\\u200bLookup agent\\nFor the lookup (i.e. question-answering) agent, weâ€™ll use a simple ReACT architecture and give the agent tools for looking up track names, artist names, and album names based on various filters. For example, you can look up albums by a particular artist, artists who released songs with a specific name, etc.\\nCopyfrom langchain.embeddings import init_embeddings\\nfrom langchain_core.tools import tool\\nfrom langchain_core.vectorstores import InMemoryVectorStore\\nfrom langgraph.prebuilt import create_react_agent\\n\\n# Our SQL queries will only work if we filter on the exact string values that are in the DB.\\n# To ensure this, we\\'ll create vectorstore indexes for all of the artists, tracks and albums\\n# ahead of time and use those to disambiguate the user input. E.g. if a user searches for\\n# songs by \"prince\" and our DB records the artist as \"Prince\", ideally when we query our\\n# artist vectorstore for \"prince\" we\\'ll get back the value \"Prince\", which we can then\\n# use in our SQL queries.\\ndef index_fields() -> tuple[InMemoryVectorStore, InMemoryVectorStore, InMemoryVectorStore]: ...\\n\\ntrack_store, artist_store, album_store = index_fields()\\n\\n# Agent tools\\n@tool\\ndef lookup_track( ...\\n\\n@tool\\ndef lookup_album( ...\\n\\n@tool\\ndef lookup_artist( ...\\n\\n# Agent model\\nqa_llm = init_chat_model(\"claude-3-5-sonnet-latest\")\\n# The prebuilt ReACT agent only expects State to have a \\'messages\\' key, so the\\n# state we defined for the refund agent can also be passed to our lookup agent.\\nqa_graph = create_react_agent(qa_llm, [lookup_track, lookup_artist, lookup_album])\\n\\nCopydisplay(Image(qa_graph.get_graph(xray=True).draw_mermaid_png()))\\n\\n\\n\\u200bParent agent\\nNow letâ€™s define a parent agent that combines our two task-specific agents. The only job of the parent agent is to route to one of the sub-agents by classifying the userâ€™s current intent, and to compile the output into a followup message.\\nCopy# Schema for routing user intent.\\n# We\\'ll use structured outputs to enforce that the model returns only\\n# the desired output.\\nclass UserIntent(TypedDict):\\n    \"\"\"The user\\'s current intent in the conversation\"\"\"\\n\\n    intent: Literal[\"refund\", \"question_answering\"]\\n\\n# Routing model with structured output\\nrouter_llm = init_chat_model(\"gpt-4o-mini\").with_structured_output(\\n    UserIntent, method=\"json_schema\", strict=True\\n)\\n\\n# Instructions for routing.\\nroute_instructions = \"\"\"You are managing an online music store that sells song tracks. \\\\\\nYou can help customers in two types of ways: (1) answering general questions about \\\\\\ntracks sold at your store, (2) helping them get a refund on a purhcase they made at your store.\\n\\nBased on the following conversation, determine if the user is currently seeking general \\\\\\ninformation about song tracks or if they are trying to refund a specific purchase.\\n\\nReturn \\'refund\\' if they are trying to get a refund and \\'question_answering\\' if they are \\\\\\nasking a general music question. Do NOT return anything else. Do NOT try to respond to \\\\\\nthe user.\\n\"\"\"\\n\\n# Node for routing.\\nasync def intent_classifier(\\n    state: State,\\n) -> Command[Literal[\"refund_agent\", \"question_answering_agent\"]]:\\n    response = router_llm.invoke(\\n        [{\"role\": \"system\", \"content\": route_instructions}, *state[\"messages\"]]\\n    )\\n    return Command(goto=response[\"intent\"] + \"_agent\")\\n\\n# Node for making sure the \\'followup\\' key is set before our agent run completes.\\ndef compile_followup(state: State) -> dict:\\n    \"\"\"Set the followup to be the last message if it hasn\\'t explicitly been set.\"\"\"\\n    if not state.get(\"followup\"):\\n        return {\"followup\": state[\"messages\"][-1].content}\\n    return {}\\n\\n# Agent definition\\ngraph_builder = StateGraph(State)\\ngraph_builder.add_node(intent_classifier)\\n# Since all of our subagents have compatible state,\\n# we can add them as nodes directly.\\ngraph_builder.add_node(\"refund_agent\", refund_graph)\\ngraph_builder.add_node(\"question_answering_agent\", qa_graph)\\ngraph_builder.add_node(compile_followup)\\n\\ngraph_builder.set_entry_point(\"intent_classifier\")\\ngraph_builder.add_edge(\"refund_agent\", \"compile_followup\")\\ngraph_builder.add_edge(\"question_answering_agent\", \"compile_followup\")\\ngraph_builder.add_edge(\"compile_followup\", END)\\n\\ngraph = graph_builder.compile()\\n\\nWe can visualize our compiled parent graph including all of its subgraphs:\\nCopydisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\n\\u200bTry it out\\nLetâ€™s give our custom support agent a whirl!\\nCopystate = await graph.ainvoke(\\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what james brown songs do you have\"}]}\\n)\\nprint(state[\"followup\"])\\n\\nCopyI found 20 James Brown songs in the database, all from the album \"Sex Machine\". Here they are: ...\\n\\nCopystate = await graph.ainvoke({\"messages\": [\\n    {\\n        \"role\": \"user\",\\n        \"content\": \"my name is Aaron Mitchell and my number is +1 (204) 452-6452. I bought some songs by Led Zeppelin that i\\'d like refunded\",\\n    }\\n]})\\nprint(state[\"followup\"])\\n\\nCopyWhich of the following purchases would you like to be refunded for? ...\\n\\n\\u200bEvaluations\\nNow that weâ€™ve got a testable version of our agent, letâ€™s run some evaluations. Agent evaluation can focus on at least 3 things:\\n\\nFinal response: The inputs are a prompt and an optional list of tools. The output is the final agent response.\\nTrajectory: As before, the inputs are a prompt and an optional list of tools. The output is the list of tool calls\\nSingle step: As before, the inputs are a prompt and an optional list of tools. The output is the tool call.\\n\\nLetâ€™s run each type of evaluation:\\n\\u200bFinal response evaluator\\nFirst, letâ€™s create a dataset that evaluates end-to-end performance of the agent. For simplicity weâ€™ll use the same dataset for final response and trajectory evaluation, so weâ€™ll add both ground-truth responses and trajectories for each example question. Weâ€™ll cover the trajectories in the next section.\\nCopyfrom langsmith import Client\\n\\nclient = Client()\\n\\n# Create a dataset\\nexamples = [\\n    {\\n        \"inputs\": {\\n            \"question\": \"How many songs do you have by James Brown\",\\n        },\\n        \"outputs\": {\\n            \"response\": \"We have 20 songs by James Brown\",\\n            \"trajectory\": [\"question_answering_agent\", \"lookup_track\"]\\n        }\\n    },\\n    {\\n        \"inputs\": {\\n            \"question\": \"My name is Aaron Mitchell and I\\'d like a refund.\",\\n        },\\n        \"outputs\": {\\n            \"response\": \"I need some more information to help you with the refund. Please specify your phone number, the invoice ID, or the line item IDs for the purchase you\\'d like refunded.\",\\n            \"trajectory\": [\"refund_agent\"],\\n        }\\n    },\\n    {\\n        \"inputs\": {\\n            \"question\": \"My name is Aaron Mitchell and I\\'d like a refund on my Led Zeppelin purchases. My number is +1 (204) 452-6452\",\\n        },\\n        \"outputs\": {\\n            \"response\": \\'Which of the following purchases would you like to be refunded for?\\\\n\\\\n  invoice_line_id  track_name                        artist_name    purchase_date          quantity_purchased    price_per_unit\\\\n-----------------  --------------------------------  -------------  -------------------  --------------------  ----------------\\\\n              267  How Many More Times               Led Zeppelin   2009-08-06 00:00:00                     1              0.99\\\\n              268  What Is And What Should Never Be  Led Zeppelin   2009-08-06 00:00:00                     1              0.99\\',\\n            \"trajectory\": [\"refund_agent\", \"lookup\"],\\n        },\\n    },\\n    {\\n        \"inputs\": {\\n            \"question\": \"Who recorded Wish You Were Here again? What other albums of there\\'s do you have?\",\\n        },\\n        \"outputs\": {\\n            \"response\": \"Wish You Were Here is an album by Pink Floyd\",\\n            \"trajectory\": [\"question_answering_agent\", \"lookup_album\"],\\n        },\\n    },\\n    {\\n        \"inputs\": {\\n            \"question\": \"I want a full refund for invoice 237\",\\n        },\\n        \"outputs\": {\\n            \"response\": \"You have been refunded $0.99.\",\\n            \"trajectory\": [\"refund_agent\", \"refund\"],\\n        }\\n    },\\n]\\n\\ndataset_name = \"Chinook Customer Service Bot: E2E\"\\n\\nif not client.has_dataset(dataset_name=dataset_name):\\n    dataset = client.create_dataset(dataset_name=dataset_name)\\n    client.create_examples(\\n        dataset_id=dataset.id,\\n        examples=examples\\n    )\\n\\nWeâ€™ll create a custom LLM-as-judge evaluator that uses another model to compare our agentâ€™s output on each example to the reference response, and judge if theyâ€™re equivalent or not:\\nCopy# LLM-as-judge instructions\\ngrader_instructions = \"\"\"You are a teacher grading a quiz.\\n\\nYou will be given a QUESTION, the GROUND TRUTH (correct) RESPONSE, and the STUDENT RESPONSE.\\n\\nHere is the grade criteria to follow:\\n(1) Grade the student responses based ONLY on their factual accuracy relative to the ground truth answer.\\n(2) Ensure that the student response does not contain any conflicting statements.\\n(3) It is OK if the student response contains more information than the ground truth response, as long as it is factually accurate relative to the  ground truth response.\\n\\nCorrectness:\\nTrue means that the student\\'s response meets all of the criteria.\\nFalse means that the student\\'s response does not meet all of the criteria.\\n\\nExplain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct.\"\"\"\\n\\n# LLM-as-judge output schema\\nclass Grade(TypedDict):\\n    \"\"\"Compare the expected and actual answers and grade the actual answer.\"\"\"\\n    reasoning: Annotated[str, ..., \"Explain your reasoning for whether the actual response is correct or not.\"]\\n    is_correct: Annotated[bool, ..., \"True if the student response is mostly or exactly correct, otherwise False.\"]\\n\\n# Judge LLM\\ngrader_llm = init_chat_model(\"gpt-4o-mini\", temperature=0).with_structured_output(Grade, method=\"json_schema\", strict=True)\\n\\n# Evaluator function\\nasync def final_answer_correct(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\\n    \"\"\"Evaluate if the final response is equivalent to reference response.\"\"\"\\n\\n    # Note that we assume the outputs has a \\'response\\' dictionary. We\\'ll need to make sure\\n    # that the target function we define includes this key.\\n    user = f\"\"\"QUESTION: {inputs[\\'question\\']}\\n    GROUND TRUTH RESPONSE: {reference_outputs[\\'response\\']}\\n    STUDENT RESPONSE: {outputs[\\'response\\']}\"\"\"\\n\\n    grade = await grader_llm.ainvoke([{\"role\": \"system\", \"content\": grader_instructions}, {\"role\": \"user\", \"content\": user}])\\n    return grade[\"is_correct\"]\\n\\nNow we can run our evaluation. Our evaluator assumes that our target function returns a â€˜responseâ€™ key, so lets define a target function that does so.\\nAlso remember that in our refund graph we made the refund node configurable, so that if we specified config={\"env\": \"test\"}, we would mock out the refunds without actually updating the DB. Weâ€™ll use this configurable variable in our target run_graph method when invoking our graph:\\nCopy# Target function\\nasync def run_graph(inputs: dict) -> dict:\\n    \"\"\"Run graph and track the trajectory it takes along with the final response.\"\"\"\\n    result = await graph.ainvoke({\"messages\": [\\n        { \"role\": \"user\", \"content\": inputs[\\'question\\']},\\n    ]}, config={\"env\": \"test\"})\\n    return {\"response\": result[\"followup\"]}\\n\\n# Evaluation job and results\\nexperiment_results = await client.aevaluate(\\n    run_graph,\\n    data=dataset_name,\\n    evaluators=[final_answer_correct],\\n    experiment_prefix=\"sql-agent-gpt4o-e2e\",\\n    num_repetitions=1,\\n    max_concurrency=4,\\n)\\nexperiment_results.to_pandas()\\n\\nYou can see what these results look like here: LangSmith link.\\n\\u200bTrajectory evaluator\\nAs agents become more complex, they have more potential points of failure. Rather than using simple pass/fail evaluations, itâ€™s often better to use evaluations that can give partial credit when an agent takes some correct steps, even if it doesnâ€™t reach the right final answer.\\nThis is where trajectory evaluations come in. A trajectory evaluation:\\n\\nCompares the actual sequence of steps the agent took against an expected sequence\\nCalculates a score based on how many of the expected steps were completed correctly\\n\\nFor this example, our end-to-end dataset contains an ordered list of steps that we expect the agent to take. Letâ€™s create an evaluator that checks the agentâ€™s actual trajectory against these expected steps and calculates what percentage were completed:\\nCopydef trajectory_subsequence(outputs: dict, reference_outputs: dict) -> float:\\n    \"\"\"Check how many of the desired steps the agent took.\"\"\"\\n    if len(reference_outputs[\\'trajectory\\']) > len(outputs[\\'trajectory\\']):\\n        return False\\n\\n    i = j = 0\\n    while i < len(reference_outputs[\\'trajectory\\']) and j < len(outputs[\\'trajectory\\']):\\n        if reference_outputs[\\'trajectory\\'][i] == outputs[\\'trajectory\\'][j]:\\n            i += 1\\n        j += 1\\n\\n    return i / len(reference_outputs[\\'trajectory\\'])\\n\\nNow we can run our evaluation. Our evaluator assumes that our target function returns a â€˜trajectoryâ€™ key, so lets define a target function that does so. Weâ€™ll need to usage LangGraphâ€™s streaming capabilities to record the trajectory.\\nNote that we are reusing the same dataset as for our final response evaluation, so we could have run both evaluators together and defined a target function that returns both â€œresponseâ€ and â€œtrajectoryâ€. In practice itâ€™s often useful to have separate datasets for each type of evaluation, which is why we show them separately here:\\nCopyasync def run_graph(inputs: dict) -> dict:\\n    \"\"\"Run graph and track the trajectory it takes along with the final response.\"\"\"\\n    trajectory = []\\n    # Set subgraph=True to stream events from subgraphs of the main graph: https://langchain-ai.github.io/langgraph/how-tos/streaming-subgraphs/\\n    # Set stream_mode=\"debug\" to stream all possible events: https://langchain-ai.github.io/langgra/langsmith/observability-concepts/streaming\\n    async for namespace, chunk in graph.astream({\"messages\": [\\n            {\\n                \"role\": \"user\",\\n                \"content\": inputs[\\'question\\'],\\n            }\\n        ]}, subgraphs=True, stream_mode=\"debug\"):\\n        # Event type for entering a node\\n        if chunk[\\'type\\'] == \\'task\\':\\n            # Record the node name\\n            trajectory.append(chunk[\\'payload\\'][\\'name\\'])\\n            # Given how we defined our dataset, we also need to track when specific tools are\\n            # called by our question answering ReACT agent. These tool calls can be found\\n            # when the ToolsNode (named \"tools\") is invoked by looking at the AIMessage.tool_calls\\n            # of the latest input message.\\n            if chunk[\\'payload\\'][\\'name\\'] == \\'tools\\' and chunk[\\'type\\'] == \\'task\\':\\n                for tc in chunk[\\'payload\\'][\\'input\\'][\\'messages\\'][-1].tool_calls:\\n                    trajectory.append(tc[\\'name\\'])\\n    return {\"trajectory\": trajectory}\\n\\nexperiment_results = await client.aevaluate(\\n    run_graph,\\n    data=dataset_name,\\n    evaluators=[trajectory_subsequence],\\n    experiment_prefix=\"sql-agent-gpt4o-trajectory\",\\n    num_repetitions=1,\\n    max_concurrency=4,\\n)\\nexperiment_results.to_pandas()\\n\\nYou can see what these results look like here: LangSmith link.\\n\\u200bSingle step evaluators\\nWhile end-to-end tests give you the most signal about your agents performance, for the sake of debugging and iterating on your agent it can be helpful to pinpoint specific steps that are difficult and evaluate them directly.\\nIn our case, a crucial part of our agent is that it routes the userâ€™s intention correctly into either the â€œrefundâ€ path or the â€œquestion answeringâ€ path. Letâ€™s create a dataset and run some evaluations to directly stress test this one component.\\nCopy# Create dataset\\nexamples = [\\n    {\\n        \"inputs\": {\"messages\": [{\"role\": \"user\", \"content\": \"i bought some tracks recently and i dont like them\"}]},\\n        \"outputs\": {\"route\": \"refund_agent\"},\\n    },\\n    {\\n        \"inputs\": {\"messages\": [{\"role\": \"user\", \"content\": \"I was thinking of purchasing some Rolling Stones tunes, any recommendations?\"}]},\\n        \"outputs\": {\"route\": \"question_answering_agent\"},\\n    },\\n    {\\n        \"inputs\": {\"messages\": [{\"role\": \"user\", \"content\": \"i want a refund on purchase 237\"}, {\"role\": \"assistant\", \"content\": \"I\\'ve refunded you a total of $1.98. How else can I help you today?\"}, {\"role\": \"user\", \"content\": \"did prince release any albums in 2000?\"}]},\\n        \"outputs\": {\"route\": \"question_answering_agent\"},\\n    },\\n    {\\n        \"inputs\": {\"messages\": [{\"role\": \"user\", \"content\": \"i purchased a cover of Yesterday recently but can\\'t remember who it was by, which versions of it do you have?\"}]},\\n        \"outputs\": {\"route\": \"question_answering_agent\"},\\n    },\\n]\\n\\ndataset_name = \"Chinook Customer Service Bot: Intent Classifier\"\\nif not client.has_dataset(dataset_name=dataset_name):\\n    dataset = client.create_dataset(dataset_name=dataset_name)\\n    client.create_examples(\\n        dataset_id=dataset.id,\\n        examples=examples\\n    )\\n\\n# Evaluator\\ndef correct(outputs: dict, reference_outputs: dict) -> bool:\\n    \"\"\"Check if the agent chose the correct route.\"\"\"\\n    return outputs[\"route\"] == reference_outputs[\"route\"]\\n\\n# Target function for running the relevant step\\nasync def run_intent_classifier(inputs: dict) -> dict:\\n    # Note that we can access and run the intent_classifier node of our graph directly.\\n    command = await graph.nodes[\\'intent_classifier\\'].ainvoke(inputs)\\n    return {\"route\": command.goto}\\n\\n# Run evaluation\\nexperiment_results = await client.aevaluate(\\n    run_intent_classifier,\\n    data=dataset_name,\\n    evaluators=[correct],\\n    experiment_prefix=\"sql-agent-gpt4o-intent-classifier\",\\n    max_concurrency=4,\\n)\\n\\nYou can see what these results look like here: LangSmith link.\\n\\u200bReference code\\nHereâ€™s a consolidated script with all the above code:\\nReference codeCopyimport json\\nimport sqlite3\\nfrom typing import Literal\\n\\nfrom langchain.chat_models import init_chat_model\\nfrom langchain.embeddings import init_embeddings\\nfrom langchain_core.runnables import RunnableConfig\\nfrom langchain_core.tools import tool\\nfrom langchain_core.vectorstores import InMemoryVectorStore\\nfrom langgraph.graph import END, StateGraph\\nfrom langgraph.graph.message import AnyMessage, add_messages\\nfrom langgraph.prebuilt import create_react_agent\\nfrom langgraph.types import Command, interrupt\\nfrom langsmith import Client\\nimport requests\\nfrom tabulate import tabulate\\nfrom typing_extensions import Annotated, TypedDict\\n\\nurl = \"https://storage.googleapis.com/benchmarks-artifacts/chinook/Chinook.db\"\\n\\nresponse = requests.get(url)\\n\\nif response.status_code == 200:\\n    # Open a local file in binary write mode\\n    with open(\"chinook.db\", \"wb\") as file:\\n        # Write the content of the response (the file) to the local file\\n        file.write(response.content)\\n    print(\"File downloaded and saved as Chinook.db\")\\nelse:\\n    print(f\"Failed to download the file. Status code: {response.status_code}\")\\n\\n\\ndef _refund(\\n    invoice_id: int | None, invoice_line_ids: list[int] | None, mock: bool = False\\n) -> float:\\n    \"\"\"Given an Invoice ID and/or Invoice Line IDs, delete the relevant Invoice/InvoiceLine records in the Chinook DB.\\n\\n    Args:\\n        invoice_id: The Invoice to delete.\\n        invoice_line_ids: The Invoice Lines to delete.\\n        mock: If True, do not actually delete the specified Invoice/Invoice Lines. Used for testing purposes.\\n\\n    Returns:\\n        float: The total dollar amount that was deleted (or mock deleted).\\n    \"\"\"\\n\\n    if invoice_id is None and invoice_line_ids is None:\\n        return 0.0\\n\\n    # Connect to the Chinook database\\n    conn = sqlite3.connect(\"chinook.db\")\\n    cursor = conn.cursor()\\n\\n    total_refund = 0.0\\n\\n    try:\\n        # If invoice_id is provided, delete entire invoice and its lines\\n        if invoice_id is not None:\\n            # First get the total amount for the invoice\\n            cursor.execute(\\n                \"\"\"\\n                SELECT Total\\n                FROM Invoice\\n                WHERE InvoiceId = ?\\n            \"\"\",\\n                (invoice_id,),\\n            )\\n\\n            result = cursor.fetchone()\\n            if result:\\n                total_refund += result[0]\\n\\n            # Delete invoice lines first (due to foreign key constraints)\\n            if not mock:\\n                cursor.execute(\\n                    \"\"\"\\n                    DELETE FROM InvoiceLine\\n                    WHERE InvoiceId = ?\\n                \"\"\",\\n                    (invoice_id,),\\n                )\\n\\n                # Then delete the invoice\\n                cursor.execute(\\n                    \"\"\"\\n                    DELETE FROM Invoice\\n                    WHERE InvoiceId = ?\\n                \"\"\",\\n                    (invoice_id,),\\n                )\\n\\n        # If specific invoice lines are provided\\n        if invoice_line_ids is not None:\\n            # Get the total amount for the specified invoice lines\\n            placeholders = \",\".join([\"?\" for _ in invoice_line_ids])\\n            cursor.execute(\\n                f\"\"\"\\n                SELECT SUM(UnitPrice * Quantity)\\n                FROM InvoiceLine\\n                WHERE InvoiceLineId IN ({placeholders})\\n            \"\"\",\\n                invoice_line_ids,\\n            )\\n\\n            result = cursor.fetchone()\\n            if result and result[0]:\\n                total_refund += result[0]\\n\\n            if not mock:\\n                # Delete the specified invoice lines\\n                cursor.execute(\\n                    f\"\"\"\\n                    DELETE FROM InvoiceLine\\n                    WHERE InvoiceLineId IN ({placeholders})\\n                \"\"\",\\n                    invoice_line_ids,\\n                )\\n\\n        # Commit the changes\\n        conn.commit()\\n\\n    except sqlite3.Error as e:\\n        # Roll back in case of error\\n        conn.rollback()\\n        raise e\\n\\n    finally:\\n        # Close the connection\\n        conn.close()\\n\\n    return float(total_refund)\\n\\n\\ndef _lookup(\\n    customer_first_name: str,\\n    customer_last_name: str,\\n    customer_phone: str,\\n    track_name: str | None,\\n    album_title: str | None,\\n    artist_name: str | None,\\n    purchase_date_iso_8601: str | None,\\n) -> list[dict]:\\n    \"\"\"Find all of the Invoice Line IDs in the Chinook DB for the given filters.\\n\\n    Returns:\\n        a list of dictionaries that contain keys: {\\n            \\'invoice_line_id\\',\\n            \\'track_name\\',\\n            \\'artist_name\\',\\n            \\'purchase_date\\',\\n            \\'quantity_purchased\\',\\n            \\'price_per_unit\\'\\n        }\\n    \"\"\"\\n\\n    # Connect to the database\\n    conn = sqlite3.connect(\"chinook.db\")\\n    cursor = conn.cursor()\\n\\n    # Base query joining all necessary tables\\n    query = \"\"\"\\n    SELECT\\n        il.InvoiceLineId,\\n        t.Name as track_name,\\n        art.Name as artist_name,\\n        i.InvoiceDate as purchase_date,\\n        il.Quantity as quantity_purchased,\\n        il.UnitPrice as price_per_unit\\n    FROM InvoiceLine il\\n    JOIN Invoice i ON il.InvoiceId = i.InvoiceId\\n    JOIN Customer c ON i.CustomerId = c.CustomerId\\n    JOIN Track t ON il.TrackId = t.TrackId\\n    JOIN Album alb ON t.AlbumId = alb.AlbumId\\n    JOIN Artist art ON alb.ArtistId = art.ArtistId\\n    WHERE c.FirstName = ?\\n    AND c.LastName = ?\\n    AND c.Phone = ?\\n    \"\"\"\\n\\n    # Parameters for the query\\n    params = [customer_first_name, customer_last_name, customer_phone]\\n\\n    # Add optional filters\\n    if track_name:\\n        query += \" AND t.Name = ?\"\\n        params.append(track_name)\\n\\n    if album_title:\\n        query += \" AND alb.Title = ?\"\\n        params.append(album_title)\\n\\n    if artist_name:\\n        query += \" AND art.Name = ?\"\\n        params.append(artist_name)\\n\\n    if purchase_date_iso_8601:\\n        query += \" AND date(i.InvoiceDate) = date(?)\"\\n        params.append(purchase_date_iso_8601)\\n\\n    # Execute query\\n    cursor.execute(query, params)\\n\\n    # Fetch results\\n    results = cursor.fetchall()\\n\\n    # Convert results to list of dictionaries\\n    output = []\\n    for row in results:\\n        output.append(\\n            {\\n                \"invoice_line_id\": row[0],\\n                \"track_name\": row[1],\\n                \"artist_name\": row[2],\\n                \"purchase_date\": row[3],\\n                \"quantity_purchased\": row[4],\\n                \"price_per_unit\": row[5],\\n            }\\n        )\\n\\n    # Close connection\\n    conn.close()\\n\\n    return output\\n\\n\\n# Graph state.\\nclass State(TypedDict):\\n    \"\"\"Agent state.\"\"\"\\n\\n    messages: Annotated[list[AnyMessage], add_messages]\\n    followup: str | None\\n\\n    invoice_id: int | None\\n    invoice_line_ids: list[int] | None\\n    customer_first_name: str | None\\n    customer_last_name: str | None\\n    customer_phone: str | None\\n    track_name: str | None\\n    album_title: str | None\\n    artist_name: str | None\\n    purchase_date_iso_8601: str | None\\n\\n\\n# Instructions for extracting the user/purchase info from the conversation.\\ngather_info_instructions = \"\"\"You are managing an online music store that sells song tracks. \\\\\\nCustomers can buy multiple tracks at a time and these purchases are recorded in a database as \\\\\\nan Invoice per purchase and an associated set of Invoice Lines for each purchased track.\\n\\nYour task is to help customers who would like a refund for one or more of the tracks they\\'ve \\\\\\npurchased. In order for you to be able refund them, the customer must specify the Invoice ID \\\\\\nto get a refund on all the tracks they bought in a single transaction, or one or more Invoice \\\\\\nLine IDs if they would like refunds on individual tracks.\\n\\nOften a user will not know the specific Invoice ID(s) or Invoice Line ID(s) for which they \\\\\\nwould like a refund. In this case you can help them look up their invoices by asking them to \\\\\\nspecify:\\n- Required: Their first name, last name, and phone number.\\n- Optionally: The track name, artist name, album name, or purchase date.\\n\\nIf the customer has not specified the required information (either Invoice/Invoice Line IDs \\\\\\nor first name, last name, phone) then please ask them to specify it.\"\"\"\\n\\n\\n# Extraction schema, mirrors the graph state.\\nclass PurchaseInformation(TypedDict):\\n    \"\"\"All of the known information about the invoice / invoice lines the customer would like refunded. Do not make up values, leave fields as null if you don\\'t know their value.\"\"\"\\n\\n    invoice_id: int | None\\n    invoice_line_ids: list[int] | None\\n    customer_first_name: str | None\\n    customer_last_name: str | None\\n    customer_phone: str | None\\n    track_name: str | None\\n    album_title: str | None\\n    artist_name: str | None\\n    purchase_date_iso_8601: str | None\\n    followup: Annotated[\\n        str | None,\\n        ...,\\n        \"If the user hasn\\'t enough identifying information, please tell them what the required information is and ask them to specify it.\",\\n    ]\\n\\n\\n# Model for performing extraction.\\ninfo_llm = init_chat_model(\"gpt-4o-mini\").with_structured_output(\\n    PurchaseInformation, method=\"json_schema\", include_raw=True\\n)\\n\\n\\n# Graph node for extracting user info and routing to lookup/refund/END.\\nasync def gather_info(state: State) -> Command[Literal[\"lookup\", \"refund\", END]]:\\n    info = await info_llm.ainvoke(\\n        [\\n            {\"role\": \"system\", \"content\": gather_info_instructions},\\n            *state[\"messages\"],\\n        ]\\n    )\\n    parsed = info[\"parsed\"]\\n    if any(parsed[k] for k in (\"invoice_id\", \"invoice_line_ids\")):\\n        goto = \"refund\"\\n    elif all(\\n        parsed[k]\\n        for k in (\"customer_first_name\", \"customer_last_name\", \"customer_phone\")\\n    ):\\n        goto = \"lookup\"\\n    else:\\n        goto = END\\n    update = {\"messages\": [info[\"raw\"]], **parsed}\\n    return Command(update=update, goto=goto)\\n\\n\\n# Graph node for executing the refund.\\n# Note that here we inspect the runtime config for an \"env\" variable.\\n# If \"env\" is set to \"test\", then we don\\'t actually delete any rows from our database.\\n# This will become important when we\\'re running our evaluations.\\ndef refund(state: State, config: RunnableConfig) -> dict:\\n    # Whether to mock the deletion. True if the configurable var \\'env\\' is set to \\'test\\'.\\n    mock = config.get(\"configurable\", {}).get(\"env\", \"prod\") == \"test\"\\n    refunded = _refund(\\n        invoice_id=state[\"invoice_id\"],\\n        invoice_line_ids=state[\"invoice_line_ids\"],\\n        mock=mock,\\n    )\\n    response = f\"You have been refunded a total of: ${refunded:.2f}. Is there anything else I can help with?\"\\n    return {\\n        \"messages\": [{\"role\": \"assistant\", \"content\": response}],\\n        \"followup\": response,\\n    }\\n\\n\\n# Graph node for looking up the users purchases\\ndef lookup(state: State) -> dict:\\n    args = (\\n        state[k]\\n        for k in (\\n            \"customer_first_name\",\\n            \"customer_last_name\",\\n            \"customer_phone\",\\n            \"track_name\",\\n            \"album_title\",\\n            \"artist_name\",\\n            \"purchase_date_iso_8601\",\\n        )\\n    )\\n    results = _lookup(*args)\\n    if not results:\\n        response = \"We did not find any purchases associated with the information you\\'ve provided. Are you sure you\\'ve entered all of your information correctly?\"\\n        followup = response\\n    else:\\n        response = f\"Which of the following purchases would you like to be refunded for?\\\\n\\\\n```json{json.dumps(results, indent=2)}\\\\n```\"\\n        followup = f\"Which of the following purchases would you like to be refunded for?\\\\n\\\\n{tabulate(results, headers=\\'keys\\')}\"\\n    return {\\n        \"messages\": [{\"role\": \"assistant\", \"content\": response}],\\n        \"followup\": followup,\\n        \"invoice_line_ids\": [res[\"invoice_line_id\"] for res in results],\\n    }\\n\\n\\n# Building our graph\\ngraph_builder = StateGraph(State)\\n\\ngraph_builder.add_node(gather_info)\\ngraph_builder.add_node(refund)\\ngraph_builder.add_node(lookup)\\n\\ngraph_builder.set_entry_point(\"gather_info\")\\ngraph_builder.add_edge(\"lookup\", END)\\ngraph_builder.add_edge(\"refund\", END)\\n\\nrefund_graph = graph_builder.compile()\\n\\n\\n# Our SQL queries will only work if we filter on the exact string values that are in the DB.\\n# To ensure this, we\\'ll create vectorstore indexes for all of the artists, tracks and albums\\n# ahead of time and use those to disambiguate the user input. E.g. if a user searches for\\n# songs by \"prince\" and our DB records the artist as \"Prince\", ideally when we query our\\n# artist vectorstore for \"prince\" we\\'ll get back the value \"Prince\", which we can then\\n# use in our SQL queries.\\ndef index_fields() -> (\\n    tuple[InMemoryVectorStore, InMemoryVectorStore, InMemoryVectorStore]\\n):\\n    \"\"\"Create an index for all artists, an index for all albums, and an index for all songs.\"\"\"\\n    try:\\n        # Connect to the chinook database\\n        conn = sqlite3.connect(\"chinook.db\")\\n        cursor = conn.cursor()\\n\\n        # Fetch all results\\n        tracks = cursor.execute(\"SELECT Name FROM Track\").fetchall()\\n        artists = cursor.execute(\"SELECT Name FROM Artist\").fetchall()\\n        albums = cursor.execute(\"SELECT Title FROM Album\").fetchall()\\n    finally:\\n        # Close the connection\\n        if conn:\\n            conn.close()\\n\\n    embeddings = init_embeddings(\"openai:text-embedding-3-small\")\\n\\n    track_store = InMemoryVectorStore(embeddings)\\n    artist_store = InMemoryVectorStore(embeddings)\\n    album_store = InMemoryVectorStore(embeddings)\\n\\n    track_store.add_texts([t[0] for t in tracks])\\n    artist_store.add_texts([a[0] for a in artists])\\n    album_store.add_texts([a[0] for a in albums])\\n    return track_store, artist_store, album_store\\n\\n\\ntrack_store, artist_store, album_store = index_fields()\\n\\n\\n# Agent tools\\n@tool\\ndef lookup_track(\\n    track_name: str | None = None,\\n    album_title: str | None = None,\\n    artist_name: str | None = None,\\n) -> list[dict]:\\n    \"\"\"Lookup a track in Chinook DB based on identifying information about.\\n\\n    Returns:\\n        a list of dictionaries per matching track that contain keys {\\'track_name\\', \\'artist_name\\', \\'album_name\\'}\\n    \"\"\"\\n    conn = sqlite3.connect(\"chinook.db\")\\n    cursor = conn.cursor()\\n\\n    query = \"\"\"\\n    SELECT DISTINCT t.Name as track_name, ar.Name as artist_name, al.Title as album_name\\n    FROM Track t\\n    JOIN Album al ON t.AlbumId = al.AlbumId\\n    JOIN Artist ar ON al.ArtistId = ar.ArtistId\\n    WHERE 1=1\\n    \"\"\"\\n    params = []\\n\\n    if track_name:\\n        track_name = track_store.similarity_search(track_name, k=1)[0].page_content\\n        query += \" AND t.Name LIKE ?\"\\n        params.append(f\"%{track_name}%\")\\n    if album_title:\\n        album_title = album_store.similarity_search(album_title, k=1)[0].page_content\\n        query += \" AND al.Title LIKE ?\"\\n        params.append(f\"%{album_title}%\")\\n    if artist_name:\\n        artist_name = artist_store.similarity_search(artist_name, k=1)[0].page_content\\n        query += \" AND ar.Name LIKE ?\"\\n        params.append(f\"%{artist_name}%\")\\n\\n    cursor.execute(query, params)\\n    results = cursor.fetchall()\\n\\n    tracks = [\\n        {\"track_name\": row[0], \"artist_name\": row[1], \"album_name\": row[2]}\\n        for row in results\\n    ]\\n\\n    conn.close()\\n    return tracks\\n\\n\\n@tool\\ndef lookup_album(\\n    track_name: str | None = None,\\n    album_title: str | None = None,\\n    artist_name: str | None = None,\\n) -> list[dict]:\\n    \"\"\"Lookup an album in Chinook DB based on identifying information about.\\n\\n    Returns:\\n        a list of dictionaries per matching album that contain keys {\\'album_name\\', \\'artist_name\\'}\\n    \"\"\"\\n    conn = sqlite3.connect(\"chinook.db\")\\n    cursor = conn.cursor()\\n\\n    query = \"\"\"\\n    SELECT DISTINCT al.Title as album_name, ar.Name as artist_name\\n    FROM Album al\\n    JOIN Artist ar ON al.ArtistId = ar.ArtistId\\n    LEFT JOIN Track t ON t.AlbumId = al.AlbumId\\n    WHERE 1=1\\n    \"\"\"\\n    params = []\\n\\n    if track_name:\\n        query += \" AND t.Name LIKE ?\"\\n        params.append(f\"%{track_name}%\")\\n    if album_title:\\n        query += \" AND al.Title LIKE ?\"\\n        params.append(f\"%{album_title}%\")\\n    if artist_name:\\n        query += \" AND ar.Name LIKE ?\"\\n        params.append(f\"%{artist_name}%\")\\n\\n    cursor.execute(query, params)\\n    results = cursor.fetchall()\\n\\n    albums = [{\"album_name\": row[0], \"artist_name\": row[1]} for row in results]\\n\\n    conn.close()\\n    return albums\\n\\n\\n@tool\\ndef lookup_artist(\\n    track_name: str | None = None,\\n    album_title: str | None = None,\\n    artist_name: str | None = None,\\n) -> list[str]:\\n    \"\"\"Lookup an album in Chinook DB based on identifying information about.\\n\\n    Returns:\\n        a list of matching artist names\\n    \"\"\"\\n    conn = sqlite3.connect(\"chinook.db\")\\n    cursor = conn.cursor()\\n\\n    query = \"\"\"\\n    SELECT DISTINCT ar.Name as artist_name\\n    FROM Artist ar\\n    LEFT JOIN Album al ON al.ArtistId = ar.ArtistId\\n    LEFT JOIN Track t ON t.AlbumId = al.AlbumId\\n    WHERE 1=1\\n    \"\"\"\\n    params = []\\n\\n    if track_name:\\n        query += \" AND t.Name LIKE ?\"\\n        params.append(f\"%{track_name}%\")\\n    if album_title:\\n        query += \" AND al.Title LIKE ?\"\\n        params.append(f\"%{album_title}%\")\\n    if artist_name:\\n        query += \" AND ar.Name LIKE ?\"\\n        params.append(f\"%{artist_name}%\")\\n\\n    cursor.execute(query, params)\\n    results = cursor.fetchall()\\n\\n    artists = [row[0] for row in results]\\n\\n    conn.close()\\n    return artists\\n\\n\\n# Agent model\\nqa_llm = init_chat_model(\"claude-3-5-sonnet-latest\")\\n# The prebuilt ReACT agent only expects State to have a \\'messages\\' key, so the\\n# state we defined for the refund agent can also be passed to our lookup agent.\\nqa_graph = create_react_agent(qa_llm, [lookup_track, lookup_artist, lookup_album])\\n\\n\\n# Schema for routing user intent.\\n# We\\'ll use structured outputs to enforce that the model returns only\\n# the desired output.\\nclass UserIntent(TypedDict):\\n    \"\"\"The user\\'s current intent in the conversation\"\"\"\\n\\n    intent: Literal[\"refund\", \"question_answering\"]\\n\\n\\n# Routing model with structured output\\nrouter_llm = init_chat_model(\"gpt-4o-mini\").with_structured_output(\\n    UserIntent, method=\"json_schema\", strict=True\\n)\\n\\n# Instructions for routing.\\nroute_instructions = \"\"\"You are managing an online music store that sells song tracks. \\\\\\nYou can help customers in two types of ways: (1) answering general questions about \\\\\\ntracks sold at your store, (2) helping them get a refund on a purhcase they made at your store.\\n\\nBased on the following conversation, determine if the user is currently seeking general \\\\\\ninformation about song tracks or if they are trying to refund a specific purchase.\\n\\nReturn \\'refund\\' if they are trying to get a refund and \\'question_answering\\' if they are \\\\\\nasking a general music question. Do NOT return anything else. Do NOT try to respond to \\\\\\nthe user.\\n\"\"\"\\n\\n\\n# Node for routing.\\nasync def intent_classifier(\\n    state: State,\\n) -> Command[Literal[\"refund_agent\", \"question_answering_agent\"]]:\\n    response = router_llm.invoke(\\n        [{\"role\": \"system\", \"content\": route_instructions}, *state[\"messages\"]]\\n    )\\n    return Command(goto=response[\"intent\"] + \"_agent\")\\n\\n\\n# Node for making sure the \\'followup\\' key is set before our agent run completes.\\ndef compile_followup(state: State) -> dict:\\n    \"\"\"Set the followup to be the last message if it hasn\\'t explicitly been set.\"\"\"\\n    if not state.get(\"followup\"):\\n        return {\"followup\": state[\"messages\"][-1].content}\\n    return {}\\n\\n\\n# Agent definition\\ngraph_builder = StateGraph(State)\\ngraph_builder.add_node(intent_classifier)\\n# Since all of our subagents have compatible state,\\n# we can add them as nodes directly.\\ngraph_builder.add_node(\"refund_agent\", refund_graph)\\ngraph_builder.add_node(\"question_answering_agent\", qa_graph)\\ngraph_builder.add_node(compile_followup)\\n\\ngraph_builder.set_entry_point(\"intent_classifier\")\\ngraph_builder.add_edge(\"refund_agent\", \"compile_followup\")\\ngraph_builder.add_edge(\"question_answering_agent\", \"compile_followup\")\\ngraph_builder.add_edge(\"compile_followup\", END)\\n\\ngraph = graph_builder.compile()\\n\\n\\nclient = Client()\\n\\n# Create a dataset\\nexamples = [\\n    {\\n        \"inputs\": {\\n            \"question\": \"How many songs do you have by James Brown\"\\n        },\\n        \"outputs\": {\\n            \"response\": \"We have 20 songs by James Brown\",\\n            \"trajectory\": [\"question_answering_agent\", \"lookup_tracks\"]\\n        },\\n    },\\n    {\\n        \"inputs\": {\\n            \"question\": \"My name is Aaron Mitchell and I\\'d like a refund.\",\\n        },\\n        \"outputs\": {\\n            \"response\": \"I need some more information to help you with the refund. Please specify your phone number, the invoice ID, or the line item IDs for the purchase you\\'d like refunded.\",\\n            \"trajectory\": [\"refund_agent\"],\\n        }\\n    },\\n    {\\n        \"inputs\": {\\n            \"question\": \"My name is Aaron Mitchell and I\\'d like a refund on my Led Zeppelin purchases. My number is +1 (204) 452-6452\",\\n        },\\n        \"outputs\": {\\n            \"response\": \"Which of the following purchases would you like to be refunded for?\\\\n\\\\n  invoice_line_id  track_name                        artist_name    purchase_date          quantity_purchased    price_per_unit\\\\n-----------------  --------------------------------  -------------  -------------------  --------------------  ----------------\\\\n              267  How Many More Times               Led Zeppelin   2009-08-06 00:00:00                     1              0.99\\\\n              268  What Is And What Should Never Be  Led Zeppelin   2009-08-06 00:00:00                     1              0.99\",\\n            \"trajectory\": [\"refund_agent\", \"lookup\"],\\n        },\\n    },\\n    {\\n        \"inputs\": {\\n            \"question\": \"Who recorded Wish You Were Here again? What other albums of there\\'s do you have?\",\\n        },\\n        \"outputs\": {\\n            \"response\": \"Wish You Were Here is an album by Pink Floyd\",\\n            \"trajectory\": [\"question_answering_agent\", \"lookup_album\"],\\n        }\\n    },\\n    {\\n        \"inputs\": {\\n            \"question\": \"I want a full refund for invoice 237\",\\n        },\\n        \"outputs\": {\\n            \"response\": \"You have been refunded $2.97.\",\\n            \"trajectory\": [\"refund_agent\", \"refund\"],\\n        },\\n    },\\n]\\n\\ndataset_name = \"Chinook Customer Service Bot: E2E\"\\n\\nif not client.has_dataset(dataset_name=dataset_name):\\n    dataset = client.create_dataset(dataset_name=dataset_name)\\n    client.create_examples(\\n        dataset_id=dataset.id,\\n        examples=examples\\n    )\\n\\n# LLM-as-judge instructions\\ngrader_instructions = \"\"\"You are a teacher grading a quiz.\\n\\nYou will be given a QUESTION, the GROUND TRUTH (correct) RESPONSE, and the STUDENT RESPONSE.\\n\\nHere is the grade criteria to follow:\\n(1) Grade the student responses based ONLY on their factual accuracy relative to the ground truth answer.\\n(2) Ensure that the student response does not contain any conflicting statements.\\n(3) It is OK if the student response contains more information than the ground truth response, as long as it is factually accurate relative to the  ground truth response.\\n\\nCorrectness:\\nTrue means that the student\\'s response meets all of the criteria.\\nFalse means that the student\\'s response does not meet all of the criteria.\\n\\nExplain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct.\"\"\"\\n\\n\\n# LLM-as-judge output schema\\nclass Grade(TypedDict):\\n    \"\"\"Compare the expected and actual answers and grade the actual answer.\"\"\"\\n\\n    reasoning: Annotated[\\n        str,\\n        ...,\\n        \"Explain your reasoning for whether the actual response is correct or not.\",\\n    ]\\n    is_correct: Annotated[\\n        bool,\\n        ...,\\n        \"True if the student response is mostly or exactly correct, otherwise False.\",\\n    ]\\n\\n\\n# Judge LLM\\ngrader_llm = init_chat_model(\"gpt-4o-mini\", temperature=0).with_structured_output(\\n    Grade, method=\"json_schema\", strict=True\\n)\\n\\n\\n# Evaluator function\\nasync def final_answer_correct(\\n    inputs: dict, outputs: dict, reference_outputs: dict\\n) -> bool:\\n    \"\"\"Evaluate if the final response is equivalent to reference response.\"\"\"\\n\\n    # Note that we assume the outputs has a \\'response\\' dictionary. We\\'ll need to make sure\\n    # that the target function we define includes this key.\\n    user = f\"\"\"QUESTION: {inputs[\\'question\\']}\\n    GROUND TRUTH RESPONSE: {reference_outputs[\\'response\\']}\\n    STUDENT RESPONSE: {outputs[\\'response\\']}\"\"\"\\n\\n    grade = await grader_llm.ainvoke(\\n        [\\n            {\"role\": \"system\", \"content\": grader_instructions},\\n            {\"role\": \"user\", \"content\": user},\\n        ]\\n    )\\n    return grade[\"is_correct\"]\\n\\n\\n# Target function\\nasync def run_graph(inputs: dict) -> dict:\\n    \"\"\"Run graph and track the trajectory it takes along with the final response.\"\"\"\\n    result = await graph.ainvoke(\\n        {\\n            \"messages\": [\\n                {\"role\": \"user\", \"content\": inputs[\"question\"]},\\n            ]\\n        },\\n        config={\"env\": \"test\"},\\n    )\\n    return {\"response\": result[\"followup\"]}\\n\\n\\n# Evaluation job and results\\nexperiment_results = await client.aevaluate(\\n    run_graph,\\n    data=dataset_name,\\n    evaluators=[final_answer_correct],\\n    experiment_prefix=\"sql-agent-gpt4o-e2e\",\\n    num_repetitions=1,\\n    max_concurrency=4,\\n)\\nexperiment_results.to_pandas()\\n\\n\\ndef trajectory_subsequence(outputs: dict, reference_outputs: dict) -> float:\\n    \"\"\"Check how many of the desired steps the agent took.\"\"\"\\n    if len(reference_outputs[\"trajectory\"]) > len(outputs[\"trajectory\"]):\\n        return False\\n\\n    i = j = 0\\n    while i < len(reference_outputs[\"trajectory\"]) and j < len(outputs[\"trajectory\"]):\\n        if reference_outputs[\"trajectory\"][i] == outputs[\"trajectory\"][j]:\\n            i += 1\\n        j += 1\\n\\n    return i / len(reference_outputs[\"trajectory\"])\\n\\n\\nasync def run_graph(inputs: dict) -> dict:\\n    \"\"\"Run graph and track the trajectory it takes along with the final response.\"\"\"\\n    trajectory = []\\n    # Set subgraph=True to stream events from subgraphs of the main graph: https://langchain-ai.github.io/langgraph/how-tos/streaming-subgraphs/\\n    # Set stream_mode=\"debug\" to stream all possible events: https://langchain-ai.github.io/langgra/langsmith/observability-concepts/streaming\\n    async for namespace, chunk in graph.astream(\\n        {\\n            \"messages\": [\\n                {\\n                    \"role\": \"user\",\\n                    \"content\": inputs[\"question\"],\\n                }\\n            ]\\n        },\\n        subgraphs=True,\\n        stream_mode=\"debug\",\\n    ):\\n        # Event type for entering a node\\n        if chunk[\"type\"] == \"task\":\\n            # Record the node name\\n            trajectory.append(chunk[\"payload\"][\"name\"])\\n            # Given how we defined our dataset, we also need to track when specific tools are\\n            # called by our question answering ReACT agent. These tool calls can be found\\n            # when the ToolsNode (named \"tools\") is invoked by looking at the AIMessage.tool_calls\\n            # of the latest input message.\\n            if chunk[\"payload\"][\"name\"] == \"tools\" and chunk[\"type\"] == \"task\":\\n                for tc in chunk[\"payload\"][\"input\"][\"messages\"][-1].tool_calls:\\n                    trajectory.append(tc[\"name\"])\\n\\n    return {\"trajectory\": trajectory}\\n\\n\\nexperiment_results = await client.aevaluate(\\n    run_graph,\\n    data=dataset_name,\\n    evaluators=[trajectory_subsequence],\\n    experiment_prefix=\"sql-agent-gpt4o-trajectory\",\\n    num_repetitions=1,\\n    max_concurrency=4,\\n)\\nexperiment_results.to_pandas()\\n\\n# Create dataset\\nexamples = [\\n    {\\n        \"inputs\": {\\n            \"messages\": [\\n                {\\n                    \"role\": \"user\",\\n                    \"content\": \"i bought some tracks recently and i dont like them\",\\n                }\\n            ],\\n        }\\n        \"outputs\": {\"route\": \"refund_agent\"},\\n    },\\n    {\\n        \"inputs\": {\\n            \"messages\": [\\n                {\\n                    \"role\": \"user\",\\n                    \"content\": \"I was thinking of purchasing some Rolling Stones tunes, any recommendations?\",\\n                }\\n            ],\\n        },\\n        \"outputs\": {\"route\": \"question_answering_agent\"},\\n    },\\n    {\\n        \"inputs\": {\\n            \"messages\": [\\n                    {\"role\": \"user\", \"content\": \"i want a refund on purchase 237\"},\\n                {\\n                    \"role\": \"assistant\",\\n                    \"content\": \"I\\'ve refunded you a total of $1.98. How else can I help you today?\",\\n                },\\n                {\"role\": \"user\", \"content\": \"did prince release any albums in 2000?\"},\\n            ],\\n        },\\n        \"outputs\": {\"route\": \"question_answering_agent\"},\\n    },\\n    {\\n        \"inputs\": {\\n            \"messages\": [\\n                {\\n                    \"role\": \"user\",\\n                    \"content\": \"i purchased a cover of Yesterday recently but can\\'t remember who it was by, which versions of it do you have?\",\\n                }\\n            ],\\n        },\\n        \"outputs\": {\"route\": \"question_answering_agent\"},\\n    },\\n]\\n\\ndataset_name = \"Chinook Customer Service Bot: Intent Classifier\"\\nif not client.has_dataset(dataset_name=dataset_name):\\n    dataset = client.create_dataset(dataset_name=dataset_name)\\n    client.create_examples(\\n        dataset_id=dataset.id,\\n        examples=examples,\\n    )\\n\\n\\n# Evaluator\\ndef correct(outputs: dict, reference_outputs: dict) -> bool:\\n    \"\"\"Check if the agent chose the correct route.\"\"\"\\n    return outputs[\"route\"] == reference_outputs[\"route\"]\\n\\n\\n# Target function for running the relevant step\\nasync def run_intent_classifier(inputs: dict) -> dict:\\n    # Note that we can access and run the intent_classifier node of our graph directly.\\n    command = await graph.nodes[\"intent_classifier\"].ainvoke(inputs)\\n    return {\"route\": command.goto}\\n\\n\\n# Run evaluation\\nexperiment_results = await client.aevaluate(\\n    run_intent_classifier,\\n    data=dataset_name,\\n    evaluators=[correct],\\n    experiment_prefix=\"sql-agent-gpt4o-intent-classifier\",\\n    max_concurrency=4,\\n)\\nexperiment_results.to_pandas()\\nWas this page helpful?YesNoSuggest editsTest a ReAct agent with Pytest/Vitest and LangSmithRun backtests on a new version of an agentâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/aligning_evaluator', 'loc': 'https://docs.smith.langchain.com/evaluation/tutorials/aligning_evaluator', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Improve LLM-as-judge evaluators using human feedback - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesImprove evaluatorsImprove LLM-as-judge evaluators using human feedbackImprove your evaluator with few-shot examplesDynamic few shot example selectionTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationImprove evaluatorsImprove LLM-as-judge evaluators using human feedbackGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageHow it worksPrerequisitesOffline evaluationsOnline evaluationsGetting started1. Select experiments or runs2. Label examples3. Test your evaluator prompt against the labeled examples4. Repeat to improve evaluator alignmentTips for improving evaluator alignmentVideo guideSet up evaluationsImprove evaluatorsImprove LLM-as-judge evaluators using human feedbackCopy pageCopy pageBefore working through this page, it might be helpful to read the following:\\nEvaluation concepts\\nCreating LLM-as-a-judge evaluators\\n\\nReliable LLM-as-a-judge evaluators are critical for making informed decisions about your AI applications (e.g., prompt, model, architecture changes). Defining the evaluator prompt correctly can be difficult, but it directly affects the trustworthiness of your evaluations.\\nThis guide describes how to align your LLM-as-a-judge evaluator using human feedback to improve your evaluatorâ€™s quality and help you build reliable AI applications.\\n\\u200bHow it works\\nLangSmithâ€™s Align Evaluator feature has a series of steps that help you align your LLM-as-a-judge evaluator with human expert feedback. You can use this feature to align evaluators that run on a dataset for offline evaluations or for online evaluations. In either case, the steps are similar:\\n\\nSelect experiments or runs that contain outputs from your application.\\nAdd the selected experiments or runs to an annotation queue where a human expert can label the data.\\nTest your LLM-as-a-judge evaluator prompt against the labeled examples. Check the cases where your evaluator result is not aligned with the labeled data. This indicates areas where your evaluator prompt needs improvement.\\nRefine and repeat to improve evaluator alignment. Update your LLM-as-a-judge evaluator prompt and test again.\\n\\n\\u200bPrerequisites\\nYouâ€™ll need the following before starting this guide for offline evaluations or online evaluations:\\n\\u200bOffline evaluations\\n\\nA dataset with at least one experiment.\\nYouâ€™ll need to upload or create datasets via the SDK or the UI and run an experiment via the SDK or the Playground.\\n\\n\\u200bOnline evaluations\\n\\nAn application thatâ€™s already sending traces to LangSmith.\\nConfigure this with one of the tracing integrations to start.\\n\\n\\u200bGetting started\\nYou can enter the alignment flow for both new and existing evaluators in datasets and tracing projects.\\nDataset EvaluatorsTracing Project EvaluatorsCreate an aligned evaluator from scratch1. Datasets & Experiments and select your dataset2. Click +\\u202fEvaluator > Create from labeled data3. Enter a descriptive feedback key name (e.g. correctness, hallucination)1. Projects and select your project2. Click +\\u202fNew > Evaluator > Create from labeled data3. Enter a descriptive feedbackâ€‘key name (e.g. correctness, hallucination)Align an existing evaluator1. Datasets & Experiments > select your dataset > Evaluators tab2. In the Align Evaluator with experiment data box, click Select\\u202fExperiments1. Projects > select your project > Evaluators tab2. In the Align Evaluator with experiment data box, click Select Experiments\\n\\u200b1. Select experiments or runs\\nSelect one or more experiments (or runs) to send for human labeling. This will add runs to an annotation queue.\\n\\nTo add any new experiments/runs to an existing annotation queue, head to the Evaluators tab, select the evaluator you are aligning and click Add to Queue.\\nDatasets should be representative of inputs and outputs you expect to see in production.While you donâ€™t need to cover every possible scenario, itâ€™s important to include examples across the full range of expected use cases. For example, if youâ€™re building a sports bot that answers questions about baseball, basketball, and football, your dataset should include at least one labeled example from each sport.\\n\\u200b2. Label examples\\nLabel examples in the annotation queue by adding a feedback score. Once youâ€™ve labeled an example, click Add to Reference Dataset.\\nIf you have a large number of examples in your experiments, you donâ€™t need to label every example to get started. We recommend starting with at least 20 examples, you can always add more later. We recommend that the examples that you label are diverse (balanced in both 0 and 1 labels) to ensure that youâ€™re building a well rounded evaluator prompt.\\n\\u200b3. Test your evaluator prompt against the labeled examples\\nOnce you have labeled examples, the next step is iterating on your evaluator prompt to mimic the labeled data as well as possible. This iteration is done in the Evaluator Playground.\\nTo go to the evaluator playground: Click the View evaluator button on the top right of the evaluator queue. This will take you to the detail page of the evaluator you are aligning. Click the Evaluator Playground button to access the playground.\\n\\nIn the evaluator playground you can create or edit your evaluator prompt and click Start Alignment to run it over the set of labeled examples that you created in Step 2. After running your evaluator, youâ€™ll see how its generated scores compare to your human labels. The alignment score is the percentage of examples where the evaluatorâ€™s judgment matches that of the human expert.\\n\\n\\u200b4. Repeat to improve evaluator alignment\\nIterate by updating your prompt and testing again to improve evaluator alignment.\\nUpdates to your evaluator prompt are not saved by default. We reccomend saving your evaluator prompt regularly, and especially after you see your alignment score improve.The evaluator playground will show the alignment score for the most recently saved version of your evaluator prompt for comparison when youâ€™re iterating on your prompt.\\nImproving the alignment score of your evaluator isnâ€™t an exact science but there are a few strategies that are helpful in increasing the alignment score.\\n\\u200bTips for improving evaluator alignment\\n1. Investigate misaligned examples\\nDigging into misaligned examples and trying to group them into common failure modes is a great first step for improving your evaluator alignment.\\nOnce you have identified the common failure modes, add instructions to your evaluator prompt so the LLM knows about them. For example, you could explain that â€œMFA stands for â€œmulti-factor authenticationâ€ if you notice it not understanding that specific acronym. Or you could tell it that â€œa good response will always contain at least 3 potential hotels to bookâ€ if it is confused on what good/bad means in your evaluatorâ€™s context.\\n2. Inspect the reasoning behind the LLM score\\nTo understand why the LLM scored an example the way it did, you can enable reasoning for your LLM-as-a-judge evaluator. Reasoning is helpful to understand the LLMâ€™s thought process and can help you identify common failure modes to incorporate into your evaluator prompt as well..\\nIn order to see the reasoning in the evaluator playground, hover over the LLM score.\\n\\nThis will show the reasoning behind the LLMâ€™s score in the evaluator playground.\\n3. Add more labeled examples and validate performance\\nTo avoid overfitting to the labeled examples, itâ€™s important to add more labeled examples and test performance, especially if you started off with a small number of examples.\\n\\u200bVideo guide\\nWas this page helpful?YesNoSuggest editsSimulate multi-turn interactionsImprove your evaluator with few-shot examplesâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/backtesting', 'loc': 'https://docs.smith.langchain.com/evaluation/tutorials/backtesting', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Run backtests on a new version of an agent - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesImprove evaluatorsTutorialsEvaluate a chatbotEvaluate a RAG applicationTest a ReAct agent with Pytest/Vitest and LangSmithEvaluate a complex agentRun backtests on a new version of an agentAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationTutorialsRun backtests on a new version of an agentGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageSetupConfigure the environmentDefine the applicationSimulate production dataConvert Production Traces to ExperimentSelect runs to backtest onConvert runs to experimentBenchmark against new systemDefine evaluatorsEvaluate baselineDefine and evaluate new systemComparing the resultsSet up evaluationsTutorialsRun backtests on a new version of an agentCopy pageCopy pageDeploying your application is just the beginning of a continuous improvement process. After you deploy to production, youâ€™ll want to refine your system by enhancing prompts, language models, tools, and architectures. Backtesting involves assessing new versions of your application using historical data and comparing the new outputs to the original ones. Compared to evaluations using pre-production datasets, backtesting offers a clearer indication of whether the new version of your application is an improvement over the current deployment.\\nHere are the basic steps for backtesting:\\n\\nSelect sample runs from your production tracing project to test against.\\nTransform the run inputs into a dataset and record the run outputs as an initial experiment against that dataset.\\nExecute your new system on the new dataset and compare the results of the experiments.\\n\\nThis process will provide you with a new dataset of representative inputs, which you can version and use for backtesting your models.\\nOften, you wonâ€™t have definitive â€œground truthâ€ answers available. In such cases, you can manually label the outputs or use evaluators that donâ€™t rely on reference data. If your application allows for capturing ground-truth labels, for example by allowing users to leave feedback, we strongly recommend doing so.\\n\\u200bSetup\\n\\u200bConfigure the environment\\nInstall and set environment variables. This guide requires langsmith>=0.2.4.\\nFor convenience weâ€™ll use the LangChain OSS framework in this tutorial, but the LangSmith functionality shown is framework-agnostic.\\npipuvCopypip install -U langsmith langchain langchain-anthropic langchainhub emoji\\n\\nCopyimport getpass\\nimport os\\n\\n# Set the project name to whichever project you\\'d like to be testing against\\nproject_name = \"Tweet Writing Task\"\\nos.environ[\"LANGSMITH_PROJECT\"] = project_name\\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\\n\\nif not os.environ.get(\"LANGSMITH_API_KEY\"):\\n    os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"YOUR API KEY\")\\n\\n# Optional. You can swap OpenAI for any other tool-calling chat model.\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR OPENAI API KEY\"\\n\\n# Optional. You can swap Tavily for the free DuckDuckGo search tool if preferred.\\n# Get Tavily API key: https://tavily.com\\nos.environ[\"TAVILY_API_KEY\"] = \"YOUR TAVILY API KEY\"\\n\\n\\u200bDefine the application\\nFor this example lets create a simple Tweet-writing application that has access to some internet search tools:\\nCopyfrom langchain.chat_models import init_chat_model\\nfrom langgraph.prebuilt import create_react_agent\\nfrom langchain_community.tools import DuckDuckGoSearchRun, TavilySearchResults\\nfrom langchain_core.rate_limiters import InMemoryRateLimiter\\n\\n# We will use GPT-3.5 Turbo as the baseline and compare against GPT-4o\\ngpt_3_5_turbo = init_chat_model(\\n    \"gpt-3.5-turbo\",\\n    temperature=1,\\n    configurable_fields=(\"model\", \"model_provider\"),\\n)\\n\\n# The instrucitons are passed as a system message to the agent\\ninstructions = \"\"\"You are a tweet writing assistant. Given a topic, do some research and write a relevant and engaging tweet about it.\\n- Use at least 3 emojis in each tweet\\n- The tweet should be no longer than 280 characters\\n- Always use the search tool to gather recent information on the tweet topic\\n- Write the tweet only based on the search content. Do not rely on your internal knowledge\\n- When relevant, link to your sources\\n- Make your tweet as engaging as possible\"\"\"\\n\\n# Define the tools our agent can use\\n# If you have a higher tiered Tavily API plan you can increase this\\nrate_limiter = InMemoryRateLimiter(requests_per_second=0.08)\\n\\n# Use DuckDuckGo if you don\\'t have a Tavily API key:\\n# tools = [DuckDuckGoSearchRun(rate_limiter=rate_limiter)]\\ntools = [TavilySearchResults(max_results=5, rate_limiter=rate_limiter)]\\n\\nagent = create_react_agent(gpt_3_5_turbo, tools=tools, state_modifier=instructions)\\n\\n\\u200bSimulate production data\\nNow lets simulate some production data:\\nCopyfake_production_inputs = [\\n    \"Alan turing\\'s early childhood\",\\n    \"Economic impacts of the European Union\",\\n    \"Underrated philosophers\",\\n    \"History of the Roxie theater in San Francisco\",\\n    \"ELI5: gravitational waves\",\\n    \"The arguments for and against a parliamentary system\",\\n    \"Pivotal moments in music history\",\\n    \"Big ideas in programming languages\",\\n    \"Big questions in biology\",\\n    \"The relationship between math and reality\",\\n    \"What makes someone funny\",\\n]\\n\\nagent.batch(\\n    [{\"messages\": [{\"role\": \"user\", \"content\": content}]} for content in fake_production_inputs],\\n)\\n\\n\\u200bConvert Production Traces to Experiment\\nThe first step is to generate a dataset based on the production inputs. Then copy over all the traces to serve as a baseline experiment.\\n\\u200bSelect runs to backtest on\\nYou can select the runs to backtest on using the filter argument of list_runs. The filter argument uses the LangSmith trace query syntax to select runs.\\nCopyfrom datetime import datetime, timedelta, timezone\\nfrom uuid import uuid4\\nfrom langsmith import Client\\nfrom langsmith.beta import convert_runs_to_test\\n\\n# Fetch the runs we want to convert to a dataset/experiment\\nclient = Client()\\n\\n# How we are sampling runs to include in our dataset\\nend_time = datetime.now(tz=timezone.utc)\\nstart_time = end_time - timedelta(days=1)\\nrun_filter = f\\'and(gt(start_time, \"{start_time.isoformat()}\"), lt(end_time, \"{end_time.isoformat()}\"))\\'\\nprod_runs = list(\\n    client.list_runs(\\n        project_name=project_name,\\n        is_root=True,\\n        filter=run_filter,\\n    )\\n)\\n\\n\\u200bConvert runs to experiment\\nconvert_runs_to_test is a function which takes some runs and does the following:\\n\\nThe inputs, and optionally the outputs, are saved to a dataset as Examples.\\nThe inputs and outputs are stored as an experiment, as if you had run the evaluate function and received those outputs.\\n\\nCopy# Name of the dataset we want to create\\ndataset_name = f\\'{project_name}-backtesting {start_time.strftime(\"%Y-%m-%d\")}-{end_time.strftime(\"%Y-%m-%d\")}\\'\\n# Name of the experiment we want to create from the historical runs\\nbaseline_experiment_name = f\"prod-baseline-gpt-3.5-turbo-{str(uuid4())[:4]}\"\\n\\n# This converts the runs to a dataset + experiment\\nconvert_runs_to_test(\\n    prod_runs,\\n    # Name of the resulting dataset\\n    dataset_name=dataset_name,\\n    # Whether to include the run outputs as reference/ground truth\\n    include_outputs=False,\\n    # Whether to include the full traces in the resulting experiment\\n    # (default is to just include the root run)\\n    load_child_runs=True,\\n    # Name of the experiment so we can apply evalautors to it after\\n    test_project_name=baseline_experiment_name\\n)\\n\\nOnce this step is complete, you should see a new dataset in your LangSmith project called â€œTweet Writing Task-backtesting TODAYS DATEâ€, with a single experiment like so:\\n\\n\\u200bBenchmark against new system\\nNow we can start the process of benchmarking our production runs against a new system.\\n\\u200bDefine evaluators\\nFirst letâ€™s define the evaluators we will use to compare the two systems. Note that we have no reference outputs, so weâ€™ll need to come up with evaluation metrics that only require the actual outputs.\\nCopyimport emoji\\nfrom pydantic import BaseModel, Field\\nfrom langchain_core.messages import convert_to_openai_messages\\n\\nclass Grade(BaseModel):\\n    \"\"\"Grade whether a response is supported by some context.\"\"\"\\n    grounded: bool = Field(..., description=\"Is the majority of the response supported by the retrieved context?\")\\n\\ngrounded_instructions = f\"\"\"You have given somebody some contextual information and asked them to write a statement grounded in that context.\\n\\nGrade whether their response is fully supported by the context you have provided. \\\\\\nIf any meaningful part of their statement is not backed up directly by the context you provided, then their response is not grounded. \\\\\\nOtherwise it is grounded.\"\"\"\\ngrounded_model = init_chat_model(model=\"gpt-4o\").with_structured_output(Grade)\\n\\ndef lt_280_chars(outputs: dict) -> bool:\\n    messages = convert_to_openai_messages(outputs[\"messages\"])\\n    return len(messages[-1][\\'content\\']) <= 280\\n\\ndef gte_3_emojis(outputs: dict) -> bool:\\n    messages = convert_to_openai_messages(outputs[\"messages\"])\\n    return len(emoji.emoji_list(messages[-1][\\'content\\'])) >= 3\\n\\nasync def is_grounded(outputs: dict) -> bool:\\n    context = \"\"\\n    messages = convert_to_openai_messages(outputs[\"messages\"])\\n    for message in messages:\\n        if message[\"role\"] == \"tool\":\\n            # Tool message outputs are the results returned from the Tavily/DuckDuckGo tool\\n            context += \"\\\\n\\\\n\" + message[\"content\"]\\n    tweet = messages[-1][\"content\"]\\n    user = f\"\"\"CONTEXT PROVIDED:\\n    {context}\\n\\n    RESPONSE GIVEN:\\n    {tweet}\"\"\"\\n    grade = await grounded_model.ainvoke([\\n        {\"role\": \"system\", \"content\": grounded_instructions},\\n        {\"role\": \"user\", \"content\": user}\\n    ])\\n    return grade.grounded\\n\\n\\u200bEvaluate baseline\\nNow, letâ€™s run our evaluators against the baseline experiment.\\nCopybaseline_results = await client.aevaluate(\\n    baseline_experiment_name,\\n    evaluators=[lt_280_chars, gte_3_emojis, is_grounded],\\n)\\n# If you have pandas installed can easily explore results as df:\\n# baseline_results.to_pandas()\\n\\n\\u200bDefine and evaluate new system\\nNow, letâ€™s define and evaluate our new system. In this example our new system will be the same as the old system, but will use GPT-4o instead of GPT-3.5. Since weâ€™ve made our model configurable we can just update the default config passed to our agent:\\nCopycandidate_results = await client.aevaluate(\\n    agent.with_config(model=\"gpt-4o\"),\\n    data=dataset_name,\\n    evaluators=[lt_280_chars, gte_3_emojis, is_grounded],\\n    experiment_prefix=\"candidate-gpt-4o\",\\n)\\n# If you have pandas installed can easily explore results as df:\\n# candidate_results.to_pandas()\\n\\n\\u200bComparing the results\\nAfter running both experiments, you can view them in your dataset:\\n\\nThe results reveal an interesting tradeoff between the two models:\\n\\nGPT-4o shows improved performance in following formatting rules, consistently including the requested number of emojis\\nHowever, GPT-4o is less reliable at staying grounded in the provided search results\\n\\nTo illustrate the grounding issue: in this example run, GPT-4o included facts about AbÅ« Bakr Muhammad ibn ZakariyyÄ al-RÄzÄ«â€™s medical contributions that werenâ€™t present in the search results. This demonstrates how itâ€™s pulling from its internal knowledge rather than strictly using the provided information.\\nThis backtesting exercise revealed that while GPT-4o is generally considered a more capable model, simply upgrading to it wouldnâ€™t improve our tweet-writer. To effectively use GPT-4o, we would need to:\\n\\nRefine our prompts to more strongly emphasize using only provided information\\nOr modify our system architecture to better constrain the modelâ€™s outputs\\n\\nThis insight demonstrates the value of backtesting - it helped us identify potential issues before deployment.\\nWas this page helpful?YesNoSuggest editsEvaluate a complex agentAnalyze an experimentâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/evaluation', 'loc': 'https://docs.smith.langchain.com/evaluation/tutorials/evaluation', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Evaluate a chatbot - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesImprove evaluatorsTutorialsEvaluate a chatbotEvaluate a RAG applicationTest a ReAct agent with Pytest/Vitest and LangSmithEvaluate a complex agentRun backtests on a new version of an agentAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationTutorialsEvaluate a chatbotGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageSetupCreate a datasetDefine metricsRun EvaluationsComparing resultsSet up automated testing to run in CI/CDTrack results over timeConclusionReference codeSet up evaluationsTutorialsEvaluate a chatbotCopy pageCopy pageIn this guide we will set up evaluations for a chatbot. These allow you to measure how well your application is performing over a set of data. Being able to get this insight quickly and reliably will allow you to iterate with confidence.\\nAt a high level, in this tutorial we will:\\n\\nCreate an initial golden dataset to measure performance\\nDefine metrics to use to measure performance\\nRun evaluations on a few different prompts or models\\nCompare results manually\\nTrack results over time\\nSet up automated testing to run in CI/CD\\n\\nFor more information on the evaluation workflows LangSmith supports, check out the how-to guides, or see the reference docs for evaluate and its asynchronous aevaluate counterpart.\\nLots to cover, letâ€™s dive in!\\n\\u200bSetup\\nFirst install the required dependencies for this tutorial. We happen to use OpenAI, but LangSmith can be used with any model:\\npipuvCopypip install -U langsmith openai\\n\\nAnd set environment variables to enable LangSmith tracing:\\nCopyexport LANGSMITH_TRACING=\"true\"\\nexport LANGSMITH_API_KEY=\"<Your LangSmith API key>\"\\nexport OPENAI_API_KEY=\"<Your OpenAI API key>\"\\n\\n\\u200bCreate a dataset\\nThe first step when getting ready to test and evaluate your application is to define the datapoints you want to evaluate. There are a few aspects to consider here:\\n\\nWhat should the schema of each datapoint be?\\nHow many datapoints should I gather?\\nHow should I gather those datapoints?\\n\\nSchema: Each datapoint should consist of, at the very least, the inputs to the application. If you are able, it is also very helpful to define the expected outputs - these represent what you would expect a properly functioning application to output. Often times you cannot define the perfect output - thatâ€™s okay! Evaluation is an iterative process. Sometimes you may also want to define more information for each example - like the expected documents to fetch in RAG, or the expected steps to take as an agent. LangSmith datasets are very flexible and allow you to define arbitrary schemas.\\nHow many: Thereâ€™s no hard and fast rule for how many you should gather. The main thing is to make sure you have proper coverage of edge cases you may want to guard against. Even 10-50 examples can provide a lot of value! Donâ€™t worry about getting a large number to start - you can (and should) always add over time!\\nHow to get: This is maybe the trickiest part. Once you know you want to gather a datasetâ€¦ how do you actually go about it? For most teams that are starting a new project, we generally see them start by collecting the first 10-20 datapoints by hand. After starting with these datapoints, these datasets are generally living constructs and grow over time. They generally grow after seeing how real users will use your application, seeing the pain points that exist, and then moving a few of those datapoints into this set. There are also methods like synthetically generating data that can be used to augment your dataset. To start, we recommend not worrying about those and just hand labeling ~10-20 examples.\\nOnce youâ€™ve got your dataset, there are a few different ways to upload them to LangSmith. For this tutorial, we will use the client, but you can also upload via the UI (or even create them in the UI).\\nFor this tutorial, we will create 5 datapoints to evaluate on. We will be evaluating a question-answering application. The input will be a question, and the output will be an answer. Since this is a question-answering application, we can define the expected answer. Letâ€™s show how to create and upload this dataset to LangSmith!\\nCopyfrom langsmith import Client\\n\\nclient = Client()\\n\\n# Define dataset: these are your test cases\\ndataset_name = \"QA Example Dataset\"\\ndataset = client.create_dataset(dataset_name)\\n\\nclient.create_examples(\\n    dataset_id=dataset.id,\\n    examples=[\\n        {\\n            \"inputs\": {\"question\": \"What is LangChain?\"},\\n            \"outputs\": {\"answer\": \"A framework for building LLM applications\"},\\n        },\\n        {\\n            \"inputs\": {\"question\": \"What is LangSmith?\"},\\n            \"outputs\": {\"answer\": \"A platform for observing and evaluating LLM applications\"},\\n        },\\n        {\\n            \"inputs\": {\"question\": \"What is OpenAI?\"},\\n            \"outputs\": {\"answer\": \"A company that creates Large Language Models\"},\\n        },\\n        {\\n            \"inputs\": {\"question\": \"What is Google?\"},\\n            \"outputs\": {\"answer\": \"A technology company known for search\"},\\n        },\\n        {\\n            \"inputs\": {\"question\": \"What is Mistral?\"},\\n            \"outputs\": {\"answer\": \"A company that creates Large Language Models\"},\\n        }\\n    ]\\n)\\n\\nNow, if we go the LangSmith UI and look for QA Example Dataset in the Datasets & Testing page, when we click into it we should see that we have five new examples.\\n\\n\\u200bDefine metrics\\nAfter creating our dataset, we can now define some metrics to evaluate our responses on. Since we have an expected answer, we can compare to that as part of our evaluation. However, we do not expect our application to output those exact answers, but rather something that is similar. This makes our evaluation a little trickier.\\nIn addition to evaluating correctness, letâ€™s also make sure our answers are short and concise. This will be a little easier - we can define a simple Python function to measure the length of the response.\\nLetâ€™s go ahead and define these two metrics.\\nFor the first, we will use an LLM to judge whether the output is correct (with respect to the expected output). This LLM-as-a-judge is relatively common for cases that are too complex to measure with a simple function. We can define our own prompt and LLM to use for evaluation here:\\nCopyimport openai\\nfrom langsmith import wrappers\\n\\nopenai_client = wrappers.wrap_openai(openai.OpenAI())\\n\\neval_instructions = \"You are an expert professor specialized in grading students\\' answers to questions.\"\\n\\ndef correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\\n    user_content = f\"\"\"You are grading the following question:\\n{inputs[\\'question\\']}\\nHere is the real answer:\\n{reference_outputs[\\'answer\\']}\\nYou are grading the following predicted answer:\\n{outputs[\\'response\\']}\\nRespond with CORRECT or INCORRECT:\\nGrade:\"\"\"\\n    response = openai_client.chat.completions.create(\\n        model=\"gpt-4o-mini\",\\n        temperature=0,\\n        messages=[\\n            {\"role\": \"system\", \"content\": eval_instructions},\\n            {\"role\": \"user\", \"content\": user_content},\\n        ],\\n    ).choices[0].message.content\\n    return response == \"CORRECT\"\\n\\nFor evaluating the length of the response, this is a lot easier! We can just define a simple function that checks whether the actual output is less than 2x the length of the expected result.\\nCopydef concision(outputs: dict, reference_outputs: dict) -> bool:\\n    return int(len(outputs[\"response\"]) < 2 * len(reference_outputs[\"answer\"]))\\n\\n\\u200bRun Evaluations\\nGreat! So now how do we run evaluations? Now that we have a dataset and evaluators, all that we need is our application! We will build a simple application that just has a system message with instructions on how to respond and then passes it to the LLM. We will build this using the OpenAI SDK directly:\\nCopydefault_instructions = \"Respond to the users question in a short, concise manner (one short sentence).\"\\n\\ndef my_app(question: str, model: str = \"gpt-4o-mini\", instructions: str = default_instructions) -> str:\\n    return openai_client.chat.completions.create(\\n        model=model,\\n        temperature=0,\\n        messages=[\\n            {\"role\": \"system\", \"content\": instructions},\\n            {\"role\": \"user\", \"content\": question},\\n        ],\\n    ).choices[0].message.content\\n\\nBefore running this through LangSmith evaluations, we need to define a simple wrapper that maps the input keys from our dataset to the function we want to call, and then also maps the output of the function to the output key we expect.\\nCopydef ls_target(inputs: str) -> dict:\\n    return {\"response\": my_app(inputs[\"question\"])}\\n\\nGreat! Now weâ€™re ready to run an evaluation. Letâ€™s do it!\\nCopyexperiment_results = client.evaluate(\\n    ls_target, # Your AI system\\n    data=dataset_name, # The data to predict and grade over\\n    evaluators=[concision, correctness], # The evaluators to score the results\\n    experiment_prefix=\"openai-4o-mini\", # A prefix for your experiment names to easily identify them\\n)\\n\\nThis will output a URL. If we click on it, we should see results of our evaluation!\\n\\nIf we go back to the dataset page and select the Experiments tab, we can now see a summary of our one run!\\n\\nLetâ€™s now try it out with a different model! Letâ€™s try gpt-4-turbo\\nCopydef ls_target_v2(inputs: str) -> dict:\\n    return {\"response\": my_app(inputs[\"question\"], model=\"gpt-4-turbo\")}\\n\\nexperiment_results = client.evaluate(\\n    ls_target_v2,\\n    data=dataset_name,\\n    evaluators=[concision, correctness],\\n    experiment_prefix=\"openai-4-turbo\",\\n)\\n\\nAnd now letâ€™s use GPT-4 but also update the prompt to be a bit more strict in requiring the answer to be short.\\nCopyinstructions_v3 = \"Respond to the users question in a short, concise manner (one short sentence). Do NOT use more than ten words.\"\\n\\ndef ls_target_v3(inputs: str) -> dict:\\n    response = my_app(\\n        inputs[\"question\"],\\n        model=\"gpt-4-turbo\",\\n        instructions=instructions_v3\\n    )\\n    return {\"response\": response}\\n\\nexperiment_results = client.evaluate(\\n    ls_target_v3,\\n    data=dataset_name,\\n    evaluators=[concision, correctness],\\n    experiment_prefix=\"strict-openai-4-turbo\",\\n)\\n\\nIf we go back to the Experiments tab on the datasets page, we should see that all three runs now show up!\\n\\n\\u200bComparing results\\nAwesome, weâ€™ve evaluated three different runs. But how can we compare results? The first way we can do this is just by looking at the runs in the Experiments tab. If we do that, we can see a high level view of the metrics for each run:\\n\\nGreat! So we can tell that GPT-4 is better than GPT-3.5 at knowing who companies are, and we can see that the strict prompt helped a lot with the length. But what if we want to explore in more detail?\\nIn order to do that, we can select all the runs we want to compare (in this case all three) and open them up in a comparison view. We immediately see all three tests side by side. Some of the cells are color coded - this is showing a regression of a certain metric compared to a certain baseline. We automatically choose defaults for the baseline and metric, but you can change those yourself. You can also choose which columns and which metrics you see by using the Display control. You can also automatically filter to only see the runs that have improvements/regressions by clicking on the icons at the top.\\n\\nIf we want to see more information, we can also select the Expand button that appears when hovering over a row to open up a side panel with more detailed information:\\n\\n\\u200bSet up automated testing to run in CI/CD\\nNow that weâ€™ve run this in a one-off manner, we can set it to run in an automated fashion. We can do this pretty easily by just including it as a pytest file that we run in CI/CD. As part of this, we can either just log the results OR set up some criteria to determine if it passes or not. For example, if I wanted to ensure that we always got at least 80% of generated responses passing the length check, we could set that up with a test like:\\nCopydef test_length_score() -> None:\\n    \"\"\"Test that the length score is at least 80%.\"\"\"\\n    experiment_results = evaluate(\\n        ls_target, # Your AI system\\n        data=dataset_name, # The data to predict and grade over\\n        evaluators=[concision, correctness], # The evaluators to score the results\\n    )\\n    # This will be cleaned up in the next release:\\n    feedback = client.list_feedback(\\n        run_ids=[r.id for r in client.list_runs(project_name=experiment_results.experiment_name)],\\n        feedback_key=\"concision\"\\n    )\\n    scores = [f.score for f in feedback]\\n    assert sum(scores) / len(scores) >= 0.8, \"Aggregate score should be at least .8\"\\n\\n\\u200bTrack results over time\\nNow that weâ€™ve got these experiments running in an automated fashion, we want to track these results over time. We can do this from the overall Experiments tab in the datasets page. By default, we show evaluation metrics over time (highlighted in red). We also automatically track git metrics, to easily associate it with the branch of your code (highlighted in yellow).\\n\\n\\u200bConclusion\\nThatâ€™s it for this tutorial!\\nWeâ€™ve gone over how to create an initial test set, define some evaluation metrics, run experiments, compare them manually, set up CI/CD, and track results over time. Hopefully this can help you iterate with confidence.\\nThis is just the start. As mentioned earlier, evaluation is an ongoing process. For example - the datapoints you will want to evaluate on will likely continue to change over time. There are many types of evaluators you may wish to explore. For information on this, check out the how-to guides.\\nAdditionally, there are other ways to evaluate data besides in this â€œofflineâ€ manner (e.g. you can evaluate production data). For more information on online evaluation, check out this guide.\\n\\u200bReference code\\nClick to see a consolidated code snippetCopyimport openai\\nfrom langsmith import Client, wrappers\\n\\n# Application code\\nopenai_client = wrappers.wrap_openai(openai.OpenAI())\\n\\ndefault_instructions = \"Respond to the users question in a short, concise manner (one short sentence).\"\\n\\ndef my_app(question: str, model: str = \"gpt-4o-mini\", instructions: str = default_instructions) -> str:\\n    return openai_client.chat.completions.create(\\n        model=model,\\n        temperature=0,\\n        messages=[\\n            {\"role\": \"system\", \"content\": instructions},\\n            {\"role\": \"user\", \"content\": question},\\n        ],\\n    ).choices[0].message.content\\n\\nclient = Client()\\n\\n# Define dataset: these are your test cases\\ndataset_name = \"QA Example Dataset\"\\ndataset = client.create_dataset(dataset_name)\\n\\nclient.create_examples(\\n    dataset_id=dataset.id,\\n    examples=[\\n        {\\n            \"inputs\": {\"question\": \"What is LangChain?\"},\\n            \"outputs\": {\"answer\": \"A framework for building LLM applications\"},\\n        },\\n        {\\n            \"inputs\": {\"question\": \"What is LangSmith?\"},\\n            \"outputs\": {\"answer\": \"A platform for observing and evaluating LLM applications\"},\\n        },\\n        {\\n            \"inputs\": {\"question\": \"What is OpenAI?\"},\\n            \"outputs\": {\"answer\": \"A company that creates Large Language Models\"},\\n        },\\n        {\\n            \"inputs\": {\"question\": \"What is Google?\"},\\n            \"outputs\": {\"answer\": \"A technology company known for search\"},\\n        },\\n        {\\n            \"inputs\": {\"question\": \"What is Mistral?\"},\\n            \"outputs\": {\"answer\": \"A company that creates Large Language Models\"},\\n        }\\n    ]\\n)\\n\\n# Define evaluators\\neval_instructions = \"You are an expert professor specialized in grading students\\' answers to questions.\"\\n\\ndef correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\\n    user_content = f\"\"\"You are grading the following question:\\n{inputs[\\'question\\']}\\nHere is the real answer:\\n{reference_outputs[\\'answer\\']}\\nYou are grading the following predicted answer:\\n{outputs[\\'response\\']}\\nRespond with CORRECT or INCORRECT:\\nGrade:\"\"\"\\n    response = openai_client.chat.completions.create(\\n        model=\"gpt-4o-mini\",\\n        temperature=0,\\n        messages=[\\n            {\"role\": \"system\", \"content\": eval_instructions},\\n            {\"role\": \"user\", \"content\": user_content},\\n        ],\\n    ).choices[0].message.content\\n    return response == \"CORRECT\"\\n\\ndef concision(outputs: dict, reference_outputs: dict) -> bool:\\n    return int(len(outputs[\"response\"]) < 2 * len(reference_outputs[\"answer\"]))\\n\\n# Run evaluations\\ndef ls_target(inputs: str) -> dict:\\n    return {\"response\": my_app(inputs[\"question\"])}\\n\\nexperiment_results_v1 = client.evaluate(\\n    ls_target, # Your AI system\\n    data=dataset_name, # The data to predict and grade over\\n    evaluators=[concision, correctness], # The evaluators to score the results\\n    experiment_prefix=\"openai-4o-mini\", # A prefix for your experiment names to easily identify them\\n)\\n\\ndef ls_target_v2(inputs: str) -> dict:\\n    return {\"response\": my_app(inputs[\"question\"], model=\"gpt-4-turbo\")}\\n\\nexperiment_results_v2 = client.evaluate(\\n    ls_target_v2,\\n    data=dataset_name,\\n    evaluators=[concision, correctness],\\n    experiment_prefix=\"openai-4-turbo\",\\n)\\n\\ninstructions_v3 = \"Respond to the users question in a short, concise manner (one short sentence). Do NOT use more than ten words.\"\\n\\ndef ls_target_v3(inputs: str) -> dict:\\n    response = my_app(\\n        inputs[\"question\"],\\n        model=\"gpt-4-turbo\",\\n        instructions=instructions_v3\\n    )\\n    return {\"response\": response}\\n\\nexperiment_results_v3 = client.evaluate(\\n    ls_target_v3,\\n    data=dataset_name,\\n    evaluators=[concision, correctness],\\n    experiment_prefix=\"strict-openai-4-turbo\",\\n)\\nWas this page helpful?YesNoSuggest editsDynamic few shot example selectionEvaluate a RAG applicationâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'loc': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Evaluate a RAG application - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesImprove evaluatorsTutorialsEvaluate a chatbotEvaluate a RAG applicationTest a ReAct agent with Pytest/Vitest and LangSmithEvaluate a complex agentRun backtests on a new version of an agentAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationTutorialsEvaluate a RAG applicationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageOverviewSetupEnvironmentApplicationIndexing and retrievalGenerationDatasetEvaluatorsCorrectness: Response vs reference answerRelevance: Response vs inputGroundedness: Response vs retrieved docsRetrieval relevance: Retrieved docs vs inputRun evaluationReference codeSet up evaluationsTutorialsEvaluate a RAG applicationCopy pageCopy pageRAG evaluation | Evaluators | LLM-as-judge evaluators\\nRetrieval Augmented Generation (RAG) is a technique that enhances Large Language Models (LLMs) by providing them with relevant external knowledge. It has become one of the most widely used approaches for building LLM applications.\\nThis tutorial will show you how to evaluate your RAG applications using LangSmith. Youâ€™ll learn:\\n\\nHow to create test datasets\\nHow to run your RAG application on those datasets\\nHow to measure your applicationâ€™s performance using different evaluation metrics\\n\\n\\u200bOverview\\nA typical RAG evaluation workflow consists of three main steps:\\n\\n\\nCreating a dataset with questions and their expected answers\\n\\n\\nRunning your RAG application on those questions\\n\\n\\nUsing evaluators to measure how well your application performed, looking at factors like:\\n\\nAnswer relevance\\nAnswer accuracy\\nRetrieval quality\\n\\n\\n\\nFor this tutorial, weâ€™ll create and evaluate a bot that answers questions about a few of Lilian Wengâ€™s insightful blog posts.\\n\\u200bSetup\\n\\u200bEnvironment\\nFirst, letâ€™s set our environment variables:\\nPythonTypeScriptCopyimport os\\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\\nos.environ[\"LANGSMITH_API_KEY\"] = \"YOUR LANGSMITH API KEY\"\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR OPENAI API KEY\"\\n\\nAnd install the dependencies weâ€™ll need:\\nPythonTypeScriptCopypip install -U langsmith langchain[openai] langchain-community\\n\\n\\u200bApplication\\nWhile this tutorial uses LangChain, the evaluation techniques and LangSmith functionality demonstrated here work with any framework. Feel free to use your preferred tools and libraries.\\nIn this section, weâ€™ll build a basic Retrieval-Augmented Generation (RAG) application.\\nWeâ€™ll stick to a simple implementation that:\\n\\nIndexing: chunks and indexes a few of Lilian Wengâ€™s blogs in a vector store\\nRetrieval: retrieves those chunks based on the user question\\nGeneration: passes the question and retrieved docs to an LLM.\\n\\n\\u200bIndexing and retrieval\\nFirst, lets load the blog posts we want to build a chatbot for and index them.\\nPythonTypeScriptCopyfrom langchain_community.document_loaders import WebBaseLoader\\nfrom langchain_core.vectorstores import InMemoryVectorStore\\nfrom langchain_openai import OpenAIEmbeddings\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\n# List of URLs to load documents from\\nurls = [\\n    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\\n    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\\n    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\\n]\\n\\n# Load documents from the URLs\\ndocs = [WebBaseLoader(url).load() for url in urls]\\ndocs_list = [item for sublist in docs for item in sublist]\\n\\n# Initialize a text splitter with specified chunk size and overlap\\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\\n    chunk_size=250, chunk_overlap=0\\n)\\n\\n# Split the documents into chunks\\ndoc_splits = text_splitter.split_documents(docs_list)\\n\\n# Add the document chunks to the \"vector store\" using OpenAIEmbeddings\\nvectorstore = InMemoryVectorStore.from_documents(\\n    documents=doc_splits,\\n    embedding=OpenAIEmbeddings(),\\n)\\n\\n# With langchain we can easily turn any vector store into a retrieval component:\\nretriever = vectorstore.as_retriever(k=6)\\n\\n\\u200bGeneration\\nWe can now define the generative pipeline.\\nPythonTypeScriptCopyfrom langchain_openai import ChatOpenAI\\nfrom langsmith import traceable\\n\\nllm = ChatOpenAI(model=\"gpt-4o\", temperature=1)\\n\\n# Add decorator so this function is traced in LangSmith\\n@traceable()\\ndef rag_bot(question: str) -> dict:\\n    # LangChain retriever will be automatically traced\\n    docs = retriever.invoke(question)\\n    docs_string = \"\".join(doc.page_content for doc in docs)\\n    instructions = f\"\"\"You are a helpful assistant who is good at analyzing source information and answering questions.\\n       Use the following source documents to answer the user\\'s questions.\\n       If you don\\'t know the answer, just say that you don\\'t know.\\n       Use three sentences maximum and keep the answer concise.\\n\\nDocuments:\\n{docs_string}\"\"\"\\n    # langchain ChatModel will be automatically traced\\n    ai_msg = llm.invoke([\\n            {\"role\": \"system\", \"content\": instructions},\\n            {\"role\": \"user\", \"content\": question},\\n        ],\\n    )\\n    return {\"answer\": ai_msg.content, \"documents\": docs}\\n\\n\\u200bDataset\\nNow that weâ€™ve got our application, letâ€™s build a dataset to evaluate it. Our dataset will be very simple in this case: weâ€™ll have example questions and reference answers.\\nPythonTypeScriptCopyfrom langsmith import Client\\n\\nclient = Client()\\n\\n# Define the examples for the dataset\\nexamples = [\\n    {\\n        \"inputs\": {\"question\": \"How does the ReAct agent use self-reflection? \"},\\n        \"outputs\": {\"answer\": \"ReAct integrates reasoning and acting, performing actions - such tools like Wikipedia search API - and then observing / reasoning about the tool outputs.\"},\\n    },\\n    {\\n        \"inputs\": {\"question\": \"What are the types of biases that can arise with few-shot prompting?\"},\\n        \"outputs\": {\"answer\": \"The biases that can arise with few-shot prompting include (1) Majority label bias, (2) Recency bias, and (3) Common token bias.\"},\\n    },\\n    {\\n        \"inputs\": {\"question\": \"What are five types of adversarial attacks?\"},\\n        \"outputs\": {\"answer\": \"Five types of adversarial attacks are (1) Token manipulation, (2) Gradient based attack, (3) Jailbreak prompting, (4) Human red-teaming, (5) Model red-teaming.\"},\\n    },\\n]\\n\\n# Create the dataset and examples in LangSmith\\ndataset_name = \"Lilian Weng Blogs Q&A\"\\ndataset = client.create_dataset(dataset_name=dataset_name)\\nclient.create_examples(\\n    dataset_id=dataset.id,\\n    examples=examples\\n)\\n\\n\\u200bEvaluators\\nOne way to think about different types of RAG evaluators is as a tuple of what is being evaluated X what its being evaluated against:\\n\\nCorrectness: Response vs reference answer\\n\\n\\nGoal: Measure â€œhow similar/correct is the RAG chain answer, relative to a ground-truth answerâ€\\nMode: Requires a ground truth (reference) answer supplied through a dataset\\nEvaluator: Use LLM-as-judge to assess answer correctness.\\n\\n\\nRelevance: Response vs input\\n\\n\\nGoal: Measure â€œhow well does the generated response address the initial user inputâ€\\nMode: Does not require reference answer, because it will compare the answer to the input question\\nEvaluator: Use LLM-as-judge to assess answer relevance, helpfulness, etc.\\n\\n\\nGroundedness: Response vs retrieved docs\\n\\n\\nGoal: Measure â€œto what extent does the generated response agree with the retrieved contextâ€\\nMode: Does not require reference answer, because it will compare the answer to the retrieved context\\nEvaluator: Use LLM-as-judge to assess faithfulness, hallucinations, etc.\\n\\n\\nRetrieval relevance: Retrieved docs vs input\\n\\n\\nGoal: Measure â€œhow relevant are my retrieved results for this queryâ€\\nMode: Does not require reference answer, because it will compare the question to the retrieved context\\nEvaluator: Use LLM-as-judge to assess relevance\\n\\n\\n\\u200bCorrectness: Response vs reference answer\\nPythonTypeScriptCopyfrom typing_extensions import Annotated, TypedDict\\n\\n# Grade output schema\\nclass CorrectnessGrade(TypedDict):\\n    # Note that the order in the fields are defined is the order in which the model will generate them.\\n    # It is useful to put explanations before responses because it forces the model to think through\\n    # its final response before generating it:\\n    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]\\n    correct: Annotated[bool, ..., \"True if the answer is correct, False otherwise.\"]\\n\\n# Grade prompt\\ncorrectness_instructions = \"\"\"You are a teacher grading a quiz. You will be given a QUESTION, the GROUND TRUTH (correct) ANSWER, and the STUDENT ANSWER. Here is the grade criteria to follow:\\n(1) Grade the student answers based ONLY on their factual accuracy relative to the ground truth answer. (2) Ensure that the student answer does not contain any conflicting statements.\\n(3) It is OK if the student answer contains more information than the ground truth answer, as long as it is factually accurate relative to the  ground truth answer.\\n\\nCorrectness:\\nA correctness value of True means that the student\\'s answer meets all of the criteria.\\nA correctness value of False means that the student\\'s answer does not meet all of the criteria.\\n\\nExplain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset.\"\"\"\\n\\n# Grader LLM\\ngrader_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0).with_structured_output(\\n    CorrectnessGrade, method=\"json_schema\", strict=True\\n)\\n\\ndef correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\\n    \"\"\"An evaluator for RAG answer accuracy\"\"\"\\n    answers = f\"\"\"\\\\\\nQUESTION: {inputs[\\'question\\']}\\nGROUND TRUTH ANSWER: {reference_outputs[\\'answer\\']}\\nSTUDENT ANSWER: {outputs[\\'answer\\']}\"\"\"\\n    # Run evaluator\\n    grade = grader_llm.invoke([\\n        {\"role\": \"system\", \"content\": correctness_instructions},\\n        {\"role\": \"user\", \"content\": answers}\\n    ])\\n    return grade[\"correct\"]\\n\\n\\u200bRelevance: Response vs input\\nThe flow is similar to above, but we simply look at the inputs and outputs without needing the reference_outputs. Without a reference answer we canâ€™t grade accuracy, but can still grade relevanceâ€”as in, did the model address the userâ€™s question or not.\\nPythonTypeScriptCopy# Grade output schema\\nclass RelevanceGrade(TypedDict):\\n    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]\\n    relevant: Annotated[\\n        bool, ..., \"Provide the score on whether the answer addresses the question\"\\n    ]\\n\\n# Grade prompt\\nrelevance_instructions = \"\"\"You are a teacher grading a quiz. You will be given a QUESTION and a STUDENT ANSWER. Here is the grade criteria to follow:\\n(1) Ensure the STUDENT ANSWER is concise and relevant to the QUESTION\\n(2) Ensure the STUDENT ANSWER helps to answer the QUESTION\\n\\nRelevance:\\nA relevance value of True means that the student\\'s answer meets all of the criteria.\\nA relevance value of False means that the student\\'s answer does not meet all of the criteria.\\n\\nExplain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset.\"\"\"\\n\\n# Grader LLM\\nrelevance_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0).with_structured_output(\\n    RelevanceGrade, method=\"json_schema\", strict=True\\n)\\n\\n# Evaluator\\ndef relevance(inputs: dict, outputs: dict) -> bool:\\n    \"\"\"A simple evaluator for RAG answer helpfulness.\"\"\"\\n    answer = f\"QUESTION: {inputs[\\'question\\']}\\\\nSTUDENT ANSWER: {outputs[\\'answer\\']}\"\\n    grade = relevance_llm.invoke([\\n        {\"role\": \"system\", \"content\": relevance_instructions},\\n        {\"role\": \"user\", \"content\": answer}\\n    ])\\n    return grade[\"relevant\"]\\n\\n\\u200bGroundedness: Response vs retrieved docs\\nAnother useful way to evaluate responses without needing reference answers is to check if the response is justified by (or â€œgrounded inâ€) the retrieved documents.\\nPythonTypeScriptCopy# Grade output schema\\nclass GroundedGrade(TypedDict):\\n    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]\\n    grounded: Annotated[\\n        bool, ..., \"Provide the score on if the answer hallucinates from the documents\"\\n    ]\\n\\n# Grade prompt\\ngrounded_instructions = \"\"\"You are a teacher grading a quiz. You will be given FACTS and a STUDENT ANSWER. Here is the grade criteria to follow:\\n(1) Ensure the STUDENT ANSWER is grounded in the FACTS. (2) Ensure the STUDENT ANSWER does not contain \"hallucinated\" information outside the scope of the FACTS.\\n\\nGrounded:\\nA grounded value of True means that the student\\'s answer meets all of the criteria.\\nA grounded value of False means that the student\\'s answer does not meet all of the criteria.\\n\\nExplain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset.\"\"\"\\n\\n# Grader LLM\\ngrounded_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0).with_structured_output(\\n    GroundedGrade, method=\"json_schema\", strict=True\\n)\\n\\n# Evaluator\\ndef groundedness(inputs: dict, outputs: dict) -> bool:\\n    \"\"\"A simple evaluator for RAG answer groundedness.\"\"\"\\n    doc_string = \"\\\\n\\\\n\".join(doc.page_content for doc in outputs[\"documents\"])\\n    answer = f\"FACTS: {doc_string}\\\\nSTUDENT ANSWER: {outputs[\\'answer\\']}\"\\n    grade = grounded_llm.invoke([\\n        {\"role\": \"system\", \"content\": grounded_instructions},\\n        {\"role\": \"user\", \"content\": answer}\\n    ])\\n    return grade[\"grounded\"]\\n\\n\\u200bRetrieval relevance: Retrieved docs vs input\\nPythonTypeScriptCopy# Grade output schema\\nclass RetrievalRelevanceGrade(TypedDict):\\n    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]\\n    relevant: Annotated[\\n        bool,\\n        ...,\\n        \"True if the retrieved documents are relevant to the question, False otherwise\",\\n    ]\\n\\n# Grade prompt\\nretrieval_relevance_instructions = \"\"\"You are a teacher grading a quiz. You will be given a QUESTION and a set of FACTS provided by the student. Here is the grade criteria to follow:\\n(1) You goal is to identify FACTS that are completely unrelated to the QUESTION\\n(2) If the facts contain ANY keywords or semantic meaning related to the question, consider them relevant\\n(3) It is OK if the facts have SOME information that is unrelated to the question as long as (2) is met\\n\\nRelevance:\\nA relevance value of True means that the FACTS contain ANY keywords or semantic meaning related to the QUESTION and are therefore relevant.\\nA relevance value of False means that the FACTS are completely unrelated to the QUESTION.\\n\\nExplain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset.\"\"\"\\n\\n# Grader LLM\\nretrieval_relevance_llm = ChatOpenAI(\\n    model=\"gpt-4o\", temperature=0\\n).with_structured_output(RetrievalRelevanceGrade, method=\"json_schema\", strict=True)\\n\\ndef retrieval_relevance(inputs: dict, outputs: dict) -> bool:\\n    \"\"\"An evaluator for document relevance\"\"\"\\n    doc_string = \"\\\\n\\\\n\".join(doc.page_content for doc in outputs[\"documents\"])\\n    answer = f\"FACTS: {doc_string}\\\\nQUESTION: {inputs[\\'question\\']}\"\\n    # Run evaluator\\n    grade = retrieval_relevance_llm.invoke([\\n        {\"role\": \"system\", \"content\": retrieval_relevance_instructions},\\n        {\"role\": \"user\", \"content\": answer}\\n    ])\\n    return grade[\"relevant\"]\\n\\n\\u200bRun evaluation\\nWe can now kick off our evaluation job with all of our different evaluators.\\nPythonTypeScriptCopydef target(inputs: dict) -> dict:\\n    return rag_bot(inputs[\"question\"])\\n\\nexperiment_results = client.evaluate(\\n    target,\\n    data=dataset_name,\\n    evaluators=[correctness, groundedness, relevance, retrieval_relevance],\\n    experiment_prefix=\"rag-doc-relevance\",\\n    metadata={\"version\": \"LCEL context, gpt-4-0125-preview\"},\\n)\\n\\n# Explore results locally as a dataframe if you have pandas installed\\n# experiment_results.to_pandas()\\n\\nYou can see an example of what these results look like here: LangSmith link\\n\\u200bReference code\\nHere\\'s a consolidated script with all the above code:PythonTypeScriptCopyfrom langchain_community.document_loaders import WebBaseLoader\\nfrom langchain_core.vectorstores import InMemoryVectorStore\\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langsmith import Client, traceable\\nfrom typing_extensions import Annotated, TypedDict\\n\\n# List of URLs to load documents from\\nurls = [\\n    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\\n    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\\n    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\\n]\\n\\n# Load documents from the URLs\\ndocs = [WebBaseLoader(url).load() for url in urls]\\ndocs_list = [item for sublist in docs for item in sublist]\\n\\n# Initialize a text splitter with specified chunk size and overlap\\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\\n    chunk_size=250, chunk_overlap=0\\n)\\n\\n# Split the documents into chunks\\ndoc_splits = text_splitter.split_documents(docs_list)\\n\\n# Add the document chunks to the \"vector store\" using OpenAIEmbeddings\\nvectorstore = InMemoryVectorStore.from_documents(\\n    documents=doc_splits,\\n    embedding=OpenAIEmbeddings(),\\n)\\n\\n# With langchain we can easily turn any vector store into a retrieval component:\\nretriever = vectorstore.as_retriever(k=6)\\n\\nllm = ChatOpenAI(model=\"gpt-4o\", temperature=1)\\n\\n# Add decorator so this function is traced in LangSmith\\n@traceable()\\ndef rag_bot(question: str) -> dict:\\n    # langchain Retriever will be automatically traced\\n    docs = retriever.invoke(question)\\n    docs_string = \"\".join(doc.page_content for doc in docs)\\n    instructions = f\"\"\"You are a helpful assistant who is good at analyzing source information and answering questions.\\n       Use the following source documents to answer the user\\'s questions.\\n       If you don\\'t know the answer, just say that you don\\'t know.\\n       Use three sentences maximum and keep the answer concise.\\n\\nDocuments:\\n{docs_string}\"\"\"\\n    # langchain ChatModel will be automatically traced\\n    ai_msg = llm.invoke([\\n            {\"role\": \"system\", \"content\": instructions},\\n            {\"role\": \"user\", \"content\": question},\\n        ],\\n    )\\n    return {\"answer\": ai_msg.content, \"documents\": docs}\\n\\nclient = Client()\\n\\n# Define the examples for the dataset\\nexamples = [\\n    {\\n        \"inputs\": {\"question\": \"How does the ReAct agent use self-reflection? \"},\\n        \"outputs\": {\"answer\": \"ReAct integrates reasoning and acting, performing actions - such tools like Wikipedia search API - and then observing / reasoning about the tool outputs.\"},\\n    },\\n    {\\n        \"inputs\": {\"question\": \"What are the types of biases that can arise with few-shot prompting?\"},\\n        \"outputs\": {\"answer\": \"The biases that can arise with few-shot prompting include (1) Majority label bias, (2) Recency bias, and (3) Common token bias.\"},\\n    },\\n    {\\n        \"inputs\": {\"question\": \"What are five types of adversarial attacks?\"},\\n        \"outputs\": {\"answer\": \"Five types of adversarial attacks are (1) Token manipulation, (2) Gradient based attack, (3) Jailbreak prompting, (4) Human red-teaming, (5) Model red-teaming.\"},\\n    },\\n]\\n\\n# Create the dataset and examples in LangSmith\\ndataset_name = \"Lilian Weng Blogs Q&A\"\\nif not client.has_dataset(dataset_name=dataset_name):\\n    dataset = client.create_dataset(dataset_name=dataset_name)\\n    client.create_examples(\\n        dataset_id=dataset.id,\\n        examples=examples\\n    )\\n\\n# Grade output schema\\nclass CorrectnessGrade(TypedDict):\\n    # Note that the order in the fields are defined is the order in which the model will generate them.\\n    # It is useful to put explanations before responses because it forces the model to think through\\n    # its final response before generating it:\\n    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]\\n    correct: Annotated[bool, ..., \"True if the answer is correct, False otherwise.\"]\\n\\n# Grade prompt\\ncorrectness_instructions = \"\"\"You are a teacher grading a quiz. You will be given a QUESTION, the GROUND TRUTH (correct) ANSWER, and the STUDENT ANSWER. Here is the grade criteria to follow:\\n(1) Grade the student answers based ONLY on their factual accuracy relative to the ground truth answer. (2) Ensure that the student answer does not contain any conflicting statements.\\n(3) It is OK if the student answer contains more information than the ground truth answer, as long as it is factually accurate relative to the  ground truth answer.\\n\\nCorrectness:\\nA correctness value of True means that the student\\'s answer meets all of the criteria.\\nA correctness value of False means that the student\\'s answer does not meet all of the criteria.\\n\\nExplain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset.\"\"\"\\n\\n# Grader LLM\\ngrader_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0).with_structured_output(\\n    CorrectnessGrade, method=\"json_schema\", strict=True\\n)\\n\\ndef correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\\n    \"\"\"An evaluator for RAG answer accuracy\"\"\"\\n    answers = f\"\"\"\\\\\\nQUESTION: {inputs[\\'question\\']}\\nGROUND TRUTH ANSWER: {reference_outputs[\\'answer\\']}\\nSTUDENT ANSWER: {outputs[\\'answer\\']}\"\"\"\\n    # Run evaluator\\n    grade = grader_llm.invoke([\\n            {\"role\": \"system\", \"content\": correctness_instructions},\\n            {\"role\": \"user\", \"content\": answers},\\n        ]\\n    )\\n    return grade[\"correct\"]\\n\\n# Grade output schema\\nclass RelevanceGrade(TypedDict):\\n    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]\\n    relevant: Annotated[\\n        bool, ..., \"Provide the score on whether the answer addresses the question\"\\n    ]\\n\\n# Grade prompt\\nrelevance_instructions = \"\"\"You are a teacher grading a quiz. You will be given a QUESTION and a STUDENT ANSWER. Here is the grade criteria to follow:\\n(1) Ensure the STUDENT ANSWER is concise and relevant to the QUESTION\\n(2) Ensure the STUDENT ANSWER helps to answer the QUESTION\\n\\nRelevance:\\nA relevance value of True means that the student\\'s answer meets all of the criteria.\\nA relevance value of False means that the student\\'s answer does not meet all of the criteria.\\n\\nExplain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset.\"\"\"\\n\\n# Grader LLM\\nrelevance_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0).with_structured_output(\\n    RelevanceGrade, method=\"json_schema\", strict=True\\n)\\n\\n# Evaluator\\ndef relevance(inputs: dict, outputs: dict) -> bool:\\n    \"\"\"A simple evaluator for RAG answer helpfulness.\"\"\"\\n    answer = f\"QUESTION: {inputs[\\'question\\']}\\\\nSTUDENT ANSWER: {outputs[\\'answer\\']}\"\\n    grade = relevance_llm.invoke([\\n            {\"role\": \"system\", \"content\": relevance_instructions},\\n            {\"role\": \"user\", \"content\": answer},\\n        ]\\n    )\\n    return grade[\"relevant\"]\\n\\n# Grade output schema\\nclass GroundedGrade(TypedDict):\\n    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]\\n    grounded: Annotated[\\n        bool, ..., \"Provide the score on if the answer hallucinates from the documents\"\\n    ]\\n\\n# Grade prompt\\ngrounded_instructions = \"\"\"You are a teacher grading a quiz. You will be given FACTS and a STUDENT ANSWER. Here is the grade criteria to follow:\\n(1) Ensure the STUDENT ANSWER is grounded in the FACTS. (2) Ensure the STUDENT ANSWER does not contain \"hallucinated\" information outside the scope of the FACTS.\\n\\nGrounded:\\nA grounded value of True means that the student\\'s answer meets all of the criteria.\\nA grounded value of False means that the student\\'s answer does not meet all of the criteria.\\n\\nExplain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset.\"\"\"\\n\\n# Grader LLM\\ngrounded_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0).with_structured_output(\\n    GroundedGrade, method=\"json_schema\", strict=True\\n)\\n\\n# Evaluator\\ndef groundedness(inputs: dict, outputs: dict) -> bool:\\n    \"\"\"A simple evaluator for RAG answer groundedness.\"\"\"\\n    doc_string = \"\\\\n\\\\n\".join(doc.page_content for doc in outputs[\"documents\"])\\n    answer = f\"FACTS: {doc_string}\\\\nSTUDENT ANSWER: {outputs[\\'answer\\']}\"\\n    grade = grounded_llm.invoke([\\n            {\"role\": \"system\", \"content\": grounded_instructions},\\n            {\"role\": \"user\", \"content\": answer},\\n        ]\\n    )\\n    return grade[\"grounded\"]\\n\\n# Grade output schema\\nclass RetrievalRelevanceGrade(TypedDict):\\n    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]\\n    relevant: Annotated[\\n        bool,\\n        ...,\\n        \"True if the retrieved documents are relevant to the question, False otherwise\",\\n    ]\\n\\n# Grade prompt\\nretrieval_relevance_instructions = \"\"\"You are a teacher grading a quiz. You will be given a QUESTION and a set of FACTS provided by the student. Here is the grade criteria to follow:\\n(1) You goal is to identify FACTS that are completely unrelated to the QUESTION\\n(2) If the facts contain ANY keywords or semantic meaning related to the question, consider them relevant\\n(3) It is OK if the facts have SOME information that is unrelated to the question as long as (2) is met\\n\\nRelevance:\\nA relevance value of True means that the FACTS contain ANY keywords or semantic meaning related to the QUESTION and are therefore relevant.\\nA relevance value of False means that the FACTS are completely unrelated to the QUESTION.\\n\\nExplain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset.\"\"\"\\n\\n# Grader LLM\\nretrieval_relevance_llm = ChatOpenAI(\\n    model=\"gpt-4o\", temperature=0\\n).with_structured_output(RetrievalRelevanceGrade, method=\"json_schema\", strict=True)\\n\\ndef retrieval_relevance(inputs: dict, outputs: dict) -> bool:\\n    \"\"\"An evaluator for document relevance\"\"\"\\n    doc_string = \"\\\\n\\\\n\".join(doc.page_content for doc in outputs[\"documents\"])\\n    answer = f\"FACTS: {doc_string}\\\\nQUESTION: {inputs[\\'question\\']}\"\\n    # Run evaluator\\n    grade = retrieval_relevance_llm.invoke([\\n            {\"role\": \"system\", \"content\": retrieval_relevance_instructions},\\n            {\"role\": \"user\", \"content\": answer},\\n        ]\\n    )\\n    return grade[\"relevant\"]\\n\\ndef target(inputs: dict) -> dict:\\n    return rag_bot(inputs[\"question\"])\\n\\nexperiment_results = client.evaluate(\\n    target,\\n    data=dataset_name,\\n    evaluators=[correctness, groundedness, relevance, retrieval_relevance],\\n    experiment_prefix=\"rag-doc-relevance\",\\n    metadata={\"version\": \"LCEL context, gpt-4-0125-preview\"},\\n)\\n\\n# Explore results locally as a dataframe if you have pandas installed\\n# experiment_results.to_pandas()\\nWas this page helpful?YesNoSuggest editsEvaluate a chatbotTest a ReAct agent with Pytest/Vitest and LangSmithâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark', 'loc': 'https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Run SWE-bench with LangSmith - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationSetupOverviewCreate an account and API keySet up a workspaceManage organizations using the APIManage billingSet up resource tagsUser managementAdditional resourcesFAQsCloud architecture and scalabilityRegions FAQAuthentication methodsData purging for complianceRelease versionsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationRun SWE-bench with LangSmithGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageLoading the dataEditing the â€˜versionâ€™ columnUpload the data to LangSmithSave to CSVUpload CSV to LangSmith ManuallyUpload CSV to LangSmith ProgrammaticallyCreate dataset split for quicker testingRunning our prediction functionEvaluating our predictions using SWE-benchSending Evaluation to LangSmithRun SWE-bench with LangSmithCopy pageCopy pageSWE-bench is one of the most popular (and difficult!) benchmarks for developers to test their coding agents against. In this walkthrough we will show you how to load the SWE-bench dataset into LangSmith and easily run evals on it, allowing you to have much better visibility into your agents behaviour then using the off-the-shelf SWE-bench eval suite. This allows you to pin specific problems quicker and iterate on your agent rapidly to improve performance!\\n\\u200bLoading the data\\nTo load the data, we will pull the dev split from Hugging Face, but for your use case you may wish to pull one of the test, or train splits, and if you want to combine multiple splits you can use pd.concat.\\nCopyimport pandas as pd\\n\\nsplits = {\\n    \\'dev\\': \\'data/dev-00000-of-00001.parquet\\',\\n    \\'test\\': \\'data/test-00000-of-00001.parquet\\',\\n    \\'train\\': \\'data/train-00000-of-00001.parquet\\'\\n}\\n\\ndf = pd.read_parquet(\"hf://datasets/princeton-nlp/SWE-bench/\" + splits[\"dev\"])\\n\\n\\u200bEditing the â€˜versionâ€™ column\\nThis is a very important step! If you skip, the rest of the code WILL NOT WORK!\\nThe version column contains all string values but all are in float format so they get converted to floats when you upload the CSV to create a LangSmith dataset. Although you can convert the values to strings during your experiments, the issue arises with values like \"0.10\". When getting converted to a float, you get the value 0.1, which would become \"0.1\" if you converted it to a string - causing a key error during execution of your proposed patch.\\nIn order to fix this, we need LangSmith to stop trying to convert the version column to floats. In order to do this, we can just append a string prefix to each of them that is not float compatible. We then need to split on this prefix when doing evaluation to get the actual version value. The prefix we choose here is the string \"version:\".\\nThe ability to select column types when uploading a CSV to LangSmith will be added in the future to avoid having to use this workaround.\\nCopydf[\\'version\\'] = df[\\'version\\'].apply(lambda x: f\"version:{x}\")\\n\\n\\u200bUpload the data to LangSmith\\n\\u200bSave to CSV\\nTo upload the data to LangSmith, we first need to save it to a CSV, which we can do using the to_csv function provided by pandas. Make sure to save this file somewhere that is easily accessible to you.\\nCopydf.to_csv(\"./../SWE-bench.csv\",index=False)\\n\\n\\u200bUpload CSV to LangSmith Manually\\nWe are now ready to upload the CSV to LangSmith. Once you are on the LangSmith website , go to the Datasets & Testing tab on the left side navigation bar, and then click the + New Dataset button in the top right corner.\\nThen click the Upload CSV button on the top, and select the CSV file you saved in the previous step. You can then give your dataset a name and description.\\nNext, select Key-Value as the dataset type. Lastly head to the Create Schema section and add ALL OF THE KEYS as Input fields. There are no Output fields in this example because our evaluator is not comparing against a reference, but instead will run the output of our experiments in docker containers to ensure that the code actually solves the PR issue.\\nOnce you have populated the Input fields (and left the Output fields empty!) you can click the blue Create button in the top right corner, and your dataset will be created!\\n\\u200bUpload CSV to LangSmith Programmatically\\nAlternatively you can upload your csv to LangSmith using the sdk as shown in the code block below:\\nCopydataset = client.upload_csv(\\n    csv_file=\"./../SWE-bench-dev.csv\",\\n    input_keys=list(df.columns),\\n    output_keys=[],\\n    name=\"swe-bench-programatic-upload\",\\n    description=\"SWE-bench dataset\",\\n    data_type=\"kv\"\\n)\\n\\n\\u200bCreate dataset split for quicker testing\\nSince running the SWE-bench evaluator takes a long time when run on all examples, you can create a â€œtestâ€ split for quickly testing the evaluator and your code. Read this guide to learn more about managing dataset splits.\\n\\u200bRunning our prediction function\\nRunning evaluation over SWE-bench works a little differently than most evals you will typically run on LangSmith since we donâ€™t have a reference output. Because of this, we first generate all of our outputs without running an evaluator (note how the evaluate call doesnâ€™t have the evaluators parameter set). In this case we returned a dummy predict function, but you can insert your agent logic inside the predict function to make it work as intended.\\nCopyfrom langsmith import evaluate\\nfrom langsmith import Client\\n\\nclient = Client()\\n\\ndef predict(inputs: dict):\\n    return {\\n        \"instance_id\": inputs[\\'instance_id\\'],\\n        \"model_patch\": \"None\",\\n        \"model_name_or_path\": \"test-model\"\\n    }\\n\\nresult = evaluate(\\n    predict,\\n    data=client.list_examples(\\n        dataset_id=\"a9bffcdf-1dfe-4aef-8805-8806f0110067\",\\n        splits=[\"test\"]\\n    ),\\n)\\n\\nView the evaluation results for experiment: â€˜perfect-lip-22â€™\\n\\u200bEvaluating our predictions using SWE-bench\\nNow we can run the following code to run the predicted patches we generated above in Docker. This code is edited slightly from the SWE-bench run_evaluation.py file.\\nBasically, the code sets up docker images to run the predictions in parallel, which greatly reduces the time needed for evaluation. This screenshot explains the basics of how SWE-bench does evaluation under the hood. To understand it in full, make sure to read through the code in the GitHub repository.\\n\\nThe function convert_runs_to_langsmith_feedback converts the logs generated by the docker file into a nice .json file that contains feedback in the typical key/score method of LangSmith.\\nCopyfrom swebench.harness.run_evaluation import run_instances\\nimport resource\\nimport docker\\nfrom swebench.harness.docker_utils import list_images, clean_images\\nfrom swebench.harness.docker_build import build_env_images\\nfrom pathlib import Path\\nimport json\\nimport os\\n\\nRUN_EVALUATION_LOG_DIR = Path(\"logs/run_evaluation\")\\nLANGSMITH_EVALUATION_DIR = \\'./langsmith_feedback/feedback.json\\'\\n\\ndef convert_runs_to_langsmith_feedback(\\n    predictions: dict,\\n    full_dataset: list,\\n    run_id: str\\n) -> float:\\n    \"\"\"\\n    Convert logs from docker containers into LangSmith feedback.\\n    Args:\\n        predictions (dict): Predictions dict generated by the model\\n        full_dataset (list): List of all instances\\n        run_id (str): Run ID\\n    \"\"\"\\n    feedback_for_all_instances = {}\\n    for instance in full_dataset:\\n        feedback_for_instance = []\\n        instance_id = instance[\\'instance_id\\']\\n        prediction = predictions[instance_id]\\n\\n        if prediction.get(\"model_patch\", None) in [\"\", None]:\\n            # Prediction returned an empty patch\\n            feedback_for_all_instances[prediction[\\'run_id\\']] = [\\n                {\"key\": \"non-empty-patch\", \"score\": 0},\\n                {\"key\": \"completed-patch\", \"score\": 0},\\n                {\"key\": \"resolved-patch\", \"score\": 0}\\n            ]\\n            continue\\n\\n        feedback_for_instance.append({\"key\": \"non-empty-patch\", \"score\": 1})\\n        report_file = (\\n            RUN_EVALUATION_LOG_DIR\\n            / run_id\\n            / prediction[\"model_name_or_path\"].replace(\"/\", \"__\")\\n            / prediction[\\'instance_id\\']\\n            / \"report.json\"\\n        )\\n\\n        if report_file.exists():\\n            # If report file exists, then the instance has been run\\n            feedback_for_instance.append({\"key\": \"completed-patch\", \"score\": 1})\\n            report = json.loads(report_file.read_text())\\n            # Check if instance actually resolved the PR\\n            if report[instance_id][\"resolved\"]:\\n                feedback_for_instance.append({\"key\": \"resolved-patch\", \"score\": 1})\\n            else:\\n                feedback_for_instance.append({\"key\": \"resolved-patch\", \"score\": 0})\\n        else:\\n            # The instance did not run successfully\\n            feedback_for_instance += [\\n                {\"key\": \"completed-patch\", \"score\": 0},\\n                {\"key\": \"resolved-patch\", \"score\": 0}\\n            ]\\n        feedback_for_all_instances[prediction[\\'run_id\\']] = feedback_for_instance\\n\\n    os.makedirs(os.path.dirname(LANGSMITH_EVALUATION_DIR), exist_ok=True)\\n    with open(LANGSMITH_EVALUATION_DIR, \\'w\\') as json_file:\\n        json.dump(feedback_for_all_instances, json_file)\\n\\ndef evaluate_predictions(\\n    dataset: list,\\n    predictions: list,\\n    max_workers: int,\\n    force_rebuild: bool,\\n    cache_level: str,\\n    clean: bool,\\n    open_file_limit: int,\\n    run_id: str,\\n    timeout: int,\\n):\\n    \"\"\"\\n    Run evaluation harness for the given dataset and predictions.\\n    \"\"\"\\n    # set open file limit\\n    assert len(run_id) > 0, \"Run ID must be provided\"\\n    resource.setrlimit(resource.RLIMIT_NOFILE, (open_file_limit, open_file_limit))\\n    client = docker.from_env()\\n    existing_images = list_images(client)\\n    print(f\"Running {len(dataset)} unevaluated instances...\")\\n\\n    # build environment images + run instances\\n    build_env_images(client, dataset, force_rebuild, max_workers)\\n    run_instances(predictions, dataset, cache_level, clean, force_rebuild, max_workers, run_id, timeout)\\n\\n    # clean images + make final report\\n    clean_images(client, existing_images, cache_level, clean)\\n    convert_runs_to_langsmith_feedback(predictions, dataset, run_id)\\n\\nCopydataset = []\\npredictions = {}\\n\\nfor res in result:\\n    predictions[res[\\'run\\'].outputs[\\'instance_id\\']] = {\\n        **res[\\'run\\'].outputs,\\n        **{\"run_id\": str(res[\\'run\\'].id)}\\n    }\\n    dataset.append(res[\\'run\\'].inputs[\\'inputs\\'])\\n\\nfor d in dataset:\\n    d[\\'version\\'] = d[\\'version\\'].split(\":\")[1]\\n\\nCopyevaluate_predictions(\\n    dataset,\\n    predictions,\\n    max_workers=8,\\n    force_rebuild=False,\\n    cache_level=\"env\",\\n    clean=False,\\n    open_file_limit=4096,\\n    run_id=\"test\",\\n    timeout=1_800\\n)\\n\\nCopy    Running 3 unevaluated instances...\\n    Base image sweb.base.arm64:latest already exists, skipping build.\\n    Base images built successfully.\\n    Total environment images to build: 2\\n    Building environment images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:47<00:00, 23.94s/it]\\n    All environment images built successfully.\\n    Running 3 instances...\\n      0%|          | 0/3 [00:00<?, ?it/s]\\n    Evaluation error for sqlfluff__sqlfluff-884: >>>>> Patch Apply Failed:\\n    patch unexpectedly ends in middle of line\\n    patch: **** Only garbage was found in the patch input.\\n    Check (logs/run_evaluation/test/test-model/sqlfluff__sqlfluff-884/run_instance.log) for more information.\\n    Evaluation error for sqlfluff__sqlfluff-4151: >>>>> Patch Apply Failed:\\n    patch unexpectedly ends in middle of line\\n    patch: **** Only garbage was found in the patch input.\\n    Check (logs/run_evaluation/test/test-model/sqlfluff__sqlfluff-4151/run_instance.log) for more information.\\n    Evaluation error for sqlfluff__sqlfluff-2849: >>>>> Patch Apply Failed:\\n    patch: **** Only garbage was found in the patch input.\\n    patch unexpectedly ends in middle of line\\n    Check (logs/run_evaluation/test/test-model/sqlfluff__sqlfluff-2849/run_instance.log) for more information.\\n    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:30<00:00, 10.04s/it]\\n    All instances run.\\n    Cleaning cached images...\\n    Removed 0 images.\\n\\n\\u200bSending Evaluation to LangSmith\\nNow, we can actually send our evaluation feedback to LangSmith by using the evaluate_existing function. Our evaluate function is incredibly simple in this case, because the convert_runs_to_langsmith_feedback function above made our life very easy by saving all the feedback to a single file.\\nCopyfrom langsmith import evaluate_existing\\nfrom langsmith.schemas import Example, Run\\n\\ndef swe_bench_evaluator(run: Run, example: Example):\\n    with open(LANGSMITH_EVALUATION_DIR, \\'r\\') as json_file:\\n        langsmith_eval = json.load(json_file)\\n    return {\"results\": langsmith_eval[str(run.id)]}\\n\\nexperiment_name = result.experiment_name\\nevaluate_existing(experiment_name, evaluators=[swe_bench_evaluator])\\n\\nCopy    View the evaluation results for experiment: \\'perfect-lip-22\\' at:\\n    https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/a9bffcdf-1dfe-4aef-8805-8806f0110067/compare?selectedSessions=182de5dc-fc9d-4065-a3e1-34527f952fd8\\n    3it [00:01,  1.52it/s]\\n    <ExperimentResults perfect-lip-22>\\n\\nAfter running, we can go to the experiments tab of our dataset, and check that our feedback keys were properly assigned. If they were, you should see something that resembles the following image:\\nWas this page helpful?YesNoSuggest editsâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/testing', 'loc': 'https://docs.smith.langchain.com/evaluation/tutorials/testing', 'changefreq': 'weekly', 'priority': '0.5'}, page_content=\"Page Not FoundSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationPage Not FoundGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForum404Page Not FoundWe couldn't find the page you were looking for. Maybe you were looking for?Test a ReAct agent with Pytest/Vitest and LangSmithGet started with LangSmithSelf-host LangSmith on KubernetesAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/langgraph_cloud', 'loc': 'https://docs.smith.langchain.com/langgraph_cloud', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Get started with LangGraph Platform - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangGraph PlatformPlatform for building and deploying AI agentsOverviewGet startedComponentsApplication structureQuickstartsRun a LangGraph application locallyDeploy on cloudUse LangGraph StudioPlansPlans and pricingOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationOverviewGet started with LangGraph PlatformGet startedBuildDeployManageReferenceGet startedBuildDeployManageReferenceGitHubForumOverviewGet started with LangGraph PlatformCopy pageCopy pageLangGraph Platform is a runtime for deploying and managing long-running, stateful agent workflows in production. It provides APIs for execution, persistence, monitoring, and scaling of agent applications. Agents built with LangGraph or other frameworks can be hosted on the platform and exposed through managed endpoints.\\nChoose from cloud, hybrid, or self-hosted deployments based on your infrastructure requirements. For more details, refer to the Deployment options page.\\n\\u200bQuickstarts\\nRun a LangGraph app locallyTest and develop your app using the LangGraph server process locally.Learn moreDeploy to cloudRun your app in a fully managed cloud deployment.Learn moreLangGraph StudioVisualize, debug, and interact with agent workflows.\\nIncludes integrations with LangSmith for tracing and evaluation.Learn more\\n\\u200bFeatures\\nStreaming SupportStream token outputs and intermediate states back to the client in real time,\\nreducing wait times during long operations.Learn moreBackground RunsRun agents asynchronously for long-duration tasks (minutes to hours),\\nwith monitoring via polling endpoints or webhooks.Learn moreBurst HandlingUse the built-in task queue to handle bursty request loads without data loss\\nor service disruption.Learn moreInterrupt HandlingManage overlapping or rapid user inputs (â€œdouble textingâ€) without breaking\\nagent state.Learn moreCheckpointers & MemoryPersist agent state with built-in checkpointing and memory storage,\\nremoving the need for custom solutions.Learn moreHuman-in-the-LoopInsert human review or intervention into an agent run through dedicated APIs.Learn moreWas this page helpful?YesNoSuggest editsOverviewâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability', 'loc': 'https://docs.smith.langchain.com/observability', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Observability - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsManual instrumentationConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationObservabilityGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumObservabilityCopy pageCopy pageWelcome to the LangSmith Observability documentation. The following sections help you set up and use tracing, monitoring, and observability features:\\n\\n\\nSet up tracing: Configure tracing for your applications with basic configuration, integrations with popular frameworks, and advanced configuration options.\\n\\n\\nView traces: Access and manage your traces through the UI and API, including filtering, exporting, sharing, and comparing traces.\\n\\n\\nMonitoring: Set up dashboards and alerts to monitor your application performance and receive notifications when issues arise.\\n\\n\\nAutomations: Configure rules, webhooks, and online evaluations to automate your observability workflows.\\n\\n\\nHuman Feedback: Collect and manage human feedback on your application outputs through annotation queues and inline annotation.\\n\\n\\nTrace a RAG application: Follow a tutorial to trace a Retrieval-Augmented Generation (RAG) application from start to finish.\\n\\n\\nFor terminology definitions and core concepts, refer to the introduction on observability.Was this page helpful?YesNoSuggest editsConceptsâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/concepts', 'loc': 'https://docs.smith.langchain.com/observability/concepts', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Observability concepts - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsManual instrumentationConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationStart tracingObservability conceptsGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageRunsTracesProjectsFeedbackTagsMetadataData storage and retentionDeleting traces from LangSmithStart tracingObservability conceptsCopy pageCopy pageThis page covers key concepts that are important to understand when logging traces to LangSmith.\\nA trace records the sequence of steps your application takesâ€”from receiving an input, through intermediate processing, to producing a final output. Each step within a trace is represented by a run. Multiple traces are grouped together within a project.\\nThe following diagram displays these concepts in the context of a simple RAG app, which retrieves documents from an index and generates an answer.\\n\\n\\u200bRuns\\nA run is a span representing a single unit of work or operation within your LLM application. This could be anything from a single call to an LLM or chain, to a prompt formatting call, to a runnable lambda invocation. If you are familiar with OpenTelemetry, you can think of a run as a span.\\n\\n\\u200bTraces\\nA trace is a collection of runs for a single operation. For example, if you have a user request that triggers a chain, and that chain makes a call to an LLM, then to an output parser, and so on, all of these runs would be part of the same trace. If you are familiar with OpenTelemetry, you can think of a LangSmith trace as a collection of spans. Runs are bound to a trace by a unique trace ID.\\n\\n\\u200bProjects\\nA project is a collection of traces. You can think of a project as a container for all the traces that are related to a single application or service. You can have multiple projects, and each project can have multiple traces.\\n\\n\\u200bFeedback\\nFeedback allows you to score an individual run based on certain criteria. Each feedback entry consists of a feedback tag and feedback score, and is bound to a run by a unique run ID. Feedback can be continuous or discrete (categorical), and you can reuse feedback tags across different runs within an organization.\\nYou can collect feedback on runs in a number of ways:\\n\\nSent up along with a trace from the LLM application.\\nGenerated by a user in the app inline or in an annotation queue.\\nGenerated by an automatic evaluator during offline evaluation.\\nGenerated by an online evaluator.\\n\\nTo learn more about how feedback is stored in the application, refer to the Feedback data format guide.\\n\\n\\u200bTags\\nTags are collections of strings that can be attached to runs. You can use tags to do the following in the LangSmith UI:\\n\\nCategorize runs for easier search.\\nFilter runs.\\nGroup runs together for analysis.\\n\\nLearn how to attach tags to your traces.\\n\\n\\u200bMetadata\\nMetadata is a collection of key-value pairs that you can attach to runs. You can use metadata to store additional information about a run, such as the version of the application that generated the run, the environment in which the run was generated, or any other information that you want to associate with a run. Similarly to tags, you can use metadata to filter runs in the LangSmith UI or group runs together for analysis.\\nLearn how to add metadata to your traces.\\n\\n\\u200bData storage and retention\\nFor traces ingested on or after Wednesday, May 22, 2024, LangSmith (SaaS) retains trace data for a maximum of 400 days past the date and time the trace was inserted into the LangSmith trace database.\\nAfter 400 days, the traces are permanently deleted from LangSmith, with a limited amount of metadata retained for the purpose of showing accurate statistics, such as historic usage and cost.\\nIf you wish to keep tracing data longer than the data retention period, you can add it to a dataset. A dataset allows you to store the trace inputs and outputs (e.g., as a key-value dataset), and will persist indefinitely, even after the trace gets deleted.\\n\\u200bDeleting traces from LangSmith\\nIf you need to remove a trace from LangSmith before its expiration date, you can do so by deleting the project that contains it.\\nYou can delete a project with one of the following ways:\\n\\nIn the LangSmith UI, select the Delete option on the projectâ€™s overflow menu.\\nWith the delete_tracer_sessions API endpoint\\nWith the delete_project() (Python) or deleteProject() (JS/TS) in the LangSmith SDK.\\n\\nLangSmith does not support self-service deletion of individual traces.\\nIf you have a need to delete a single trace (or set of traces) from LangSmith project before its expiration date, the account owner should reach out to LangSmith Support with the organization ID and trace IDs.Was this page helpful?YesNoSuggest editsOverviewTutorial - Trace a RAG applicationâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Trace with LangChain (Python and JS/TS) - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsOverviewLangChainLangGraphAnthropic (Python only)OpenAIAutoGenClaude CodeCrewAIGoogle ADKInstructor (Python only)OpenAI Agents SDKOpenTelemetrySemantic KernelVercel AI SDKManual instrumentationConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationIntegrationsTrace with LangChain (Python and JS/TS)Get startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageInstallationQuick start1. Configure your environment2. Log a trace3. View your traceTrace selectivelyLog to a specific projectStaticallyDynamicallyAdd metadata and tags to tracesCustomize run nameCustomize run IDAccess run (span) ID for LangChain invocationsEnsure all traces are submitted before exitingTrace without setting environment variablesDistributed tracing with LangChain (Python)Interoperability between LangChain (Python) and LangSmith SDKInteroperability between LangChain.JS and LangSmith SDKTracing LangChain objects inside traceable (JS only)Tracing LangChain child runs via traceable / RunTree API (JS only)Tracing setupIntegrationsTrace with LangChain (Python and JS/TS)Copy pageCopy pageLangSmith integrates seamlessly with LangChain (Python and JavaScript), the popular open-source framework for building LLM applications.\\n\\u200bInstallation\\nInstall the core library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below).\\nFor a full list of packages available, see the LangChain Python docs and LangChain JS docs.\\npipyarnnpmpnpmCopypip install langchain_openai langchain_core\\n\\n\\u200bQuick start\\n\\u200b1. Configure your environment\\nPythonTypeScriptCopyexport LANGSMITH_TRACING=true\\nexport LANGSMITH_API_KEY=<your-api-key>\\n# This example uses OpenAI, but you can use any LLM provider of choice\\nexport OPENAI_API_KEY=<your-openai-api-key>\\n# For LangSmith API keys linked to multiple workspaces, set the LANGSMITH_WORKSPACE_ID environment variable to specify which workspace to use.\\nexport LANGSMITH_WORKSPACE_ID=<your-workspace-id>\\n\\nIf you are using LangChain.js with LangSmith and are not in a serverless environment, we also recommend setting the following explicitly to reduce latency:export LANGCHAIN_CALLBACKS_BACKGROUND=trueIf you are in a serverless environment, we recommend setting the reverse to allow tracing to finish before your function ends:export LANGCHAIN_CALLBACKS_BACKGROUND=falseSee this LangChain.js guide for more information.\\n\\u200b2. Log a trace\\nNo extra code is needed to log a trace to LangSmith. Just run your LangChain code as you normally would.\\nPythonTypeScriptCopyfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\nprompt = ChatPromptTemplate.from_messages([\\n    (\"system\", \"You are a helpful assistant. Please respond to the user\\'s request only based on the given context.\"),\\n    (\"user\", \"Question: {question}\\\\nContext: {context}\")\\n])\\n\\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\\noutput_parser = StrOutputParser()\\nchain = prompt | model | output_parser\\n\\nquestion = \"Can you summarize this morning\\'s meetings?\"\\ncontext = \"During this morning\\'s meeting, we solved all world conflict.\"\\n\\nchain.invoke({\"question\": question, \"context\": context})\\n\\n\\u200b3. View your trace\\nBy default, the trace will be logged to the project with the name default. An example of a trace logged using the above code is made public and can be viewed here.\\n\\n\\u200bTrace selectively\\nThe previous section showed how to trace all invocations of a LangChain runnables within your applications by setting a single environment variable. While this is a convenient way to get started, you may want to trace only specific invocations or parts of your application.\\nThere are two ways to do this in Python: by manually passing in a LangChainTracer (reference docs) instance as a callback, or by using the tracing_context context manager (reference docs).\\nIn JS/TS, you can pass a LangChainTracer (reference docs) instance as a callback.\\nPythonTypeScriptCopy# You can opt-in to specific invocations..\\nimport langsmith as ls\\n\\nwith ls.tracing_context(enabled=True):\\n    chain.invoke({\"question\": \"Am I using a callback?\", \"context\": \"I\\'m using a callback\"})\\n\\n# This will NOT be traced (assuming LANGSMITH_TRACING is not set)\\nchain.invoke({\"question\": \"Am I being traced?\", \"context\": \"I\\'m not being traced\"})\\n\\n# This would not be traced, even if LANGSMITH_TRACING=true\\nwith ls.tracing_context(enabled=False):\\n    chain.invoke({\"question\": \"Am I being traced?\", \"context\": \"I\\'m not being traced\"})\\n\\n\\u200bLog to a specific project\\n\\u200bStatically\\nAs mentioned in the tracing conceptual guide LangSmith uses the concept of a Project to group traces. If left unspecified, the tracer project is set to default. You can set the LANGSMITH_PROJECT environment variable to configure a custom project name for an entire application run. This should be done before executing your application.\\nCopyexport LANGSMITH_PROJECT=my-project\\n\\nThe LANGSMITH_PROJECT flag is only supported in JS SDK versions >= 0.2.16, use LANGCHAIN_PROJECT instead if you are using an older version.\\n\\u200bDynamically\\nThis largely builds off of the previous section and allows you to set the project name for a specific LangChainTracer instance or as parameters to the tracing_context context manager in Python.\\nPythonTypeScriptCopy# You can set the project name using the project_name parameter.\\nimport langsmith as ls\\n\\nwith ls.tracing_context(project_name=\"My Project\", enabled=True):\\n    chain.invoke({\"question\": \"Am I using a context manager?\", \"context\": \"I\\'m using a context manager\"})\\n\\n\\u200bAdd metadata and tags to traces\\nYou can annotate your traces with arbitrary metadata and tags by providing them in the RunnableConfig. This is useful for associating additional information with a trace, such as the environment in which it was executed, or the user who initiated it. For information on how to query traces and runs by metadata and tags, see this guide\\nWhen you attach metadata or tags to a runnable (either through the RunnableConfig or at runtime with invocation params), they are inherited by all child runnables of that runnable.\\nPythonTypeScriptCopyfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\nprompt = ChatPromptTemplate.from_messages([\\n    (\"system\", \"You are a helpful AI.\"),\\n    (\"user\", \"{input}\")\\n])\\n\\n# The tag \"model-tag\" and metadata {\"model-key\": \"model-value\"} will be attached to the ChatOpenAI run only\\nchat_model = ChatOpenAI().with_config({\"tags\": [\"model-tag\"], \"metadata\": {\"model-key\": \"model-value\"}})\\noutput_parser = StrOutputParser()\\n\\n# Tags and metadata can be configured with RunnableConfig\\nchain = (prompt | chat_model | output_parser).with_config({\"tags\": [\"config-tag\"], \"metadata\": {\"config-key\": \"config-value\"}})\\n\\n# Tags and metadata can also be passed at runtime\\nchain.invoke({\"input\": \"What is the meaning of life?\"}, {\"tags\": [\"invoke-tag\"], \"metadata\": {\"invoke-key\": \"invoke-value\"}})\\n\\n\\u200bCustomize run name\\nYou can customize the name of a given run when invoking or streaming your LangChain code by providing it in the Config. This name is used to identify the run in LangSmith and can be used to filter and group runs. The name is also used as the title of the run in the LangSmith UI. This can be done by setting a run_name in the RunnableConfig object at construction or by passing a run_name in the invocation parameters in JS/TS.\\nPythonTypeScriptCopy# When tracing within LangChain, run names default to the class name of the traced object (e.g., \\'ChatOpenAI\\').\\nconfigured_chain = chain.with_config({\"run_name\": \"MyCustomChain\"})\\nconfigured_chain.invoke({\"input\": \"What is the meaning of life?\"})\\n\\n# You can also configure the run name at invocation time, like below\\nchain.invoke({\"input\": \"What is the meaning of life?\"}, {\"run_name\": \"MyCustomChain\"})\\n\\nThe run_name parameter only changes the name of the runnable you invoke (e.g., a chain, function). It does not rename the nested run automatically created when you invoke an LLM object like ChatOpenAI (gpt-4o-mini). In the example, the enclosing run will appear in LangSmith as MyCustomChain, while the nested LLM run still shows the modelâ€™s default name.To give the LLM run a more meaningful name, you can either:\\nWrap the model in another runnable and assign a run_name to that step.\\nUse a tracing decorator or helper (e.g., @traceable in Python, or traceable from langsmith in JS/TS) to create a custom run around the model call.\\n\\n\\u200bCustomize run ID\\nYou can customize the ID of a given run when invoking or streaming your LangChain code by providing it in the Config. This ID is used to uniquely identify the run in LangSmith and can be used to query specific runs. The ID can be useful for linking runs across different systems or for implementing custom tracking logic. This can be done by setting a run_id in the RunnableConfig object at construction or by passing a run_id in the invocation parameters.\\nThis feature is not currently supported directly for LLM objects.\\nPythonTypeScriptCopyimport uuid\\n\\nmy_uuid = uuid.uuid4()\\n\\n# You can configure the run ID at invocation time:\\nchain.invoke({\"input\": \"What is the meaning of life?\"}, {\"run_id\": my_uuid})\\n\\nNote that if you do this at the root of a trace (i.e., the top-level run, that run ID will be used as the trace_id).\\n\\u200bAccess run (span) ID for LangChain invocations\\nWhen you invoke a LangChain object, you can manually specify the run ID of the invocation. This run ID can be used to query the run in LangSmith.\\nIn JS/TS, you can use a RunCollectorCallbackHandler instance to access the run ID.\\nPythonTypeScriptCopyimport uuid\\n\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\nprompt = ChatPromptTemplate.from_messages([\\n    (\"system\", \"You are a helpful assistant. Please respond to the user\\'s request only based on the given context.\"),\\n    (\"user\", \"Question: {question}\\\\n\\\\nContext: {context}\")\\n])\\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\\noutput_parser = StrOutputParser()\\n\\nchain = prompt | model | output_parser\\n\\nquestion = \"Can you summarize this morning\\'s meetings?\"\\ncontext = \"During this morning\\'s meeting, we solved all world conflict.\"\\nmy_uuid = uuid.uuid4()\\nresult = chain.invoke({\"question\": question, \"context\": context}, {\"run_id\": my_uuid})\\nprint(my_uuid)\\n\\n\\u200bEnsure all traces are submitted before exiting\\nIn LangChain Python, LangSmithâ€™s tracing is done in a background thread to avoid obstructing your production application. This means that your process may end before all traces are successfully posted to LangSmith. This is especially prevalent in a serverless environment, where your VM may be terminated immediately once your chain or agent completes.\\nYou can make callbacks synchronous by setting the LANGCHAIN_CALLBACKS_BACKGROUND environment variable to \"false\".\\nFor both languages, LangChain exposes methods to wait for traces to be submitted before exiting your application. Below is an example:\\nPythonTypeScriptCopyfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.tracers.langchain import wait_for_all_tracers\\n\\nllm = ChatOpenAI()\\n\\ntry:\\n  llm.invoke(\"Hello, World!\")\\nfinally:\\n  wait_for_all_tracers()\\n\\n\\u200bTrace without setting environment variables\\nAs mentioned in other guides, the following environment variables allow you to configure tracing enabled, the api endpoint, the api key, and the tracing project:\\n\\nLANGSMITH_TRACING\\nLANGSMITH_API_KEY\\nLANGSMITH_ENDPOINT\\nLANGSMITH_PROJECT\\n\\nHowever, in some environments, it is not possible to set environment variables. In these cases, you can set the tracing configuration programmatically.\\nThis largely builds off of the previous section.\\nPythonTypeScriptCopyimport langsmith as ls\\n\\n# You can create a client instance with an api key and api url\\nclient = ls.Client(\\n    api_key=\"YOUR_API_KEY\",  # This can be retrieved from a secrets manager\\n    api_url=\"https://api.smith.langchain.com\",  # Update appropriately for self-hosted installations or the EU region\\n)\\n\\n# You can pass the client and project_name to the tracing_context\\nwith ls.tracing_context(client=client, project_name=\"test-no-env\", enabled=True):\\n    chain.invoke({\"question\": \"Am I using a callback?\", \"context\": \"I\\'m using a callback\"})\\n\\n\\u200bDistributed tracing with LangChain (Python)\\nLangSmith supports distributed tracing with LangChain Python. This allows you to link runs (spans) across different services and applications. The principles are similar to the distributed tracing guide for the LangSmith SDK.\\nCopyimport langsmith\\nfrom langchain_core.runnables import chain\\nfrom langsmith.run_helpers import get_current_run_tree\\n\\n# -- This code should be in a separate file or service --\\n@chain\\ndef child_chain(inputs):\\n    return inputs[\"test\"] + 1\\n\\ndef child_wrapper(x, headers):\\n    with langsmith.tracing_context(parent=headers):\\n        child_chain.invoke({\"test\": x})\\n\\n# -- This code should be in a separate file or service --\\n@chain\\ndef parent_chain(inputs):\\n    rt = get_current_run_tree()\\n    headers = rt.to_headers()\\n    # ... make a request to another service with the headers\\n    # The headers should be passed to the other service, eventually to the child_wrapper function\\n\\nparent_chain.invoke({\"test\": 1})\\n\\n\\u200bInteroperability between LangChain (Python) and LangSmith SDK\\nIf you are using LangChain for part of your application and the LangSmith SDK (see this guide) for other parts, you can still trace the entire application seamlessly.\\nLangChain objects will be traced when invoked within a traceable function and be bound as a child run of the traceable function.\\nCopyfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langsmith import traceable\\n\\nprompt = ChatPromptTemplate.from_messages([\\n    (\"system\", \"You are a helpful assistant. Please respond to the user\\'s request only based on the given context.\"),\\n    (\"user\", \"Question: {question}\\\\nContext: {context}\")\\n])\\n\\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\\noutput_parser = StrOutputParser()\\nchain = prompt | model | output_parser\\n\\n# The above chain will be traced as a child run of the traceable function\\n@traceable(\\n    tags=[\"openai\", \"chat\"],\\n    metadata={\"foo\": \"bar\"}\\n)\\ndef invoke_runnnable(question, context):\\n    result = chain.invoke({\"question\": question, \"context\": context})\\n    return \"The response is: \" + result\\n\\ninvoke_runnnable(\"Can you summarize this morning\\'s meetings?\", \"During this morning\\'s meeting, we solved all world conflict.\")\\n\\nThis will produce the following trace tree: \\n\\u200bInteroperability between LangChain.JS and LangSmith SDK\\n\\u200bTracing LangChain objects inside traceable (JS only)\\nStarting with langchain@0.2.x, LangChain objects are traced automatically when used inside @traceable functions, inheriting the client, tags, metadata and project name of the traceable function.\\nFor older versions of LangChain below 0.2.x, you will need to manually pass an instance LangChainTracer created from the tracing context found in @traceable.\\nCopyimport { ChatOpenAI } from \"@langchain/openai\";\\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\\nimport { StringOutputParser } from \"@langchain/core/output_parsers\";\\nimport { getLangchainCallbacks } from \"langsmith/langchain\";\\n\\nconst prompt = ChatPromptTemplate.fromMessages([\\n  [\\n    \"system\",\\n    \"You are a helpful assistant. Please respond to the user\\'s request only based on the given context.\",\\n  ],\\n  [\"user\", \"Question: {question}\\\\nContext: {context}\"],\\n]);\\n\\nconst model = new ChatOpenAI({ modelName: \"gpt-4o-mini\" });\\nconst outputParser = new StringOutputParser();\\nconst chain = prompt.pipe(model).pipe(outputParser);\\n\\nconst main = traceable(\\n  async (input: { question: string; context: string }) => {\\n    const callbacks = await getLangchainCallbacks();\\n    const response = await chain.invoke(input, { callbacks });\\n    return response;\\n  },\\n  { name: \"main\" }\\n);\\n\\n\\u200bTracing LangChain child runs via traceable / RunTree API (JS only)\\nWeâ€™re working on improving the interoperability between traceable and LangChain. The following limitations are present when using combining LangChain with traceable:\\nMutating RunTree obtained from getCurrentRunTree() of the RunnableLambda context will result in a no-op.\\nItâ€™s discouraged to traverse the RunTree obtained from RunnableLambda via getCurrentRunTree() as it may not contain all the RunTree nodes.\\nDifferent child runs may have the same execution_order and child_execution_order value. Thus in extreme circumstances, some runs may end up in a different order, depending on the start_time.\\n\\nIn some uses cases, you might want to run traceable functions as part of the RunnableSequence or trace child runs of LangChain run imperatively via the RunTree API. Starting with LangSmith 0.1.39 and @langchain/core 0.2.18, you can directly invoke traceable-wrapped functions within RunnableLambda.\\nCopyimport { traceable } from \"langsmith/traceable\";\\nimport { RunnableLambda } from \"@langchain/core/runnables\";\\nimport { RunnableConfig } from \"@langchain/core/runnables\";\\n\\nconst tracedChild = traceable((input: string) => `Child Run: ${input}`, {\\n  name: \"Child Run\",\\n});\\n\\nconst parrot = new RunnableLambda({\\n  func: async (input: { text: string }, config?: RunnableConfig) => {\\n    return await tracedChild(input.text);\\n  },\\n});\\n\\n\\nAlternatively, you can convert LangChainâ€™s RunnableConfig to a equivalent RunTree object by using RunTree.fromRunnableConfig or pass the RunnableConfig as the first argument of traceable-wrapped function.\\nTraceableRun TreeCopyimport { traceable } from \"langsmith/traceable\";\\nimport { RunnableLambda } from \"@langchain/core/runnables\";\\nimport { RunnableConfig } from \"@langchain/core/runnables\";\\n\\nconst tracedChild = traceable((input: string) => `Child Run: ${input}`, {\\n  name: \"Child Run\",\\n});\\n\\nconst parrot = new RunnableLambda({\\n  func: async (input: { text: string }, config?: RunnableConfig) => {\\n    // Pass the config to existing traceable function\\n    await tracedChild(config, input.text);\\n    return input.text;\\n  },\\n});\\n\\nIf you prefer a video tutorial, check out the Alternative Ways to Trace video from the Introduction to LangSmith Course.Was this page helpful?YesNoSuggest editsOverviewLangGraphâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/access_current_span', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/access_current_span', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Access the current run (span) within a traced function - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationSetupOverviewCreate an account and API keySet up a workspaceManage organizations using the APIManage billingSet up resource tagsUser managementAdditional resourcesFAQsCloud architecture and scalabilityRegions FAQAuthentication methodsData purging for complianceRelease versionsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationAccess the current run (span) within a traced functionGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumAccess the current run (span) within a traced functionCopy pageCopy pageIn some cases you will want to access the current run (span) within a traced function. This can be useful for extracting UUIDs, tags, or other information from the current run.\\nYou can access the current run by calling the get_current_run_tree/getCurrentRunTree function in the Python or TypeScript SDK, respectively.\\nFor a full list of available properties on the RunTree object, see this reference.\\nPythonTypeScriptCopyfrom langsmith import traceable\\nfrom langsmith.run_helpers import get_current_run_tree\\nfrom openai import Client\\n\\n    openai = Client()\\n\\n    @traceable\\n    def format_prompt(subject):\\n        run = get_current_run_tree()\\n        print(f\"format_prompt Run Id: {run.id}\")\\n        print(f\"format_prompt Trace Id: {run.trace_id}\")\\n        print(f\"format_prompt Parent Run Id: {run.parent_run.id}\")\\n        return [\\n            {\\n                \"role\": \"system\",\\n                \"content\": \"You are a helpful assistant.\",\\n            },\\n            {\\n                \"role\": \"user\",\\n                \"content\": f\"What\\'s a good name for a store that sells {subject}?\"\\n            }\\n        ]\\n\\n    @traceable(run_type=\"llm\")\\n    def invoke_llm(messages):\\n        run = get_current_run_tree()\\n        print(f\"invoke_llm Run Id: {run.id}\")\\n        print(f\"invoke_llm Trace Id: {run.trace_id}\")\\n        print(f\"invoke_llm Parent Run Id: {run.parent_run.id}\")\\n        return openai.chat.completions.create(\\n            messages=messages, model=\"gpt-4o-mini\", temperature=0\\n        )\\n\\n    @traceable\\n    def parse_output(response):\\n        run = get_current_run_tree()\\n        print(f\"parse_output Run Id: {run.id}\")\\n        print(f\"parse_output Trace Id: {run.trace_id}\")\\n        print(f\"parse_output Parent Run Id: {run.parent_run.id}\")\\n        return response.choices[0].message.content\\n\\n    @traceable\\n    def run_pipeline():\\n        run = get_current_run_tree()\\n        print(f\"run_pipeline Run Id: {run.id}\")\\n        print(f\"run_pipeline Trace Id: {run.trace_id}\")\\n        messages = format_prompt(\"colorful socks\")\\n        response = invoke_llm(messages)\\n        return parse_output(response)\\n\\nrun_pipeline()\\nWas this page helpful?YesNoSuggest editsâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Add metadata and tags to traces - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsManual instrumentationConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyAdd metadata and tags to tracesPrevent logging of sensitive data in tracesUpload files with tracesTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationData & privacyAdd metadata and tags to tracesGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumConfiguration & troubleshootingData & privacyAdd metadata and tags to tracesCopy pageCopy pageLangSmith supports sending arbitrary metadata and tags along with traces.\\nTags are strings that can be used to categorize or label a trace. Metadata is a dictionary of key-value pairs that can be used to store additional information about a trace.\\nBoth are useful for associating additional information with a trace, such as the environment in which it was executed, the user who initiated it, or an internal correlation ID. For more information on tags and metadata, see the Concepts page. For information on how to query traces and runs by metadata and tags, see the Filter traces in the application page.\\nPythonTypeScriptCopyimport openai\\nimport langsmith as ls\\nfrom langsmith.wrappers import wrap_openai\\n\\nclient = openai.Client()\\nmessages = [\\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\\n    {\"role\": \"user\", \"content\": \"Hello!\"}\\n]\\n\\n    # You can set metadata & tags **statically** when decorating a function\\n    # Use the @traceable decorator with tags and metadata\\n    # Ensure that the LANGSMITH_TRACING environment variables are set for @traceable to work\\n    @ls.traceable(\\n        run_type=\"llm\",\\n        name=\"OpenAI Call Decorator\",\\n        tags=[\"my-tag\"],\\n        metadata={\"my-key\": \"my-value\"}\\n    )\\n    def call_openai(\\n        messages: list[dict], model: str = \"gpt-4o-mini\"\\n    ) -> str:\\n        # You can also dynamically set metadata on the parent run:\\n        rt = ls.get_current_run_tree()\\n        rt.metadata[\"some-conditional-key\"] = \"some-val\"\\n        rt.tags.extend([\"another-tag\"])\\n        return client.chat.completions.create(\\n            model=model,\\n            messages=messages,\\n        ).choices[0].message.content\\n\\n    call_openai(\\n        messages,\\n        # To add at **invocation time**, when calling the function.\\n        # via the langsmith_extra parameter\\n        langsmith_extra={\"tags\": [\"my-other-tag\"], \"metadata\": {\"my-other-key\": \"my-value\"}}\\n    )\\n\\n    # Alternatively, you can use the context manager\\n    with ls.trace(\\n        name=\"OpenAI Call Trace\",\\n        run_type=\"llm\",\\n        inputs={\"messages\": messages},\\n        tags=[\"my-tag\"],\\n        metadata={\"my-key\": \"my-value\"},\\n    ) as rt:\\n        chat_completion = client.chat.completions.create(\\n            model=\"gpt-4o-mini\",\\n            messages=messages,\\n        )\\n        rt.metadata[\"some-conditional-key\"] = \"some-val\"\\n        rt.end(outputs={\"output\": chat_completion})\\n\\n# You can use the same techniques with the wrapped client\\npatched_client = wrap_openai(\\n    client, tracing_extra={\"metadata\": {\"my-key\": \"my-value\"}, \"tags\": [\"a-tag\"]}\\n)\\nchat_completion = patched_client.chat.completions.create(\\n    model=\"gpt-4o-mini\",\\n    messages=messages,\\n    langsmith_extra={\\n        \"tags\": [\"my-other-tag\"],\\n        \"metadata\": {\"my-other-key\": \"my-value\"},\\n    },\\n)\\nWas this page helpful?YesNoSuggest editsTrace generator functionsPrevent logging of sensitive data in tracesâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/alerts', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/alerts', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Configure webhook notifications for LangSmith alerts - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsManual instrumentationConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationMonitoring & alertingConfigure webhook notifications for LangSmith alertsGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageOverviewPrerequisitesIntegration ConfigurationStep 1: Prepare Your Receiving EndpointStep 2: Configure Webhook ParametersStep 3: Test the WebhookTroubleshootingSecurity ConsiderationsSending alerts to Slack using a webhookPrerequisitesStep 1: Create a Slack AppStep 2: Configure Bot PermissionsStep 3: Install the App to Your WorkspaceStep 4: Configure the Webhook Alert in LangSmithStep 5: Test the Integration(Optional) Step 6: Link to the Alert Preview in the Request BodyAdditional ResourcesMonitoring & alertingConfigure webhook notifications for LangSmith alertsCopy pageCopy page\\u200bOverview\\nThis guide details the process for setting up webhook notifications for LangSmith alerts. Before proceeding, make sure you have followed the steps leading up to the notification step of creating the alert by following this guide. Webhooks enable integration with custom services and third-party platforms by sending HTTP POST requests when alert conditions are triggered. Use webhooks to forward alert data to ticketing systems, chat applications, or custom monitoring solutions.\\n\\u200bPrerequisites\\n\\nAn endpoint that can receive HTTP POST requests\\nAppropriate authentication credentials for your receiving service (if required)\\n\\n\\u200bIntegration Configuration\\n\\u200bStep 1: Prepare Your Receiving Endpoint\\nBefore configuring the webhook in LangSmith, ensure your receiving endpoint:\\n\\nAccepts HTTP POST requests\\nCan process JSON payloads\\nIs accessible from external services\\nHas appropriate authentication mechanisms (if required)\\n\\nAdditionally, if on a custom deployment of LangSmith, make sure there are no firewall settings blocking egress traffic from LangSmith services.\\n\\u200bStep 2: Configure Webhook Parameters\\n\\nIn the notification section of your alert complete the webhook configuration with the following parameters:\\nRequired Fields\\n\\nURL: The complete URL of your receiving endpoint\\n\\nExample: https://api.example.com/incident-webhook\\n\\n\\n\\nOptional Fields\\n\\n\\nHeaders: JSON Key-value pairs sent with the webhook request\\n\\n\\nCommon headers include:\\n\\nAuthorization: For authentication tokens\\nContent-Type: Usually set to application/json (default)\\nX-Source: To identify the source as LangSmith\\n\\n\\n\\nIf no headers, then simply use {}\\n\\n\\n\\n\\nRequest Body Template: Customize the JSON payload sent to your endpoint\\n\\n\\nDefault: LangSmith sends the payload defined and the following additonal key-value pairs appended to the payload:\\n\\nproject_name: Name of the triggered alert\\nalert_rule_id: A UUID to identify the LangSmith alert. This can be used as a de-duplication key in the webhook service.\\nalert_rule_name: The name of the alert rule.\\nalert_rule_type: The type of alert (as of 04/01/2025 all alerts are of type threshold).\\nalert_rule_attribute: The attribute associated with the alert rule - error_count, feedback_score or latency.\\ntriggered_metric_value: The value of the metric at the time the threshold was triggered.\\ntriggered_threshold: The threshold that triggered the alert.\\ntimestamp: The timestamp that triggered the alert.\\n\\n\\n\\n\\n\\n\\u200bStep 3: Test the Webhook\\nClick Send Test Alert to send the webhook notification to ensure the notification works as intended.\\n\\u200bTroubleshooting\\nIf webhook notifications arenâ€™t being delivered:\\n\\nVerify the webhook URL is correct and accessible\\nEnsure any authentication headers are properly formatted\\nCheck that your receiving endpoint accepts POST requests\\nExamine your endpointâ€™s logs for received but rejected requests\\nVerify your custom payload template is valid JSON format\\n\\n\\u200bSecurity Considerations\\n\\nUse HTTPS for your webhook endpoints\\nImplement authentication for your webhook endpoint\\nConsider adding a shared secret in your headers to verify webhook sources\\nValidate incoming webhook requests before processing them\\n\\n\\u200bSending alerts to Slack using a webhook\\nHere is an example for configuring LangSmith alerts to send notifications to Slack channels using the chat.postMessage API.\\n\\u200bPrerequisites\\n\\nAccess to a Slack workspace\\nA LangSmith project to set up alerts\\nPermissions to create Slack applications\\n\\n\\u200bStep 1: Create a Slack App\\n\\nVisit the Slack API Applications page\\nClick Create New App\\nSelect From scratch\\nProvide an App Name (e.g., â€œLangSmith Alertsâ€)\\nSelect the workspace where you want to install the app\\nClick Create App\\n\\n\\u200bStep 2: Configure Bot Permissions\\n\\n\\nIn the left sidebar of your Slack app configuration, click OAuth & Permissions\\n\\n\\nScroll down to Bot Token Scopes under Scopes and click Add an OAuth Scope\\n\\n\\nAdd the following scopes:\\n\\nchat:write (Send messages as the app)\\nchat:write.public (Send messages to channels the app isnâ€™t in)\\nchannels:read (View basic channel information)\\n\\n\\n\\n\\u200bStep 3: Install the App to Your Workspace\\n\\nScroll up to the top of the OAuth & Permissions page\\nClick Install to Workspace\\nReview the permissions and click Allow\\nCopy the Bot User OAuth Token that appears (begins with xoxb-)\\n\\n\\u200bStep 4: Configure the Webhook Alert in LangSmith\\n\\nIn LangSmith, navigate to your project\\nSelect Alerts â†’ Create Alert\\nDefine your alert metrics and conditions\\nIn the notification section, select Webhook\\nConfigure the webhook with the following settings:\\n\\nWebhook URL\\nCopyhttps://slack.com/api/chat.postMessage\\n\\nHeaders\\nCopy{\\n  \"Content-Type\": \"application/json\",\\n  \"Authorization\": \"Bearer xoxb-your-token-here\"\\n}\\n\\n\\nNote: Replace xoxb-your-token-here with your actual Bot User OAuth Token\\n\\nRequest Body Template\\nCopy{\\n  \"channel\": \"{channel_id}\",\\n  \"text\": \"{alert_name} triggered for {project_name}\",\\n  \"blocks\": [\\n    {\\n      \"type\": \"section\",\\n      \"text\": {\\n        \"type\": \"mrkdwn\",\\n        \"text\": \"ðŸš¨{alert_name} has been triggered\"\\n      }\\n    },\\n    {\\n      \"type\": \"section\",\\n      \"text\": {\\n        \"type\": \"mrkdwn\",\\n        \"text\": \"Please check the following link for more information:\"\\n      }\\n    },\\n    {\\n      \"type\": \"section\",\\n      \"text\": {\\n        \"type\": \"mrkdwn\",\\n        \"text\": \"<{project-url}|View in LangSmith>\"\\n      }\\n    }\\n  ]\\n}\\n\\nNOTE: Fill in the channel_id, alert_name, project_name and project_url when creating the alert. You can find your project_url in the browserâ€™s URL bar. Copy the portion up to but not including any query parameters.\\n\\nClick Save to activate the webhook configuration\\n\\n\\u200bStep 5: Test the Integration\\n\\nIn the LangSmith alert configuration, click Test Alert\\nCheck your specified Slack channel for the test notification\\nVerify that the message contains the expected alert information\\n\\n\\u200b(Optional) Step 6: Link to the Alert Preview in the Request Body\\nAfter creating an alert, you can optionally link to its preview in the webhookâ€™s request body.\\n\\nTo configure this:\\n\\nSave your alert\\nFind your saved alert in the alerts table and click it\\nCopy the dsiplayed URL\\nClick â€œEdit Alertâ€\\nReplace the existing project URL with the copied alert preview URL\\n\\n\\u200bAdditional Resources\\n\\nLangSmith Alerts Documentation\\nSlack chat.postMessage API Documentation\\nSlack Block Kit Builder\\nWas this page helpful?YesNoSuggest editsAlertsInsights (Beta)âŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Configure PagerDuty integration for LangSmith alerts - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsManual instrumentationConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationConfigure PagerDuty integration for LangSmith alertsGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageOverviewPrerequisitesIntegration StepsStep 1: Create a Service in PagerDutyStep 2: Obtain Integration KeyStep 3: Configure LangSmith Alert with PagerDutyTroubleshootingAdditional ResourcesConfigure PagerDuty integration for LangSmith alertsCopy pageCopy page\\u200bOverview\\nThis guide walks through the process of configuring PagerDuty as a notification channel for LangSmith alerts using PagerDutyâ€™s Events API v2. This integration allows critical LLM application issues to trigger PagerDuty incidents, enabling rapid response through your established incident management workflow.\\n\\u200bPrerequisites\\n\\nAn active PagerDuty account with administrator access\\nAppropriate service-level permissions in PagerDuty\\n\\nAdditionally, if on a custom deployment of LangSmith, make sure there are no firewall settings blocking egress traffic from LangSmith services.\\n\\u200bIntegration Steps\\n\\u200bStep 1: Create a Service in PagerDuty\\n\\n\\nLog in to your PagerDuty account\\n\\n\\nNavigate to Services â†’ Service Directory\\n\\n\\nClick + New Service\\n\\n\\nComplete the following fields:\\n\\nName: Provide a descriptive name (e.g., â€œLangSmith Monitoringâ€)\\nDescription: Add details about the monitored application\\nEscalation Policy: Select the appropriate team escalation policy\\nIntegration Type: Select â€œEvents API V2â€\\n\\n\\n\\nClick Add Service to create the service\\n\\n\\n\\u200bStep 2: Obtain Integration Key\\nAfter creating the service, youâ€™ll need to retrieve the Integration Key:\\n\\nFrom the Service Directory under the Service dropdown, locate and click on your newly created service\\nSelect the Integrations tab\\nFind the â€œEvents API V2â€ integration\\nCopy the Integration Key (a 32-character alphanumeric string)\\n\\n\\n\\u200bStep 3: Configure LangSmith Alert with PagerDuty\\nTo receive the same alert again within an hour of it being triggered, you must resolve the active incident created by the alert in PagerDuty.\\n\\n\\nIn the notification section of your alert set-up in LangSmith, select PagerDuty\\nClick the key icon to save the Integration Key as a Workspace secret or select an existing Workspace secret. As a best practice, we recommend saving the Integration Key as a Workspace Secret rather than adding it directy. This will allow you to re-use the same key across alerts for a workspace.\\nConfigure additional notification options:\\n\\nSeverity: Maps to PagerDuty incident priority\\n\\n\\nSend a test alert by clicking Send Test Alert\\nVerify the incident is triggered by PagerDuty and contains relevant LangSmith alert information\\n\\n\\u200bTroubleshooting\\nIf incidents arenâ€™t being created in PagerDuty:\\n\\nVerify the Integration Key is entered correctly in LangSmith\\nEnsure the PagerDuty service is active and not in maintenance mode\\nCheck that your PagerDuty account has Events API v2 enabled\\nIf an alert trigger appears to be missing in PagerDuty, check whether the expected trigger occurred within one hour of a previous trigger from the same alert rule, and whether the incident created by the previous alert is still open.\\nReview network connectivity if your LangSmith instance is behind a firewall\\n\\n\\u200bAdditional Resources\\n\\nPagerDuty Events API v2 Documentation\\nPagerDuty Integration Guide\\nLangSmith Alerts Documentation\\nWas this page helpful?YesNoSuggest editsâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Configure webhook notifications for LangSmith alerts - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsManual instrumentationConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationMonitoring & alertingConfigure webhook notifications for LangSmith alertsGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageOverviewPrerequisitesIntegration ConfigurationStep 1: Prepare Your Receiving EndpointStep 2: Configure Webhook ParametersStep 3: Test the WebhookTroubleshootingSecurity ConsiderationsSending alerts to Slack using a webhookPrerequisitesStep 1: Create a Slack AppStep 2: Configure Bot PermissionsStep 3: Install the App to Your WorkspaceStep 4: Configure the Webhook Alert in LangSmithStep 5: Test the Integration(Optional) Step 6: Link to the Alert Preview in the Request BodyAdditional ResourcesMonitoring & alertingConfigure webhook notifications for LangSmith alertsCopy pageCopy page\\u200bOverview\\nThis guide details the process for setting up webhook notifications for LangSmith alerts. Before proceeding, make sure you have followed the steps leading up to the notification step of creating the alert by following this guide. Webhooks enable integration with custom services and third-party platforms by sending HTTP POST requests when alert conditions are triggered. Use webhooks to forward alert data to ticketing systems, chat applications, or custom monitoring solutions.\\n\\u200bPrerequisites\\n\\nAn endpoint that can receive HTTP POST requests\\nAppropriate authentication credentials for your receiving service (if required)\\n\\n\\u200bIntegration Configuration\\n\\u200bStep 1: Prepare Your Receiving Endpoint\\nBefore configuring the webhook in LangSmith, ensure your receiving endpoint:\\n\\nAccepts HTTP POST requests\\nCan process JSON payloads\\nIs accessible from external services\\nHas appropriate authentication mechanisms (if required)\\n\\nAdditionally, if on a custom deployment of LangSmith, make sure there are no firewall settings blocking egress traffic from LangSmith services.\\n\\u200bStep 2: Configure Webhook Parameters\\n\\nIn the notification section of your alert complete the webhook configuration with the following parameters:\\nRequired Fields\\n\\nURL: The complete URL of your receiving endpoint\\n\\nExample: https://api.example.com/incident-webhook\\n\\n\\n\\nOptional Fields\\n\\n\\nHeaders: JSON Key-value pairs sent with the webhook request\\n\\n\\nCommon headers include:\\n\\nAuthorization: For authentication tokens\\nContent-Type: Usually set to application/json (default)\\nX-Source: To identify the source as LangSmith\\n\\n\\n\\nIf no headers, then simply use {}\\n\\n\\n\\n\\nRequest Body Template: Customize the JSON payload sent to your endpoint\\n\\n\\nDefault: LangSmith sends the payload defined and the following additonal key-value pairs appended to the payload:\\n\\nproject_name: Name of the triggered alert\\nalert_rule_id: A UUID to identify the LangSmith alert. This can be used as a de-duplication key in the webhook service.\\nalert_rule_name: The name of the alert rule.\\nalert_rule_type: The type of alert (as of 04/01/2025 all alerts are of type threshold).\\nalert_rule_attribute: The attribute associated with the alert rule - error_count, feedback_score or latency.\\ntriggered_metric_value: The value of the metric at the time the threshold was triggered.\\ntriggered_threshold: The threshold that triggered the alert.\\ntimestamp: The timestamp that triggered the alert.\\n\\n\\n\\n\\n\\n\\u200bStep 3: Test the Webhook\\nClick Send Test Alert to send the webhook notification to ensure the notification works as intended.\\n\\u200bTroubleshooting\\nIf webhook notifications arenâ€™t being delivered:\\n\\nVerify the webhook URL is correct and accessible\\nEnsure any authentication headers are properly formatted\\nCheck that your receiving endpoint accepts POST requests\\nExamine your endpointâ€™s logs for received but rejected requests\\nVerify your custom payload template is valid JSON format\\n\\n\\u200bSecurity Considerations\\n\\nUse HTTPS for your webhook endpoints\\nImplement authentication for your webhook endpoint\\nConsider adding a shared secret in your headers to verify webhook sources\\nValidate incoming webhook requests before processing them\\n\\n\\u200bSending alerts to Slack using a webhook\\nHere is an example for configuring LangSmith alerts to send notifications to Slack channels using the chat.postMessage API.\\n\\u200bPrerequisites\\n\\nAccess to a Slack workspace\\nA LangSmith project to set up alerts\\nPermissions to create Slack applications\\n\\n\\u200bStep 1: Create a Slack App\\n\\nVisit the Slack API Applications page\\nClick Create New App\\nSelect From scratch\\nProvide an App Name (e.g., â€œLangSmith Alertsâ€)\\nSelect the workspace where you want to install the app\\nClick Create App\\n\\n\\u200bStep 2: Configure Bot Permissions\\n\\n\\nIn the left sidebar of your Slack app configuration, click OAuth & Permissions\\n\\n\\nScroll down to Bot Token Scopes under Scopes and click Add an OAuth Scope\\n\\n\\nAdd the following scopes:\\n\\nchat:write (Send messages as the app)\\nchat:write.public (Send messages to channels the app isnâ€™t in)\\nchannels:read (View basic channel information)\\n\\n\\n\\n\\u200bStep 3: Install the App to Your Workspace\\n\\nScroll up to the top of the OAuth & Permissions page\\nClick Install to Workspace\\nReview the permissions and click Allow\\nCopy the Bot User OAuth Token that appears (begins with xoxb-)\\n\\n\\u200bStep 4: Configure the Webhook Alert in LangSmith\\n\\nIn LangSmith, navigate to your project\\nSelect Alerts â†’ Create Alert\\nDefine your alert metrics and conditions\\nIn the notification section, select Webhook\\nConfigure the webhook with the following settings:\\n\\nWebhook URL\\nCopyhttps://slack.com/api/chat.postMessage\\n\\nHeaders\\nCopy{\\n  \"Content-Type\": \"application/json\",\\n  \"Authorization\": \"Bearer xoxb-your-token-here\"\\n}\\n\\n\\nNote: Replace xoxb-your-token-here with your actual Bot User OAuth Token\\n\\nRequest Body Template\\nCopy{\\n  \"channel\": \"{channel_id}\",\\n  \"text\": \"{alert_name} triggered for {project_name}\",\\n  \"blocks\": [\\n    {\\n      \"type\": \"section\",\\n      \"text\": {\\n        \"type\": \"mrkdwn\",\\n        \"text\": \"ðŸš¨{alert_name} has been triggered\"\\n      }\\n    },\\n    {\\n      \"type\": \"section\",\\n      \"text\": {\\n        \"type\": \"mrkdwn\",\\n        \"text\": \"Please check the following link for more information:\"\\n      }\\n    },\\n    {\\n      \"type\": \"section\",\\n      \"text\": {\\n        \"type\": \"mrkdwn\",\\n        \"text\": \"<{project-url}|View in LangSmith>\"\\n      }\\n    }\\n  ]\\n}\\n\\nNOTE: Fill in the channel_id, alert_name, project_name and project_url when creating the alert. You can find your project_url in the browserâ€™s URL bar. Copy the portion up to but not including any query parameters.\\n\\nClick Save to activate the webhook configuration\\n\\n\\u200bStep 5: Test the Integration\\n\\nIn the LangSmith alert configuration, click Test Alert\\nCheck your specified Slack channel for the test notification\\nVerify that the message contains the expected alert information\\n\\n\\u200b(Optional) Step 6: Link to the Alert Preview in the Request Body\\nAfter creating an alert, you can optionally link to its preview in the webhookâ€™s request body.\\n\\nTo configure this:\\n\\nSave your alert\\nFind your saved alert in the alerts table and click it\\nCopy the dsiplayed URL\\nClick â€œEdit Alertâ€\\nReplace the existing project URL with the copied alert preview URL\\n\\n\\u200bAdditional Resources\\n\\nLangSmith Alerts Documentation\\nSlack chat.postMessage API Documentation\\nSlack Block Kit Builder\\nWas this page helpful?YesNoSuggest editsAlertsInsights (Beta)âŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/annotate_code', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/annotate_code', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Custom instrumentation - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsManual instrumentationCustom instrumentationTrace with APILog custom LLM tracesLog retriever tracesConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationManual instrumentationCustom instrumentationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageUse @traceable / traceableUse the trace context manager (Python only)Use the RunTree APIExample usageEnsure all traces are submitted before exitingUsing the LangSmith SDKUsing LangChainTracing setupManual instrumentationCustom instrumentationCopy pageCopy pageIf youâ€™ve decided you no longer want to trace your runs, you can remove the LANGSMITH_TRACING environment variable. Note that this does not affect the RunTree objects or API users, as these are meant to be low-level and not affected by the tracing toggle.\\nThere are several ways to log traces to LangSmith.\\nIf you are using LangChain (either Python or JS/TS), you can skip this section and go directly to the LangChain-specific instructions.\\n\\u200bUse @traceable / traceable\\nLangSmith makes it easy to log traces with minimal changes to your existing code with the @traceable decorator in Python and traceable function in TypeScript.\\nThe LANGSMITH_TRACING environment variable must be set to \\'true\\' in order for traces to be logged to LangSmith, even when using @traceable or traceable. This allows you to toggle tracing on and off without changing your code.Additionally, you will need to set the LANGSMITH_API_KEY environment variable to your API key (see Setup for more information).By default, the traces will be logged to a project named default. To log traces to a different project, see this section.\\nThe @traceable decorator is a simple way to log traces from the LangSmith Python SDK. Simply decorate any function with @traceable.\\nNote that when wrapping a sync function with traceable, (e.g. formatPrompt in the example below), you should use the await keyword when calling it to\\nensure the trace is logged correctly.\\nPythonTypeScriptCopyfrom langsmith import traceable\\nfrom openai import Client\\n\\nopenai = Client()\\n\\n@traceable\\ndef format_prompt(subject):\\n  return [\\n      {\\n          \"role\": \"system\",\\n          \"content\": \"You are a helpful assistant.\",\\n      },\\n      {\\n          \"role\": \"user\",\\n          \"content\": f\"What\\'s a good name for a store that sells {subject}?\"\\n      }\\n  ]\\n\\n@traceable(run_type=\"llm\")\\ndef invoke_llm(messages):\\n  return openai.chat.completions.create(\\n      messages=messages, model=\"gpt-4o-mini\", temperature=0\\n  )\\n\\n@traceable\\ndef parse_output(response):\\n  return response.choices[0].message.content\\n\\n@traceable\\ndef run_pipeline():\\n  messages = format_prompt(\"colorful socks\")\\n  response = invoke_llm(messages)\\n  return parse_output(response)\\n\\nrun_pipeline()\\n\\n\\n\\u200bUse the trace context manager (Python only)\\nIn Python, you can use the trace context manager to log traces to LangSmith. This is useful in situations where:\\n\\nYou want to log traces for a specific block of code.\\nYou want control over the inputs, outputs, and other attributes of the trace.\\nIt is not feasible to use a decorator or wrapper.\\nAny or all of the above.\\n\\nThe context manager integrates seamlessly with the traceable decorator and wrap_openai wrapper, so you can use them together in the same application.\\nCopyimport openai\\nimport langsmith as ls\\nfrom langsmith.wrappers import wrap_openai\\n\\nclient = wrap_openai(openai.Client())\\n\\n@ls.traceable(run_type=\"tool\", name=\"Retrieve Context\")\\ndef my_tool(question: str) -> str:\\n    return \"During this morning\\'s meeting, we solved all world conflict.\"\\n\\ndef chat_pipeline(question: str):\\n    context = my_tool(question)\\n    messages = [\\n        { \"role\": \"system\", \"content\": \"You are a helpful assistant. Please respond to the user\\'s request only based on the given context.\" },\\n        { \"role\": \"user\", \"content\": f\"Question: {question}\\\\nContext: {context}\"}\\n    ]\\n    chat_completion = client.chat.completions.create(\\n        model=\"gpt-4o-mini\", messages=messages\\n    )\\n    return chat_completion.choices[0].message.content\\n\\napp_inputs = {\"input\": \"Can you summarize this morning\\'s meetings?\"}\\n\\nwith ls.trace(\"Chat Pipeline\", \"chain\", project_name=\"my_test\", inputs=app_inputs) as rt:\\n    output = chat_pipeline(\"Can you summarize this morning\\'s meetings?\")\\n    rt.end(outputs={\"output\": output})\\n\\n\\u200bUse the RunTree API\\nAnother, more explicit way to log traces to LangSmith is via the RunTree API. This API allows you more control over your tracing - you can manually create runs and children runs to assemble your trace. You still need to set your LANGSMITH_API_KEY, but LANGSMITH_TRACING is not necessary for this method.\\nThis method is not recommended, as itâ€™s easier to make mistakes in propagating trace context.\\nPythonTypeScriptCopyimport openai\\nfrom langsmith.run_trees import RunTree\\n\\n# This can be a user input to your app\\nquestion = \"Can you summarize this morning\\'s meetings?\"\\n\\n# Create a top-level run\\npipeline = RunTree(\\n  name=\"Chat Pipeline\",\\n  run_type=\"chain\",\\n  inputs={\"question\": question}\\n)\\npipeline.post()\\n\\n# This can be retrieved in a retrieval step\\ncontext = \"During this morning\\'s meeting, we solved all world conflict.\"\\nmessages = [\\n  { \"role\": \"system\", \"content\": \"You are a helpful assistant. Please respond to the user\\'s request only based on the given context.\" },\\n  { \"role\": \"user\", \"content\": f\"Question: {question}\\\\nContext: {context}\"}\\n]\\n\\n# Create a child run\\nchild_llm_run = pipeline.create_child(\\n  name=\"OpenAI Call\",\\n  run_type=\"llm\",\\n  inputs={\"messages\": messages},\\n)\\nchild_llm_run.post()\\n\\n# Generate a completion\\nclient = openai.Client()\\nchat_completion = client.chat.completions.create(\\n  model=\"gpt-4o-mini\", messages=messages\\n)\\n\\n# End the runs and log them\\nchild_llm_run.end(outputs=chat_completion)\\nchild_llm_run.patch()\\npipeline.end(outputs={\"answer\": chat_completion.choices[0].message.content})\\npipeline.patch()\\n\\n\\u200bExample usage\\nYou can extend the utilities above to conveniently trace any code. Below are some example extensions:\\nTrace any public method in a class:\\nCopyfrom typing import Any, Callable, Type, TypeVar\\n\\nT = TypeVar(\"T\")\\n\\ndef traceable_cls(cls: Type[T]) -> Type[T]:\\n    \"\"\"Instrument all public methods in a class.\"\"\"\\n    def wrap_method(name: str, method: Any) -> Any:\\n        if callable(method) and not name.startswith(\"__\"):\\n            return traceable(name=f\"{cls.__name__}.{name}\")(method)\\n        return method\\n\\n    # Handle __dict__ case\\n    for name in dir(cls):\\n        if not name.startswith(\"_\"):\\n            try:\\n                method = getattr(cls, name)\\n                setattr(cls, name, wrap_method(name, method))\\n            except AttributeError:\\n                # Skip attributes that can\\'t be set (e.g., some descriptors)\\n                pass\\n\\n    # Handle __slots__ case\\n    if hasattr(cls, \"__slots__\"):\\n        for slot in cls.__slots__:  # type: ignore[attr-defined]\\n            if not slot.startswith(\"__\"):\\n                try:\\n                    method = getattr(cls, slot)\\n                    setattr(cls, slot, wrap_method(slot, method))\\n                except AttributeError:\\n                    # Skip slots that don\\'t have a value yet\\n                    pass\\n\\n    return cls\\n\\n@traceable_cls\\nclass MyClass:\\n    def __init__(self, some_val: int):\\n        self.some_val = some_val\\n\\n    def combine(self, other_val: int):\\n        return self.some_val + other_val\\n\\n# See trace: https://smith.langchain.com/public/882f9ecf-5057-426a-ae98-0edf84fdcaf9/r\\nMyClass(13).combine(29)\\n\\n\\u200bEnsure all traces are submitted before exiting\\nLangSmithâ€™s tracing is done in a background thread to avoid obstructing your production application. This means that your process may end before all traces are successfully posted to LangSmith. Here are some options for ensuring all traces are submitted before exiting your application.\\n\\u200bUsing the LangSmith SDK\\nIf you are using the LangSmith SDK standalone, you can use the flush method before exit:\\nPythonTypeScriptCopyfrom langsmith import Client\\n\\nclient = Client()\\n\\n@traceable(client=client)\\nasync def my_traced_func():\\n  # Your code here...\\n  pass\\n\\ntry:\\n  await my_traced_func()\\nfinally:\\n  await client.flush()\\n\\n\\u200bUsing LangChain\\nIf you are using LangChain, please refer to our LangChain tracing guide.\\nIf you prefer a video tutorial, check out the Tracing Basics video from the Introduction to LangSmith Course.Was this page helpful?YesNoSuggest editsVercel AI SDKTrace with APIâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Calculate token-based costs for traces - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationSetupOverviewCreate an account and API keySet up a workspaceManage organizations using the APIManage billingSet up resource tagsUser managementAdditional resourcesFAQsCloud architecture and scalabilityRegions FAQAuthentication methodsData purging for complianceRelease versionsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationCalculate token-based costs for tracesGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageSend token countsSpecify model nameSet model pricesCost formulaSend costs directlyCalculate token-based costs for tracesCopy pageCopy page\\nProviding token counts for LLM runs (spans)\\n\\nLangSmith allows you to track token-based costs for LLM runs. The costs are rolled up to the trace and project level.\\nThere are two ways costs can be tracked:\\n\\nDerived from token counts and model prices\\nDirectly specified as part of the run data\\n\\nIn most cases it is easier to include token counts in the run data and specify model pricing in LangSmith. LangSmith assumes that costs are linear in token counts, broken down by token type. For the few models with non-linear pricing (e.g., above X input tokens the per-token price changes), we recommend computing costs client-side and sending them as part of the run data.\\n\\u200bSend token counts\\nFor LangSmith to accurately derive costs for an LLM run, you need to provide token counts:\\n\\nIf you are using the LangSmith Python or TS/JS SDK with OpenAI or Anthropic models, the built-in wrappers will automatically send up token counts, model provider and model name data to LangSmith.\\nIf you are using the LangSmith SDKâ€™s with other model providers, you should carefully read through this guide.\\nIf you are using LangChain Python or TS/JS, token counts, model provider, and model name are automatically sent up to LangSmith for most chat model integrations. If there is a chat model integration that is missing token counts and for which the underlying API includes token counts in the model response, please open a GitHub issue in the LangChain repo.\\n\\nIf token counts are not explicitly specified, LangSmith will approximate the token counts of the LLM messages using tiktoken.\\n\\u200bSpecify model name\\nLangSmith reads the LLM model name from the ls_model_name field in run metadata. The SDK built-in wrappers and any LangChain integrations will automatically handle specifying this metadata for you.\\n\\u200bSet model prices\\nTo compute costs from token counts and model names, we need to know the per-token prices for the model youâ€™re using. LangSmith has a model pricing table for this. The table comes with pricing information for most OpenAI, Anthropic, and Gemini models. You can add prices for other models, or overwrite pricing for default models.\\nYou can specify prices for prompt (input) and completion (output) tokens. If needed you can provide a more detailed breakdown of prices. For example, some model providers have different pricing for multimodal or cached tokens.\\n\\nHovering over the ... next to the prompt/completion prices shows you the price breakdown by token type. You can see, for example, if audio and image prompt tokens have different prices versus default text prompt tokens.\\nTo create a new entry in the model pricing map, click on the Add new model button in the top right corner.\\n\\nHere, you can specify the following fields:\\n\\nModel Name: The human-readable name of the model.\\nMatch Pattern: A regex pattern to match the model name. This is used to match the value for ls_model_name in the run metadata.\\nPrompt (Input) Price: The cost per 1M input tokens for the model. This number is multiplied by the number of tokens in the prompt to calculate the prompt cost.\\nCompletion (Output) Price: The cost per 1M output tokens for the model. This number is multiplied by the number of tokens in the completion to calculate the completion cost.\\nPrompt (Input) Price Breakdown (Optional): The breakdown of price for each different type of prompt token, e.g. cache_read, video, audio, etc.\\nCompletion (Output) Price Breakdown (Optional): The breakdown of price for each different type of completion token, e.g. reasoning, image, etc.\\nModel Activation Date (Optional): The date from which the pricing is applicable. Only runs after this date will apply this model price.\\nProvider (Optional): The provider of the model. If specified, this is matched against ls_provider in the run metadata.\\n\\nOnce you have set up the model pricing map, LangSmith will automatically calculate and aggregate the token-based costs for traces based on the token counts provided in the LLM invocations.\\nPlease note that updates to the model pricing map are will not be reflected in the costs for traces already logged. We do not currently support backfilling model pricing changes.\\nFor specifying pricing breakdowns, here are the detailed token count types used by LangChain chat model integrations and LangSmith SDK wrappers:\\nCopy# Standardized\\ncache_read\\ncache_creation\\nreasoning\\naudio\\nimage\\nvideo\\n# Anthropic-only\\nephemeral_1h_input_tokens\\nephemeral_5m_input_tokens\\n\\n\\u200bCost formula\\nThe cost for a run is computed greedily from most-to-lease specific token type. Suppose we set a price of 2per1Mprompttokenswithadetailedpriceof2 per 1M prompt tokens with a detailed price of 2per1Mprompttokenswithadetailedpriceof1 per 1M cache_read prompt tokens, and $3 per 1M completion tokens. If we uploaded the following usage metadata:\\nCopy{\\n  \"input_tokens\": 20,\\n  \"input_token_details\": {\"cache_read\": 5},\\n  \"output_tokens\": 10,\\n  \"output_token_details\": {},\\n  \"total_tokens\": 30,\\n}\\n\\nthen weâ€™d compute the token costs as follows:\\nCopy# A.K.A. prompt_cost\\n# Notice that we compute the cache_read cost and then for any\\n# remaining input_tokens we apply the default input price.\\ninput_cost = 5 * 1e-6 + (20 - 5) * 2e-6  # 3.5e-5\\n# A.K.A. completion_cost\\noutput_cost = 10 * 3e-6  # 3e-5\\ntotal_cost = input_cost + output_cost  # 6.5e-5\\n\\n\\u200bSend costs directly\\nIf you are tracing an LLM call that returns token cost information, are tracing an API with a non-token based pricing scheme, or otherwise have accurate information around costs at runtime, you may instead populate a usage_metadata dict while tracing rather than relying on LangSmithâ€™s built-in cost calculations.\\nSee this guide to learn how to manually provide cost information for a run.Was this page helpful?YesNoSuggest editsâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/claude_code', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/claude_code', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Trace Claude Code - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsOverviewLangChainLangGraphAnthropic (Python only)OpenAIAutoGenClaude CodeCrewAIGoogle ADKInstructor (Python only)OpenAI Agents SDKOpenTelemetrySemantic KernelVercel AI SDKManual instrumentationConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationIntegrationsTrace Claude CodeGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageQuick StartTracing setupIntegrationsTrace Claude CodeCopy pageCopy pageClaude Code is one of the most impressive and useful AI coding tools to date. Claude code emits  for monitoring and observability. LangSmith can collect and display these events to give you a full detailed log on what Claude Code does under the hood.\\n\\u200bQuick Start\\nYou can integrate LangSmith tracing with Claude Code by setting the following environment variables in the environment in which you run Claude Code.\\nCopy# Enables Claude Code to emit OTEL events\\nexport CLAUDE_CODE_ENABLE_TELEMETRY=1\\n\\n# Sets the output format to use Open Telemetry Protocol\\nexport OTEL_LOGS_EXPORTER=otlp\\n\\n# LangSmith ingests JSON format events\\nexport OTEL_EXPORTER_OTLP_LOGS_PROTOCOL=http/json\\n\\n# Claude Code Logs are translated to Spans by LangSmith\\nexport OTEL_EXPORTER_OTLP_LOGS_ENDPOINT=https://api.smith.langchain.com/otel/v1/claude_code\\n\\n# Pass your API key and desired tracing project through headers\\nexport OTEL_EXPORTER_OTLP_HEADERS=\"x-api-key=<api-key>,Langsmith-Project=<project-name>\"\\n\\n# Set this to true in order to log the input user prompts\\nexport OTEL_LOG_USER_PROMPTS=1\\n\\n# Once these are set, start Claude Code, and events will be traced to LangSmith\\nclaude\\n\\nClaude Code emits open telemetry standard events for monitoring usage, but this does not include the actual prompts and messages that go to the LLM.\\nIf youâ€™re self-hosting LangSmith, replace the base endpoint with your LangSmith api endpoint and append /api/v1. For example: OTEL_EXPORTER_OTLP_LOGS_ENDPOINT=https://ai-company.com/api/v1/otel/v1/claude_code\\nWas this page helpful?YesNoSuggest editsAutoGenCrewAIâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/collector_proxy', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/collector_proxy', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Beta LangSmith Collector-Proxy - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsManual instrumentationConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyTroubleshooting guidesTroubleshoot trace nestingTroubleshoot variable cachingBeta LangSmith Collector-ProxyViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationTroubleshooting guidesBeta LangSmith Collector-ProxyGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageWhen to Use the Collector-ProxyKey FeaturesConfigurationProject ConfigurationAuthenticationDeployment (Docker)UsageHealth & ScalingHorizontal ScalingFork & ExtendConfiguration & troubleshootingTroubleshooting guidesBeta LangSmith Collector-ProxyCopy pageCopy pageThis is a beta feature. The API may change in future releases.\\nThe LangSmith Collector-Proxy is a lightweight, high-performance proxy server that sits between your application and the LangSmith backend. It batches and compresses trace data before sending it to LangSmith, reducing network overhead and improving performance.\\n\\u200bWhen to Use the Collector-Proxy\\nThe Collector-Proxy is particularly valuable when:\\n\\nYouâ€™re running multiple instances of your application in parallel and need to efficiently aggregate traces\\nYou want more efficient tracing than direct OTEL API calls to LangSmith (the collector optimizes batching and compression)\\nYouâ€™re using a language that doesnâ€™t have a native LangSmith SDK\\n\\n\\u200bKey Features\\n\\nEfficient Data Transfer Batches multiple spans into fewer, larger uploads.\\nCompression Uses zstd to minimize payload size.\\nOTLP Support Accepts OTLP JSON and Protobuf over HTTP POST.\\nSemantic Translation Maps GenAI/OpenInference conventions to the LangSmith Run model.\\nFlexible Batching Flush by span count or time interval.\\n\\n\\u200bConfiguration\\nConfigure via environment variables:\\nVariableDescriptionDefaultHTTP_PORTPort to run the proxy server4318LANGSMITH_ENDPOINTLangSmith backend URLhttps://api.smith.langchain.comLANGSMITH_API_KEYAPI key for LangSmithRequired (env var or header)LANGSMITH_PROJECTDefault tracing projectDefault project if not specifiedBATCH_SIZESpans per upload batch100FLUSH_INTERVAL_MSFlush interval in milliseconds1000MAX_BUFFER_BYTESMax uncompressed buffer size10485760 (10 MB)MAX_BODY_BYTESMax incoming request body size209715200 (200 MB)MAX_RETRIESRetry attempts for failed uploads3RETRY_BACKOFF_MSInitial backoff in milliseconds100\\n\\u200bProject Configuration\\nThe Collector-Proxy supports LangSmith project configuration with the following priority:\\n\\nIf a project is specified in the request headers (Langsmith-Project), that project will be used\\nIf no project is specified in headers, it will use the project set in the LANGSMITH_PROJECT environment variable\\nIf neither is set, it will trace to the default project.\\n\\n\\u200bAuthentication\\nThe API key can be provided either:\\n\\nAs an environment variable (LANGSMITH_API_KEY)\\nIn the request headers (X-API-Key)\\n\\n\\u200bDeployment (Docker)\\nYou can deploy the Collector-Proxy with Docker:\\n\\n\\nBuild the image\\nCopydocker build \\\\\\n  -t langsmith-collector-proxy:beta .\\n\\n\\n\\nRun the container\\nCopydocker run -d \\\\\\n  -p 4318:4318 \\\\\\n  -e LANGSMITH_API_KEY=<your_api_key> \\\\\\n  -e LANGSMITH_PROJECT=<your_project> \\\\\\n  langsmith-collector-proxy:beta\\n\\n\\n\\n\\u200bUsage\\nPoint any OTLP-compatible client or the OpenTelemetry Collector exporter at:\\nCopyexport OTEL_EXPORTER_OTLP_ENDPOINT=http://<host>:4318/v1/traces\\nexport OTEL_EXPORTER_OTLP_HEADERS=\"X-API-Key=<your_api_key>,Langsmith-Project=<your_project>\"\\n\\nSend a test trace:\\nCopycurl -X POST http://localhost:4318/v1/traces \\\\\\n  -H \"Content-Type: application/json\" \\\\\\n  --data \\'{\\n    \"resourceSpans\": [\\n      {\\n        \"resource\": {\\n          \"attributes\": [\\n            {\\n              \"key\": \"service.name\",\\n              \"value\": { \"stringValue\": \"test-service\" }\\n            }\\n          ]\\n        },\\n        \"scopeSpans\": [\\n          {\\n            \"scope\": {\\n              \"name\": \"example/instrumentation\",\\n              \"version\": \"1.0.0\"\\n            },\\n            \"spans\": [\\n              {\\n                \"traceId\": \"T6nh/mMkIONaoHewS9UWIw==\",\\n                \"spanId\": \"0tEqJwCpvU0=\",\\n                \"name\": \"parent-span\",\\n                \"kind\": \"SPAN_KIND_INTERNAL\",\\n                \"startTimeUnixNano\": 1747675155185223936,\\n                \"endTimeUnixNano\":   1747675156185223936,\\n                \"attributes\": [\\n                  {\\n                    \"key\": \"gen_ai.prompt\",\\n                    \"value\": {\\n                      \"stringValue\": \"{\\\\\"text\\\\\":\\\\\"Hello, world!\\\\\"}\"\\n                    }\\n                  },\\n                  {\\n                    \"key\": \"gen_ai.usage.input_tokens\",\\n                    \"value\": {\\n                      \"intValue\": \"5\"\\n                    }\\n                  },\\n                  {\\n                    \"key\": \"gen_ai.completion\",\\n                    \"value\": {\\n                      \"stringValue\": \"{\\\\\"text\\\\\":\\\\\"Hi there!\\\\\"}\"\\n                    }\\n                  },\\n                  {\\n                    \"key\": \"gen_ai.usage.output_tokens\",\\n                    \"value\": {\\n                      \"intValue\": \"3\"\\n                    }\\n                  }\\n                ],\\n                \"droppedAttributesCount\": 0,\\n                \"events\": [],\\n                \"links\": [],\\n                \"status\": {}\\n              }\\n            ]\\n          }\\n        ]\\n      }\\n    ]\\n  }\\'\\n\\n\\u200bHealth & Scaling\\n\\nLiveness: GET /live â†’ 200\\nReadiness: GET /ready â†’ 200\\n\\n\\u200bHorizontal Scaling\\nTo ensure full traces are batched correctly, route spans with the same trace ID to the same instance (e.g., via consistent hashing).\\n\\u200bFork & Extend\\nFork the Collector-Proxy repo on GitHub and implement your own converter:\\n\\nCreate a custom GenAiConverter or modify the existing one in internal/translator/otel_converter.go\\nRegister the custom converter in internal/translator/translator.go\\nWas this page helpful?YesNoSuggest editsTroubleshoot variable cachingFilter tracesâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/compare_traces', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/compare_traces', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Compare traces - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsManual instrumentationConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationViewing & managing tracesCompare tracesGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumViewing & managing tracesCompare tracesCopy pageCopy pageTo compare traces, click on the Compare button in the upper right hand side of any trace view.\\n\\nThis will show the trace run table. Select the trace you want to compare against the original trace.\\n\\nThe pane will open with both traces selected in a side by side comparison view.\\n\\nTo stop comparing, close the pane or click on Stop comparing in the upper right hand side of the pane.Was this page helpful?YesNoSuggest editsQuery traces (SDK)Share or unshare a trace publiclyâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/dashboards', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/dashboards', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Monitor projects with dashboards - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsManual instrumentationConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationMonitoring & alertingMonitor projects with dashboardsGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pagePrebuilt dashboardsDashboard sectionsGroup byCustom DashboardsCreating a new dashboardAdding charts to your dashboardChart configurationSelect tracing projects and filter runsPick a metricSplit the dataPick a chart typeSave and manage chartsLinking to a dashboard from a tracing projectExample: user-journey monitoringVideo guideMonitoring & alertingMonitor projects with dashboardsCopy pageCopy pageDashboards give you high-level insights into your trace data, helping you spot trends and monitor the health of your applications. Dashboards are available in the Monitoring tab in the left sidebar.\\nLangSmith offers two dashboard types:\\n\\nPrebuilt dashboards: Automatically generated for every tracing project.\\nCustom dashboards: Fully configurable collections of charts tailored to your needs.\\n\\n\\u200bPrebuilt dashboards\\nPrebuilt dashboards are created automatically for each project and cover essential metrics, such as trace count, error rates, token usage, and more. By default, the prebuilt dashboard for your tracing project can be accessed using the Dashboard button on the top right of the tracing project page.\\n\\nYou cannot modify a prebuilt dashboard. In the future, we plan to allow you to clone a default dashboard in order to have a starting point to customize it.\\n\\u200bDashboard sections\\nPrebuilt dashboards are broken down into the following sections:\\nSectionWhat it showsTracesTrace count, latency and error rates. A trace is a collection of runs related to a single operation. For example, if a user request triggers an agent, all runs for that agent invocation would be part of the same trace.LLM CallsLLM call count and latency. Includes all runs where run type is â€œllmâ€.Cost & TokensTotal and per-trace token counts and costs, broken down by token type. Costs are measured using LangSmithâ€™s cost tracking.ToolsRun counts, error rates, and latency stats for tool runs broken down by tool name. Includes runs where run type is â€œtoolâ€. Limits to top 5 most frequently occurring tools.Run TypesRun counts, error rates, and latency stats for runs that are immediate children of the root run. This helps in understanding the high-level execution path of agents. Limits to top 5 most frequently occurring run names. Refer to the image following this table.Feedback ScoresAggregate stats for the top 5 most frequently occurring types of feedback. Charts show average score for numerical feedback and category counts for categorical feedback.\\nFor example, for the following trace, the following runs have a depth of 1:\\n\\n\\u200bGroup by\\nGroup by run tag or metadata can be used to split data over attributes that are important to your application. The global group by setting appears on the top right hand side of the dashboard. Note that the Tool and Run Type charts already have a group by applied, so the global group by wonâ€™t take effect; the global group by will apply to all other charts.\\nWhen adding metadata to runs, we recommend having the same metadata on the trace, as well as the specific run (e.g. LLM call). Metadata and tags are not propagated from parent to child runs, or vice versa. So, if you want to see e.g. both your trace charts and your LLM call charts grouped on some metadata key then both your traces (root runs) and your LLM runs need to have that metadata attached.\\n\\u200bCustom Dashboards\\nCreate tailored collections of charts for tracking metrics that matter most for your application.\\n\\u200bCreating a new dashboard\\n\\nNavigate to the Monitor tab in the left sidebar.\\nClick on the + New Dashboard button.\\nGive your dashboard a name and a description.\\nClick on Create.\\n\\n\\u200bAdding charts to your dashboard\\n\\nWithin a dashboard, click on the + New Chart button to open up the chart creation pane.\\nGive your chart a name and a description.\\nConfigure the chart.\\n\\n\\u200bChart configuration\\n\\u200bSelect tracing projects and filter runs\\n\\nSelect one or more tracing projects to track metrics for.\\nUse the Chart filters section to refine the matching runs. This filter applies to all data series in the chart. For more information on filtering traces, view our guide on filtering traces in application.\\n\\n\\u200bPick a metric\\n\\nChoose a metric from the dropdown menu to set the y-axis of your chart. With a project and a metric selected, youâ€™ll see a preview of your chart and the matching runs.\\nFor certain metrics (such as latency, token usage, cost), we support comparing multiple metrics with the same unit. For example, you may want one chart where you can see prompt tokens and completion tokens. Each metric appears as a separate line.\\n\\n\\n\\u200bSplit the data\\nThere are two ways to create multiple series in a chart (i.e. create multiple lines in a chart):\\n\\n\\nGroup by: Group runs by run tag or metadata, run name, or run type. Group by automatically splits the data into multiple series based on the field selected. Note that group by is limited to the top 5 elements by frequency.\\n\\n\\nData series: Manually define multiple series with individual filters. This is useful for comparing granular data within a single metric.\\n\\n\\n\\n\\u200bPick a chart type\\n\\nChoose between a line chart and a bar chart for visualizing\\n\\n\\u200bSave and manage charts\\n\\nClick Save to save your chart to the dashboard.\\nEdit or delete a chart by clicking the triple dot button in the top right of the chart.\\nClone a chart by clicking the triple line button in the top right of the chart and selecting + Clone. This will open a new chart creation pane with the same configurations as the original.\\n\\n\\n\\n\\u200bLinking to a dashboard from a tracing project\\nYou can link to any dashboard directly from a tracing project. By default, the prebuilt dashboard for your tracing project is selected. If you have a custom dashboard that you would like to link instead:\\n\\nIn your tracing project, click the three dots next to the Dashboard button.\\nChoose a dashboard to set as the new default.\\n\\n\\n\\u200bExample: user-journey monitoring\\nUse monitoring charts for mapping the decisions made by an agent at a particular node.\\nConsider an email assistant agent. At a particular node it makes a decision about an email to:\\n\\nsend an email back\\nnotify the user\\nno response needed\\n\\nWe can create a chart to track and visualize the breakdown of these decisions.\\nCreating the chart\\n\\n\\nMetric Selection: Select the metric Run count.\\n\\n\\nChart Filters: Add a tree filter to include all of the traces with name triage_input. This means we only include traces that hit the triage_input node. Also add a chart filter for Is Root is true, so our count is not inflated by the number of nodes in the trace.\\n\\n\\n\\nData Series: Create a data series for each decision made at the triage_input node. The output of the decision is stored in the triage.response field of the output object, and the value of the decision is either no, email, or notify. Each of these decisions generates a separate data series in the chart.\\n\\n\\n\\nNow we can visualize the decisions made at the triage_input node over time.\\n\\u200bVideo guide\\nWas this page helpful?YesNoSuggest editsSet up online evaluatorsAlertsâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/data_export', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/data_export', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Bulk Exporting Trace Data - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsManual instrumentationConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationViewing & managing tracesBulk Exporting Trace DataGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageDestinationsExporting DataDestinations - Providing a S3 bucketPreparing the DestinationCredentials configurationAWS S3 bucketGoogle GCS XML S3 compatible bucketCreate an export jobScheduled exportsMonitoring the Export JobMonitor Export StatusList Runs for an ExportList All ExportsStop an ExportPartitioning SchemeImporting Data into other systemsBigQuerySnowflakeRedShiftClickhouseDuckDBError HandlingDebugging Destination ErrorsMonitoring RunsCommon ErrorsViewing & managing tracesBulk Exporting Trace DataCopy pageCopy pagePlan restrictions applyPlease note that the Data Export functionality is only supported for LangSmith Plus or Enterprise tiers.\\nLangSmithâ€™s bulk data export functionality allows you to export your traces into an external destination. This can be useful if you want to analyze the\\ndata offline in a tool such as BigQuery, Snowflake, RedShift, Jupyter Notebooks, etc.\\nAn export can be launched to target a specific LangSmith project and date range. Once a batch export is launched, our system will handle the orchestration and resilience of the export process.\\nPlease note that exporting your data may take some time depending on the size of your data. We also have a limit on how many of your exports can run at the same time.\\nBulk exports also have a runtime timeout of 24 hours.\\n\\u200bDestinations\\nCurrently we support exporting to an S3 bucket or S3 API compatible bucket that you provide. The data will be exported in\\nParquet columnar format. This format will allow you to easily import the data into\\nother systems. The data export will contain equivalent data fields as the Run data format.\\n\\u200bExporting Data\\n\\u200bDestinations - Providing a S3 bucket\\nTo export LangSmith data, you will need to provide an S3 bucket where the data will be exported to.\\nThe following information is needed for the export:\\n\\nBucket Name: The name of the S3 bucket where the data will be exported to.\\nPrefix: The root prefix within the bucket where the data will be exported to.\\nS3 Region: The region of the bucket - this is needed for AWS S3 buckets.\\nEndpoint URL: The endpoint URL for the S3 bucket - this is needed for S3 API compatible buckets.\\nAccess Key: The access key for the S3 bucket.\\nSecret Key: The secret key for the S3 bucket.\\n\\nWe support any S3 compatible bucket, for non AWS buckets such as GCS or MinIO, you will need to provide the endpoint URL.\\n\\u200bPreparing the Destination\\nFor self-hosted and EU region deploymentsUpdate the LangSmith URL appropriately for self-hosted installations or organizations in the EU region in the requests below.\\nFor the EU region, use eu.api.smith.langchain.com.\\nPermissions requiredBoth the backend and queue services require write access to the destination bucket:\\nThe backend service attempts to write a test file to the destination bucket when the export destination is created.\\nIt will delete the test file if it has permission to do so (delete access is optional).\\nThe queue service is responsible for bulk export execution and uploading the files to the bucket.\\n\\nThe following example demonstrates how to create a destination using cURL. Replace the placeholder values with your actual configuration details.\\nNote that credentials will be stored securely in an encrypted form in our system.\\nCopycurl --request POST \\\\\\n  --url \\'https://api.smith.langchain.com/api/v1/bulk-exports/destinations\\' \\\\\\n  --header \\'Content-Type: application/json\\' \\\\\\n  --header \\'X-API-Key: YOUR_API_KEY\\' \\\\\\n  --header \\'X-Tenant-Id: YOUR_WORKSPACE_ID\\' \\\\\\n  --data \\'{\\n    \"destination_type\": \"s3\",\\n    \"display_name\": \"My S3 Destination\",\\n    \"config\": {\\n      \"bucket_name\": \"your-s3-bucket-name\",\\n      \"prefix\": \"root_folder_prefix\",\\n      \"region\": \"your aws s3 region\",\\n      \"endpoint_url\": \"your endpoint url for s3 compatible buckets\"\\n    },\\n    \"credentials\": {\\n      \"access_key_id\": \"YOUR_S3_ACCESS_KEY_ID\",\\n      \"secret_access_key\": \"YOUR_S3_SECRET_ACCESS_KEY\"\\n    }\\n  }\\'\\n\\nUse the returned id to reference this destination in subsequent bulk export operations.\\nIf you receive an error while creating a destination, see debug destination errors for details on how to debug this.\\n\\u200bCredentials configuration\\nRequires LangSmith Helm version >= 0.10.34 (application version >= 0.10.91)\\nWe support the following additional credentials formats besides static access_key_id and secret_access_key:\\n\\nTo use temporary credentials that include an AWS session token,\\nadditionally provide the credentials.session_token key when creating the bulk export destination.\\n(Self-hosted only): To use environment-based credentials such as with AWS IAM Roles for Service Accounts (IRSA),\\nomit the credentials key from the request when creating the bulk export destination.\\nIn this case, the standard Boto3 credentials locations will be checked in the order defined by the library.\\n\\n\\u200bAWS S3 bucket\\nFor AWS S3, you can leave off the endpoint_url and supply the region that matches the region of your bucket.\\nCopycurl --request POST \\\\\\n  --url \\'https://api.smith.langchain.com/api/v1/bulk-exports/destinations\\' \\\\\\n  --header \\'Content-Type: application/json\\' \\\\\\n  --header \\'X-API-Key: YOUR_API_KEY\\' \\\\\\n  --header \\'X-Tenant-Id: YOUR_WORKSPACE_ID\\' \\\\\\n  --data \\'{\\n    \"destination_type\": \"s3\",\\n    \"display_name\": \"My AWS S3 Destination\",\\n    \"config\": {\\n      \"bucket_name\": \"my_bucket\",\\n      \"prefix\": \"data_exports\",\\n      \"region\": \"us-east-1\"\\n    },\\n    \"credentials\": {\\n      \"access_key_id\": \"YOUR_S3_ACCESS_KEY_ID\",\\n      \"secret_access_key\": \"YOUR_S3_SECRET_ACCESS_KEY\"\\n    }\\n  }\\'\\n\\n\\u200bGoogle GCS XML S3 compatible bucket\\nWhen using Googleâ€™s GCS bucket, you need to use the XML S3 compatible API, and supply the endpoint_url\\nwhich is typically https://storage.googleapis.com.\\nHere is an example of the API request when using the GCS XML API which is compatible with S3:\\nCopycurl --request POST \\\\\\n  --url \\'https://api.smith.langchain.com/api/v1/bulk-exports/destinations\\' \\\\\\n  --header \\'Content-Type: application/json\\' \\\\\\n  --header \\'X-API-Key: YOUR_API_KEY\\' \\\\\\n  --header \\'X-Tenant-Id: YOUR_WORKSPACE_ID\\' \\\\\\n  --data \\'{\\n    \"destination_type\": \"s3\",\\n    \"display_name\": \"My GCS Destination\",\\n    \"config\": {\\n      \"bucket_name\": \"my_bucket\",\\n      \"prefix\": \"data_exports\",\\n      \"endpoint_url\": \"https://storage.googleapis.com\"\\n    },\\n    \"credentials\": {\\n      \"access_key_id\": \"YOUR_S3_ACCESS_KEY_ID\",\\n      \"secret_access_key\": \"YOUR_S3_SECRET_ACCESS_KEY\"\\n    }\\n  }\\'\\n\\nSee Google documentation for more info\\n\\u200bCreate an export job\\nTo export data, you will need to create an export job. This job will specify the destination, the project, the date range, and filter expression of the data to export. The filter expression is used to narrow down the set of runs exported and is optional. Not setting the filter field will export all runs. Refer to our filter query language and examples to determine the correct filter expression for your export.\\nYou can use the following cURL command to create the job:\\nCopycurl --request POST \\\\\\n  --url \\'https://api.smith.langchain.com/api/v1/bulk-exports\\' \\\\\\n  --header \\'Content-Type: application/json\\' \\\\\\n  --header \\'X-API-Key: YOUR_API_KEY\\' \\\\\\n  --header \\'X-Tenant-Id: YOUR_WORKSPACE_ID\\' \\\\\\n  --data \\'{\\n    \"bulk_export_destination_id\": \"your_destination_id\",\\n    \"session_id\": \"project_uuid\",\\n    \"start_time\": \"2024-01-01T00:00:00Z\",\\n    \"end_time\": \"2024-01-02T23:59:59Z\",\\n    \"filter\": \"and(eq(run_type, \\\\\"llm\\\\\"), eq(name, \\\\\"ChatOpenAI\\\\\"), eq(input_key, \\\\\"messages.content\\\\\"), like(input_value, \\\\\"%messages.content%\\\\\"))\"\\n  }\\'\\n\\nThe session_id is also known as the Tracing Project ID, which can be copied from the individual project view by clicking into the project in the Tracing Projects list.\\nUse the returned id to reference this export in subsequent bulk export operations.\\n\\u200bScheduled exports\\nRequires LangSmith Helm version >= 0.10.42 (application version >= 0.10.109)\\nScheduled exports collect runs periodically and export to the configured destination.\\nTo create a scheduled export, include interval_hours and remove end_time:\\nCopycurl --request POST \\\\\\n  --url \\'https://api.smith.langchain.com/api/v1/bulk-exports\\' \\\\\\n  --header \\'Content-Type: application/json\\' \\\\\\n  --header \\'X-API-Key: YOUR_API_KEY\\' \\\\\\n  --header \\'X-Tenant-Id: YOUR_WORKSPACE_ID\\' \\\\\\n  --data \\'{\\n    \"bulk_export_destination_id\": \"your_destination_id\",\\n    \"session_id\": \"project_uuid\",\\n    \"start_time\": \"2024-01-01T00:00:00Z\",\\n    \"filter\": \"and(eq(run_type, \\\\\"llm\\\\\"), eq(name, \\\\\"ChatOpenAI\\\\\"), eq(input_key, \\\\\"messages.content\\\\\"), like(input_value, \\\\\"%messages.content%\\\\\"))\",\\n    \"interval_hours\": 1\\n  }\\'\\n\\nDetails\\n\\ninterval_hours must be between 1 hour and 168 hours (1 week) inclusive.\\nFor spawned exports, the first time range exported is start_time=(scheduled_export_start_time), end_time=(start_time + interval_hours).\\nThen start_time=(previous_export_end_time), end_time=(this_export_start_time + interval_hours), and so on.\\nend_time must be omitted for scheduled exports. end_time is still required for non-scheduled exports.\\nScheduled exports can be stopped by cancelling the export.\\n\\nExports that have been spawned by a scheduled export have the source_bulk_export_id attribute filled.\\nIf desired, these spawned bulk exports must be canceled separately from the source scheduled bulk export -\\ncanceling the source bulk export does not cancel the spawned bulk exports.\\n\\n\\nSpawned exports run at end_time + 10 minutes to account for any runs that are submitted with end_time in the recent past.\\n\\nExample\\nIf a scheduled bulk export is created with start_time=2025-07-16T00:00:00Z and interval_hours=6:\\nExportStart TimeEnd TimeRuns At12025-07-16T00:00:00Z2025-07-16T06:00:00Z2025-07-16T06:10:00Z22025-07-16T06:00:00Z2025-07-16T12:00:00Z2025-07-16T12:10:00Z32025-07-16T12:00:00Z2025-07-16T18:00:00Z2025-07-16T18:10:00Z\\n\\u200bMonitoring the Export Job\\n\\u200bMonitor Export Status\\nTo monitor the status of an export job, use the following cURL command:\\nCopycurl --request GET \\\\\\n  --url \\'https://api.smith.langchain.com/api/v1/bulk-exports/{export_id}\\' \\\\\\n  --header \\'Content-Type: application/json\\' \\\\\\n  --header \\'X-API-Key: YOUR_API_KEY\\' \\\\\\n  --header \\'X-Tenant-Id: YOUR_WORKSPACE_ID\\'\\n\\nReplace {export_id} with the ID of the export you want to monitor. This command retrieves the current status of the specified export job.\\n\\u200bList Runs for an Export\\nAn export is typically broken up into multiple runs which correspond to a specific date partition to export.\\nTo list all runs associated with a specific export, use the following cURL command:\\nCopycurl --request GET \\\\\\n  --url \\'https://api.smith.langchain.com/api/v1/bulk-exports/{export_id}/runs\\' \\\\\\n  --header \\'Content-Type: application/json\\' \\\\\\n  --header \\'X-API-Key: YOUR_API_KEY\\' \\\\\\n  --header \\'X-Tenant-Id: YOUR_WORKSPACE_ID\\'\\n\\nThis command fetches all runs related to the specified export, providing details such as run ID, status, creation time, rows exported, etc.\\n\\u200bList All Exports\\nTo retrieve a list of all export jobs, use the following cURL command:\\nCopycurl --request GET \\\\\\n  --url \\'https://api.smith.langchain.com/api/v1/bulk-exports\\' \\\\\\n  --header \\'Content-Type: application/json\\' \\\\\\n  --header \\'X-API-Key: YOUR_API_KEY\\' \\\\\\n  --header \\'X-Tenant-Id: YOUR_WORKSPACE_ID\\'\\n\\nThis command returns a list of all export jobs along with their current statuses and creation timestamps.\\n\\u200bStop an Export\\nTo stop an existing export, use the following cURL command:\\nCopycurl --request PATCH \\\\\\n  --url \\'https://api.smith.langchain.com/api/v1/bulk-exports/{export_id}\\' \\\\\\n  --header \\'Content-Type: application/json\\' \\\\\\n  --header \\'X-API-Key: YOUR_API_KEY\\' \\\\\\n  --header \\'X-Tenant-Id: YOUR_WORKSPACE_ID\\' \\\\\\n  --data \\'{\\n    \"status\": \"Cancelled\"\\n}\\'\\n\\nReplace {export_id} with the ID of the export you wish to cancel. Note that a job cannot be restarted once it has been cancelled,\\nyou will need to create a new export job instead.\\n\\u200bPartitioning Scheme\\nData will be exported into your bucket into the follow Hive partitioned format:\\nCopy<bucket>/<prefix>/export_id=<export_id>/tenant_id=<tenant_id>/session_id=<session_id>/runs/year=<year>/month=<month>/day=<day>\\n\\n\\u200bImporting Data into other systems\\nImporting data from S3 and Parquet format is commonly supported by the majority of analytical systems. See below for documentation links:\\n\\u200bBigQuery\\nTo import your data into BigQuery, see Loading Data from Parquet and also\\nHive Partitioned loads.\\n\\u200bSnowflake\\nYou can load data into Snowflake from S3 by following the Load from Cloud Document.\\n\\u200bRedShift\\nYou can COPY data from S3 / Parquet into RedShift by following the AWS COPY Instructions.\\n\\u200bClickhouse\\nYou can directly query data in S3 / Parquet format in Clickhouse. As an example, if using GCS, you can query the data as follows:\\nCopySELECT count(distinct id) FROM s3(\\'https://storage.googleapis.com/<bucket>/<prefix>/export_id=<export_id>/**\\',\\n \\'access_key_id\\', \\'access_secret\\', \\'Parquet\\')\\n\\nSee Clickhouse S3 Integration Documentation for more information.\\n\\u200bDuckDB\\nYou can query the data from S3 in-memory with SQL using DuckDB. See S3 import Documentation.\\n\\u200bError Handling\\n\\u200bDebugging Destination Errors\\nThe destinations API endpoint will validate that the destination and credentials are valid and that write access is\\nis present for the bucket.\\nIf you receive an error, and would like to debug this error, you can use the AWS CLI\\nto test the connectivity to the bucket. You should be able to write a file with the CLI using the same\\ndata that you supplied to the destinations API above.\\nAWS S3:\\nCopyaws configure\\n\\n# set the same access key credentials and region as you used for the destination\\n> AWS Access Key ID: <access_key_id>\\n> AWS Secret Access Key: <secret_access_key>\\n> Default region name [us-east-1]: <region>\\n\\n# List buckets\\naws s3 ls /\\n\\n# test write permissions\\ntouch ./test.txt\\naws s3 cp ./test.txt s3://<bucket-name>/tmp/test.txt\\n\\nGCS Compatible Buckets:\\nYou will need to supply the endpoint_url with --endpoint-url option.\\nFor GCS, the endpoint_url is typically https://storage.googleapis.com:\\nCopyaws configure\\n\\n# set the same access key credentials and region as you used for the destination\\n> AWS Access Key ID: <access_key_id>\\n> AWS Secret Access Key: <secret_access_key>\\n> Default region name [us-east-1]: <region>\\n\\n# List buckets\\naws s3 --endpoint-url=<endpoint_url> ls /\\n\\n# test write permissions\\ntouch ./test.txt\\naws s3 --endpoint-url=<endpoint_url> cp ./test.txt s3://<bucket-name>/tmp/test.txt\\n\\n\\u200bMonitoring Runs\\nYou can monitor your runs using the List Runs API. If this is a known error, this will be added to the errors field of the run.\\n\\u200bCommon Errors\\nHere are some common errors:\\nErrorDescriptionAccess deniedThe blob store credentials or bucket are not valid. This error occurs when the provided access key and secret key combination doesnâ€™t have the necessary permissions to access the specified bucket or perform the required operations.Bucket is not validThe specified blob store bucket is not valid. This error is thrown when the bucket doesnâ€™t exist or there is not enough access to perform writes on the bucket.Key ID you provided does not existThe blob store credentials provided are not valid. This error occurs when the access key ID used for authentication is not a valid key.Invalid endpointThe endpoint_url provided is invalid. This error is raised when the specified endpoint is an invalid endpoint. Only S3 compatible endpoints are supported, for example https://storage.googleapis.com for GCS, https://play.min.io for minio, etc. If using AWS, you should omit the endpoint_url.Was this page helpful?YesNoSuggest editsView server logs for a traceSet up automation rulesâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Implement distributed tracing - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsManual instrumentationConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesImplement distributed tracingThreadsTrace JS functions in serverless environmentsLog multimodal tracesTrace generator functionsData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationAdvanced tracing techniquesImplement distributed tracingGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageDistributed tracing in PythonDistributed tracing in TypeScriptConfiguration & troubleshootingAdvanced tracing techniquesImplement distributed tracingCopy pageCopy pageSometimes, you need to trace a request across multiple services.\\nLangSmith supports distributed tracing out of the box, linking runs within a trace across services using context propagation headers (langsmith-trace and optional baggage for metadata/tags).\\nExample client-server setup:\\n\\nTrace starts on client\\nContinues on server\\n\\n\\u200bDistributed tracing in Python\\nCopy# client.py\\nfrom langsmith.run_helpers import get_current_run_tree, traceable\\nimport httpx\\n\\n@traceable\\nasync def my_client_function():\\n    headers = {}\\n    async with httpx.AsyncClient(base_url=\"...\") as client:\\n        if run_tree := get_current_run_tree():\\n            # add langsmith-id to headers\\n            headers.update(run_tree.to_headers())\\n        return await client.post(\"/my-route\", headers=headers)\\n\\nThen the server (or other service) can continue the trace by handling the headers appropriately. If you are using an asgi app Starlette or FastAPI, you can connect the distributed trace using LangSmithâ€™s TracingMiddleware.\\nThe TracingMiddleware class was added in langsmith==0.1.133.\\nExample using FastAPI:\\nCopyfrom langsmith import traceable\\nfrom langsmith.middleware import TracingMiddleware\\nfrom fastapi import FastAPI, Request\\n\\napp = FastAPI()  # Or Flask, Django, or any other framework\\napp.add_middleware(TracingMiddleware)\\n\\n@traceable\\nasync def some_function():\\n    ...\\n\\n@app.post(\"/my-route\")\\nasync def fake_route(request: Request):\\n    return await some_function()\\n\\nOr in Starlette:\\nCopyfrom starlette.applications import Starlette\\nfrom starlette.middleware import Middleware\\nfrom langsmith.middleware import TracingMiddleware\\n\\nroutes = ...\\nmiddleware = [\\n    Middleware(TracingMiddleware),\\n]\\napp = Starlette(..., middleware=middleware)\\n\\nIf you are using other server frameworks, you can always â€œreceiveâ€ the distributed trace by passing the headers in through langsmith_extra:\\nCopy# server.py\\nimport langsmith as ls\\nfrom fastapi import FastAPI, Request\\n\\n@ls.traceable\\nasync def my_application():\\n    ...\\n\\napp = FastAPI()  # Or Flask, Django, or any other framework\\n\\n@app.post(\"/my-route\")\\nasync def fake_route(request: Request):\\n    # request.headers:  {\"langsmith-trace\": \"...\"}\\n    # as well as optional metadata/tags in `baggage`\\n    with ls.tracing_context(parent=request.headers):\\n        return await my_application()\\n\\nThe example above uses the tracing_context context manager. You can also directly specify the parent run context in the langsmith_extra parameter of a method wrapped with @traceable.\\nCopy# ... same as above\\n\\n@app.post(\"/my-route\")\\nasync def fake_route(request: Request):\\n    # request.headers:  {\"langsmith-trace\": \"...\"}\\n    my_application(langsmith_extra={\"parent\": request.headers})\\n\\n\\u200bDistributed tracing in TypeScript\\nDistributed tracing in TypeScript requires langsmith version >=0.1.31\\nFirst, we obtain the current run tree from the client and convert it to langsmith-trace and baggage header values, which we can pass to the server:\\nCopy// client.mts\\nimport { getCurrentRunTree, traceable } from \"langsmith/traceable\";\\n\\nconst client = traceable(\\n    async () => {\\n        const runTree = getCurrentRunTree();\\n        return await fetch(\"...\", {\\n            method: \"POST\",\\n            headers: runTree.toHeaders(),\\n        }).then((a) => a.text());\\n    },\\n    { name: \"client\" }\\n);\\n\\nawait client();\\n\\nThen, the server converts the headers back to a run tree, which it uses to further continue the tracing.\\nTo pass the newly created run tree to a traceable function, we can use the withRunTree helper, which will ensure the run tree is propagated within traceable invocations.\\nExpress.JSHonoCopy// server.mts\\nimport { RunTree } from \"langsmith\";\\nimport { traceable, withRunTree } from \"langsmith/traceable\";\\nimport express from \"express\";\\nimport bodyParser from \"body-parser\";\\n\\n    const server = traceable(\\n        (text: string) => `Hello from the server! Received \"${text}\"`,\\n        { name: \"server\" }\\n    );\\n\\n    const app = express();\\n    app.use(bodyParser.text());\\n\\napp.post(\"/\", async (req, res) => {\\n    const runTree = RunTree.fromHeaders(req.headers);\\n    const result = await withRunTree(runTree, () => server(req.body));\\n    res.send(result);\\n});\\nWas this page helpful?YesNoSuggest editsSet a sampling rate for tracesThreadsâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/export_traces', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/export_traces', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Query traces (SDK) - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsManual instrumentationConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationViewing & managing tracesQuery traces (SDK)Get startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageUse filter argumentsList all runs in a projectList LLM and Chat runs in the last 24 hoursList root runs in a projectList runs without errorsList runs by run IDUse filter query languageList all root runs in a conversational threadList all runs called â€œextractorâ€ whose root of the trace was assigned feedback â€œuser_scoreâ€ score of 1List runs with â€œstar_ratingâ€ key whose score is greater than 4List runs that took longer than 5 seconds to completeList all runs that have â€œerrorâ€ not equal to nullList all runs where start_time is greater than a specific timestampList all runs that contain the string â€œsubstringâ€List all runs that are tagged with the git hash â€œ2aa1cf4â€List all runs that started after a specific timestamp and either have â€œerrorâ€ not equal to null or a â€œCorrectnessâ€ feedback score equal to 0Complex query: List all runs where tags include â€œexperimentalâ€ or â€œbetaâ€ and latency is greater than 2 secondsSearch trace trees by full textCheck for presence of metadataCheck for environment details in metadataCheck for conversation ID in metadataNegative filtering on key-value pairsCombine multiple filtersTree FilterAdvanced: export flattened trace view with child tool usageAdvanced: export retriever IO for traces with feedbackViewing & managing tracesQuery traces (SDK)Copy pageCopy pageRecommended ReadingBefore diving into this content, it might be helpful to read the following:\\nRun (span) data format\\n\\n\\n\\nLangSmith trace query syntax\\n\\nIf you are looking to export a large volume of traces, we recommend that you use the Bulk Data Export functionality, as it will better handle large data volumes and will support automatic retries and parallelization across partitions.\\nThe recommended way to query runs (the span data in LangSmith traces) is to use the list_runs method in the SDK or /runs/query endpoint in the API.\\nLangSmith stores traces in a simple format that is specified in the Run (span) data format.\\n\\u200bUse filter arguments\\nFor simple queries, you donâ€™t have to rely on our query syntax. You can use the filter arguments specified in the filter arguments reference.\\nPrerequisitesInitialize the client before running the below code snippets.\\nPythonTypeScriptCopyfrom langsmith import Client\\n\\nclient = Client()\\n\\nBelow are some examples of ways to list runs using keyword arguments:\\n\\u200bList all runs in a project\\nPythonTypeScriptCopyproject_runs = client.list_runs(project_name=\"<your_project>\")\\n\\n\\u200bList LLM and Chat runs in the last 24 hours\\nPythonTypeScriptCopytodays_llm_runs = client.list_runs(\\n    project_name=\"<your_project>\",\\n    start_time=datetime.now() - timedelta(days=1),\\n    run_type=\"llm\",\\n)\\n\\n\\u200bList root runs in a project\\nRoot runs are runs that have no parents. These are assigned a value of True for is_root. You can use this to filter for root runs.\\nPythonTypeScriptCopyroot_runs = client.list_runs(\\n    project_name=\"<your_project>\",\\n    is_root=True\\n)\\n\\n\\u200bList runs without errors\\nPythonTypeScriptCopycorrect_runs = client.list_runs(project_name=\"<your_project>\", error=False)\\n\\n\\u200bList runs by run ID\\nIgnores Other ArgumentsIf you provide a list of run IDs in the way described above, it will ignore all other filtering arguments like project_name, run_type, etc. and directly return the runs matching the given IDs.\\nIf you have a list of run IDs, you can list them directly:\\nPythonTypeScriptCopyrun_ids = [\\'a36092d2-4ad5-4fb4-9c0d-0dba9a2ed836\\',\\'9398e6be-964f-4aa4-8ae9-ad78cd4b7074\\']\\nselected_runs = client.list_runs(id=run_ids)\\n\\n\\u200bUse filter query language\\nFor more complex queries, you can use the query language described in the filter query language reference.\\n\\u200bList all root runs in a conversational thread\\nThis is the way to fetch runs in a conversational thread. For more information on setting up threads, refer to our how-to guide on setting up threads.\\nThreads are grouped by setting a shared thread ID. The LangSmith UI lets you use any one of the following three metadata keys: session_id, conversation_id, or thread_id. The session ID is also known as the tracing project ID. The following query matches on any of them.\\nPythonTypeScriptCopygroup_key = \"<your_thread_id>\"\\nfilter_string = f\\'and(in(metadata_key, [\"session_id\",\"conversation_id\",\"thread_id\"]), eq(metadata_value, \"{group_key}\"))\\'\\nthread_runs = client.list_runs(\\n    project_name=\"<your_project>\",\\n    filter=filter_string,\\n    is_root=True\\n)\\n\\n\\u200bList all runs called â€œextractorâ€ whose root of the trace was assigned feedback â€œuser_scoreâ€ score of 1\\nPythonTypeScriptCopyclient.list_runs(\\n    project_name=\"<your_project>\",\\n    filter=\\'eq(name, \"extractor\")\\',\\n    trace_filter=\\'and(eq(feedback_key, \"user_score\"), eq(feedback_score, 1))\\'\\n)\\n\\n\\u200bList runs with â€œstar_ratingâ€ key whose score is greater than 4\\nPythonTypeScriptCopyclient.list_runs(\\n    project_name=\"<your_project>\",\\n    filter=\\'and(eq(feedback_key, \"star_rating\"), gt(feedback_score, 4))\\'\\n)\\n\\n\\u200bList runs that took longer than 5 seconds to complete\\nPythonTypeScriptCopyclient.list_runs(project_name=\"<your_project>\", filter=\\'gt(latency, \"5s\")\\')\\n\\n\\u200bList all runs that have â€œerrorâ€ not equal to null\\nPythonTypeScriptCopyclient.list_runs(project_name=\"<your_project>\", filter=\\'neq(error, null)\\')\\n\\n\\u200bList all runs where start_time is greater than a specific timestamp\\nPythonTypeScriptCopyclient.list_runs(project_name=\"<your_project>\", filter=\\'gt(start_time, \"2023-07-15T12:34:56Z\")\\')\\n\\n\\u200bList all runs that contain the string â€œsubstringâ€\\nPythonTypeScriptCopyclient.list_runs(project_name=\"<your_project>\", filter=\\'search(\"substring\")\\')\\n\\n\\u200bList all runs that are tagged with the git hash â€œ2aa1cf4â€\\nPythonTypeScriptCopyclient.list_runs(project_name=\"<your_project>\", filter=\\'has(tags, \"2aa1cf4\")\\')\\n\\n\\u200bList all runs that started after a specific timestamp and either have â€œerrorâ€ not equal to null or a â€œCorrectnessâ€ feedback score equal to 0\\nPythonTypeScriptCopyclient.list_runs(\\n  project_name=\"<your_project>\",\\n  filter=\\'and(gt(start_time, \"2023-07-15T12:34:56Z\"), or(neq(error, null), and(eq(feedback_key, \"Correctness\"), eq(feedback_score, 0.0))))\\'\\n)\\n\\n\\u200bComplex query: List all runs where tags include â€œexperimentalâ€ or â€œbetaâ€ and latency is greater than 2 seconds\\nPythonTypeScriptCopyclient.list_runs(\\n  project_name=\"<your_project>\",\\n  filter=\\'and(or(has(tags, \"experimental\"), has(tags, \"beta\")), gt(latency, 2))\\'\\n)\\n\\n\\u200bSearch trace trees by full text\\nYou can use the search() function without any specific field to do a full text search across all string fields in a run. This allows you to quickly find traces that match a search term.\\nPythonTypeScriptCopyclient.list_runs(\\n  project_name=\"<your_project>\",\\n  filter=\\'search(\"image classification\")\\'\\n)\\n\\n\\u200bCheck for presence of metadata\\nIf you want to check for the presence of metadata, you can use the eq operator, optionally with an and statement to match by value. This is useful if you want to log more structured information about your runs.\\nPythonTypeScriptCopyto_search = {\\n    \"user_id\": \"\"\\n}\\n\\n# Check for any run with the \"user_id\" metadata key\\nclient.list_runs(\\n  project_name=\"default\",\\n  filter=\"eq(metadata_key, \\'user_id\\')\"\\n)\\n# Check for runs with user_id=4070f233-f61e-44eb-bff1-da3c163895a3\\nclient.list_runs(\\n  project_name=\"default\",\\n  filter=\"and(eq(metadata_key, \\'user_id\\'), eq(metadata_value, \\'4070f233-f61e-44eb-bff1-da3c163895a3\\'))\"\\n)\\n\\n\\u200bCheck for environment details in metadata\\nA common pattern is to add environment information to your traces via metadata. If you want to filter for runs containing environment metadata, you can use the same pattern as above:\\nPythonTypeScriptCopyclient.list_runs(\\n  project_name=\"default\",\\n  filter=\"and(eq(metadata_key, \\'environment\\'), eq(metadata_value, \\'production\\'))\"\\n)\\n\\n\\u200bCheck for conversation ID in metadata\\nAnother common way to associate traces in the same conversation is by using a shared conversation ID. If you want to filter runs based on a conversation ID in this way, you can search for that ID in the metadata.\\nPythonTypeScriptCopyclient.list_runs(\\n  project_name=\"default\",\\n  filter=\"and(eq(metadata_key, \\'conversation_id\\'), eq(metadata_value, \\'a1b2c3d4-e5f6-7890\\'))\"\\n)\\n\\n\\u200bNegative filtering on key-value pairs\\nYou can use negative filtering on metadata, input, and output key-value pairs to exclude specific runs from your results. Here are some examples for metadata key-value pairs but the same logic applies to input and output key-value pairs.\\nPythonTypeScriptCopy# Find all runs where the metadata does not contain a \"conversation_id\" key\\nclient.list_runs(\\n  project_name=\"default\",\\n  filter=\"and(neq(metadata_key, \\'conversation_id\\'))\"\\n)\\n\\n# Find all runs where the conversation_id in metadata is not \"a1b2c3d4-e5f6-7890\"\\nclient.list_runs(\\n  project_name=\"default\",\\n  filter=\"and(eq(metadata_key, \\'conversation_id\\'), neq(metadata_value, \\'a1b2c3d4-e5f6-7890\\'))\"\\n)\\n\\n# Find all runs where there is no \"conversation_id\" metadata key and the \"a1b2c3d4-e5f6-7890\" value is not present\\nclient.list_runs(\\n  project_name=\"default\",\\n  filter=\"and(neq(metadata_key, \\'conversation_id\\'), neq(metadata_value, \\'a1b2c3d4-e5f6-7890\\'))\"\\n)\\n\\n# Find all runs where the conversation_id metadata key is not present but the \"a1b2c3d4-e5f6-7890\" value is present\\nclient.list_runs(\\n  project_name=\"default\",\\n  filter=\"and(neq(metadata_key, \\'conversation_id\\'), eq(metadata_value, \\'a1b2c3d4-e5f6-7890\\'))\"\\n)\\n\\n\\u200bCombine multiple filters\\nIf you want to combine multiple conditions to refine your search, you can use the and operator along with other filtering functions. Hereâ€™s how you can search for runs named â€œChatOpenAIâ€ that also have a specific conversation_id in their metadata:\\nPythonTypeScriptCopyclient.list_runs(\\n  project_name=\"default\",\\n  filter=\"and(eq(name, \\'ChatOpenAI\\'), eq(metadata_key, \\'conversation_id\\'), eq(metadata_value, \\'69b12c91-b1e2-46ce-91de-794c077e8151\\'))\"\\n)\\n\\n\\u200bTree Filter\\nList all runs named â€œRetrieveDocsâ€ whose root run has a â€œuser_scoreâ€ feedback of 1 and any run in the full trace is named â€œExpandQueryâ€.\\nThis type of query is useful if you want to extract a specific run conditional on various states or steps being reached within the trace.\\nPythonTypeScriptCopyclient.list_runs(\\n    project_name=\"<your_project>\",\\n    filter=\\'eq(name, \"RetrieveDocs\")\\',\\n    trace_filter=\\'and(eq(feedback_key, \"user_score\"), eq(feedback_score, 1))\\',\\n    tree_filter=\\'eq(name, \"ExpandQuery\")\\'\\n)\\n\\n\\u200bAdvanced: export flattened trace view with child tool usage\\nThe following Python example demonstrates how to export a flattened view of traces, including information on the tools (from nested runs) used by the agent within each trace.\\nThis can be used to analyze the behavior of your agents across multiple traces.\\nThis example queries all tool runs within a specified number of days and groups them by their parent (root) run ID. It then fetches the relevant information for each root run, such as the run name, inputs, outputs, and combines that information with the child run information.\\nTo optimize the query, the example:\\n\\nSelects only the necessary fields when querying tool runs to reduce query time.\\nFetches root runs in batches while processing tool runs concurrently.\\n\\nPythonCopyfrom collections import defaultdict\\nfrom concurrent.futures import Future, ThreadPoolExecutor\\nfrom datetime import datetime, timedelta\\n\\nfrom langsmith import Client\\nfrom tqdm.auto import tqdm\\n\\nclient = Client()\\nproject_name = \"my-project\"\\nnum_days = 30\\n\\n# List all tool runs\\ntool_runs = client.list_runs(\\n    project_name=project_name,\\n    start_time=datetime.now() - timedelta(days=num_days),\\n    run_type=\"tool\",\\n    # We don\\'t need to fetch inputs, outputs, and other values that # may increase the query time\\n    select=[\"trace_id\", \"name\", \"run_type\"],\\n)\\n\\ndata = []\\nfutures: list[Future] = []\\ntrace_cursor = 0\\ntrace_batch_size = 50\\n\\ntool_runs_by_parent = defaultdict(lambda: defaultdict(set))\\n# Do not exceed rate limit\\nwith ThreadPoolExecutor(max_workers=2) as executor:\\n    # Group tool runs by parent run ID\\n    for run in tqdm(tool_runs):\\n        # Collect all tools invoked within a given trace\\n        tool_runs_by_parent[run.trace_id][\"tools_involved\"].add(run.name)\\n        # maybe send a batch of parent run IDs to the server\\n        # this lets us query for the root runs in batches\\n        # while still processing the tool runs\\n        if len(tool_runs_by_parent) % trace_batch_size == 0:\\n            if this_batch := list(tool_runs_by_parent.keys())[\\n                trace_cursor : trace_cursor + trace_batch_size\\n            ]:\\n                trace_cursor += trace_batch_size\\n                futures.append(\\n                    executor.submit(\\n                        client.list_runs,\\n                        project_name=project_name,\\n                        run_ids=this_batch,\\n                        select=[\"name\", \"inputs\", \"outputs\", \"run_type\"],\\n                    )\\n                )\\n    if this_batch := list(tool_runs_by_parent.keys())[trace_cursor:]:\\n        futures.append(\\n            executor.submit(\\n                client.list_runs,\\n                project_name=project_name,\\n                run_ids=this_batch,\\n                select=[\"name\", \"inputs\", \"outputs\", \"run_type\"],\\n            )\\n        )\\n\\nfor future in tqdm(futures):\\n    root_runs = future.result()\\n    for root_run in root_runs:\\n        root_data = tool_runs_by_parent[root_run.id]\\n        data.append(\\n            {\\n                \"run_id\": root_run.id,\\n                \"run_name\": root_run.name,\\n                \"run_type\": root_run.run_type,\\n                \"inputs\": root_run.inputs,\\n                \"outputs\": root_run.outputs,\\n                \"tools_involved\": list(root_data[\"tools_involved\"]),\\n            }\\n        )\\n\\n# (Optional): Convert to a pandas DataFrame\\nimport pandas as pd\\n\\ndf = pd.DataFrame(data)\\ndf.head()\\n\\n\\u200bAdvanced: export retriever IO for traces with feedback\\nThis query is useful if you want to fine-tune embeddings or diagnose end-to-end system performance issues based on retriever behavior.\\nThe following Python example demonstrates how to export retriever inputs and outputs within traces that have a specific feedback score.\\nPythonCopyfrom collections import defaultdict\\nfrom concurrent.futures import Future, ThreadPoolExecutor\\nfrom datetime import datetime, timedelta\\n\\nimport pandas as pd\\nfrom langsmith import Client\\nfrom tqdm.auto import tqdm\\n\\nclient = Client()\\nproject_name = \"your-project-name\"\\nnum_days = 1\\n\\n# List all tool runs\\nretriever_runs = client.list_runs(\\n    project_name=project_name,\\n    start_time=datetime.now() - timedelta(days=num_days),\\n    run_type=\"retriever\",\\n    # This time we do want to fetch the inputs and outputs, since they\\n    # may be adjusted by query expansion steps.\\n    select=[\"trace_id\", \"name\", \"run_type\", \"inputs\", \"outputs\"],\\n    trace_filter=\\'eq(feedback_key, \"user_score\")\\',\\n)\\n\\ndata = []\\nfutures: list[Future] = []\\ntrace_cursor = 0\\ntrace_batch_size = 50\\n\\nretriever_runs_by_parent = defaultdict(lambda: defaultdict(list))\\n# Do not exceed rate limit\\nwith ThreadPoolExecutor(max_workers=2) as executor:\\n    # Group retriever runs by parent run ID\\n    for run in tqdm(retriever_runs):\\n        # Collect all retriever calls invoked within a given trace\\n        for k, v in run.inputs.items():\\n            retriever_runs_by_parent[run.trace_id][f\"retriever.inputs.{k}\"].append(v)\\n        for k, v in (run.outputs or {}).items():\\n            # Extend the docs\\n            retriever_runs_by_parent[run.trace_id][f\"retriever.outputs.{k}\"].extend(v)\\n        # maybe send a batch of parent run IDs to the server\\n        # this lets us query for the root runs in batches\\n        # while still processing the retriever runs\\n        if len(retriever_runs_by_parent) % trace_batch_size == 0:\\n            if this_batch := list(retriever_runs_by_parent.keys())[\\n                trace_cursor : trace_cursor + trace_batch_size\\n            ]:\\n                trace_cursor += trace_batch_size\\n                futures.append(\\n                    executor.submit(\\n                        client.list_runs,\\n                        project_name=project_name,\\n                        run_ids=this_batch,\\n                        select=[\\n                            \"name\",\\n                            \"inputs\",\\n                            \"outputs\",\\n                            \"run_type\",\\n                            \"feedback_stats\",\\n                        ],\\n                    )\\n                )\\n    if this_batch := list(retriever_runs_by_parent.keys())[trace_cursor:]:\\n        futures.append(\\n            executor.submit(\\n                client.list_runs,\\n                project_name=project_name,\\n                run_ids=this_batch,\\n                select=[\"name\", \"inputs\", \"outputs\", \"run_type\"],\\n            )\\n        )\\n\\nfor future in tqdm(futures):\\n    root_runs = future.result()\\n    for root_run in root_runs:\\n        root_data = retriever_runs_by_parent[root_run.id]\\n        feedback = {\\n            f\"feedback.{k}\": v.get(\"avg\")\\n            for k, v in (root_run.feedback_stats or {}).items()\\n        }\\n        inputs = {f\"inputs.{k}\": v for k, v in root_run.inputs.items()}\\n        outputs = {f\"outputs.{k}\": v for k, v in (root_run.outputs or {}).items()}\\n        data.append(\\n            {\\n                \"run_id\": root_run.id,\\n                \"run_name\": root_run.name,\\n                **inputs,\\n                **outputs,\\n                **feedback,\\n                **root_data,\\n            }\\n        )\\n\\n# (Optional): Convert to a pandas DataFrame\\nimport pandas as pd\\ndf = pd.DataFrame(data)\\ndf.head()\\nWas this page helpful?YesNoSuggest editsFilter tracesCompare tracesâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Filter traces - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsManual instrumentationConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationViewing & managing tracesFilter tracesGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageCreating and Applying FiltersFiltering by run attributesFiltering by time rangeFilter operatorsSpecific Filtering TechniquesFilter for intermediate runs (spans)Filter based on inputs and outputsFilter based on input / output key-value pairsExample: Filtering for tool callsNegative filtering on key-value pairsSave a filterSave a filterUse a saved filterUpdate a saved filterDelete a saved filterCopy a filterFiltering runs within the trace viewManually specify a raw query in LangSmith query languageUse an AI Query to auto-generate a query (Experimental)Advanced filtersFilter for intermediate runs (spans) on properties of the rootFilter for runs (spans) whose child runs have some attributeExample: Filtering on all runs whose tree contains the tool call filterViewing & managing tracesFilter tracesCopy pageCopy pageRecommended reading: It might be helpful to read the Conceptual guide on tracing to gain familiarity with the concepts mentioned on this page.\\nTracing projects can contain a significant amount of data. Filters are used for effectively navigating and analyzing this data, allowing you to:\\n\\nHave focused investigations: Quickly narrow down to specific runs for ad-hoc analysis\\nDebug and analyze: Identify and examine errors, failed runs, and performance bottlenecks\\n\\nThis page contains a series of guides for how to filter runs in a tracing project. If you are programmatically exporting runs for analysis via the API or SDK, please refer to the exporting traces guide for more information.\\n\\u200bCreating and Applying Filters\\n\\u200bFiltering by run attributes\\nThere are two ways to filter runs in a tracing project:\\n\\nFilters: Located towards the top-left of the tracing projects page. This is where you construct and manage detailed filter criteria.\\n\\n\\n\\nFilter Shortcuts: Positioned on the right sidebar of the tracing projects page. The filter shortcuts bar provides quick access to filters based on the most frequently occurring attributes in your projectâ€™s runs.\\n\\n\\nDefault filterBy default, the IsTrace is true filter is applied. This displays only top-level traces. Removing this filter will show all runs, including intermediate spans, in the project.\\n\\u200bFiltering by time range\\nIn addition to filtering by run attributes, you can also filter runs within a specific time range. This option is available towards the top-left of the tracing projects page.\\n\\n\\u200bFilter operators\\nThe available filter operators depend on the data type of the attribute you are filtering on. Hereâ€™s an overview of common operators:\\n\\nis: Exact match on the filter value\\nis not: Negative match on the filter value\\ncontains: Partial match on the filter value\\ndoes not contain: Negative partial match on the filter value\\nis one of: Match on any of the values in the list\\n> / <: Available for numeric fields\\n\\n\\u200bSpecific Filtering Techniques\\n\\u200bFilter for intermediate runs (spans)\\nIn order to filter for intermediate runs (spans), you first need to remove the default IsTrace is true filter. For example, you would do this if you wanted to filter by run name for sub runs or filter by run type.\\nRun metadata and tags are also powerful to filter on. These rely on good tagging across all parts of your pipeline. To learn more, you can check out this guide.\\n\\u200bFilter based on inputs and outputs\\nYou can filter runs based on the content in the inputs and outputs of the run.\\nTo filter either inputs or outputs, you can use the Full-Text Search filter which will match keywords in either field. For more targeted search, you can use the Input or Output filters which will only match content based on the respective field.\\nFor performance, we index up to 250 characters of data for full-text search. If your search query exceeds this limit, we recommend using Input/Output key-value search instead.\\nYou can also specify multiple matches, either by including multiple terms separated by whitespace, or adding multiple filters - which will try to match all terms provided.\\nNote that keyword search is done by splitting the text and finding any partial matches on the search keywords, so it is not done in specific order. We exclude common stop words from the search (from the nltk stop word list along with a few other common JSON keywords).\\n\\nBased on the filters above, the system will search for python and tensorflow in either inputs or outputs, and embedding in the inputs along with fine and tune in the outputs.\\n\\u200bFilter based on input / output key-value pairs\\nIn addition to full-text search, you can filter runs based on specific key-value pairs in the inputs and outputs. This allows for more precise filtering, especially when dealing with structured data.\\nWe index up to 100 unique keys to keep your data organized and searchable. Each key also has a character limit of 250 characters per value. If your data exceeds either of these limits, the text wonâ€™t be indexed. This helps us ensure fast, reliable performance.\\nTo filter based on key-value pairs, select the Input Key or Output Key filter from the filters dropdown.\\nFor example, to match the following input:\\nCopy{\\n  \"input\": \"What is the capital of France?\"\\n}\\n\\nSelect Filters, Add Filter to bring up the filtering options. Then select Input Key, enter input as the key and enter What is the capital of France? as the value.\\n\\nYou can also match nested keys by using dot notation to select the nested key name. For example, to match nested keys in the output:\\nCopy{\\n  \"documents\": [\\n    {\\n      \"page_content\": \"The capital of France is Paris\",\\n      \"metadata\": {},\\n      \"type\": \"Document\"\\n    }\\n  ]\\n}\\n\\nSelect Output Key, enter documents.page_content as the key and enter The capital of France is Paris as the value. This will match the nested key documents.page_content with the specified value.\\n\\nYou can add multiple key-value filters to create more complex queries. You can also use the Filter Shortcuts on the right side to quickly filter based on common key-value pairs as shown below:\\n\\n\\u200bExample: Filtering for tool calls\\nItâ€™s common to want to search for traces that contain specific tool calls. Tool calls are typically indicated in the output of an LLM run. To filter for tool calls, you would use the Output Key filter.\\nWhile this example will show you how to filter for tool calls, the same logic can be applied to filter for any key-value pair in the output.\\nIn this case, letâ€™s assume this is the output you want to filter for:\\nCopy{\\n  \"generations\": [\\n    [\\n      {\\n        \"text\": \"\",\\n        \"type\": \"ChatGeneration\",\\n        \"message\": {\\n          \"lc\": 1,\\n          \"type\": \"constructor\",\\n          \"id\": [],\\n          \"kwargs\": {\\n            \"type\": \"ai\",\\n            \"id\": \"run-ca7f7531-f4de-4790-9c3e-960be7f8b109\",\\n            \"tool_calls\": [\\n              {\\n                \"name\": \"Plan\",\\n                \"args\": {\\n                  \"steps\": [\\n                    \"Research LangGraph\\'s node configuration capabilities\",\\n                    \"Investigate how to add a Python code execution node\",\\n                    \"Find an example or create a sample implementation of a code execution node\"\\n                  ]\\n                },\\n                \"id\": \"toolu_01XexPzAVknT3gRmUB5PK5BP\",\\n                \"type\": \"tool_call\"\\n              }\\n            ]\\n          }\\n        }\\n      }\\n    ]\\n  ],\\n  \"llm_output\": null,\\n  \"run\": null,\\n  \"type\": \"LLMResult\"\\n}\\n\\nWith the example above, the KV search will map each nested JSON path as a key-value pair that you can use to search and filter.\\nLangSmith will break it into the following set of searchable key-value pairs:\\nKeyValuegenerations.typeChatGenerationgenerations.message.typeconstructorgenerations.message.kwargs.typeaigenerations.message.kwargs.idrun-ca7f7531-f4de-4790-9c3e-960be7f8b109generations.message.kwargs.tool_calls.namePlangenerations.message.kwargs.tool_calls.args.stepsResearch LangGraph\\'s node configuration capabilitiesgenerations.message.kwargs.tool_calls.args.stepsInvestigate how to add a Python code execution nodegenerations.message.kwargs.tool_calls.args.stepsFind an example or create a sample implementation of a code execution nodegenerations.message.kwargs.tool_calls.idtoolu_01XexPzAVknT3gRmUB5PK5BPgenerations.message.kwargs.tool_calls.typetool_calltypeLLMResult\\nTo search for a specific tool call, you can use the following Output Key search while removing the root runs filter:\\ngenerations.message.kwargs.tool_calls.name = Plan\\nThis will match root and non-root runs where the tool_calls name is Plan.\\n\\n\\u200bNegative filtering on key-value pairs\\nDifferent types of negative filtering can be applied to Metadata, Input Key, and Output Key fields to exclude specific runs from your results.\\nFor example, to find all runs where the metadata key phone is not equal to 1234567890, set the Metadata Key operator to is and Key field to phone, then set the Value operator to is not and the Value field to 1234567890. This will match all runs that have a metadata key phone with any value except 1234567890.\\n\\nTo find runs that donâ€™t have a specific metadata key, set the Key operator to is not. For example, setting the Key operator to is not with phone as the key will match all runs that donâ€™t have a phone field in their metadata.\\n\\nYou can also filter for runs that neither have a specific key nor a specific value. To find runs where the metadata has neither the key phone nor any field with the value 1234567890, set the Key operator to is not with key phone, and the Value operator to is not with value 1234567890.\\n\\nFinally, you can also filter for runs that do not have a specific key but have a specific value. To find runs where there is no phone key but there is a value of 1234567890 for some other key, set the Key operator to is not with key phone, and the Value operator to is with value 1234567890.\\n\\nNote that you can use the does not contain operator instead of is not to perform a substring match.\\n\\u200bSave a filter\\nSaving filters allows you to store and reuse frequently used filter configurations. Saved filters are specific to a tracing project.\\n\\u200bSave a filter\\nIn the filter box, click the Save filter button after you have constructed your filter. This will bring up a dialog to specify the name and a description of the filter.\\n\\n\\u200bUse a saved filter\\nAfter saving a filter, it is available in the filter bar as a quick filter for you to use. If you have more than three saved filters, only two will be displayed directly, with the rest accessible via a â€œmoreâ€ menu. You can use the settings icon in the saved filter bar to optionally hide default saved filters.\\n\\n\\u200bUpdate a saved filter\\nWith the filter selected, make any changes to filter parameters. Then click Update filter > Update to update the filter.\\nIn the same menu, you can also create a new saved filter by clicking Update filter > Create new.\\n\\u200bDelete a saved filter\\nClick the settings icon in the saved filter bar, and delete a filter using the trash icon.\\n\\u200bCopy a filter\\nYou can copy a constructed filter to share it with colleagues, reuse it later, or query runs programmatically in the API or SDK.\\nIn order to copy the filter, you can first create it in the UI. From there, you can click the copy button in the upper right hand corner. If you have constructed tree or trace filters, you can also copy those.\\nThis will give you a string representing the filter in the LangSmith query language. For example: and(eq(is_root, true), and(eq(feedback_key, \"user_score\"), eq(feedback_score, 1))). For more information on the query language syntax, please refer to this reference.\\n\\n\\u200bFiltering runs within the trace view\\nYou can also apply filters directly within the trace view, which is useful for sifting through traces with a large number of runs. The same filters available in the main runs table view can be applied here.\\nBy default, only the runs that match the filters will be shown. To see the matched runs within the broader context of the trace tree, switch the view option from â€œFiltered Onlyâ€ to â€œShow Allâ€ or â€œMost relevantâ€.\\n\\n\\u200bManually specify a raw query in LangSmith query language\\nIf you have copied a previously constructed filter, you may want to manually apply this raw query in a future session.\\nIn order to do this, you can click on Advanced filters on the bottom of the filters popover. From there you can paste a raw query into the text box.\\nNote that this will add that query to the existing queries, not overwrite it.\\n\\n\\u200bUse an AI Query to auto-generate a query (Experimental)\\nSometimes figuring out the exact query to specify can be difficult! In order to make it easier, weâ€™ve added an AI Query functionality. With this, you can type in the filter you want to construct in natural language and it will convert it into a valid query.\\nFor example: â€œAll runs longer than 10 secondsâ€\\n\\n\\u200bAdvanced filters\\n\\u200bFilter for intermediate runs (spans) on properties of the root\\nA common concept is to filter for intermediate runs which are part of a trace whose root run has some attribute. An example is filtering for intermediate runs of a particular type whose root run has positive (or negative) feedback associated with it.\\nIn order to do this, first set up a filter for intermediate runs (per the above section). After that, you can then add another filter rule. You can then click the Advanced Filters link all the way at the bottom of the filter. This will open up a new modal where you can add Trace filters. These filters will apply to the traces of all the parent runs of the individual runs youâ€™ve already filtered for.\\n\\n\\u200bFilter for runs (spans) whose child runs have some attribute\\nThis is the opposite of the above. You may want to search for runs who have specific types of sub runs. An example of this could be searching for all traces that had a sub run with name Foo. This is useful when Foo is not always called, but you want to analyze the cases where it is.\\nIn order to do this, you can click on the Advanced Filters link all the way at the bottom of the filter. This will open up a new modal where you can add Tree filters. This will make the rule you specify apply to all child runs of the individual runs youâ€™ve already filtered for.\\n\\n\\u200bExample: Filtering on all runs whose tree contains the tool call filter\\nExtending the tool call filtering example from above, if you would like to filter for all runs whose tree contains the tool filter call, you can use the tree filter in the advanced filters setting:\\nWas this page helpful?YesNoSuggest editsBeta LangSmith Collector-ProxyQuery traces (SDK)âŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/langgraph_platform_logs', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/langgraph_platform_logs', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='View server logs for a trace - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsManual instrumentationConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationViewing & managing tracesView server logs for a traceGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageAccess server logs from trace viewServer logs viewFiltering logs by trace IDViewing & managing tracesView server logs for a traceCopy pageCopy pageWhen viewing a trace that was generated by a run in LangGraph Platform, you can access the associated server logs directly from the trace view.\\nViewing server logs for a trace only works with the Cloud SaaS and fully self-hosted deployment options.\\n\\u200bAccess server logs from trace view\\nIn the trace view, use the See Logs button in the top right corner, next to the Run in Studio button.\\n\\nClicking this button will take you to the server logs view for the associated deployment in LangGraph Platform.\\n\\u200bServer logs view\\nThe server logs view displays logs from both:\\n\\nLangGraph Serverâ€™s own operational logs - Internal server operations, API calls, and system events\\nUser application logs - Logs written in your graph with:\\n\\nPython: Use the logging or structlog libraries\\nJavaScript: Use the re-exported Winston logger from @langchain/langgraph-sdk/logging:\\n\\n\\n\\nCopyimport { getLogger } from \"@langchain/langgraph-sdk/logging\";\\n\\nconst logger = getLogger();\\nlogger.info(\"Your log message\");\\n\\n\\u200bFiltering logs by trace ID\\nWhen you navigate from the trace view, the Filters box will automatically pre-fill with the Trace ID from the trace you just viewed.\\nThis allows you to quickly filter the logs to see only those related to your specific trace execution.\\nWas this page helpful?YesNoSuggest editsShare or unshare a trace publiclyBulk export trace dataâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Log custom LLM traces - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsManual instrumentationCustom instrumentationTrace with APILog custom LLM tracesLog retriever tracesConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationManual instrumentationLog custom LLM tracesGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageChat-style modelsUsing LangChain OSS or LangSmith wrappersImplementing your own custom chat-modelProvide token and cost informationSetting run metadataSetting run outputsTime-to-first-tokenInstruct-style modelsTracing setupManual instrumentationLog custom LLM tracesCopy pageCopy pageNothing will break if you donâ€™t log LLM traces in the correct format - data will still be logged. However, the data will not be processed or rendered in a way that is specific to LLMs.\\nLangSmith provides special rendering and processing for LLM traces, including token counting (assuming token counts are not available from the model provider) and token-based cost calculation. In order to make the most of this feature, you must log your LLM traces in a specific format.\\nThe examples below uses the traceable decorator/wrapper to log the model run (which is the recommended approach for Python and JS/TS). However, the same idea applies if you are using the RunTree or API directly.\\n\\u200bChat-style models\\n\\u200bUsing LangChain OSS or LangSmith wrappers\\nIf you are using LangChain OSS or LangSmith wrappers, you donâ€™t need to do anything special. The wrappers will automatically log traces in the correct format.\\n\\u200bImplementing your own custom chat-model\\nIf you are implementing your own custom chat-model, you need to ensure that your inputs contain a key messages with a list of dictionaries/objects. Each dictionary/object must contain the keys role and content with string values. The output must return an object that, when serialized, contains the key choices with a list of dictionaries/objects. Each must contain the key message with a dictionary/object that contains the keys role and content with string values.\\nTo make your custom LLM traces appear well-formatted in the LangSmith UI, your trace inputs and outputs must conform to a format LangSmith recognizes:\\n\\n\\nA list of messages in OpenAI or Anthropic format, represented as Python dictionaries or TypeScript objects.\\n\\nEach message must contain the key role and content.\\n\\nMessages with the \"assistant\" role may optionally contain tool_calls. These tool_calls may be in OpenAI format or LangChainâ€™s format.\\n\\n\\n\\n\\n\\nAn dict/object containing \"messages\" key with a list of messages in the above format.\\n\\nLangSmith may use additional parameters in this input dict that match OpenAIâ€™s chat completion endpoint for rendering in the trace view, such as a list of available tools for the model to call.\\n\\n\\n\\nHere are some examples:\\nList of messagesMessages dictCopy# Format 1: List of messages\\ninputs = [\\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\\n    {\"role\": \"user\", \"content\": \"What\\'s the weather like?\"},\\n    {\\n        \"role\": \"assistant\",\\n        \"content\": \"I need to check the weather for you.\",\\n        \"tool_calls\": [\\n            {\\n                \"id\": \"call_123\",\\n                \"type\": \"function\",\\n                \"function\": {\\n                    \"name\": \"get_weather\",\\n                    \"arguments\": \\'{\"location\": \"current\"}\\'\\n                }\\n            }\\n        ]\\n    }\\n]\\n\\n@traceable(run_type=\"llm\")\\ndef chat_model(messages: list):\\n    ...\\n\\nchat_model(inputs)\\n\\nThe output is accepted in any of the following formats:\\n\\nA dictionary/object that contains the key choices with a value that is a list of dictionaries/objects. Each dictionary/object must contain the key message, which maps to a message object with the keys role and content.\\nA dictionary/object that contains the key message with a value that is a message object with the keys role and content.\\nA tuple/array of two elements, where the first element is the role and the second element is the content.\\nA dictionary/object that contains the key role and content.\\n\\nHere are some examples:\\nChoices formatMessage formatTuple formatDirect formatCopyfrom langsmith import traceable\\n\\n@traceable(run_type=\"llm\")\\ndef chat_model_choices(messages):\\n    # Your model logic here\\n    return {\\n        \"choices\": [\\n            {\\n                \"message\": {\\n                    \"role\": \"assistant\",\\n                    \"content\": \"Sure, what time would you like to book the table for?\"\\n                }\\n            }\\n        ]\\n    }\\n\\n# Usage\\ninputs = [\\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\\n    {\"role\": \"user\", \"content\": \"I\\'d like to book a table for two.\"}\\n]\\nchat_model_choices(inputs)\\n\\nYou can also provide the following metadata fields to help LangSmith identify the model - which if recognized, LangSmith will use to automatically calculate costs. To learn more about how to use the metadata fields, see this guide.\\n\\nls_provider: The provider of the model, eg â€œopenaiâ€, â€œanthropicâ€, etc.\\nls_model_name: The name of the model, eg â€œgpt-4o-miniâ€, â€œclaude-3-opus-20240307â€, etc.\\n\\nPythonTypeScriptCopyfrom langsmith import traceable\\n\\ninputs = [\\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\\n    {\"role\": \"user\", \"content\": \"I\\'d like to book a table for two.\"},\\n]\\noutput = {\\n    \"choices\": [\\n        {\\n            \"message\": {\\n                \"role\": \"assistant\",\\n                \"content\": \"Sure, what time would you like to book the table for?\"\\n            }\\n        }\\n    ]\\n}\\n\\n# Can also use one of:\\n# output = {\\n#     \"message\": {\\n#         \"role\": \"assistant\",\\n#         \"content\": \"Sure, what time would you like to book the table for?\"\\n#     }\\n# }\\n#\\n# output = {\\n#     \"role\": \"assistant\",\\n#     \"content\": \"Sure, what time would you like to book the table for?\"\\n# }\\n#\\n# output = [\"assistant\", \"Sure, what time would you like to book the table for?\"]\\n\\n@traceable(\\n    run_type=\"llm\",\\n    metadata={\"ls_provider\": \"my_provider\", \"ls_model_name\": \"my_model\"}\\n)\\ndef chat_model(messages: list):\\n    return output\\n\\nchat_model(inputs)\\n\\nThe above code will log the following trace:\\n\\nIf you implement a custom streaming chat_model, you can â€œreduceâ€ the outputs into the same format as the non-streaming version. This is currently only supported in Python.\\nCopydef _reduce_chunks(chunks: list):\\n    all_text = \"\".join([chunk[\"choices\"][0][\"message\"][\"content\"] for chunk in chunks])\\n    return {\"choices\": [{\"message\": {\"content\": all_text, \"role\": \"assistant\"}}]}\\n\\n@traceable(\\n    run_type=\"llm\",\\n    reduce_fn=_reduce_chunks,\\n    metadata={\"ls_provider\": \"my_provider\", \"ls_model_name\": \"my_model\"}\\n)\\ndef my_streaming_chat_model(messages: list):\\n    for chunk in [\"Hello, \" + messages[1][\"content\"]]:\\n        yield {\\n            \"choices\": [\\n                {\\n                    \"message\": {\\n                        \"content\": chunk,\\n                        \"role\": \"assistant\",\\n                    }\\n                }\\n            ]\\n        }\\n\\nlist(\\n    my_streaming_chat_model(\\n        [\\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Please greet the user.\"},\\n            {\"role\": \"user\", \"content\": \"polly the parrot\"},\\n        ],\\n    )\\n)\\n\\nIf ls_model_name is not present in extra.metadata, other fields might be used from the extra.metadata for estimating token counts. The following fields are used in the order of precedence:\\nmetadata.ls_model_name\\ninputs.model\\ninputs.model_name\\n\\n\\u200bProvide token and cost information\\nBy default, LangSmith uses tiktoken to count tokens, utilizing a best guess at the modelâ€™s tokenizer based on the ls_model_name provided. It also calculates costs automatically by using the model pricing table. To learn how LangSmith calculates token-based costs, see this guide.\\nHowever, many models already include exact token counts as part of the response. If you have this information, you can override the default token calculation in LangSmith in one of two ways:\\n\\nExtract usage within your traced function and set a usage_metadata field on the runâ€™s metadata.\\nReturn a usage_metadata field in your traced function outputs.\\n\\nIn both cases, the usage metadata you send should contain a subset of the following LangSmith-recognized fields:\\nYou cannot set any fields other than the ones listed below. You do not need to include all fields.\\nCopyclass UsageMetadata(TypedDict, total=False):\\n    input_tokens: int\\n    \"\"\"The number of tokens used for the prompt.\"\"\"\\n    output_tokens: int\\n    \"\"\"The number of tokens generated as output.\"\"\"\\n    total_tokens: int\\n    \"\"\"The total number of tokens used.\"\"\"\\n    input_token_details: dict[str, float]\\n    \"\"\"The details of the input tokens.\"\"\"\\n    output_token_details: dict[str, float]\\n    \"\"\"The details of the output tokens.\"\"\"\\n    input_cost: float\\n    \"\"\"The cost of the input tokens.\"\"\"\\n    output_cost: float\\n    \"\"\"The cost of the output tokens.\"\"\"\\n    total_cost: float\\n    \"\"\"The total cost of the tokens.\"\"\"\\n    input_cost_details: dict[str, float]\\n    \"\"\"The cost details of the input tokens.\"\"\"\\n    output_cost_details: dict[str, float]\\n    \"\"\"The cost details of the output tokens.\"\"\"\\n\\nNote that the usage data can also include cost information, in case you do not want to rely on LangSmithâ€™s token-based cost formula. This is useful for models with pricing that is not linear by token type.\\n\\u200bSetting run metadata\\nYou can modify the current runâ€™s metadata with usage information within your traced function. The advantage of this approach is that you do not need to change your traced functionâ€™s runtime outputs. Hereâ€™s an example:\\nRequires langsmith>=0.3.43 (Python) and langsmith>=0.3.30 (JS/TS).\\nPythonTypeScriptCopyfrom langsmith import traceable, get_current_run_tree\\n\\ninputs = [\\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\\n    {\"role\": \"user\", \"content\": \"I\\'d like to book a table for two.\"},\\n]\\n\\n@traceable(\\n    run_type=\"llm\",\\n    metadata={\"ls_provider\": \"my_provider\", \"ls_model_name\": \"my_model\"}\\n)\\ndef chat_model(messages: list):\\n    llm_output = {\\n        \"choices\": [\\n            {\\n                \"message\": {\\n                    \"role\": \"assistant\",\\n                    \"content\": \"Sure, what time would you like to book the table for?\"\\n                }\\n            }\\n        ],\\n        \"usage_metadata\": {\\n            \"input_tokens\": 27,\\n            \"output_tokens\": 13,\\n            \"total_tokens\": 40,\\n            \"input_token_details\": {\"cache_read\": 10},\\n            # If you wanted to specify costs:\\n            # \"input_cost\": 1.1e-6,\\n            # \"input_cost_details\": {\"cache_read\": 2.3e-7},\\n            # \"output_cost\": 5.0e-6,\\n        },\\n    }\\n    run = get_current_run_tree()\\n    run.set(usage_metadata=llm_output[\"usage_metadata\"])\\n    return llm_output[\"choices\"][0][\"message\"]\\n\\nchat_model(inputs)\\n\\n\\u200bSetting run outputs\\nYou can add a usage_metadata key to the functionâ€™s response to set manual token counts and costs.\\nPythonTypeScriptCopyfrom langsmith import traceable\\n\\ninputs = [\\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\\n    {\"role\": \"user\", \"content\": \"I\\'d like to book a table for two.\"},\\n]\\noutput = {\\n    \"choices\": [\\n        {\\n            \"message\": {\\n                \"role\": \"assistant\",\\n                \"content\": \"Sure, what time would you like to book the table for?\"\\n            }\\n        }\\n    ],\\n    \"usage_metadata\": {\\n        \"input_tokens\": 27,\\n        \"output_tokens\": 13,\\n        \"total_tokens\": 40,\\n        \"input_token_details\": {\"cache_read\": 10},\\n        # If you wanted to specify costs:\\n        # \"input_cost\": 1.1e-6,\\n        # \"input_cost_details\": {\"cache_read\": 2.3e-7},\\n        # \"output_cost\": 5.0e-6,\\n    },\\n}\\n\\n@traceable(\\n    run_type=\"llm\",\\n    metadata={\"ls_provider\": \"my_provider\", \"ls_model_name\": \"my_model\"}\\n)\\ndef chat_model(messages: list):\\n    return output\\n\\nchat_model(inputs)\\n\\n\\u200bTime-to-first-token\\nIf you are using traceable or one of our SDK wrappers, LangSmith will automatically populate time-to-first-token for streaming LLM runs.\\nHowever, if you are using the RunTree API directly, you will need to add a new_token event to the run tree in order to properly populate time-to-first-token.\\nHereâ€™s an example:\\nPythonTypeScriptCopyfrom langsmith.run_trees import RunTree\\nrun_tree = RunTree(\\n    name=\"CustomChatModel\",\\n    run_type=\"llm\",\\n    inputs={ ... }\\n)\\nrun_tree.post()\\nllm_stream = ...\\nfirst_token = None\\nfor token in llm_stream:\\n    if first_token is None:\\n      first_token = token\\n      run_tree.add_event({\\n        \"name\": \"new_token\"\\n      })\\nrun_tree.end(outputs={ ... })\\nrun_tree.patch()\\n\\n\\u200bInstruct-style models\\nFor instruct-style models (string in, string out), your inputs must contain a key prompt with a string value. Other inputs are also permitted. The output must return an object that, when serialized, contains the key choices with a list of dictionaries/objects. Each must contain the key text with a string value. The same rules for metadata and usage_metadata apply as for chat-style models.\\nPythonTypeScriptCopyfrom langsmith import traceable\\n\\n@traceable(\\n    run_type=\"llm\",\\n    metadata={\"ls_provider\": \"my_provider\", \"ls_model_name\": \"my_model\"}\\n)\\ndef hello_llm(prompt: str):\\n    return {\\n        \"choices\": [\\n            {\"text\": \"Hello, \" + prompt}\\n        ],\\n        \"usage_metadata\": {\\n            \"input_tokens\": 4,\\n            \"output_tokens\": 5,\\n            \"total_tokens\": 9,\\n        },\\n    }\\n\\nhello_llm(\"polly the parrot\\\\n\")\\n\\nThe above code will log the following trace:\\nWas this page helpful?YesNoSuggest editsTrace with APILog retriever tracesâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Include multimodal content in a prompt - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsCreate and update promptsCreate a promptManage promptsManage prompts programmaticallyConfigure prompt settingsUse tools in a promptInclude multimodal content in a promptWrite your prompt with AIConnect to modelsTutorialsOptimize a classifierSync prompts with GitHubTest multi-turn conversationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationCreate and update promptsInclude multimodal content in a promptGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageInline contentTemplate variablesPopulate the template variableRun an evaluationCreate and update promptsInclude multimodal content in a promptCopy pageCopy pageSome applications are based around multimodal content, like a chatbot that can answer questions about a PDF or image. In these cases, youâ€™ll want to include multimodal content in your prompt and test the modelâ€™s ability to answer questions about the content.\\nThe LangSmith Playground supports two methods for incorporating multimodal content in your prompts:\\n\\n\\nInline content: Embed static files (images, PDFs, audio) directly in your prompt. This is ideal when you want to consistently include the same multimodal content across all uses of the prompt. For example, you might include a reference image that helps ground the modelâ€™s responses.\\n\\n\\nTemplate variables: Create dynamic placeholders for attachments that can be populated with different content each time. This approach offers more flexibility, allowing you to:\\n\\nTest how the model handles different inputs\\nCreate reusable prompts that work with varying content\\n\\n\\n\\nNot all models support multimodal content. Before using multimodal features in the playground, make sure your selected model supports the file types you want to use.\\n\\u200bInline content\\nClick the file icon in the message where you want to add multimodal content. Under the Upload content tab, you can upload a file and include it inline in the prompt.\\n\\n\\u200bTemplate variables\\nClick the file icon in the message where you want to add multimodal content. Under the Template variables tab, you can create a template variable for a specific attachment type. Currently, only images, PDFs, and audio files (.wav, .mp3) are supported.\\n\\n\\u200bPopulate the template variable\\nOnce youâ€™ve added a template variable, you can provide content for it using the panel on the right side of the screen. Simply click the + button to upload or select content that will be used to populate the template variable.\\n\\n\\u200bRun an evaluation\\nAfter testing out your prompt manually, you can run an evaluation to see how the prompt performs over a golden dataset of examples.Was this page helpful?YesNoSuggest editsUse tools in a promptWrite your prompt with AIâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Log retriever traces - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsManual instrumentationCustom instrumentationTrace with APILog custom LLM tracesLog retriever tracesConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationManual instrumentationLog retriever tracesGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumTracing setupManual instrumentationLog retriever tracesCopy pageCopy pageNothing will break if you donâ€™t log retriever traces in the correct format and data will still be logged. However, the data will not be rendered in a way that is specific to retriever steps.\\nMany LLM applications require looking up documents from vector databases, knowledge graphs, or other types of indexes. Retriever traces are a way to log the documents that are retrieved by the retriever. LangSmith provides special rendering for retrieval steps in traces to make it easier to understand and diagnose retrieval issues. In order for retrieval steps to be rendered correctly, a few small steps need to be taken.\\n\\n\\nAnnotate the retriever step with run_type=\"retriever\".\\n\\n\\nReturn a list of Python dictionaries or TypeScript objects from the retriever step. Each dictionary should contain the following keys:\\n\\npage_content: The text of the document.\\ntype: This should always be â€œDocumentâ€.\\nmetadata: A python dictionary or TypeScript object containing metadata about the document. This metadata will be displayed in the trace.\\n\\n\\n\\nThe following code snippets show how to log a retrieval steps in Python and TypeScript.\\nPythonTypeScriptCopyfrom langsmith import traceable\\n\\ndef _convert_docs(results):\\n    return [\\n        {\\n            \"page_content\": r,\\n            \"type\": \"Document\",\\n            \"metadata\": {\"foo\": \"bar\"}\\n        }\\n        for r in results\\n    ]\\n\\n@traceable(run_type=\"retriever\")\\ndef retrieve_docs(query):\\n    # Foo retriever returning hardcoded dummy documents.\\n    # In production, this could be a real vector datatabase or other document index.\\n    contents = [\"Document contents 1\", \"Document contents 2\", \"Document contents 3\"]\\n    return _convert_docs(contents)\\n\\nretrieve_docs(\"User query\")\\n\\nThe following image shows how a retriever step is rendered in a trace. The contents along with the metadata are displayed with each document.\\nWas this page helpful?YesNoSuggest editsLog custom LLM tracesLog traces to a specific projectâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Log traces to a specific project - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsManual instrumentationConfiguration & troubleshootingProject & environment settingsLog traces to a specific projectTrace without env varsSet a sampling rate for tracesAdvanced tracing techniquesData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationProject & environment settingsLog traces to a specific projectGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageSet the destination project staticallySet the destination project dynamicallyConfiguration & troubleshootingProject & environment settingsLog traces to a specific projectCopy pageCopy pageYou can change the destination project of your traces both statically through environment variables and dynamically at runtime.\\n\\u200bSet the destination project statically\\nAs mentioned in the Tracing Concepts section, LangSmith uses the concept of a Project to group traces. If left unspecified, the project is set to default. You can set the LANGSMITH_PROJECT environment variable to configure a custom project name for an entire application run. This should be done before executing your application.\\nCopyexport LANGSMITH_PROJECT=my-custom-project\\n\\nThe LANGSMITH_PROJECT flag is only supported in JS SDK versions >= 0.2.16, use LANGCHAIN_PROJECT instead if you are using an older version.\\nIf the project specified does not exist, it will be created automatically when the first trace is ingested.\\n\\u200bSet the destination project dynamically\\nYou can also set the project name at program runtime in various ways, depending on how you are annotating your code for tracing. This is useful when you want to log traces to different projects within the same application.\\nSetting the project name dynamically using one of the below methods overrides the project name set by the LANGSMITH_PROJECT environment variable.\\nPythonTypeScriptCopyimport openai\\nfrom langsmith import traceable\\nfrom langsmith.run_trees import RunTree\\n\\nclient = openai.Client()\\nmessages = [\\n  {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\\n  {\"role\": \"user\", \"content\": \"Hello!\"}\\n]\\n\\n# Use the @traceable decorator with the \\'project_name\\' parameter to log traces to LangSmith\\n# Ensure that the LANGSMITH_TRACING environment variables is set for @traceable to work\\n@traceable(\\n  run_type=\"llm\",\\n  name=\"OpenAI Call Decorator\",\\n  project_name=\"My Project\"\\n)\\ndef call_openai(\\n  messages: list[dict], model: str = \"gpt-4o-mini\"\\n) -> str:\\n  return client.chat.completions.create(\\n      model=model,\\n      messages=messages,\\n  ).choices[0].message.content\\n\\n# Call the decorated function\\ncall_openai(messages)\\n\\n# You can also specify the Project via the project_name parameter\\n# This will override the project_name specified in the @traceable decorator\\ncall_openai(\\n  messages,\\n  langsmith_extra={\"project_name\": \"My Overridden Project\"},\\n)\\n\\n# The wrapped OpenAI client accepts all the same langsmith_extra parameters\\n# as @traceable decorated functions, and logs traces to LangSmith automatically.\\n# Ensure that the LANGSMITH_TRACING environment variables is set for the wrapper to work.\\nfrom langsmith import wrappers\\nwrapped_client = wrappers.wrap_openai(client)\\nwrapped_client.chat.completions.create(\\n  model=\"gpt-4o-mini\",\\n  messages=messages,\\n  langsmith_extra={\"project_name\": \"My Project\"},\\n)\\n\\n# Alternatively, create a RunTree object\\n# You can set the project name using the project_name parameter\\nrt = RunTree(\\n  run_type=\"llm\",\\n  name=\"OpenAI Call RunTree\",\\n  inputs={\"messages\": messages},\\n  project_name=\"My Project\"\\n)\\nchat_completion = client.chat.completions.create(\\n  model=\"gpt-4o-mini\",\\n  messages=messages,\\n)\\n# End and submit the run\\nrt.end(outputs=chat_completion)\\nrt.post()\\nWas this page helpful?YesNoSuggest editsLog retriever tracesTrace without env varsâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Prevent logging of sensitive data in traces - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsManual instrumentationConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyAdd metadata and tags to tracesPrevent logging of sensitive data in tracesUpload files with tracesTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationData & privacyPrevent logging of sensitive data in tracesGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageRule-based masking of inputs and outputsProcessing Inputs & Outputs for a Single FunctionQuick startsRegexMicrosoft PresidioAmazon ComprehendConfiguration & troubleshootingData & privacyPrevent logging of sensitive data in tracesCopy pageCopy pageIn some situations, you may need to prevent the inputs and outputs of your traces from being logged for privacy or security reasons. LangSmith provides a way to filter the inputs and outputs of your traces before they are sent to the LangSmith backend.\\nIf you want to completely hide the inputs and outputs of your traces, you can set the following environment variables when running your application:\\nCopyLANGSMITH_HIDE_INPUTS=true\\nLANGSMITH_HIDE_OUTPUTS=true\\n\\nThis works for both the LangSmith SDK (Python and TypeScript) and LangChain.\\nYou can also customize and override this behavior for a given Client instance. This can be done by setting the hide_inputs and hide_outputs parameters on the Client object (hideInputs and hideOutputs in TypeScript).\\nFor the example below, we will simply return an empty object for both hide_inputs and hide_outputs, but you can customize this to your needs.\\nPythonTypeScriptCopyimport openai\\nfrom langsmith import Client\\nfrom langsmith.wrappers import wrap_openai\\n\\nopenai_client = wrap_openai(openai.Client())\\nlangsmith_client = Client(\\n    hide_inputs=lambda inputs: {}, hide_outputs=lambda outputs: {}\\n)\\n\\n# The trace produced will have its metadata present, but the inputs will be hidden\\nopenai_client.chat.completions.create(\\n    model=\"gpt-4o-mini\",\\n    messages=[\\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\\n        {\"role\": \"user\", \"content\": \"Hello!\"},\\n    ],\\n    langsmith_extra={\"client\": langsmith_client},\\n)\\n\\n# The trace produced will not have hidden inputs and outputs\\nopenai_client.chat.completions.create(\\n    model=\"gpt-4o-mini\",\\n    messages=[\\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\\n        {\"role\": \"user\", \"content\": \"Hello!\"},\\n    ],\\n)\\n\\n\\u200bRule-based masking of inputs and outputs\\nThis feature is available in the following LangSmith SDK versions:\\nPython: 0.1.81 and above\\nTypeScript: 0.1.33 and above\\n\\nTo mask specific data in inputs and outputs, you can use the create_anonymizer / createAnonymizer function and pass the newly created anonymizer when instantiating the client. The anonymizer can be either constructed from a list of regex patterns and the replacement values or from a function that accepts and returns a string value.\\nThe anonymizer will be skipped for inputs if LANGSMITH_HIDE_INPUTS = true. Same applies for outputs if LANGSMITH_HIDE_OUTPUTS = true.\\nHowever, if inputs or outputs are to be sent to client, the anonymizer method will take precedence over functions found in hide_inputs and hide_outputs. By default, the create_anonymizer will only look at maximum of 10 nesting levels deep, which can be configured via the max_depth parameter.\\nPythonTypeScriptCopyfrom langsmith.anonymizer import create_anonymizer\\nfrom langsmith import Client, traceable\\nimport re\\n\\n# create anonymizer from list of regex patterns and replacement values\\nanonymizer = create_anonymizer([\\n    { \"pattern\": r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}\", \"replace\": \"<email-address>\" },\\n    { \"pattern\": r\"[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}\", \"replace\": \"<UUID>\" }\\n])\\n\\n# or create anonymizer from a function\\nemail_pattern = re.compile(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}\")\\nuuid_pattern = re.compile(r\"[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}\")\\nanonymizer = create_anonymizer(\\n    lambda text: email_pattern.sub(\"<email-address>\", uuid_pattern.sub(\"<UUID>\", text))\\n)\\n\\nclient = Client(anonymizer=anonymizer)\\n\\n@traceable(client=client)\\ndef main(inputs: dict) -> dict:\\n    ...\\n\\nPlease note, that using the anonymizer might incur a performance hit with complex regular expressions or large payloads, as the anonymizer serializes the payload to JSON before processing.\\nImproving the performance of anonymizer API is on our roadmap! If you are encountering performance issues, please contact us at support@langchain.dev.\\n\\nOlder versions of LangSmith SDKs can use the hide_inputs and hide_outputs parameters to achieve the same effect. You can also use these parameters to process the inputs and outputs more efficiently as well.\\nPythonTypeScriptCopyimport re\\nfrom langsmith import Client, traceable\\n\\n# Define the regex patterns for email addresses and UUIDs\\nEMAIL_REGEX = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}\"\\nUUID_REGEX = r\"[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}\"\\n\\ndef replace_sensitive_data(data, depth=10):\\n    if depth == 0:\\n        return data\\n    if isinstance(data, dict):\\n        return {k: replace_sensitive_data(v, depth-1) for k, v in data.items()}\\n    elif isinstance(data, list):\\n        return [replace_sensitive_data(item, depth-1) for item in data]\\n    elif isinstance(data, str):\\n        data = re.sub(EMAIL_REGEX, \"<email-address>\", data)\\n        data = re.sub(UUID_REGEX, \"<UUID>\", data)\\n        return data\\n    else:\\n        return data\\n\\nclient = Client(\\n    hide_inputs=lambda inputs: replace_sensitive_data(inputs),\\n    hide_outputs=lambda outputs: replace_sensitive_data(outputs)\\n)\\n\\ninputs = {\"role\": \"user\", \"content\": \"Hello! My email is user@example.com and my ID is 123e4567-e89b-12d3-a456-426614174000.\"}\\noutputs = {\"role\": \"assistant\", \"content\": \"Hi! I\\'ve noted your email as user@example.com and your ID as 123e4567-e89b-12d3-a456-426614174000.\"}\\n\\n@traceable(client=client)\\ndef child(inputs: dict) -> dict:\\n    return outputs\\n\\n@traceable(client=client)\\ndef parent(inputs: dict) -> dict:\\n    child_outputs = child(inputs)\\n    return child_outputs\\n\\nparent(inputs)\\n\\n\\u200bProcessing Inputs & Outputs for a Single Function\\nThe process_outputs parameter is available in LangSmith SDK version 0.1.98 and above for Python.\\nIn addition to client-level input and output processing, LangSmith provides function-level processing through the process_inputs and process_outputs parameters of the @traceable decorator.\\nThese parameters accept functions that allow you to transform the inputs and outputs of a specific function before they are logged to LangSmith. This is useful for reducing payload size, removing sensitive information, or customizing how an object should be serialized and represented in LangSmith for a particular function.\\nHereâ€™s an example of how to use process_inputs and process_outputs:\\nCopyfrom langsmith import traceable\\n\\ndef process_inputs(inputs: dict) -> dict:\\n    # inputs is a dictionary where keys are argument names and values are the provided arguments\\n    # Return a new dictionary with processed inputs\\n    return {\\n        \"processed_key\": inputs.get(\"my_cool_key\", \"default\"),\\n        \"length\": len(inputs.get(\"my_cool_key\", \"\"))\\n    }\\n\\ndef process_outputs(output: Any) -> dict:\\n    # output is the direct return value of the function\\n    # Transform the output into a dictionary\\n    # In this case, \"output\" will be an integer\\n    return {\"processed_output\": str(output)}\\n\\n@traceable(process_inputs=process_inputs, process_outputs=process_outputs)\\ndef my_function(my_cool_key: str) -> int:\\n    # Function implementation\\n    return len(my_cool_key)\\n\\nresult = my_function(\"example\")\\n\\nIn this example, process_inputs creates a new dictionary with processed input data, and process_outputs transforms the output into a specific format before logging to LangSmith.\\nItâ€™s recommended to avoid mutating the source objects in the processor functions. Instead, create and return new objects with the processed data.\\nFor asynchronous functions, the usage is similar:\\nCopy@traceable(process_inputs=process_inputs, process_outputs=process_outputs)\\nasync def async_function(key: str) -> int:\\n    # Async implementation\\n    return len(key)\\n\\nThese function-level processors take precedence over client-level processors (hide_inputs and hide_outputs) when both are defined.\\n\\u200bQuick starts\\nYou can combine rule-based masking with various anonymizers to scrub sensitive information from inputs and outputs. In this how-to-guide, weâ€™ll cover working with regex, Microsoft Presidio, and Amazon Comprehend.\\n\\u200bRegex\\nThe implementation below is not exhaustive and may miss some formats or edge cases. Test any implementation thoroughly before using it in production.\\nYou can use regex to mask inputs and outputs before they are sent to LangSmith. The implementation below masks email addresses, phone numbers, full names, credit card numbers, and SSNs.\\nCopyimport re\\nimport openai\\nfrom langsmith import Client\\nfrom langsmith.wrappers import wrap_openai\\n\\n# Define regex patterns for various PII\\nSSN_PATTERN = re.compile(r\\'\\\\b\\\\d{3}-\\\\d{2}-\\\\d{4}\\\\b\\')\\nCREDIT_CARD_PATTERN = re.compile(r\\'\\\\b(?:\\\\d[ -]*?){13,16}\\\\b\\')\\nEMAIL_PATTERN = re.compile(r\\'\\\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Z|a-z]{2,7}\\\\b\\')\\nPHONE_PATTERN = re.compile(r\\'\\\\b(?:\\\\+?1[-.\\\\s]?)?\\\\(?\\\\d{3}\\\\)?[-.\\\\s]?\\\\d{3}[-.\\\\s]?\\\\d{4}\\\\b\\')\\nFULL_NAME_PATTERN = re.compile(r\\'\\\\b([A-Z][a-z]*\\\\s[A-Z][a-z]*)\\\\b\\')\\n\\ndef regex_anonymize(text):\\n    \"\"\"\\n    Anonymize sensitive information in the text using regex patterns.\\n    Args:\\n        text (str): The input text to be anonymized.\\n    Returns:\\n        str: The anonymized text.\\n    \"\"\"\\n    # Replace sensitive information with placeholders\\n    text = SSN_PATTERN.sub(\\'[REDACTED SSN]\\', text)\\n    text = CREDIT_CARD_PATTERN.sub(\\'[REDACTED CREDIT CARD]\\', text)\\n    text = EMAIL_PATTERN.sub(\\'[REDACTED EMAIL]\\', text)\\n    text = PHONE_PATTERN.sub(\\'[REDACTED PHONE]\\', text)\\n    text = FULL_NAME_PATTERN.sub(\\'[REDACTED NAME]\\', text)\\n    return text\\n\\ndef recursive_anonymize(data, depth=10):\\n    \"\"\"\\n    Recursively traverse the data structure and anonymize sensitive information.\\n    Args:\\n        data (any): The input data to be anonymized.\\n        depth (int): The current recursion depth to prevent excessive recursion.\\n    Returns:\\n        any: The anonymized data.\\n    \"\"\"\\n    if depth == 0:\\n        return data\\n    if isinstance(data, dict):\\n        anonymized_dict = {}\\n        for k, v in data.items():\\n            anonymized_value = recursive_anonymize(v, depth - 1)\\n            anonymized_dict[k] = anonymized_value\\n        return anonymized_dict\\n    elif isinstance(data, list):\\n        anonymized_list = []\\n        for item in data:\\n            anonymized_item = recursive_anonymize(item, depth - 1)\\n            anonymized_list.append(anonymized_item)\\n        return anonymized_list\\n    elif isinstance(data, str):\\n        anonymized_data = regex_anonymize(data)\\n        return anonymized_data\\n    else:\\n        return data\\n\\nopenai_client = wrap_openai(openai.Client())\\n\\n# Initialize the LangSmith client with the anonymization functions\\nlangsmith_client = Client(\\n    hide_inputs=recursive_anonymize, hide_outputs=recursive_anonymize\\n)\\n\\n# The trace produced will have its metadata present, but the inputs and outputs will be anonymized\\nresponse_with_anonymization = openai_client.chat.completions.create(\\n    model=\"gpt-4o-mini\",\\n    messages=[\\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\\n        {\"role\": \"user\", \"content\": \"My name is John Doe, my SSN is 123-45-6789, my credit card number is 4111 1111 1111 1111, my email is john.doe@example.com, and my phone number is (123) 456-7890.\"},\\n    ],\\n    langsmith_extra={\"client\": langsmith_client},\\n)\\n\\n# The trace produced will not have anonymized inputs and outputs\\nresponse_without_anonymization = openai_client.chat.completions.create(\\n    model=\"gpt-4o-mini\",\\n    messages=[\\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\\n        {\"role\": \"user\", \"content\": \"My name is John Doe, my SSN is 123-45-6789, my credit card number is 4111 1111 1111 1111, my email is john.doe@example.com, and my phone number is (123) 456-7890.\"},\\n    ],\\n)\\n\\nThe anonymized run will look like this in LangSmith: \\nThe non-anonymized run will look like this in LangSmith: \\n\\u200bMicrosoft Presidio\\nThe implementation below provides a general example of how to anonymize sensitive information in messages exchanged between a user and an LLM. It is not exhaustive and does not account for all cases. Test any implementation thoroughly before using it in production.\\nMicrosoft Presidio is a data protection and de-identification SDK. The implementation below uses Presidio to anonymize inputs and outputs before they are sent to LangSmith. For up to date information, please refer to Presidioâ€™s official documentation.\\nTo use Presidio and its spaCy model, install the following:\\npipuvCopypip install presidio-analyzer\\npip install presidio-anonymizer\\npython -m spacy download en_core_web_lg\\n\\nAlso, install OpenAI:\\npipuvCopypip install openai\\n\\nCopyimport openai\\nfrom langsmith import Client\\nfrom langsmith.wrappers import wrap_openai\\nfrom presidio_anonymizer import AnonymizerEngine\\nfrom presidio_analyzer import AnalyzerEngine\\n\\nanonymizer = AnonymizerEngine()\\nanalyzer = AnalyzerEngine()\\n\\ndef presidio_anonymize(data):\\n    \"\"\"\\n    Anonymize sensitive information sent by the user or returned by the model.\\n    Args:\\n        data (any): The data to be anonymized.\\n    Returns:\\n        any: The anonymized data.\\n    \"\"\"\\n    message_list = (\\n        data.get(\\'messages\\') or [data.get(\\'choices\\', [{}])[0].get(\\'message\\')]\\n    )\\n    if not message_list or not all(isinstance(msg, dict) and msg for msg in message_list):\\n        return data\\n\\n    for message in message_list:\\n        content = message.get(\\'content\\', \\'\\')\\n        if not content.strip():\\n            print(\"Empty content detected. Skipping anonymization.\")\\n            continue\\n\\n        results = analyzer.analyze(\\n            text=content,\\n            entities=[\"PERSON\", \"PHONE_NUMBER\", \"EMAIL_ADDRESS\", \"US_SSN\"],\\n            language=\\'en\\'\\n        )\\n        anonymized_result = anonymizer.anonymize(\\n            text=content,\\n            analyzer_results=results\\n        )\\n        message[\\'content\\'] = anonymized_result.text\\n\\n    return data\\n\\nopenai_client = wrap_openai(openai.Client())\\n\\n# initialize the langsmith client with the anonymization functions\\nlangsmith_client = Client(\\n  hide_inputs=presidio_anonymize, hide_outputs=presidio_anonymize\\n)\\n\\n# The trace produced will have its metadata present, but the inputs and outputs will be anonymized\\nresponse_with_anonymization = openai_client.chat.completions.create(\\n  model=\"gpt-4o-mini\",\\n  messages=[\\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\\n      {\"role\": \"user\", \"content\": \"My name is Slim Shady, call me at 313-666-7440 or email me at real.slim.shady@gmail.com\"},\\n  ],\\n  langsmith_extra={\"client\": langsmith_client},\\n)\\n\\n# The trace produced will not have anonymized inputs and outputs\\nresponse_without_anonymization = openai_client.chat.completions.create(\\n  model=\"gpt-4o-mini\",\\n  messages=[\\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\\n      {\"role\": \"user\", \"content\": \"My name is Slim Shady, call me at 313-666-7440 or email me at real.slim.shady@gmail.com\"},\\n  ],\\n)\\n\\nThe anonymized run will look like this in LangSmith: \\nThe non-anonymized run will look like this in LangSmith: \\n\\u200bAmazon Comprehend\\nThe implementation below provides a general example of how to anonymize sensitive information in messages exchanged between a user and an LLM. It is not exhaustive and does not account for all cases. Test any implementation thoroughly before using it in production.\\nComprehend is a natural language processing service that can detect personally identifiable information. The implementation below uses Comprehend to anonymize inputs and outputs before they are sent to LangSmith. For up to date information, please refer to Comprehendâ€™s official documentation.\\nTo use Comprehend, install boto3:\\npipuvCopypip install boto3\\n\\nAlso, install OpenAI:\\npipuvCopypip install openai\\n\\nYou will need to set up credentials in AWS and authenticate using the AWS CLI. Follow the instructions here.\\nCopyimport openai\\nimport boto3\\nfrom langsmith import Client\\nfrom langsmith.wrappers import wrap_openai\\n\\ncomprehend = boto3.client(\\'comprehend\\', region_name=\\'us-east-1\\')\\n\\ndef redact_pii_entities(text, entities):\\n    \"\"\"\\n    Redact PII entities in the text based on the detected entities.\\n    Args:\\n        text (str): The original text containing PII.\\n        entities (list): A list of detected PII entities.\\n    Returns:\\n        str: The text with PII entities redacted.\\n    \"\"\"\\n    sorted_entities = sorted(entities, key=lambda x: x[\\'BeginOffset\\'], reverse=True)\\n    redacted_text = text\\n    for entity in sorted_entities:\\n        begin = entity[\\'BeginOffset\\']\\n        end = entity[\\'EndOffset\\']\\n        entity_type = entity[\\'Type\\']\\n        # Define the redaction placeholder based on entity type\\n        placeholder = f\"[{entity_type}]\"\\n        # Replace the PII in the text with the placeholder\\n        redacted_text = redacted_text[:begin] + placeholder + redacted_text[end:]\\n    return redacted_text\\n\\ndef detect_pii(text):\\n    \"\"\"\\n    Detect PII entities in the given text using AWS Comprehend.\\n    Args:\\n        text (str): The text to analyze.\\n    Returns:\\n        list: A list of detected PII entities.\\n    \"\"\"\\n    try:\\n        response = comprehend.detect_pii_entities(\\n            Text=text,\\n            LanguageCode=\\'en\\',\\n        )\\n        entities = response.get(\\'Entities\\', [])\\n        return entities\\n    except Exception as e:\\n        print(f\"Error detecting PII: {e}\")\\n        return []\\n\\ndef comprehend_anonymize(data):\\n    \"\"\"\\n    Anonymize sensitive information sent by the user or returned by the model.\\n    Args:\\n        data (any): The input data to be anonymized.\\n    Returns:\\n        any: The anonymized data.\\n    \"\"\"\\n    message_list = (\\n        data.get(\\'messages\\') or [data.get(\\'choices\\', [{}])[0].get(\\'message\\')]\\n    )\\n    if not message_list or not all(isinstance(msg, dict) and msg for msg in message_list):\\n        return data\\n\\n    for message in message_list:\\n        content = message.get(\\'content\\', \\'\\')\\n        if not content.strip():\\n            print(\"Empty content detected. Skipping anonymization.\")\\n            continue\\n\\n        entities = detect_pii(content)\\n        if entities:\\n            anonymized_text = redact_pii_entities(content, entities)\\n            message[\\'content\\'] = anonymized_text\\n        else:\\n            print(\"No PII detected. Content remains unchanged.\")\\n\\n    return data\\n\\nopenai_client = wrap_openai(openai.Client())\\n\\n# initialize the langsmith client with the anonymization functions\\nlangsmith_client = Client(\\n  hide_inputs=comprehend_anonymize, hide_outputs=comprehend_anonymize\\n)\\n\\n# The trace produced will have its metadata present, but the inputs and outputs will be anonymized\\nresponse_with_anonymization = openai_client.chat.completions.create(\\n  model=\"gpt-4o-mini\",\\n  messages=[\\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\\n      {\"role\": \"user\", \"content\": \"My name is Slim Shady, call me at 313-666-7440 or email me at real.slim.shady@gmail.com\"},\\n  ],\\n  langsmith_extra={\"client\": langsmith_client},\\n)\\n\\n# The trace produced will not have anonymized inputs and outputs\\nresponse_without_anonymization = openai_client.chat.completions.create(\\n  model=\"gpt-4o-mini\",\\n  messages=[\\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\\n      {\"role\": \"user\", \"content\": \"My name is Slim Shady, call me at 313-666-7440 or email me at real.slim.shady@gmail.com\"},\\n  ],\\n)\\n\\nThe anonymized run will look like this in LangSmith: \\nThe non-anonymized run will look like this in LangSmith: Was this page helpful?YesNoSuggest editsAdd metadata and tags to tracesUpload files with tracesâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/nest_traces', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/nest_traces', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Troubleshoot trace nesting - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsManual instrumentationConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyTroubleshooting guidesTroubleshoot trace nestingTroubleshoot variable cachingBeta LangSmith Collector-ProxyViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationTroubleshooting guidesTroubleshoot trace nestingGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pagePythonContext propagation using asyncioWhyTo resolveContext propagation using threadingWhyTo resolveConfiguration & troubleshootingTroubleshooting guidesTroubleshoot trace nestingCopy pageCopy pageWhen tracing with the LangSmith SDK, LangGraph, and LangChain, tracing should automatically propagate the correct context so that code executed within a parent trace will be rendered in the expected location in the UI.\\nIf you see a child run go to a separate trace (and appear on the top level), it may be caused by one of the following known â€œedge casesâ€.\\n\\u200bPython\\nThe following outlines common causes for â€œsplitâ€ traces when building with python.\\n\\u200bContext propagation using asyncio\\nWhen using async calls (especially with streaming) in Python versions < 3.11, you may encounter issues with trace nesting. This is because Pythonâ€™s asyncio only added full support for passing context in version 3.11.\\n\\u200bWhy\\nLangChain and LangSmith SDK use contextvars to propagate tracing information implicitly. In Python 3.11 and above, this works seamlessly. However, in earlier versions (3.8, 3.9, 3.10), asyncio tasks lack proper contextvar support, which can lead to disconnected traces.\\n\\u200bTo resolve\\n\\n\\nUpgrade Python Version (Recommended) If possible, upgrade to Python 3.11 or later for automatic context propagation.\\n\\n\\nManual Context Propagation If upgrading isnâ€™t an option, youâ€™ll need to manually propagate the tracing context. The method varies depending on your setup:\\na) Using LangGraph or LangChain Pass the parent config to the child call:\\nCopyimport asyncio\\nfrom langchain_core.runnables import RunnableConfig, RunnableLambda\\n\\n@RunnableLambda\\nasync def my_child_runnable(\\n    inputs: str,\\n    # The config arg (present in parent_runnable below) is optional\\n):\\n    yield \"A\"\\n    yield \"response\"\\n\\n@RunnableLambda\\nasync def parent_runnable(inputs: str, config: RunnableConfig):\\n    async for chunk in my_child_runnable.astream(inputs, config):\\n        yield chunk\\n\\nasync def main():\\n    return [val async for val in parent_runnable.astream(\"call\")]\\n\\nasyncio.run(main())\\n\\nb) Using LangSmith Directly Pass the run tree directly:\\nCopyimport asyncio\\nimport langsmith as ls\\n\\n@ls.traceable\\nasync def my_child_function(inputs: str):\\n    yield \"A\"\\n    yield \"response\"\\n\\n@ls.traceable\\nasync def parent_function(\\n    inputs: str,\\n    # The run tree can be auto-populated by the decorator\\n    run_tree: ls.RunTree,\\n):\\n    async for chunk in my_child_function(inputs, langsmith_extra={\"parent\": run_tree}):\\n        yield chunk\\n\\nasync def main():\\n    return [val async for val in parent_function(\"call\")]\\n\\nasyncio.run(main())\\n\\nc) Combining Decorated Code with LangGraph/LangChain Use a combination of techniques for manual handoff:\\nCopyimport asyncio\\nimport langsmith as ls\\nfrom langchain_core.runnables import RunnableConfig, RunnableLambda\\n\\n@RunnableLambda\\nasync def my_child_runnable(inputs: str):\\n    yield \"A\"\\n    yield \"response\"\\n\\n@ls.traceable\\nasync def my_child_function(inputs: str, run_tree: ls.RunTree):\\n    with ls.tracing_context(parent=run_tree):\\n        async for chunk in my_child_runnable.astream(inputs):\\n            yield chunk\\n\\n@RunnableLambda\\nasync def parent_runnable(inputs: str, config: RunnableConfig):\\n    # @traceable decorated functions can directly accept a RunnableConfig when passed in via \"config\"\\n    async for chunk in my_child_function(inputs, langsmith_extra={\"config\": config}):\\n        yield chunk\\n\\n@ls.traceable\\nasync def parent_function(inputs: str, run_tree: ls.RunTree):\\n    # You can set the tracing context manually\\n    with ls.tracing_context(parent=run_tree):\\n        async for chunk in parent_runnable.astream(inputs):\\n            yield chunk\\n\\nasync def main():\\n    return [val async for val in parent_function(\"call\")]\\n\\nasyncio.run(main())\\n\\n\\n\\n\\u200bContext propagation using threading\\nItâ€™s common to start tracing and want to apply some parallelism on child tasks all within a single trace. Pythonâ€™s stdlib ThreadPoolExecutor by default breaks tracing.\\n\\u200bWhy\\nPythonâ€™s contextvars start empty within new threads. Here are two approaches to handle maintain trace contiguity:\\n\\u200bTo resolve\\n\\n\\nUsing LangSmithâ€™s ContextThreadPoolExecutor\\nLangSmith provides a ContextThreadPoolExecutor that automatically handles context propagation:\\nCopyfrom langsmith.utils import ContextThreadPoolExecutor\\nfrom langsmith import traceable\\n\\n@traceable\\ndef outer_func():\\n    with ContextThreadPoolExecutor() as executor:\\n        inputs = [1, 2]\\n        r = list(executor.map(inner_func, inputs))\\n\\n@traceable\\ndef inner_func(x):\\n    print(x)\\n\\nouter_func()\\n\\n\\n\\nManually providing the parent run tree\\nAlternatively, you can manually pass the parent run tree to the inner function:\\nCopyfrom langsmith import traceable, get_current_run_tree\\nfrom concurrent.futures import ThreadPoolExecutor\\n\\n@traceable\\ndef outer_func():\\n    rt = get_current_run_tree()\\n    with ThreadPoolExecutor() as executor:\\n        r = list(\\n            executor.map(\\n                lambda x: inner_func(x, langsmith_extra={\"parent\": rt}), [1, 2]\\n            )\\n        )\\n\\n@traceable\\ndef inner_func(x):\\n    print(x)\\n\\nouter_func()\\n\\n\\n\\nIn this approach, we use get_current_run_tree() to obtain the current run tree and pass it to the inner function using the langsmith_extra parameter.\\nBoth methods ensure that the inner function calls are correctly aggregated under the initial trace stack, even when executed in separate threads.Was this page helpful?YesNoSuggest editsUpload files with tracesTroubleshoot variable cachingâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Set up online evaluators - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsManual instrumentationConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationFeedback & evaluationSet up online evaluatorsGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageView online evaluatorsConfigure online evaluators1. Navigate to online evaluators2. Name your evaluator3. Create a filter4. (Optional) Configure a sampling rate5. (Optional) Apply rule to past runs6. Select evaluator typeConfigure a LLM-as-a-judge online evaluatorConfigure a custom code evaluatorWrite your evaluation functionTest and save your evaluation functionVideo guideFeedback & evaluationSet up online evaluatorsCopy pageCopy pageRecommended ReadingBefore diving into this content, it might be helpful to read the following:\\nRunning online evaluations\\n\\nOnline evaluations provide real-time feedback on your production traces. This is useful to continuously monitor the performance of your applicationâ€”to identify issues, measure improvements, and ensure consistent quality over time.\\nThere are two types of online evaluations supported in LangSmith:\\n\\nLLM-as-a-judge: Use an LLM to evaluate your traces. Used as a scalable way to provide human-like judgement to your output (e.g. toxicity, hallucination, correctness, etc.).\\nCustom Code: Write an evaluator in Python directly in LangSmith. Often used for validating structure or statistical properties of your data.\\n\\n\\u200bView online evaluators\\nHead to the Tracing Projects tab and select a tracing project. To view existing online evaluators for that project, click on the Evaluators tab.\\n\\n\\u200bConfigure online evaluators\\n\\u200b1. Navigate to online evaluators\\nHead to the Tracing Projects tab and select a tracing project. Click on + New in the top right corner of the tracing project page, then click on New Evaluator.\\n\\u200b2. Name your evaluator\\n\\u200b3. Create a filter\\nFor example, you may want to apply specific evaluators based on:\\n\\nRuns where a user left feedback indicating the response was unsatisfactory.\\nRuns that invoke a specific tool call. See filtering for tool calls for more information.\\nRuns that match a particular piece of metadata (e.g. if you log traces with a plan_type and only want to run evaluations on traces from your enterprise customers). See adding metadata to your traces for more information.\\n\\nFilters on evaluators work the same way as when youâ€™re filtering traces in a project. For more information on filters, you can refer to this guide.\\nItâ€™s often helpful to inspect runs as youâ€™re creating a filter for your evaluator. With the evaluator configuration panel open, you can inspect runs and apply filters to them. Any filters you apply to the runs table will automatically be reflected in filters on your evaluator.\\n\\u200b4. (Optional) Configure a sampling rate\\nConfigure a sampling rate to control the percentage of filtered runs that trigger the automation action. For example, to control costs, you may want to set a filter to only apply the evaluator to 10% of traces. In order to do this, you would set the sampling rate to 0.1.\\n\\u200b5. (Optional) Apply rule to past runs\\nApply rule to past runs by toggling the Apply to past runs and entering a â€œBackfill fromâ€ date. This is only possible upon rule creation. Note: the backfill is processed as a background job, so you will not see the results immediately.\\nIn order to track progress of the backfill, you can view logs for your evaluator by heading to the Evaluators tab within a tracing project and clicking the Logs button for the evaluator you created. Online evaluator logs are similar to automation rule logs.\\n\\nAdd an evaluator name\\nOptionally filter runs that you would like to apply your evaluator on or configure a sampling rate.\\nSelect Apply Evaluator\\n\\n\\u200b6. Select evaluator type\\n\\nConfiguring LLM-as-a-judge evaluators\\nConfiguring custom code evaluators\\n\\n\\u200bConfigure a LLM-as-a-judge online evaluator\\nView this guide to configure an LLM-as-a-judge evaluator.\\n\\u200bConfigure a custom code evaluator\\nSelect custom code evaluator.\\n\\u200bWrite your evaluation function\\nCustom code evaluators restrictions.Allowed Libraries: You can import all standard library functions, as well as the following public packages:Copynumpy (v2.2.2): \"numpy\"\\npandas (v1.5.2): \"pandas\"\\njsonschema (v4.21.1): \"jsonschema\"\\nscipy (v1.14.1): \"scipy\"\\nsklearn (v1.26.4): \"scikit-learn\"\\nNetwork Access: You cannot access the internet from a custom code evaluator.\\nCustom code evaluators must be written inline. We recommend testing locally before setting up your custom code evaluator in LangSmith.\\nIn the UI, you will see a panel that lets you write your code inline, with some starter code:\\n\\nCustom code evaluators take in one argument:\\n\\nA Run (reference). This represents the sampled run to evaluate.\\n\\nThey return a single value:\\n\\nFeedback(s) Dictionary: A dictionary whose keys are the type of feedback you want to return, and values are the score you will give for that feedback key. For example, {\"correctness\": 1, \"silliness\": 0} would create two types of feedback on the run, one saying it is correct, and the other saying it is not silly.\\n\\nIn the below screenshot, you can see an example of a simple function that validates that each run in the experiment has a known json field:\\nPythonJavaScriptCopyimport json\\n\\ndef perform_eval(run):\\n  output_to_validate = run[\\'outputs\\']\\n  is_valid_json = 0\\n\\n  # assert you can serialize/deserialize as json\\n  try:\\n    json.loads(json.dumps(output_to_validate))\\n  except Exception as e:\\n    return { \"formatted\": False }\\n\\n  # assert output facts exist\\n  if \"facts\" not in output_to_validate:\\n    return { \"formatted\": False }\\n\\n  # assert required fields exist\\n  if \"years_mentioned\" not in output_to_validate[\"facts\"]:\\n    return { \"formatted\": False }\\n\\n  return {\"formatted\": True}\\n\\n\\u200bTest and save your evaluation function\\nBefore saving, you can test your evaluator function on a recent run by clicking Test Code to make sure that your code executes properly.\\nOnce you Save, your online evaluator will run over newly sampled runs (or backfilled ones too if you chose the backfill option).\\nIf you prefer a video tutorial, check out the Online Evaluations video from the Introduction to LangSmith Course.\\n\\u200bVideo guide\\nWas this page helpful?YesNoSuggest editsLog user feedback using the SDKMonitor projects with dashboardsâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='How to print detailed logs (Python SDK) - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewSetupInstall on KubernetesInstall on DockerInteract with an installationUpgrade an installationConfigure egress for subscription metricsView trace counts across an organizationLangSmith-managed ClickHouseConfigurationConfigure for scaleEnable TTL & data retentionCreate an Ingress for installations (Kubernetes)Mirror images for your installationUse environment variables for model providersTroubleshootingAuthentication & access controlSet up basic authenticationSet up SSO with OAuth2.0 & OIDCCustomize user managementConfigure custom TLS certificatesUse an existing secret for your installation (Kubernetes)Connect external servicesEnable blob storageConnect to an external ClickHouse databaseConnect to an external PostgreSQL databaseConnect to an external Redis databaseScriptsDelete workspacesDelete organizationsDelete tracesGenerate ClickHouse StatsGenerate query statsRun support queries against PostgreSQLRun support queries against ClickHouseObservabilityExport LangSmith telemetry to your observability backendConfigure your collector for telemetryDeploy an observability stackOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationHow to print detailed logs (Python SDK)Get startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageEnsure logging is configuredIncrease the loggerâ€™s verbosityHow to print detailed logs (Python SDK)Copy pageCopy pageThe LangSmith package uses Pythonâ€™s built in  mechanism to output logs about its behavior to standard output.\\n\\u200bEnsure logging is configured\\nBy default, Jupyter notebooks send logs to standard error instead of standard output, which means your logs will not show up in your notebook cell output unless you configure logging as we do below.\\nIf logging is not currently configured to send logs to standard output for your Python environment, youâ€™ll need to explicitly turn it on as follows:\\nCopyimport logging\\n# Note: this will affect _all_ packages that use python\\'s built-in logging mechanism,\\n#       so may increase your log volume. Pick the right log level for your use case.\\nlogging.basicConfig(level=logging.WARNING)\\n\\n\\u200bIncrease the loggerâ€™s verbosity\\nWhen debugging an issue, itâ€™s helpful to increase logs to a higher level verbosity so more info is outputted to standard output. Python loggers default to using WARNING log level, but you can choose different values to get different levels of verbosity. The values, from least verbose to most, are ERROR, WARNING, INFO, and DEBUG. You can set this as follows:\\nCopyimport langsmith\\nimport logging\\n\\n# Loggers are hierarchical, so setting the log level on \"langsmith\" will\\n# set it on all modules inside the \"langsmith\" package\\nlangsmith_logger = logging.getLogger(\"langsmith\")\\nlangsmith_logger.setLevel(level=logging.DEBUG)\\nWas this page helpful?YesNoSuggest editsâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/rules', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/rules', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Set up automation rules - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsManual instrumentationConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationAutomationsSet up automation rulesGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageView automation rulesCreate a rule1. Navigate to rule creation2. Name your rule3. Create a filter4. Configure a sampling rate5. (Optional) Apply rule to past runs6. Select an action to trigger when the rule is applied.View logs for your automationsVideo guideAutomationsSet up automation rulesCopy pageCopy pageWhile you can manually sift through and process production logs from your LLM application, it often becomes difficult as your application scales to more users.\\nLangSmith provides a powerful feature called Automations that allow you to trigger certain actions on your trace data.\\nAt a high level, automations are defined by a filter, sampling rate, and action.\\nAutomation rules can trigger actions such as: adding traces to a dataset, adding to an annotation queue, triggering a webhook (e.g. for remote evaluations) or extending data retention. Some examples of automations you can set up:\\n\\nSend all traces with negative feedback to an annotation queue for human review\\nSend 10% of all traces to an annotation queue for human review to spot check for issues\\nUpgrade all traces with errors for extended data retention\\n\\nTo configure online evaluations, visit the online evaluations page.\\n\\u200bView automation rules\\nHead to the Tracing Projects tab and select a tracing project. To view existing automation rules for that tracing project, click on the Automations tab.\\n\\n\\u200bCreate a rule\\n\\n\\u200b1. Navigate to rule creation\\nHead to the Tracing Projects tab and select a tracing project. Click on + New in the top right corner of the tracing project page, then click on New Automation.\\n\\u200b2. Name your rule\\n\\u200b3. Create a filter\\nAutomation rule filters work the same way as filters applied to traces in the project. For more information on filters, you can refer to this guide\\n\\u200b4. Configure a sampling rate\\nConfigure a sampling rate to control the percentage of filtered runs that trigger the automation action.\\nYou can specify a sampling rate between 0 and 1 for automations. This will control the percent of the filtered runs that are sent to an automation action. For example, if you set the sampling rate to 0.5, then 50% of the traces that pass the filter will be sent to the action.\\n\\u200b5. (Optional) Apply rule to past runs\\nApply rule to past runs by toggling the Apply to past runs and entering a â€œBackfill fromâ€ date. This is only possible upon rule creation. Note: the backfill is processed as a background job, so you will not see the results immediately. In order to track progress of the backfill, you can view logs for your automations\\n\\u200b6. Select an action to trigger when the rule is applied.\\nThere are four actions you can take with an automation rule:\\n\\nAdd to dataset: Add the inputs and outputs of the trace to a dataset.\\nAdd to annotation queue: Add the trace to an annotation queue.\\nTrigger webhook: Trigger a webhook with the trace data. For more information on webhooks, you can refer to this guide.\\nExtend data retention: Extends the data retention period on matching traces that use base retention (see data retention docs for more details).\\nNote that all other rules will also extend data retention on matching traces through the\\nauto-upgrade mechanism described in the aforementioned data retention docs,\\nbut this rule takes no additional action.\\n\\n\\u200bView logs for your automations\\nLogs allow you to gain confidence that your rules are working as expected. You can view logs for your automations by heading to the Automations tab within a tracing project and clicking the Logs button for the rule you created.\\nThe logs tab allows you to:\\n\\nView all runs processed by a given rule for the time period selected\\nIf a particular rule execution has triggered an error, you can view the error message by hovering over the error icon\\nYou can monitor the progress of a backfill job by filtering to the ruleâ€™s creation timestamp. This is because the backfill starts from when the rule was created.\\nInspect the run that the automation rule applied to using the View run button. For rules that add runs as examples to datasets, you can view the example produced.\\n\\n\\n\\u200bVideo guide\\nWas this page helpful?YesNoSuggest editsBulk export trace dataConfigure webhook notifications for rulesâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/sample_traces', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/sample_traces', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Set a sampling rate for traces - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsManual instrumentationConfiguration & troubleshootingProject & environment settingsLog traces to a specific projectTrace without env varsSet a sampling rate for tracesAdvanced tracing techniquesData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationProject & environment settingsSet a sampling rate for tracesGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageSet a global sampling rateSet different sampling rates per clientConfiguration & troubleshootingProject & environment settingsSet a sampling rate for tracesCopy pageCopy pageWhen working with high-volume applications, you may not want to log every trace to LangSmith. Sampling rates allow you to control what percentage of traces are logged, helping you balance observability needs with cost considerations.\\n\\u200bSet a global sampling rate\\nThis section is relevant for those using the LangSmith SDK or LangChain, not for those logging directly with the LangSmith API.\\nBy default, all traces are logged to LangSmith. To down-sample the number of traces logged to LangSmith, set the LANGSMITH_TRACING_SAMPLING_RATE environment variable to any float between 0 (no traces) and 1 (all traces). For instance, setting the following environment variable will log 75% of the traces.\\nCopyexport LANGSMITH_TRACING_SAMPLING_RATE=0.75\\n\\nThis works for the traceable decorator and RunTree objects.\\n\\u200bSet different sampling rates per client\\nYou can also set sampling rates on specific Client instances and use the tracing_context context manager:\\nCopyfrom langsmith import Client, tracing_context\\n\\n# Create clients with different sampling rates\\nclient_1 = Client(tracing_sampling_rate=0.5)  # 50% sampling\\nclient_2 = Client(tracing_sampling_rate=0.25)  # 25% sampling\\nclient_no_trace = Client(tracing_sampling_rate=0.0)  # No tracing\\n\\n# Use different sampling rates for different operations\\nwith tracing_context(client=client_1):\\n    # Your code here - will be traced with 50% sampling rate\\n    agent_1.invoke(...)\\n\\nwith tracing_context(client=client_2):\\n    # Your code here - will be traced with 25% sampling rate\\n    agent_1.invoke(...)\\n\\nwith tracing_context(client=client_no_trace):\\n    # Your code here - will not be traced\\n    agent_1.invoke(...)\\n\\nThis allows you to control sampling rates at the operation level.Was this page helpful?YesNoSuggest editsTrace without env varsImplement distributed tracingâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Trace JS functions in serverless environments - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsManual instrumentationConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesImplement distributed tracingThreadsTrace JS functions in serverless environmentsLog multimodal tracesTrace generator functionsData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationAdvanced tracing techniquesTrace JS functions in serverless environmentsGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageRate limits at high concurrency\\x0bConfiguration & troubleshootingAdvanced tracing techniquesTrace JS functions in serverless environmentsCopy pageCopy pageThis section is relevant for those using the LangSmith JS SDK version 0.2.0 and higher. If you are tracing using LangChain.js or LangGraph.js in serverless environments, see this guide.\\nWhen tracing JavaScript functions, LangSmith will trace runs in the background by default to avoid adding latency. In serverless environments where the execution context may be terminated abruptly, itâ€™s important to ensure that all tracing data is properly flushed before the function completes.\\nTo make sure this occurs, you can either:\\n\\nSet an environment variable named LANGSMITH_TRACING_BACKGROUND to \"false\". This will cause your traced functions to wait for tracing to complete before returning.\\n\\nNote that this is named differently from the environment variable in LangChain.js because LangSmith can be used without LangChain.\\n\\n\\nPass a custom client into your traced runs and await the client.awaitPendingTraceBatches(); method.\\n\\nHereâ€™s an example of using awaitPendingTraceBatches alongside the traceable method:\\nCopyimport { Client } from \"langsmith\";\\nimport { traceable } from \"langsmith/traceable\";\\nconst langsmithClient = new Client({});\\nconst tracedFn = traceable(\\n  async () => {\\n    return \"Some return value\";\\n  },\\n  {\\n    client: langsmithClient,\\n  }\\n);\\nconst res = await tracedFn();\\nawait langsmithClient.awaitPendingTraceBatches();\\n\\n\\u200bRate limits at high concurrency\\x0b\\nBy default, the LangSmith client will batch operations as your traced run executions, sending a new batch every few milliseconds.\\nThis works well in most situations, but if your traced function is long-running and you have very high concurrency, you may also hit rate limits related to overall request count.\\nIf you are seeing rate limit errors related to this, you can try setting manualFlushMode: true in your client like this:\\nCopyimport { Client } from \"langsmith\";\\nconst langsmithClient = new Client({  manualFlushMode: true,});\\nconst myTracedFunc = traceable(\\n  async () => {\\n    // Your logic here...\\n  },\\n  { client: langsmithClient }\\n);\\n\\nAnd then manually calling client.flush() like this before your serverless function closes:\\nCopytry {\\n  await myTracedFunc();\\n} finally {\\n  await langsmithClient.flush();\\n}\\n\\nNote that this will prevent runs from appearing in the LangSmith UI until you call .flush().Was this page helpful?YesNoSuggest editsThreadsLog multimodal tracesâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/share_trace', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/share_trace', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Share or unshare a trace publicly - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsManual instrumentationConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationViewing & managing tracesShare or unshare a trace publiclyGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumViewing & managing tracesShare or unshare a trace publiclyCopy pageCopy pageSharing a trace publicly will make it accessible to anyone with the link. Make sure youâ€™re not sharing sensitive information.If your self-hosted or hybrid LangSmith deployment is within a VPC, then the public link is accessible only to members authenticated within your VPC. For enhanced security, we recommend configuring your instance with a private URL accessible only to users with access to your network.\\nTo share a trace publicly, simply click on the Share button in the upper right hand side of any trace view.\\n\\nThis will open a dialog where you can copy the link to the trace.\\nShared traces will be accessible to anyone with the link, even if they donâ€™t have a LangSmith account. They will be able to view the trace, but not edit it.\\nTo â€œunshareâ€ a trace, either:\\n\\n\\nClick on Unshare by clicking on Public in the upper right hand corner of any publicly shared trace, then Unshare in the dialog.\\n\\n\\n\\nNavigate to your organizationâ€™s list of publicly shared traces, by clicking on Settings -> Shared URLs, then click on Unshare next to the trace you want to unshare.\\n\\n\\nWas this page helpful?YesNoSuggest editsCompare tracesView server logs for a traceâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/threads', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/threads', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Configure threads - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsManual instrumentationConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesImplement distributed tracingThreadsTrace JS functions in serverless environmentsLog multimodal tracesTrace generator functionsData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationAdvanced tracing techniquesConfigure threadsGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageGroup traces into threadsCode exampleView threadsConfiguration & troubleshootingAdvanced tracing techniquesConfigure threadsCopy pageCopy pageRecommended ReadingBefore diving into this content, it might be helpful to read the following:\\nAdd metadata and tags to traces\\n\\nMany LLM applications have a chatbot-like interface in which the user and the LLM application engage in a multi-turn conversation. In order to track these conversations, you can use the Threads feature in LangSmith.\\n\\u200bGroup traces into threads\\nA Thread is a sequence of traces representing a single conversation. Each response is represented as its own trace, but these traces are linked together by being part of the same thread.\\nTo associate traces together, you need to pass in a special metadata key where the value is the unique identifier for that thread.\\nThe key value is the unique identifier for that conversation.\\nThe key name should be one of:\\n\\nsession_id\\nthread_id\\nconversation_id.\\n\\nThe value can be any string you want, but we recommend using UUIDs, such as f47ac10b-58cc-4372-a567-0e02b2c3d479.\\n\\u200bCode example\\nThis example demonstrates how to log and retrieve conversation history from LangSmith to maintain long-running chats.\\nYou can add metadata to your traces in LangSmith in a variety of ways, this code will show how to do so dynamically, but read the\\npreviously linked guide to learn about all the ways you can add thread identifier metadata to your traces.\\nPythonTypeScriptCopyimport openai\\nfrom langsmith import traceable\\nfrom langsmith import Client\\nimport langsmith as ls\\nfrom langsmith.wrappers import wrap_openai\\n\\nclient = wrap_openai(openai.Client())\\nlangsmith_client = Client()\\n\\n# Config used for this example\\nlangsmith_project = \"project-with-threads\"\\n\\nsession_id = \"thread-id-1\"\\n\\nlangsmith_extra={\"project_name\": langsmith_project, \"metadata\":{\"session_id\": session_id}}\\n\\n# gets a history of all LLM calls in the thread to construct conversation history\\ndef get_thread_history(thread_id: str, project_name: str):\\n    # Filter runs by the specific thread and project\\n    filter_string = f\\'and(in(metadata_key, [\"session_id\",\"conversation_id\",\"thread_id\"]), eq(metadata_value, \"{thread_id}\"))\\'\\n    # Only grab the LLM runs\\n    runs = [r for r in langsmith_client.list_runs(project_name=project_name, filter=filter_string, run_type=\"llm\")]\\n\\n    # Sort by start time to get the most recent interaction\\n    runs = sorted(runs, key=lambda run: run.start_time, reverse=True)\\n    # The current state of the conversation\\n    return runs[0].inputs[\\'messages\\'] + [runs[0].outputs[\\'choices\\'][0][\\'message\\']]\\n\\n# if an existing conversation is continued, this function looks up the current run\\'s metadata to get the session_id, calls get_thread_history, and appends the new user question before making a call to the chat model\\n@traceable(name=\"Chat Bot\")\\ndef chat_pipeline(question: str, get_chat_history: bool = False):\\n    # Whether to continue an existing thread or start a new one\\n    if get_chat_history:\\n        run_tree = ls.get_current_run_tree()\\n        messages = get_thread_history(run_tree.extra[\"metadata\"][\"session_id\"],run_tree.session_name) + [{\"role\": \"user\", \"content\": question}]\\n    else:\\n        messages = [{\"role\": \"user\", \"content\": question}]\\n\\n    # Invoke the model\\n    chat_completion = client.chat.completions.create(\\n        model=\"gpt-4o-mini\", messages=messages\\n    )\\n    return chat_completion.choices[0].message.content\\n\\n# Start the conversation\\nchat_pipeline(\"Hi, my name is Bob\", langsmith_extra=langsmith_extra)\\n\\nAfter waiting a few seconds, you can make the following calls to continue the conversation. By passing getChatHistory: true,\\nyou can continue the conversation from where it left off. This means that the LLM will receive the entire message history and respond to it,\\ninstead of just responding to the latest message.\\nPythonTypeScriptCopy# Continue the conversation (WAIT A FEW SECONDS BEFORE RUNNING THIS SO THE FIRST TRACE CAN BE INGESTED)\\nchat_pipeline(\"What is my name?\", get_chat_history=True, langsmith_extra=langsmith_extra)\\n\\n# Keep the conversation going (WAIT A FEW SECONDS BEFORE RUNNING THIS SO THE PREVIOUS TRACE CAN BE INGESTED)\\nchat_pipeline(\"What was the first message I sent you\", get_chat_history=True, langsmith_extra=langsmith_extra)\\n\\n\\u200bView threads\\nYou can view threads by clicking on the Threads tab in any project details page. You will then see a list of all threads, sorted by the most recent activity.\\n\\nYou can then click into a particular thread. This will open the history for a particular thread. If your threads are formatted as chat messages, you will a chatbot-like UI where you can see a history of inputs and outputs.\\n\\nYou can open up the trace or annotate the trace in a side panel by clicking on Annotate and Open trace, respectively.Was this page helpful?YesNoSuggest editsImplement distributed tracingTrace JS functions in serverless environmentsâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Troubleshoot variable caching - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsManual instrumentationConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyTroubleshooting guidesTroubleshoot trace nestingTroubleshoot variable cachingBeta LangSmith Collector-ProxyViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationTroubleshooting guidesTroubleshoot variable cachingGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this page1. Verify Your Environment Variables2. Clear the cache3. Reload the Environment VariablesConfiguration & troubleshootingTroubleshooting guidesTroubleshoot variable cachingCopy pageCopy pageIf youâ€™re not seeing traces in your tracing project or notice traces logged to the wrong project/workspace, the issue might be due to LangSmithâ€™s default environment variable caching. This is especially common when running LangSmith within a Jupyter notebook. Follow these steps to diagnose and resolve the issue:\\n\\u200b1. Verify Your Environment Variables\\nFirst, check that the environment variables are set correctly by running:\\nCopyimport os\\nprint(os.getenv(\"LANGSMITH_PROJECT\"))\\nprint(os.getenv(\"LANGSMITH_TRACING\"))\\nprint(os.getenv(\"LANGSMITH_ENDPOINT\"))\\nprint(os.getenv(\"LANGSMITH_API_KEY\"))\\n\\nIf the output does not match whatâ€™s defined in your .env file, itâ€™s likely due to environment variable caching.\\n\\u200b2. Clear the cache\\nClear the cached environment variables with the following command:\\nCopyutils.get_env_var.cache_clear()\\n\\n\\u200b3. Reload the Environment Variables\\nReload your environment variables from the .env file by executing:\\nCopyfrom dotenv import load_dotenv\\nimport os\\nload_dotenv(<path to .env file>, override=True)\\n\\nAfter reloading, your environment variables should be set correctly.\\nIf you continue to experience issues, please reach out to us via a shared Slack channel or email support (available for Plus and Enterprise plans), or in the LangChain Forum.Was this page helpful?YesNoSuggest editsTroubleshoot trace nestingBeta LangSmith Collector-ProxyâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Trace generator functions - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsManual instrumentationConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesImplement distributed tracingThreadsTrace JS functions in serverless environmentsLog multimodal tracesTrace generator functionsData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationAdvanced tracing techniquesTrace generator functionsGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageAggregate Results\\x0bConfiguration & troubleshootingAdvanced tracing techniquesTrace generator functionsCopy pageCopy pageIn most LLM applications, you will want to stream outputs to minimize the time to the first token seen by the user.\\nLangSmithâ€™s tracing functionality natively supports streamed outputs via generator functions. Below is an example.\\nPythonTypeScriptCopyfrom langsmith import traceable\\n@traceable\\ndef my_generator():\\n  for chunk in [\"Hello\", \"World\", \"!\"]:\\n      yield chunk\\n# Stream to the user\\nfor output in my_generator():\\n  print(output)\\n# It also works with async functions\\nimport asyncio\\n@traceable\\nasync def my_async_generator():\\n  for chunk in [\"Hello\", \"World\", \"!\"]:\\n      yield chunk\\n# Stream to the user\\nasync def main():\\n  async for output in my_async_generator():\\n      print(output)\\nasyncio.run(main())\\n\\n\\u200bAggregate Results\\x0b\\nBy default, the outputs of the traced function are aggregated into a single array in LangSmith. If you want to customize how it is stored (for instance, concatenating the outputs into a single string), you can use the aggregate option (reduce_fn in python). This is especially useful for aggregating streamed LLM outputs.\\nAggregating outputs only impacts the traced representation of the outputs. It doesn not alter the values returned by your function.\\nPythonTypeScriptCopyfrom langsmith import traceable\\ndef concatenate_strings(outputs: list):\\n  return \"\".join(outputs)\\n@traceable(reduce_fn=concatenate_strings)\\ndef my_generator():\\n  for chunk in [\"Hello\", \"World\", \"!\"]:\\n      yield chunk\\n# Stream to the user\\nfor output in my_generator():\\n  print(output)\\n# It also works with async functions\\nimport asyncio\\n@traceable(reduce_fn=concatenate_strings)\\nasync def my_async_generator():\\n  for chunk in [\"Hello\", \"World\", \"!\"]:\\n      yield chunk\\n# Stream to the user\\nasync def main():\\n  async for output in my_async_generator():\\n      print(output)\\nasyncio.run(main())\\nWas this page helpful?YesNoSuggest editsLog multimodal tracesAdd metadata and tags to tracesâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Trace with API - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsManual instrumentationCustom instrumentationTrace with APILog custom LLM tracesLog retriever tracesConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationManual instrumentationTrace with APIGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageBasic tracingBatch IngestionTracing setupManual instrumentationTrace with APICopy pageCopy pageLearn how to trace your LLM applications using the LangSmith API directly.\\nIt is highly recommended to use our Python or TypeScript SDKs to send traces to LangSmith. We have designed these SDKs with optimizations like batching and backgrounding to ensure that your applicationâ€™s performance is not impacted by sending traces to LangSmith. However, if you are unable to use our SDKs, you can use the LangSmith REST API to send traces. Performance may be impacted if you send traces synchronously in your application. This guide will show you how to trace a request using the LangSmith REST API. Please view our API documentation  for a full list of endpoints and request/response schemas.\\n\\u200bBasic tracing\\nThe simplest way to log runs is via the POST and PATCH /runs endpoint. These routes expect minimal contextual information about the tree structure to\\nWhen using the LangSmith REST API, you will need to provide your API key in the request headers as \"x-api-key\".If your API key is linked to multiple workspaces, you will need to specify the workspace being used in the header with \"x-tenant-id\".In the simple example, you do not need to set the dotted_order opr trace_id fields in the request body. These fields will be automatically generated by the system. Though this is simpler, it is slower and has a lower rate limit in LangSmith.\\nThe following example shows how you might leverage our API directly in Python. The same principles apply to other languages.\\nCopyimport openai\\nimport os\\nimport requests\\nfrom datetime import datetime, timezone\\nfrom uuid import uuid4\\n\\n# Send your API Key in the request headers\\nheaders = {\\n    \"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"],\\n    \"x-tenant-id\": os.environ[\"LANGSMITH_WORKSPACE_ID\"]\\n}\\n\\ndef post_run(run_id, name, run_type, inputs, parent_id=None):\\n    \"\"\"Function to post a new run to the API.\"\"\"\\n    data = {\\n        \"id\": run_id.hex,\\n        \"name\": name,\\n        \"run_type\": run_type,\\n        \"inputs\": inputs,\\n        \"start_time\": datetime.utcnow().isoformat(),\\n        # \"session_name\": \"project-name\",  # the name of the project to trace to\\n        # \"session_id\": \"project-id\",  # the ID of the project to trace to. specify one of session_name or session_id\\n    }\\n    if parent_id:\\n        data[\"parent_run_id\"] = parent_id.hex\\n\\n    requests.post(\\n        \"https://api.smith.langchain.com/runs\",  # Update appropriately for self-hosted installations or the EU region\\n        json=data,\\n        headers=headers\\n    )\\n\\ndef patch_run(run_id, outputs):\\n    \"\"\"Function to patch a run with outputs.\"\"\"\\n    requests.patch(\\n        f\"https://api.smith.langchain.com/runs/{run_id}\",\\n        json={\\n            \"outputs\": outputs,\\n            \"end_time\": datetime.now(timezone.utc).isoformat(),\\n        },\\n        headers=headers,\\n    )\\n\\n# This can be a user input to your app\\nquestion = \"Can you summarize this morning\\'s meetings?\"\\n\\n# This can be retrieved in a retrieval step\\ncontext = \"During this morning\\'s meeting, we solved all world conflict.\"\\n\\nmessages = [\\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Please respond to the user\\'s request only based on the given context.\"},\\n    {\"role\": \"user\", \"content\": f\"Question: {question}\\\\nContext: {context}\"}\\n]\\n\\n# Create parent run\\nparent_run_id = uuid4()\\npost_run(parent_run_id, \"Chat Pipeline\", \"chain\", {\"question\": question})\\n\\n# Create child run\\nchild_run_id = uuid4()\\npost_run(child_run_id, \"OpenAI Call\", \"llm\", {\"messages\": messages}, parent_run_id)\\n\\n# Generate a completion\\nclient = openai.Client()\\nchat_completion = client.chat.completions.create(\\n    model=\"gpt-4o-mini\",\\n    messages=messages\\n)\\n\\n# End runs\\npatch_run(child_run_id, chat_completion.dict())\\npatch_run(parent_run_id, {\"answer\": chat_completion.choices[0].message.content})\\n\\nSee the doc on the Run (span) data format for more information.\\n\\u200bBatch Ingestion\\nFor faster ingestion of runs and higher rate limits, you can use the POST /runs/multipart link endpoint. Below is an example. It requires orjson (for fast json ) and requests_toolbelt to run\\nCopyimport json\\nimport os\\nimport uuid\\nfrom datetime import datetime, timezone\\nfrom typing import Dict, List, Optional\\nimport requests\\nfrom requests_toolbelt import MultipartEncoder\\n\\ndef create_dotted_order(\\n    start_time: Optional[datetime] = None,\\n    run_id: Optional[uuid.UUID] = None\\n) -> str:\\n    \"\"\"Create a dotted order string for run ordering and hierarchy.\\n\\n    The dotted order is used to establish the sequence and relationships between runs.\\n    It combines a timestamp with a unique identifier to ensure proper ordering and tracing.\\n    \"\"\"\\n    st = start_time or datetime.now(timezone.utc)\\n    id_ = run_id or uuid.uuid4()\\n    return f\"{st.strftime(\\'%Y%m%dT%H%M%S%fZ\\')}{id_}\"\\n\\ndef create_run_base(\\n    name: str,\\n    run_type: str,\\n    inputs: dict,\\n    start_time: datetime\\n) -> dict:\\n    \"\"\"Create the base structure for a run.\"\"\"\\n    run_id = uuid.uuid4()\\n    return {\\n        \"id\": str(run_id),\\n        \"trace_id\": str(run_id),\\n        \"name\": name,\\n        \"start_time\": start_time.isoformat(),\\n        \"inputs\": inputs,\\n        \"run_type\": run_type,\\n    }\\n\\ndef construct_run(\\n    name: str,\\n    run_type: str,\\n    inputs: dict,\\n    parent_dotted_order: Optional[str] = None,\\n) -> dict:\\n    \"\"\"Construct a run dictionary with the given parameters.\\n\\n    This function creates a run with a unique ID and dotted order, establishing its place\\n    in the trace hierarchy if it\\'s a child run.\\n    \"\"\"\\n    start_time = datetime.now(timezone.utc)\\n    run = create_run_base(name, run_type, inputs, start_time)\\n    current_dotted_order = create_dotted_order(start_time, uuid.UUID(run[\"id\"]))\\n\\n    if parent_dotted_order:\\n        current_dotted_order = f\"{parent_dotted_order}.{current_dotted_order}\"\\n        run[\"trace_id\"] = parent_dotted_order.split(\".\")[0].split(\"Z\")[1]\\n        run[\"parent_run_id\"] = parent_dotted_order.split(\".\")[-1].split(\"Z\")[1]\\n\\n    run[\"dotted_order\"] = current_dotted_order\\n    return run\\n\\ndef serialize_run(operation: str, run_data: dict) -> List[tuple]:\\n    \"\"\"Serialize a run for the multipart request.\\n\\n    This function separates the run data into parts for efficient transmission and storage.\\n    The main run data and optional fields (inputs, outputs, events) are serialized separately.\\n    \"\"\"\\n    run_id = run_data.get(\"id\", str(uuid.uuid4()))\\n\\n    # Separate optional fields\\n    inputs = run_data.pop(\"inputs\", None)\\n    outputs = run_data.pop(\"outputs\", None)\\n    events = run_data.pop(\"events\", None)\\n\\n    parts = []\\n\\n    # Serialize main run data\\n    run_data_json = json.dumps(run_data).encode(\"utf-8\")\\n    parts.append(\\n        (\\n            f\"{operation}.{run_id}\",\\n            (\\n                None,\\n                run_data_json,\\n                \"application/json\",\\n                {\"Content-Length\": str(len(run_data_json))},\\n            ),\\n        )\\n    )\\n\\n    # Serialize optional fields\\n    for key, value in [(\"inputs\", inputs), (\"outputs\", outputs), (\"events\", events)]:\\n        if value:\\n            serialized_value = json.dumps(value).encode(\"utf-8\")\\n            parts.append(\\n                (\\n                    f\"{operation}.{run_id}.{key}\",\\n                    (\\n                        None,\\n                        serialized_value,\\n                        \"application/json\",\\n                        {\"Content-Length\": str(len(serialized_value))},\\n                    ),\\n                )\\n            )\\n\\n    return parts\\n\\ndef batch_ingest_runs(\\n    api_url: str,\\n    api_key: str,\\n    posts: Optional[List[dict]] = None,\\n    patches: Optional[List[dict]] = None,\\n) -> None:\\n    \"\"\"Ingest multiple runs in a single batch request.\\n\\n    This function handles both creating new runs (posts) and updating existing runs (patches).\\n    It\\'s more efficient for ingesting multiple runs compared to individual API calls.\\n    \"\"\"\\n    boundary = uuid.uuid4().hex\\n    all_parts = []\\n\\n    for operation, runs in zip((\"post\", \"patch\"), (posts, patches)):\\n        if runs:\\n            all_parts.extend(\\n                [part for run in runs for part in serialize_run(operation, run)]\\n            )\\n\\n    encoder = MultipartEncoder(fields=all_parts, boundary=boundary)\\n    headers = {\"Content-Type\": encoder.content_type, \"x-api-key\": api_key}\\n\\n    try:\\n        response = requests.post(\\n            f\"{api_url}/runs/multipart\",\\n            data=encoder,\\n            headers=headers\\n        )\\n        response.raise_for_status()\\n        print(\"Successfully ingested runs.\")\\n    except requests.RequestException as e:\\n        print(f\"Error ingesting runs: {e}\")\\n        # In a production environment, you might want to log this error or handle it more robustly\\n\\n# Configure API URL and key\\n# For production use, consider using a configuration file or environment variables\\napi_url = \"https://api.smith.langchain.com\"\\napi_key = os.environ.get(\"LANGSMITH_API_KEY\")\\n\\nif not api_key:\\n    raise ValueError(\"LANGSMITH_API_KEY environment variable is not set\")\\n\\n# Create a parent run\\nparent_run = construct_run(\\n    name=\"Parent Run\",\\n    run_type=\"chain\",\\n    inputs={\"main_question\": \"Tell me about France\"},\\n)\\n\\n# Create a child run, linked to the parent\\nchild_run = construct_run(\\n    name=\"Child Run\",\\n    run_type=\"llm\",\\n    inputs={\"question\": \"What is the capital of France?\"},\\n    parent_dotted_order=parent_run[\"dotted_order\"],\\n)\\n\\n# First, post the runs to create them\\nposts = [parent_run, child_run]\\nbatch_ingest_runs(api_url, api_key, posts=posts)\\n\\n# Then, update the runs with their end times and any outputs\\nchild_run_update = {\\n    **child_run,\\n    \"end_time\": datetime.now(timezone.utc).isoformat(),\\n    \"outputs\": {\"answer\": \"Paris is the capital of France.\"},\\n}\\n\\nparent_run_update = {\\n    **parent_run,\\n    \"end_time\": datetime.now(timezone.utc).isoformat(),\\n    \"outputs\": {\"summary\": \"Discussion about France, including its capital.\"},\\n}\\n\\npatches = [parent_run_update, child_run_update]\\nbatch_ingest_runs(api_url, api_key, patches=patches)\\n\\n# Note: This example requires the `requests` and `requests_toolbelt` libraries.\\n# You can install them using pip:\\n# pip install requests requests_toolbelt\\nWas this page helpful?YesNoSuggest editsCustom instrumentationLog custom LLM tracesâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Trace with Instructor - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsOverviewLangChainLangGraphAnthropic (Python only)OpenAIAutoGenClaude CodeCrewAIGoogle ADKInstructor (Python only)OpenAI Agents SDKOpenTelemetrySemantic KernelVercel AI SDKManual instrumentationConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationIntegrationsTrace with InstructorGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumTracing setupIntegrationsTrace with InstructorCopy pageCopy pageLangSmith provides a convenient integration with Instructor, a popular open-source library for generating structured outputs with LLMs.\\nIn order to use, you first need to set your LangSmith API key.\\nCopyexport LANGSMITH_API_KEY=<your-api-key>\\n# For LangSmith API keys linked to multiple workspaces, set the LANGSMITH_WORKSPACE_ID environment variable to specify which workspace to use.\\nexport LANGSMITH_WORKSPACE_ID=<your-workspace-id>\\n\\nNext, you will need to install the LangSmith SDK:\\npipuvCopypip install -U langsmith\\n\\nWrap your OpenAI client with langsmith.wrappers.wrap_openai\\nCopyfrom openai import OpenAI\\nfrom langsmith import wrappers\\n\\nclient = wrappers.wrap_openai(OpenAI())\\n\\nAfter this, you can patch the wrapped OpenAI client using instructor:\\nCopyimport instructor\\n\\nclient = instructor.patch(client)\\n\\nNow, you can use instructor as you normally would, but now everything is logged to LangSmith!\\nCopyfrom pydantic import BaseModel\\n\\n\\nclass UserDetail(BaseModel):\\n    name: str\\n    age: int\\n\\n\\nuser = client.chat.completions.create(\\n    model=\"gpt-4o-mini\",\\n    response_model=UserDetail,\\n    messages=[\\n        {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"},\\n    ]\\n)\\n\\nOftentimes, you use instructor inside of other functions.\\nYou can get nested traces by using this wrapped client and decorating those functions with @traceable.\\nPlease see this guide for more information on how to annotate your code for tracing with the @traceable decorator.\\nCopy# You can customize the run name with the `name` keyword argument\\n@traceable(name=\"Extract User Details\")\\ndef my_function(text: str) -> UserDetail:\\n    return client.chat.completions.create(\\n        model=\"gpt-4o-mini\",\\n        response_model=UserDetail,\\n        messages=[\\n            {\"role\": \"user\", \"content\": f\"Extract {text}\"},\\n        ]\\n    )\\n\\nmy_function(\"Jason is 25 years old\")\\nWas this page helpful?YesNoSuggest editsGoogle ADKOpenAI Agents SDKâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Trace with LangChain (Python and JS/TS) - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsOverviewLangChainLangGraphAnthropic (Python only)OpenAIAutoGenClaude CodeCrewAIGoogle ADKInstructor (Python only)OpenAI Agents SDKOpenTelemetrySemantic KernelVercel AI SDKManual instrumentationConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationIntegrationsTrace with LangChain (Python and JS/TS)Get startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageInstallationQuick start1. Configure your environment2. Log a trace3. View your traceTrace selectivelyLog to a specific projectStaticallyDynamicallyAdd metadata and tags to tracesCustomize run nameCustomize run IDAccess run (span) ID for LangChain invocationsEnsure all traces are submitted before exitingTrace without setting environment variablesDistributed tracing with LangChain (Python)Interoperability between LangChain (Python) and LangSmith SDKInteroperability between LangChain.JS and LangSmith SDKTracing LangChain objects inside traceable (JS only)Tracing LangChain child runs via traceable / RunTree API (JS only)Tracing setupIntegrationsTrace with LangChain (Python and JS/TS)Copy pageCopy pageLangSmith integrates seamlessly with LangChain (Python and JavaScript), the popular open-source framework for building LLM applications.\\n\\u200bInstallation\\nInstall the core library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below).\\nFor a full list of packages available, see the LangChain Python docs and LangChain JS docs.\\npipyarnnpmpnpmCopypip install langchain_openai langchain_core\\n\\n\\u200bQuick start\\n\\u200b1. Configure your environment\\nPythonTypeScriptCopyexport LANGSMITH_TRACING=true\\nexport LANGSMITH_API_KEY=<your-api-key>\\n# This example uses OpenAI, but you can use any LLM provider of choice\\nexport OPENAI_API_KEY=<your-openai-api-key>\\n# For LangSmith API keys linked to multiple workspaces, set the LANGSMITH_WORKSPACE_ID environment variable to specify which workspace to use.\\nexport LANGSMITH_WORKSPACE_ID=<your-workspace-id>\\n\\nIf you are using LangChain.js with LangSmith and are not in a serverless environment, we also recommend setting the following explicitly to reduce latency:export LANGCHAIN_CALLBACKS_BACKGROUND=trueIf you are in a serverless environment, we recommend setting the reverse to allow tracing to finish before your function ends:export LANGCHAIN_CALLBACKS_BACKGROUND=falseSee this LangChain.js guide for more information.\\n\\u200b2. Log a trace\\nNo extra code is needed to log a trace to LangSmith. Just run your LangChain code as you normally would.\\nPythonTypeScriptCopyfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\nprompt = ChatPromptTemplate.from_messages([\\n    (\"system\", \"You are a helpful assistant. Please respond to the user\\'s request only based on the given context.\"),\\n    (\"user\", \"Question: {question}\\\\nContext: {context}\")\\n])\\n\\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\\noutput_parser = StrOutputParser()\\nchain = prompt | model | output_parser\\n\\nquestion = \"Can you summarize this morning\\'s meetings?\"\\ncontext = \"During this morning\\'s meeting, we solved all world conflict.\"\\n\\nchain.invoke({\"question\": question, \"context\": context})\\n\\n\\u200b3. View your trace\\nBy default, the trace will be logged to the project with the name default. An example of a trace logged using the above code is made public and can be viewed here.\\n\\n\\u200bTrace selectively\\nThe previous section showed how to trace all invocations of a LangChain runnables within your applications by setting a single environment variable. While this is a convenient way to get started, you may want to trace only specific invocations or parts of your application.\\nThere are two ways to do this in Python: by manually passing in a LangChainTracer (reference docs) instance as a callback, or by using the tracing_context context manager (reference docs).\\nIn JS/TS, you can pass a LangChainTracer (reference docs) instance as a callback.\\nPythonTypeScriptCopy# You can opt-in to specific invocations..\\nimport langsmith as ls\\n\\nwith ls.tracing_context(enabled=True):\\n    chain.invoke({\"question\": \"Am I using a callback?\", \"context\": \"I\\'m using a callback\"})\\n\\n# This will NOT be traced (assuming LANGSMITH_TRACING is not set)\\nchain.invoke({\"question\": \"Am I being traced?\", \"context\": \"I\\'m not being traced\"})\\n\\n# This would not be traced, even if LANGSMITH_TRACING=true\\nwith ls.tracing_context(enabled=False):\\n    chain.invoke({\"question\": \"Am I being traced?\", \"context\": \"I\\'m not being traced\"})\\n\\n\\u200bLog to a specific project\\n\\u200bStatically\\nAs mentioned in the tracing conceptual guide LangSmith uses the concept of a Project to group traces. If left unspecified, the tracer project is set to default. You can set the LANGSMITH_PROJECT environment variable to configure a custom project name for an entire application run. This should be done before executing your application.\\nCopyexport LANGSMITH_PROJECT=my-project\\n\\nThe LANGSMITH_PROJECT flag is only supported in JS SDK versions >= 0.2.16, use LANGCHAIN_PROJECT instead if you are using an older version.\\n\\u200bDynamically\\nThis largely builds off of the previous section and allows you to set the project name for a specific LangChainTracer instance or as parameters to the tracing_context context manager in Python.\\nPythonTypeScriptCopy# You can set the project name using the project_name parameter.\\nimport langsmith as ls\\n\\nwith ls.tracing_context(project_name=\"My Project\", enabled=True):\\n    chain.invoke({\"question\": \"Am I using a context manager?\", \"context\": \"I\\'m using a context manager\"})\\n\\n\\u200bAdd metadata and tags to traces\\nYou can annotate your traces with arbitrary metadata and tags by providing them in the RunnableConfig. This is useful for associating additional information with a trace, such as the environment in which it was executed, or the user who initiated it. For information on how to query traces and runs by metadata and tags, see this guide\\nWhen you attach metadata or tags to a runnable (either through the RunnableConfig or at runtime with invocation params), they are inherited by all child runnables of that runnable.\\nPythonTypeScriptCopyfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\nprompt = ChatPromptTemplate.from_messages([\\n    (\"system\", \"You are a helpful AI.\"),\\n    (\"user\", \"{input}\")\\n])\\n\\n# The tag \"model-tag\" and metadata {\"model-key\": \"model-value\"} will be attached to the ChatOpenAI run only\\nchat_model = ChatOpenAI().with_config({\"tags\": [\"model-tag\"], \"metadata\": {\"model-key\": \"model-value\"}})\\noutput_parser = StrOutputParser()\\n\\n# Tags and metadata can be configured with RunnableConfig\\nchain = (prompt | chat_model | output_parser).with_config({\"tags\": [\"config-tag\"], \"metadata\": {\"config-key\": \"config-value\"}})\\n\\n# Tags and metadata can also be passed at runtime\\nchain.invoke({\"input\": \"What is the meaning of life?\"}, {\"tags\": [\"invoke-tag\"], \"metadata\": {\"invoke-key\": \"invoke-value\"}})\\n\\n\\u200bCustomize run name\\nYou can customize the name of a given run when invoking or streaming your LangChain code by providing it in the Config. This name is used to identify the run in LangSmith and can be used to filter and group runs. The name is also used as the title of the run in the LangSmith UI. This can be done by setting a run_name in the RunnableConfig object at construction or by passing a run_name in the invocation parameters in JS/TS.\\nPythonTypeScriptCopy# When tracing within LangChain, run names default to the class name of the traced object (e.g., \\'ChatOpenAI\\').\\nconfigured_chain = chain.with_config({\"run_name\": \"MyCustomChain\"})\\nconfigured_chain.invoke({\"input\": \"What is the meaning of life?\"})\\n\\n# You can also configure the run name at invocation time, like below\\nchain.invoke({\"input\": \"What is the meaning of life?\"}, {\"run_name\": \"MyCustomChain\"})\\n\\nThe run_name parameter only changes the name of the runnable you invoke (e.g., a chain, function). It does not rename the nested run automatically created when you invoke an LLM object like ChatOpenAI (gpt-4o-mini). In the example, the enclosing run will appear in LangSmith as MyCustomChain, while the nested LLM run still shows the modelâ€™s default name.To give the LLM run a more meaningful name, you can either:\\nWrap the model in another runnable and assign a run_name to that step.\\nUse a tracing decorator or helper (e.g., @traceable in Python, or traceable from langsmith in JS/TS) to create a custom run around the model call.\\n\\n\\u200bCustomize run ID\\nYou can customize the ID of a given run when invoking or streaming your LangChain code by providing it in the Config. This ID is used to uniquely identify the run in LangSmith and can be used to query specific runs. The ID can be useful for linking runs across different systems or for implementing custom tracking logic. This can be done by setting a run_id in the RunnableConfig object at construction or by passing a run_id in the invocation parameters.\\nThis feature is not currently supported directly for LLM objects.\\nPythonTypeScriptCopyimport uuid\\n\\nmy_uuid = uuid.uuid4()\\n\\n# You can configure the run ID at invocation time:\\nchain.invoke({\"input\": \"What is the meaning of life?\"}, {\"run_id\": my_uuid})\\n\\nNote that if you do this at the root of a trace (i.e., the top-level run, that run ID will be used as the trace_id).\\n\\u200bAccess run (span) ID for LangChain invocations\\nWhen you invoke a LangChain object, you can manually specify the run ID of the invocation. This run ID can be used to query the run in LangSmith.\\nIn JS/TS, you can use a RunCollectorCallbackHandler instance to access the run ID.\\nPythonTypeScriptCopyimport uuid\\n\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\nprompt = ChatPromptTemplate.from_messages([\\n    (\"system\", \"You are a helpful assistant. Please respond to the user\\'s request only based on the given context.\"),\\n    (\"user\", \"Question: {question}\\\\n\\\\nContext: {context}\")\\n])\\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\\noutput_parser = StrOutputParser()\\n\\nchain = prompt | model | output_parser\\n\\nquestion = \"Can you summarize this morning\\'s meetings?\"\\ncontext = \"During this morning\\'s meeting, we solved all world conflict.\"\\nmy_uuid = uuid.uuid4()\\nresult = chain.invoke({\"question\": question, \"context\": context}, {\"run_id\": my_uuid})\\nprint(my_uuid)\\n\\n\\u200bEnsure all traces are submitted before exiting\\nIn LangChain Python, LangSmithâ€™s tracing is done in a background thread to avoid obstructing your production application. This means that your process may end before all traces are successfully posted to LangSmith. This is especially prevalent in a serverless environment, where your VM may be terminated immediately once your chain or agent completes.\\nYou can make callbacks synchronous by setting the LANGCHAIN_CALLBACKS_BACKGROUND environment variable to \"false\".\\nFor both languages, LangChain exposes methods to wait for traces to be submitted before exiting your application. Below is an example:\\nPythonTypeScriptCopyfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.tracers.langchain import wait_for_all_tracers\\n\\nllm = ChatOpenAI()\\n\\ntry:\\n  llm.invoke(\"Hello, World!\")\\nfinally:\\n  wait_for_all_tracers()\\n\\n\\u200bTrace without setting environment variables\\nAs mentioned in other guides, the following environment variables allow you to configure tracing enabled, the api endpoint, the api key, and the tracing project:\\n\\nLANGSMITH_TRACING\\nLANGSMITH_API_KEY\\nLANGSMITH_ENDPOINT\\nLANGSMITH_PROJECT\\n\\nHowever, in some environments, it is not possible to set environment variables. In these cases, you can set the tracing configuration programmatically.\\nThis largely builds off of the previous section.\\nPythonTypeScriptCopyimport langsmith as ls\\n\\n# You can create a client instance with an api key and api url\\nclient = ls.Client(\\n    api_key=\"YOUR_API_KEY\",  # This can be retrieved from a secrets manager\\n    api_url=\"https://api.smith.langchain.com\",  # Update appropriately for self-hosted installations or the EU region\\n)\\n\\n# You can pass the client and project_name to the tracing_context\\nwith ls.tracing_context(client=client, project_name=\"test-no-env\", enabled=True):\\n    chain.invoke({\"question\": \"Am I using a callback?\", \"context\": \"I\\'m using a callback\"})\\n\\n\\u200bDistributed tracing with LangChain (Python)\\nLangSmith supports distributed tracing with LangChain Python. This allows you to link runs (spans) across different services and applications. The principles are similar to the distributed tracing guide for the LangSmith SDK.\\nCopyimport langsmith\\nfrom langchain_core.runnables import chain\\nfrom langsmith.run_helpers import get_current_run_tree\\n\\n# -- This code should be in a separate file or service --\\n@chain\\ndef child_chain(inputs):\\n    return inputs[\"test\"] + 1\\n\\ndef child_wrapper(x, headers):\\n    with langsmith.tracing_context(parent=headers):\\n        child_chain.invoke({\"test\": x})\\n\\n# -- This code should be in a separate file or service --\\n@chain\\ndef parent_chain(inputs):\\n    rt = get_current_run_tree()\\n    headers = rt.to_headers()\\n    # ... make a request to another service with the headers\\n    # The headers should be passed to the other service, eventually to the child_wrapper function\\n\\nparent_chain.invoke({\"test\": 1})\\n\\n\\u200bInteroperability between LangChain (Python) and LangSmith SDK\\nIf you are using LangChain for part of your application and the LangSmith SDK (see this guide) for other parts, you can still trace the entire application seamlessly.\\nLangChain objects will be traced when invoked within a traceable function and be bound as a child run of the traceable function.\\nCopyfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langsmith import traceable\\n\\nprompt = ChatPromptTemplate.from_messages([\\n    (\"system\", \"You are a helpful assistant. Please respond to the user\\'s request only based on the given context.\"),\\n    (\"user\", \"Question: {question}\\\\nContext: {context}\")\\n])\\n\\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\\noutput_parser = StrOutputParser()\\nchain = prompt | model | output_parser\\n\\n# The above chain will be traced as a child run of the traceable function\\n@traceable(\\n    tags=[\"openai\", \"chat\"],\\n    metadata={\"foo\": \"bar\"}\\n)\\ndef invoke_runnnable(question, context):\\n    result = chain.invoke({\"question\": question, \"context\": context})\\n    return \"The response is: \" + result\\n\\ninvoke_runnnable(\"Can you summarize this morning\\'s meetings?\", \"During this morning\\'s meeting, we solved all world conflict.\")\\n\\nThis will produce the following trace tree: \\n\\u200bInteroperability between LangChain.JS and LangSmith SDK\\n\\u200bTracing LangChain objects inside traceable (JS only)\\nStarting with langchain@0.2.x, LangChain objects are traced automatically when used inside @traceable functions, inheriting the client, tags, metadata and project name of the traceable function.\\nFor older versions of LangChain below 0.2.x, you will need to manually pass an instance LangChainTracer created from the tracing context found in @traceable.\\nCopyimport { ChatOpenAI } from \"@langchain/openai\";\\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\\nimport { StringOutputParser } from \"@langchain/core/output_parsers\";\\nimport { getLangchainCallbacks } from \"langsmith/langchain\";\\n\\nconst prompt = ChatPromptTemplate.fromMessages([\\n  [\\n    \"system\",\\n    \"You are a helpful assistant. Please respond to the user\\'s request only based on the given context.\",\\n  ],\\n  [\"user\", \"Question: {question}\\\\nContext: {context}\"],\\n]);\\n\\nconst model = new ChatOpenAI({ modelName: \"gpt-4o-mini\" });\\nconst outputParser = new StringOutputParser();\\nconst chain = prompt.pipe(model).pipe(outputParser);\\n\\nconst main = traceable(\\n  async (input: { question: string; context: string }) => {\\n    const callbacks = await getLangchainCallbacks();\\n    const response = await chain.invoke(input, { callbacks });\\n    return response;\\n  },\\n  { name: \"main\" }\\n);\\n\\n\\u200bTracing LangChain child runs via traceable / RunTree API (JS only)\\nWeâ€™re working on improving the interoperability between traceable and LangChain. The following limitations are present when using combining LangChain with traceable:\\nMutating RunTree obtained from getCurrentRunTree() of the RunnableLambda context will result in a no-op.\\nItâ€™s discouraged to traverse the RunTree obtained from RunnableLambda via getCurrentRunTree() as it may not contain all the RunTree nodes.\\nDifferent child runs may have the same execution_order and child_execution_order value. Thus in extreme circumstances, some runs may end up in a different order, depending on the start_time.\\n\\nIn some uses cases, you might want to run traceable functions as part of the RunnableSequence or trace child runs of LangChain run imperatively via the RunTree API. Starting with LangSmith 0.1.39 and @langchain/core 0.2.18, you can directly invoke traceable-wrapped functions within RunnableLambda.\\nCopyimport { traceable } from \"langsmith/traceable\";\\nimport { RunnableLambda } from \"@langchain/core/runnables\";\\nimport { RunnableConfig } from \"@langchain/core/runnables\";\\n\\nconst tracedChild = traceable((input: string) => `Child Run: ${input}`, {\\n  name: \"Child Run\",\\n});\\n\\nconst parrot = new RunnableLambda({\\n  func: async (input: { text: string }, config?: RunnableConfig) => {\\n    return await tracedChild(input.text);\\n  },\\n});\\n\\n\\nAlternatively, you can convert LangChainâ€™s RunnableConfig to a equivalent RunTree object by using RunTree.fromRunnableConfig or pass the RunnableConfig as the first argument of traceable-wrapped function.\\nTraceableRun TreeCopyimport { traceable } from \"langsmith/traceable\";\\nimport { RunnableLambda } from \"@langchain/core/runnables\";\\nimport { RunnableConfig } from \"@langchain/core/runnables\";\\n\\nconst tracedChild = traceable((input: string) => `Child Run: ${input}`, {\\n  name: \"Child Run\",\\n});\\n\\nconst parrot = new RunnableLambda({\\n  func: async (input: { text: string }, config?: RunnableConfig) => {\\n    // Pass the config to existing traceable function\\n    await tracedChild(config, input.text);\\n    return input.text;\\n  },\\n});\\n\\nIf you prefer a video tutorial, check out the Alternative Ways to Trace video from the Introduction to LangSmith Course.Was this page helpful?YesNoSuggest editsOverviewLangGraphâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Trace with LangGraph - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsOverviewLangChainLangGraphAnthropic (Python only)OpenAIAutoGenClaude CodeCrewAIGoogle ADKInstructor (Python only)OpenAI Agents SDKOpenTelemetrySemantic KernelVercel AI SDKManual instrumentationConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationIntegrationsTrace with LangGraphGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageWith LangChain1. Installation2. Configure your environment3. Log a traceWithout LangChain1. Installation2. Configure your environment3. Log a traceTracing setupIntegrationsTrace with LangGraphCopy pageCopy pageLangSmith smoothly integrates with LangGraph (Python and JS) to help you trace agentic workflows, whether youâ€™re using LangChain modules or other SDKs.\\n\\u200bWith LangChain\\nIf you are using LangChain modules within LangGraph, you only need to set a few environment variables to enable tracing.\\nThis guide will walk through a basic example. For more detailed information on configuration, see the Trace With LangChain guide.\\n\\u200b1. Installation\\nInstall the LangGraph library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below).\\nFor a full list of packages available, see the LangChain Python docs and LangChain JS docs.\\npipyarnnpmpnpmCopypip install langchain_openai langgraph\\n\\n\\u200b2. Configure your environment\\nPythonTypeScriptCopyexport LANGSMITH_TRACING=true\\nexport LANGSMITH_API_KEY=<your-api-key>\\n# This example uses OpenAI, but you can use any LLM provider of choice\\nexport OPENAI_API_KEY=<your-openai-api-key>\\n# For LangSmith API keys linked to multiple workspaces, set the LANGSMITH_WORKSPACE_ID environment variable to specify which workspace to use.\\nexport LANGSMITH_WORKSPACE_ID=<your-workspace-id>\\n\\nIf you are using LangChain.js with LangSmith and are not in a serverless environment, we also recommend setting the following explicitly to reduce latency:export LANGCHAIN_CALLBACKS_BACKGROUND=trueIf you are in a serverless environment, we recommend setting the reverse to allow tracing to finish before your function ends:export LANGCHAIN_CALLBACKS_BACKGROUND=falseSee this LangChain.js guide for more information.\\n\\u200b3. Log a trace\\nOnce youâ€™ve set up your environment, you can call LangChain runnables as normal. LangSmith will infer the proper tracing config:\\nPythonTypeScriptCopyfrom typing import Literal\\nfrom langchain_core.messages import HumanMessage\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.tools import tool\\nfrom langgraph.graph import StateGraph, MessagesState\\nfrom langgraph.prebuilt import ToolNode\\n\\n@tool\\ndef search(query: str):\\n    \"\"\"Call to surf the web.\"\"\"\\n    if \"sf\" in query.lower() or \"san francisco\" in query.lower():\\n        return \"It\\'s 60 degrees and foggy.\"\\n    return \"It\\'s 90 degrees and sunny.\"\\n\\ntools = [search]\\ntool_node = ToolNode(tools)\\n\\nmodel = ChatOpenAI(model=\"gpt-4o\", temperature=0).bind_tools(tools)\\n\\ndef should_continue(state: MessagesState) -> Literal[\"tools\", \"__end__\"]:\\n    messages = state[\\'messages\\']\\n    last_message = messages[-1]\\n    if last_message.tool_calls:\\n        return \"tools\"\\n    return \"__end__\"\\n\\ndef call_model(state: MessagesState):\\n    messages = state[\\'messages\\']\\n    # Invoking `model` will automatically infer the correct tracing context\\n    response = model.invoke(messages)\\n    return {\"messages\": [response]}\\n\\nworkflow = StateGraph(MessagesState)\\nworkflow.add_node(\"agent\", call_model)\\nworkflow.add_node(\"tools\", tool_node)\\nworkflow.add_edge(\"__start__\", \"agent\")\\nworkflow.add_conditional_edges(\\n    \"agent\",\\n    should_continue,\\n)\\nworkflow.add_edge(\"tools\", \\'agent\\')\\n\\napp = workflow.compile()\\n\\nfinal_state = app.invoke(\\n    {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]},\\n    config={\"configurable\": {\"thread_id\": 42}}\\n)\\n\\nfinal_state[\"messages\"][-1].content\\n\\nAn example trace from running the above code looks like this:\\n\\n\\u200bWithout LangChain\\nIf you are using other SDKs or custom functions within LangGraph, you will need to wrap or decorate them appropriately (with the @traceable decorator in Python or the traceable function in JS, or something like e.g. wrap_openai for SDKs). If you do so, LangSmith will automatically nest traces from those wrapped methods.\\nHereâ€™s an example. You can also see this page for more information.\\n\\u200b1. Installation\\nInstall the LangGraph library and the OpenAI SDK for Python and JS (we use the OpenAI integration for the code snippets below).\\npipyarnnpmpnpmCopypip install openai langsmith langgraph\\n\\n\\u200b2. Configure your environment\\nPythonTypeScriptCopyexport LANGSMITH_TRACING=true\\nexport LANGSMITH_API_KEY=<your-api-key>\\n# This example uses OpenAI, but you can use any LLM provider of choice\\nexport OPENAI_API_KEY=<your-openai-api-key>\\n\\nIf you are using LangChain.js with LangSmith and are not in a serverless environment, we also recommend setting the following explicitly to reduce latency:export LANGCHAIN_CALLBACKS_BACKGROUND=trueIf you are in a serverless environment, we recommend setting the reverse to allow tracing to finish before your function ends:export LANGCHAIN_CALLBACKS_BACKGROUND=falseSee this LangChain.js guide for more information.\\n\\u200b3. Log a trace\\nOnce youâ€™ve set up your environment, wrap or decorate the custom functions/SDKs you want to trace. LangSmith will then infer the proper tracing config:\\nPythonTypeScriptCopyimport json\\nimport openai\\nimport operator\\nfrom langsmith import traceable\\nfrom langsmith.wrappers import wrap_openai\\nfrom typing import Annotated, Literal, TypedDict\\nfrom langgraph.graph import StateGraph\\n\\nclass State(TypedDict):\\n    messages: Annotated[list, operator.add]\\n\\ntool_schema = {\\n    \"type\": \"function\",\\n    \"function\": {\\n        \"name\": \"search\",\\n        \"description\": \"Call to surf the web.\",\\n        \"parameters\": {\\n            \"type\": \"object\",\\n            \"properties\": {\"query\": {\"type\": \"string\"}},\\n            \"required\": [\"query\"],\\n        },\\n    },\\n}\\n\\n# Decorating the tool function will automatically trace it with the correct context\\n@traceable(run_type=\"tool\", name=\"Search Tool\")\\ndef search(query: str):\\n    \"\"\"Call to surf the web.\"\"\"\\n    if \"sf\" in query.lower() or \"san francisco\" in query.lower():\\n        return \"It\\'s 60 degrees and foggy.\"\\n    return \"It\\'s 90 degrees and sunny.\"\\n\\ntools = [search]\\n\\ndef call_tools(state):\\n    function_name_to_function = {\"search\": search}\\n    messages = state[\"messages\"]\\n    tool_call = messages[-1][\"tool_calls\"][0]\\n    function_name = tool_call[\"function\"][\"name\"]\\n    function_arguments = tool_call[\"function\"][\"arguments\"]\\n    arguments = json.loads(function_arguments)\\n    function_response = function_name_to_function[function_name](**arguments)\\n    tool_message = {\\n        \"tool_call_id\": tool_call[\"id\"],\\n        \"role\": \"tool\",\\n        \"name\": function_name,\\n        \"content\": function_response,\\n    }\\n    return {\"messages\": [tool_message]}\\n\\nwrapped_client = wrap_openai(openai.Client())\\n\\ndef should_continue(state: State) -> Literal[\"tools\", \"__end__\"]:\\n    messages = state[\"messages\"]\\n    last_message = messages[-1]\\n    if last_message[\"tool_calls\"]:\\n        return \"tools\"\\n    return \"__end__\"\\n\\ndef call_model(state: State):\\n    messages = state[\"messages\"]\\n    # Calling the wrapped client will automatically infer the correct tracing context\\n    response = wrapped_client.chat.completions.create(\\n        messages=messages, model=\"gpt-4o-mini\", tools=[tool_schema]\\n    )\\n    raw_tool_calls = response.choices[0].message.tool_calls\\n    tool_calls = [tool_call.to_dict() for tool_call in raw_tool_calls] if raw_tool_calls else []\\n    response_message = {\\n        \"role\": \"assistant\",\\n        \"content\": response.choices[0].message.content,\\n        \"tool_calls\": tool_calls,\\n    }\\n    return {\"messages\": [response_message]}\\n\\nworkflow = StateGraph(State)\\nworkflow.add_node(\"agent\", call_model)\\nworkflow.add_node(\"tools\", call_tools)\\nworkflow.add_edge(\"__start__\", \"agent\")\\nworkflow.add_conditional_edges(\\n    \"agent\",\\n    should_continue,\\n)\\nworkflow.add_edge(\"tools\", \\'agent\\')\\n\\napp = workflow.compile()\\n\\nfinal_state = app.invoke(\\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\\n)\\n\\nfinal_state[\"messages\"][-1][\"content\"]\\n\\nAn example trace from running the above code looks like this:\\nWas this page helpful?YesNoSuggest editsLangChainAnthropic (Python only)âŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Trace with OpenAI - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsOverviewLangChainLangGraphAnthropic (Python only)OpenAIAutoGenClaude CodeCrewAIGoogle ADKInstructor (Python only)OpenAI Agents SDKOpenTelemetrySemantic KernelVercel AI SDKManual instrumentationConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationIntegrationsTrace with OpenAIGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumTracing setupIntegrationsTrace with OpenAICopy pageCopy pageThe wrap_openai/wrapOpenAI methods in Python/TypeScript allow you to wrap your OpenAI client in order to automatically log traces â€” no decorator or function wrapping required! Using the wrapper ensures that messages, including tool calls and multimodal content blocks will be rendered nicely in LangSmith. Also note that the wrapper works seamlessly with the @traceable decorator or traceable function and you can use both in the same application.\\nThe LANGSMITH_TRACING environment variable must be set to \\'true\\' in order for traces to be logged to LangSmith, even when using wrap_openai or wrapOpenAI. This allows you to toggle tracing on and off without changing your code.Additionally, you will need to set the LANGSMITH_API_KEY environment variable to your API key (see Setup for more information).If your LangSmith API key is linked to multiple workspaces, set the LANGSMITH_WORKSPACE_ID environment variable to specify which workspace to use.By default, the traces will be logged to a project named default. To log traces to a different project, see this section.\\nPythonTypeScriptCopyimport openai\\nfrom langsmith import traceable\\nfrom langsmith.wrappers import wrap_openai\\n\\nclient = wrap_openai(openai.Client())\\n\\n@traceable(run_type=\"tool\", name=\"Retrieve Context\")\\ndef my_tool(question: str) -> str:\\n  return \"During this morning\\'s meeting, we solved all world conflict.\"\\n\\n@traceable(name=\"Chat Pipeline\")\\ndef chat_pipeline(question: str):\\n  context = my_tool(question)\\n  messages = [\\n      { \"role\": \"system\", \"content\": \"You are a helpful assistant. Please respond to the user\\'s request only based on the given context.\" },\\n      { \"role\": \"user\", \"content\": f\"Question: {question}\\\\nContext: {context}\"}\\n  ]\\n  chat_completion = client.chat.completions.create(\\n      model=\"gpt-4o-mini\", messages=messages\\n  )\\n  return chat_completion.choices[0].message.content\\n\\nchat_pipeline(\"Can you summarize this morning\\'s meetings?\")\\nWas this page helpful?YesNoSuggest editsAnthropic (Python only)AutoGenâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='\\n\\n\\n\\n\\nTrace with OpenTelemetry | ðŸ¦œï¸ðŸ› ï¸ LangSmith\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentLangSmith docs have moved! Find the LangSmith docs at the new LangChain Docs site.API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityQuick StartTutorialsAdd observability to your LLM applicationHow-to GuidesAnnotate code for tracingFilter traces in the applicationUpload files with tracesDashboardsLog traces to specific projectSet up automation rulesOnline EvaluationSet a sampling rate for tracesAdd metadata and tags to tracesImplement distributed tracingAccess the current run (span) within a traced functionLog multimodal tracesLog retriever tracesLog custom LLM tracesPrevent logging of sensitive data in tracesQuery tracesShare or unshare a trace publiclyCompare tracesTrace generator functionsTrace with LangChain (Python and JS/TS)View server logs for a traceTrace with LangGraph (Python and JS/TS)Trace with Instructor (Python only)Trace with OpenTelemetryTrace with the Vercel AI SDK (JS/TS only)Trace with the Vercel AI SDK (Legacy)Trace without setting environment variablesTrace using the LangSmith REST APITrace with OpenAI Agents SDKCalculate token-based costs for tracesTroubleshoot trace nestingBulk Exporting Trace DataAlerts in LangSmithConfiguring PagerDuty Integration for LangSmith AlertsConfiguring Webhook Notifications for LangSmith AlertsTracing Claude Code[Beta] LangSmith Collector-ProxyHow to print detailed logs (Python SDK)Trace JS functions in serverless environmentsSet up threadsToubleshooting variable cachingSet up webhook notifications for rulesConceptual GuideEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceObservabilityHow-to GuidesTrace with OpenTelemetryOn this pageTrace with OpenTelemetry\\nLangSmith supports OpenTelemetry-based tracing, allowing you to send traces from any OpenTelemetry-compatible application. This guide covers both automatic instrumentation for LangChain applications and manual instrumentation for other frameworks.\\nFor self-hosted and EU region deploymentsUpdate the LangSmith URL appropriately for self-hosted installations or organizations in the EU region in the requests below. For the EU region, use eu.api.smith.langchain.com.\\nTrace a LangChain application\\u200b\\nIf you\\'re using LangChain or LangGraph, use the built-in integration to trace your application:\\n\\n\\nInstall the LangSmith package with OpenTelemetry support:\\npip install \"langsmith[otel]\"pip install langchain\\ninfoRequires Python SDK version langsmith>=0.3.18.\\n\\n\\nIn your LangChain/LangGraph App, enable the OpenTelemetry integration by setting the LANGSMITH_OTEL_ENABLED environment variable:\\nLANGSMITH_OTEL_ENABLED=trueLANGSMITH_TRACING=trueLANGSMITH_ENDPOINT=https://api.smith.langchain.comLANGSMITH_API_KEY=<your_langsmith_api_key>\\n\\n\\nCreate a LangChain application with tracing. For example:\\nimport osfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplate# Create a chainprompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")model = ChatOpenAI()chain = prompt | model# Run the chainresult = chain.invoke({\"topic\": \"programming\"})print(result.content)\\n\\n\\nView the traces in your LangSmith dashboard (example) once your application runs.\\n\\n\\nTrace a non-LangChain application\\u200b\\nFor non-LangChain applications or custom instrumentation, you can trace your application in LangSmith with a standard OpenTelemetry client:\\n\\n\\nInstall the OpenTelemetry SDK, OpenTelemetry exporter packages, as well as the OpenAI package:\\npip install openaipip install opentelemetry-sdkpip install opentelemetry-exporter-otlp\\n\\n\\nSetup environment variables for the endpoint, substitute your specific values:\\nOTEL_EXPORTER_OTLP_ENDPOINT=https://api.smith.langchain.com/otelOTEL_EXPORTER_OTLP_HEADERS=\"x-api-key=<your langsmith api key>\"\\nTrace endpointDepending on how your otel exporter is configured, you may need to append /v1/traces to the endpoint if you are only sending traces.\\nSelf-hosted deploymentsIf you\\'re self-hosting LangSmith, replace the base endpoint with your LangSmith api endpoint and append /api/v1. For example: OTEL_EXPORTER_OTLP_ENDPOINT=https://ai-company.com/api/v1/otel\\nOptional: Specify a custom project name other than \"default\":\\nOTEL_EXPORTER_OTLP_ENDPOINT=https://api.smith.langchain.com/otelOTEL_EXPORTER_OTLP_HEADERS=\"x-api-key=<your langsmith api key>,Langsmith-Project=<project name>\"\\n\\n\\nLog a trace.\\nThis code sets up an OTEL tracer and exporter that will send traces to LangSmith. It then\\ncalls OpenAI and sends the required OpenTelemetry attributes.\\nfrom openai import OpenAIfrom opentelemetry import tracefrom opentelemetry.sdk.trace import TracerProviderfrom opentelemetry.sdk.trace.export import (    BatchSpanProcessor,)from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporterclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))otlp_exporter = OTLPSpanExporter(    timeout=10,)trace.set_tracer_provider(TracerProvider())trace.get_tracer_provider().add_span_processor(    BatchSpanProcessor(otlp_exporter))tracer = trace.get_tracer(__name__)def call_openai():    model = \"gpt-4o-mini\"    with tracer.start_as_current_span(\"call_open_ai\") as span:        span.set_attribute(\"langsmith.span.kind\", \"LLM\")        span.set_attribute(\"langsmith.metadata.user_id\", \"user_123\")        span.set_attribute(\"gen_ai.system\", \"OpenAI\")        span.set_attribute(\"gen_ai.request.model\", model)        span.set_attribute(\"llm.request.type\", \"chat\")        messages = [            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},            {                \"role\": \"user\",                \"content\": \"Write a haiku about recursion in programming.\"            }        ]        for i, message in enumerate(messages):            span.set_attribute(f\"gen_ai.prompt.{i}.content\", str(message[\"content\"]))            span.set_attribute(f\"gen_ai.prompt.{i}.role\", str(message[\"role\"]))        completion = client.chat.completions.create(            model=model,            messages=messages        )        span.set_attribute(\"gen_ai.response.model\", completion.model)        span.set_attribute(\"gen_ai.completion.0.content\", str(completion.choices[0].message.content))        span.set_attribute(\"gen_ai.completion.0.role\", \"assistant\")        span.set_attribute(\"gen_ai.usage.prompt_tokens\", completion.usage.prompt_tokens)        span.set_attribute(\"gen_ai.usage.completion_tokens\", completion.usage.completion_tokens)        span.set_attribute(\"gen_ai.usage.total_tokens\", completion.usage.total_tokens)        return completion.choices[0].messageif __name__ == \"__main__\":    call_openai()\\n\\n\\nView the trace in your LangSmith dashboard (example).\\n\\n\\nSend traces to an alternate provider\\u200b\\nWhile LangSmith is the default destination for OpenTelemetry traces, you can also configure OpenTelemetry to send traces to other observability platforms.\\ninfoIn langsmith Python SDK version â‰¥ 0.4.1,\\nsetting LANGSMITH_OTEL_ENABLED=true will by default send traces to both LangSmith and your instrumented OTEL endpoint. No extra code is needed for fan-out.\\nUse environment variables for global configuration\\u200b\\nBy default, the LangSmith OpenTelemetry exporter will send data to the LangSmith API OTEL endpoint, but this can be customized by setting standard OTEL environment variables:\\nOTEL_EXPORTER_OTLP_ENDPOINT: Override the endpoint URLOTEL_EXPORTER_OTLP_HEADERS: Add custom headers (LangSmith API keys and Project are added automatically)OTEL_SERVICE_NAME: Set a custom service name (defaults to \"langsmith\")\\nLangSmith uses the HTTP trace exporter by default. If you\\'d like to use your own tracing provider, you can either:\\n\\nSet the OTEL environment variables as shown above, or\\nSet a global trace provider before initializing LangChain components, which LangSmith will detect and use instead of creating its own.\\n\\nConfigure alternate OTLP endpoints\\u200b\\nTo send traces to a different provider, configure the OTLP exporter with your provider\\'s endpoint:\\nimport osfrom opentelemetry import tracefrom opentelemetry.sdk.trace import TracerProviderfrom opentelemetry.sdk.trace.export import BatchSpanProcessorfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporterfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplate# Set environment variables for LangChainos.environ[\"LANGSMITH_OTEL_ENABLED\"] = \"true\"os.environ[\"LANGSMITH_TRACING\"] = \"true\"# Configure the OTLP exporter for your custom endpointprovider = TracerProvider()otlp_exporter = OTLPSpanExporter(    # Change to your provider\\'s endpoint    endpoint=\"https://otel.your-provider.com/v1/traces\",    # Add any required headers for authentication    headers={\"api-key\": \"your-api-key\"})processor = BatchSpanProcessor(otlp_exporter)provider.add_span_processor(processor)trace.set_tracer_provider(provider)# Create and run a LangChain applicationprompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")model = ChatOpenAI()chain = prompt | modelresult = chain.invoke({\"topic\": \"programming\"})print(result.content)\\ninfoTo disable hybrid tracing behavior after 0.4.1 and send traces only to your OTEL endpoint and exclude sending to LangSmith, add an additional env var:LANGSMITH_OTEL_ONLY = \"true\"\\nSupported OpenTelemetry attribute and event mapping\\u200b\\nWhen sending traces to LangSmith via OpenTelemetry, the following attributes are mapped to LangSmith fields:\\nCore LangSmith attributes\\u200b\\nOpenTelemetry attributeLangSmith fieldNoteslangsmith.trace.nameRun nameOverrides the span name for the runlangsmith.span.kindRun typeValues: llm, chain, tool, retriever, embedding, prompt, parserlangsmith.trace.session_idSession IDSession identifier for related traceslangsmith.trace.session_nameSession nameName of the sessionlangsmith.span.tagsTagsCustom tags attached to the span (comma-separated)langsmith.metadata.{key}metadata.{key}Custom metadata with langsmith prefix\\nGenAI standard attributes\\u200b\\nOpenTelemetry attributeLangSmith fieldNotesgen_ai.systemmetadata.ls_providerThe GenAI system (e.g., \"openai\", \"anthropic\")gen_ai.operation.nameRun typeMaps \"chat\"/\"completion\" to \"llm\", \"embedding\" to \"embedding\"gen_ai.promptinputsThe input prompt sent to the modelgen_ai.completionoutputsThe output generated by the modelgen_ai.prompt.{n}.roleinputs.messages[n].roleRole for the nth input messagegen_ai.prompt.{n}.contentinputs.messages[n].contentContent for the nth input messagegen_ai.prompt.{n}.message.roleinputs.messages[n].roleAlternative format for rolegen_ai.prompt.{n}.message.contentinputs.messages[n].contentAlternative format for contentgen_ai.completion.{n}.roleoutputs.messages[n].roleRole for the nth output messagegen_ai.completion.{n}.contentoutputs.messages[n].contentContent for the nth output messagegen_ai.completion.{n}.message.roleoutputs.messages[n].roleAlternative format for rolegen_ai.completion.{n}.message.contentoutputs.messages[n].contentAlternative format for contentgen_ai.tool.nameinvocation_params.tool_nameTool name, also sets run type to \"tool\"\\nGenAI request parameters\\u200b\\nOpenTelemetry attributeLangSmith fieldNotesgen_ai.request.modelinvocation_params.modelThe model name used for the requestgen_ai.response.modelinvocation_params.modelThe model name returned in the responsegen_ai.request.temperatureinvocation_params.temperatureTemperature settinggen_ai.request.top_pinvocation_params.top_pTop-p sampling settinggen_ai.request.max_tokensinvocation_params.max_tokensMaximum tokens settinggen_ai.request.frequency_penaltyinvocation_params.frequency_penaltyFrequency penalty settinggen_ai.request.presence_penaltyinvocation_params.presence_penaltyPresence penalty settinggen_ai.request.seedinvocation_params.seedRandom seed used for generationgen_ai.request.stop_sequencesinvocation_params.stopSequences that stop generationgen_ai.request.top_kinvocation_params.top_kTop-k sampling parametergen_ai.request.encoding_formatsinvocation_params.encoding_formatsOutput encoding formats\\nGenAI usage metrics\\u200b\\nOpenTelemetry attributeLangSmith fieldNotesgen_ai.usage.input_tokensusage_metadata.input_tokensNumber of input tokens usedgen_ai.usage.output_tokensusage_metadata.output_tokensNumber of output tokens usedgen_ai.usage.total_tokensusage_metadata.total_tokensTotal number of tokens usedgen_ai.usage.prompt_tokensusage_metadata.input_tokensNumber of input tokens used (deprecated)gen_ai.usage.completion_tokensusage_metadata.output_tokensNumber of output tokens used (deprecated)\\nTraceLoop attributes\\u200b\\nOpenTelemetry attributeLangSmith fieldNotestraceloop.entity.inputinputsFull input value from TraceLooptraceloop.entity.outputoutputsFull output value from TraceLooptraceloop.entity.nameRun nameEntity name from TraceLooptraceloop.span.kindRun typeMaps to LangSmith run typestraceloop.llm.request.typeRun type\"embedding\" maps to \"embedding\", others to \"llm\"traceloop.association.properties.{key}metadata.{key}Custom metadata with traceloop prefix\\nOpenInference attributes\\u200b\\nOpenTelemetry attributeLangSmith fieldNotesinput.valueinputsFull input value, can be string or JSONoutput.valueoutputsFull output value, can be string or JSONopeninference.span.kindRun typeMaps various kinds to LangSmith run typesllm.systemmetadata.ls_providerLLM system providerllm.model_namemetadata.ls_model_nameModel name from OpenInferencetool.nameRun nameTool name when span kind is \"TOOL\"metadatametadata.*JSON string of metadata to be merged\\nLLM attributes\\u200b\\nOpenTelemetry attributeLangSmith fieldNotesllm.input_messagesinputs.messagesInput messagesllm.output_messagesoutputs.messagesOutput messagesllm.token_count.promptusage_metadata.input_tokensPrompt token countllm.token_count.completionusage_metadata.output_tokensCompletion token countllm.token_count.totalusage_metadata.total_tokensTotal token countllm.usage.total_tokensusage_metadata.total_tokensAlternative total token countllm.invocation_parametersinvocation_params.*JSON string of invocation parametersllm.presence_penaltyinvocation_params.presence_penaltyPresence penaltyllm.frequency_penaltyinvocation_params.frequency_penaltyFrequency penaltyllm.request.functionsinvocation_params.functionsFunction definitions\\nPrompt template attributes\\u200b\\nOpenTelemetry attributeLangSmith fieldNotesllm.prompt_template.variablesRun typeSets run type to \"prompt\", used with input.value\\nRetriever attributes\\u200b\\nOpenTelemetry attributeLangSmith fieldNotesretrieval.documents.{n}.document.contentoutputs.documents[n].page_contentContent of the nth retrieved documentretrieval.documents.{n}.document.metadataoutputs.documents[n].metadataMetadata of the nth retrieved document (JSON)\\nTool attributes\\u200b\\nOpenTelemetry attributeLangSmith fieldNotestoolsinvocation_params.toolsArray of tool definitionstool_argumentsinvocation_params.tool_argumentsTool arguments as JSON or key-value pairs\\nLogfire attributes\\u200b\\nOpenTelemetry attributeLangSmith fieldNotespromptinputsLogfire prompt inputall_messages_eventsoutputsLogfire message events outputeventsinputs/outputsLogfire events array, splits input/choice events\\nOpenTelemetry event mapping\\u200b\\nEvent nameLangSmith fieldNotesgen_ai.content.promptinputsExtracts prompt content from event attributesgen_ai.content.completionoutputsExtracts completion content from event attributesgen_ai.system.messageinputs.messages[]System message in conversationgen_ai.user.messageinputs.messages[]User message in conversationgen_ai.assistant.messageoutputs.messages[]Assistant message in conversationgen_ai.tool.messageoutputs.messages[]Tool response messagegen_ai.choiceoutputsModel choice/response with finish reasonexceptionstatus, errorSets status to \"error\" and extracts exception message/stacktrace\\nEvent attribute extraction\\u200b\\nFor message events, the following attributes are extracted:\\n\\ncontent â†’ message content\\nrole â†’ message role\\nid â†’ tool_call_id (for tool messages)\\ngen_ai.event.content â†’ full message JSON\\n\\nFor choice events:\\n\\nfinish_reason â†’ choice finish reason\\nmessage.content â†’ choice message content\\nmessage.role â†’ choice message role\\ntool_calls.{n}.id â†’ tool call ID\\ntool_calls.{n}.function.name â†’ tool function name\\ntool_calls.{n}.function.arguments â†’ tool function arguments\\ntool_calls.{n}.type â†’ tool call type\\n\\nFor exception events:\\n\\nexception.message â†’ error message\\nexception.stacktrace â†’ error stacktrace (appended to message)\\n\\nImplementation examples\\u200b\\nTrace using the Traceloop SDK\\u200b\\nThe Traceloop SDK is an OpenTelemetry compatible SDK that covers a range of models, vector databases and frameworks.\\nIf there are integrations that you are interested in instrumenting that are covered by this SDK, you\\ncan use this SDK with OpenTelemetry to log traces to LangSmith.\\nTo see what integrations are supported by the Traceloop SDK, see the Traceloop SDK documentation.\\nTo get started, follow these steps:\\n\\n\\nInstall the Traceloop SDK and OpenAI:\\npip install traceloop-sdkpip install openai\\n\\n\\nConfigure your environment:\\nTRACELOOP_BASE_URL=https://api.smith.langchain.com/otelTRACELOOP_HEADERS=x-api-key=<your_langsmith_api_key>\\nOptional: Specify a custom project name other than \"default\":\\nTRACELOOP_HEADERS=x-api-key=<your_langsmith_api_key>,Langsmith-Project=<langsmith_project_name>\\n\\n\\nTo use the SDK, you need to initialize it before logging traces:\\nfrom traceloop.sdk import TraceloopTraceloop.init()\\n\\n\\nLog a trace:\\nimport osfrom openai import OpenAIfrom traceloop.sdk import Traceloopclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))Traceloop.init()completion = client.chat.completions.create(    model=\"gpt-4o-mini\",    messages=[        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},        {            \"role\": \"user\",            \"content\": \"Write a haiku about recursion in programming.\"        }    ])print(completion.choices[0].message)\\n\\n\\nView the trace in your LangSmith dashboard (example).\\n\\n\\nTrace using the Arize SDK\\u200b\\nWith the Arize SDK and OpenTelemetry, you can log traces from multiple other frameworks to LangSmith.\\nBelow is an example of tracing CrewAI to LangSmith, you can find a full list of supported\\nframeworks here. To make this example\\nwork with other frameworks, you just need to change the instrumentor to match the framework.\\n\\n\\nInstall the required packages:\\npip install -qU arize-phoenix-otel openinference-instrumentation-crewai crewai crewai-tools\\n\\n\\nSet the following environment variables:\\nOPENAI_API_KEY=<your_openai_api_key>SERPER_API_KEY=<your_serper_api_key>\\n\\n\\nBefore running any application code, set up our instrumentor (you can replace this with any of the frameworks supported here):\\nfrom opentelemetry.sdk.trace import TracerProviderfrom opentelemetry.sdk.trace.export import BatchSpanProcessorfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter# Add LangSmith API Key for tracingLANGSMITH_API_KEY = \"YOUR_API_KEY\"# Set the endpoint for OTEL collectionENDPOINT = \"https://api.smith.langchain.com/otel/v1/traces\"# Select the project to trace toLANGSMITH_PROJECT = \"YOUR_PROJECT_NAME\"# Create the OTLP exporterotlp_exporter = OTLPSpanExporter(    endpoint=ENDPOINT,    headers={\"x-api-key\": LANGSMITH_API_KEY, \"Langsmith-Project\": LANGSMITH_PROJECT})# Set up the trace providerprovider = TracerProvider()processor = BatchSpanProcessor(otlp_exporter)provider.add_span_processor(processor)# Now instrument CrewAIfrom openinference.instrumentation.crewai import CrewAIInstrumentorCrewAIInstrumentor().instrument(tracer_provider=provider)\\n\\n\\nRun a CrewAI workflow and the trace will automatically be logged to LangSmith:\\nfrom crewai import Agent, Task, Crew, Processfrom crewai_tools import SerperDevToolsearch_tool = SerperDevTool()# Define your agents with roles and goalsresearcher = Agent(    role=\\'Senior Research Analyst\\',    goal=\\'Uncover cutting-edge developments in AI and data science\\',    backstory=\"\"\"You work at a leading tech think tank.    Your expertise lies in identifying emerging trends.    You have a knack for dissecting complex data and presenting actionable insights.\"\"\",    verbose=True,    allow_delegation=False,    # You can pass an optional llm attribute specifying what model you wanna use.    # llm=ChatOpenAI(model_name=\"gpt-3.5\", temperature=0.7),    tools=[search_tool])writer = Agent(    role=\\'Tech Content Strategist\\',    goal=\\'Craft compelling content on tech advancements\\',    backstory=\"\"\"You are a renowned Content Strategist, known for your insightful and engaging articles.    You transform complex concepts into compelling narratives.\"\"\",    verbose=True,    allow_delegation=True)# Create tasks for your agentstask1 = Task(    description=\"\"\"Conduct a comprehensive analysis of the latest advancements in AI in 2024.    Identify key trends, breakthrough technologies, and potential industry impacts.\"\"\",    expected_output=\"Full analysis report in bullet points\",    agent=researcher)task2 = Task(    description=\"\"\"Using the insights provided, develop an engaging blog    post that highlights the most significant AI advancements.    Your post should be informative yet accessible, catering to a tech-savvy audience.    Make it sound cool, avoid complex words so it doesn\\'t sound like AI.\"\"\",    expected_output=\"Full blog post of at least 4 paragraphs\",    agent=writer)# Instantiate your crew with a sequential processcrew = Crew(    agents=[researcher, writer],    tasks=[task1, task2],    verbose= False,    process = Process.sequential)# Get your crew to work!result = crew.kickoff()print(\"######################\")print(result)\\n\\n\\nView the trace in your LangSmith dashboard (example).\\n\\n\\nAdvanced configuration\\u200b\\nUse OpenTelemetry Collector for fan-out\\u200b\\nFor more advanced scenarios, you can use the OpenTelemetry Collector to fan out your telemetry data to multiple destinations. This is a more scalable approach than configuring multiple exporters in your application code.\\n\\n\\nInstall the OpenTelemetry Collector for your environment.\\n\\n\\nCreate a configuration file (e.g., otel-collector-config.yaml) that exports to multiple destinations:\\nreceivers:  otlp:    protocols:      grpc:        endpoint: 0.0.0.0:4317      http:        endpoint: 0.0.0.0:4318processors:  batch:exporters:  otlphttp/langsmith:    endpoint: https://api.smith.langchain.com/otel/v1/traces    headers:      x-api-key: ${env:LANGSMITH_API_KEY}      Langsmith-Project: my_project  otlphttp/other_provider:    endpoint: https://otel.your-provider.com/v1/traces    headers:      api-key: ${env:OTHER_PROVIDER_API_KEY}service:  pipelines:    traces:      receivers: [otlp]      processors: [batch]      exporters: [otlphttp/langsmith, otlphttp/other_provider]\\n\\n\\nConfigure your application to send to the collector:\\nimport osfrom opentelemetry import tracefrom opentelemetry.sdk.trace import TracerProviderfrom opentelemetry.sdk.trace.export import BatchSpanProcessorfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporterfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplate# Point to your local OpenTelemetry Collectorotlp_exporter = OTLPSpanExporter(    endpoint=\"http://localhost:4318/v1/traces\")provider = TracerProvider()processor = BatchSpanProcessor(otlp_exporter)provider.add_span_processor(processor)trace.set_tracer_provider(provider)# Set environment variables for LangChainos.environ[\"LANGSMITH_OTEL_ENABLED\"] = \"true\"os.environ[\"LANGSMITH_TRACING\"] = \"true\"# Create and run a LangChain applicationprompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")model = ChatOpenAI()chain = prompt | modelresult = chain.invoke({\"topic\": \"programming\"})print(result.content)\\n\\n\\nThis approach offers several advantages:\\n\\nCentralized configuration for all your telemetry destinations\\nReduced overhead in your application code\\nBetter scalability and resilience\\nAbility to add or remove destinations without changing application code\\n\\nDistributed tracing with LangChain and OpenTelemetry\\u200b\\nDistributed tracing is essential when your LLM application spans multiple services or processes. OpenTelemetry\\'s context propagation capabilities ensure that traces remain connected across service boundaries.\\nContext propagation in distributed tracing\\u200b\\nIn distributed systems, context propagation passes trace metadata between services so that related spans are linked to the same trace:\\n\\nTrace ID: A unique identifier for the entire trace\\nSpan ID: A unique identifier for the current span\\nSampling Decision: Indicates whether this trace should be sampled\\n\\nSet up distributed tracing with LangChain\\u200b\\nTo enable distributed tracing across multiple services:\\nimport osfrom opentelemetry import tracefrom opentelemetry.propagate import inject, extractfrom opentelemetry.sdk.trace import TracerProviderfrom opentelemetry.sdk.trace.export import BatchSpanProcessorfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporterimport requestsfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplate# Set up OpenTelemetry trace providerprovider = TracerProvider()otlp_exporter = OTLPSpanExporter(    endpoint=\"https://api.smith.langchain.com/otel/v1/traces\",    headers={\"x-api-key\": os.getenv(\"LANGSMITH_API_KEY\"), \"Langsmith-Project\": \"my_project\"})processor = BatchSpanProcessor(otlp_exporter)provider.add_span_processor(processor)trace.set_tracer_provider(provider)tracer = trace.get_tracer(__name__)# Service A: Create a span and propagate context to Service Bdef service_a():    with tracer.start_as_current_span(\"service_a_operation\") as span:        # Create a chain        prompt = ChatPromptTemplate.from_template(\"Summarize: {text}\")        model = ChatOpenAI()        chain = prompt | model        # Run the chain        result = chain.invoke({\"text\": \"OpenTelemetry is an observability framework\"})        # Propagate context to Service B        headers = {}        inject(headers)  # Inject trace context into headers        # Call Service B with the trace context        response = requests.post(            \"http://service-b.example.com/process\",            headers=headers,            json={\"summary\": result.content}        )        return response.json()# Service B: Extract the context and continue the tracefrom flask import Flask, request, jsonifyapp = Flask(__name__)@app.route(\"/process\", methods=[\"POST\"])def service_b_endpoint():    # Extract the trace context from the request headers    context = extract(request.headers)    with tracer.start_as_current_span(\"service_b_operation\", context=context) as span:        data = request.json        summary = data.get(\"summary\", \"\")        # Process the summary with another LLM chain        prompt = ChatPromptTemplate.from_template(\"Analyze the sentiment of: {text}\")        model = ChatOpenAI()        chain = prompt | model        result = chain.invoke({\"text\": summary})        return jsonify({\"analysis\": result.content})if __name__ == \"__main__\":    app.run(port=5000)Was this page helpful?You can leave detailed feedback on GitHub.PreviousTrace with Instructor (Python only)NextTrace with the Vercel AI SDK (JS/TS only)Trace a LangChain applicationTrace a non-LangChain applicationSend traces to an alternate providerUse environment variables for global configurationConfigure alternate OTLP endpointsSupported OpenTelemetry attribute and event mappingCore LangSmith attributesGenAI standard attributesGenAI request parametersGenAI usage metricsTraceLoop attributesOpenInference attributesLLM attributesPrompt template attributesRetriever attributesTool attributesLogfire attributesOpenTelemetry event mappingImplementation examplesTrace using the Traceloop SDKTrace using the Arize SDKAdvanced configurationUse OpenTelemetry Collector for fan-outDistributed tracing with LangChain and OpenTelemetryCommunityLangChain ForumTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright Â© 2025 LangChain, Inc.\\n\\n'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Trace with the Vercel AI SDK (JS/TS only) - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsOverviewLangChainLangGraphAnthropic (Python only)OpenAIAutoGenClaude CodeCrewAIGoogle ADKInstructor (Python only)OpenAI Agents SDKOpenTelemetrySemantic KernelVercel AI SDKManual instrumentationConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationIntegrationsTrace with the Vercel AI SDK (JS/TS only)Get startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageInstallationEnvironment configurationBasic setupWith traceableTracing in serverless environmentsPassing LangSmith configRedacting dataTracing setupIntegrationsTrace with the Vercel AI SDK (JS/TS only)Copy pageCopy pageYou can use LangSmith to trace runs from the Vercel AI SDK. This guide will walk through an example.\\n\\u200bInstallation\\nThis wrapper requires AI SDK v5 and langsmith>=0.3.63. If you are using an older version of the AI SDK or langsmith, see the OpenTelemetry (OTEL)\\nbased approach on this page.\\nInstall the Vercel AI SDK. This guide uses Vercelâ€™s OpenAI integration for the code snippets below, but you can use any of their other options as well.\\nnpmyarnpnpmCopynpm install ai @ai-sdk/openai zod\\n\\n\\u200bEnvironment configuration\\nShellCopyexport LANGSMITH_TRACING=true\\nexport LANGSMITH_API_KEY=<your-api-key>\\n\\n# The examples use OpenAI, but you can use any LLM provider of choice\\nexport OPENAI_API_KEY=<your-openai-api-key>\\n\\n# For LangSmith API keys linked to multiple workspaces, set the LANGSMITH_WORKSPACE_ID environment variable to specify which workspace to use.\\nexport LANGSMITH_WORKSPACE_ID=<your-workspace-id>\\n\\n\\u200bBasic setup\\nImport and wrap AI SDK methods, then use them as you normally would:\\nCopyimport { openai } from \"@ai-sdk/openai\";\\nimport * as ai from \"ai\";\\n\\nimport { wrapAISDK } from \"langsmith/experimental/vercel\";\\n\\nconst { generateText, streamText, generateObject, streamObject } =\\n  wrapAISDK(ai);\\n\\nawait generateText({\\n  model: openai(\"gpt-5-nano\"),\\n  prompt: \"Write a vegetarian lasagna recipe for 4 people.\",\\n});\\n\\nYou should see a trace in your LangSmith dashboard like this one.\\nYou can also trace runs with tool calls:\\nCopyimport * as ai from \"ai\";\\nimport { tool, stepCountIs } from \"ai\";\\nimport { openai } from \"@ai-sdk/openai\";\\nimport { z } from \"zod\";\\n\\nimport { wrapAISDK } from \"langsmith/experimental/vercel\";\\n\\nconst { generateText, streamText, generateObject, streamObject } =\\n  wrapAISDK(ai);\\n\\nawait generateText({\\n  model: openai(\"gpt-5-nano\"),\\n  messages: [\\n    {\\n      role: \"user\",\\n      content: \"What are my orders and where are they? My user ID is 123\",\\n    },\\n  ],\\n  tools: {\\n    listOrders: tool({\\n      description: \"list all orders\",\\n      inputSchema: z.object({ userId: z.string() }),\\n      execute: async ({ userId }) =>\\n        `User ${userId} has the following orders: 1`,\\n    }),\\n    viewTrackingInformation: tool({\\n      description: \"view tracking information for a specific order\",\\n      inputSchema: z.object({ orderId: z.string() }),\\n      execute: async ({ orderId }) =>\\n        `Here is the tracking information for ${orderId}`,\\n    }),\\n  },\\n  stopWhen: stepCountIs(5),\\n});\\n\\nWhich results in a trace like this one.\\nYou can use other AI SDK methods exactly as you usually would.\\n\\u200bWith traceable\\nYou can wrap traceable calls around AI SDK calls or within AI SDK tool calls. This is useful if you\\nwant to group runs together in LangSmith:\\nCopyimport * as ai from \"ai\";\\nimport { tool, stepCountIs } from \"ai\";\\nimport { openai } from \"@ai-sdk/openai\";\\nimport { z } from \"zod\";\\n\\nimport { traceable } from \"langsmith/traceable\";\\nimport { wrapAISDK } from \"langsmith/experimental/vercel\";\\n\\nconst { generateText, streamText, generateObject, streamObject } =\\n  wrapAISDK(ai);\\n\\nconst wrapper = traceable(async (input: string) => {\\n  const { text } = await generateText({\\n    model: openai(\"gpt-5-nano\"),\\n    messages: [\\n      {\\n        role: \"user\",\\n        content: input,\\n      },\\n    ],\\n    tools: {\\n      listOrders: tool({\\n        description: \"list all orders\",\\n        inputSchema: z.object({ userId: z.string() }),\\n        execute: async ({ userId }) =>\\n          `User ${userId} has the following orders: 1`,\\n      }),\\n      viewTrackingInformation: tool({\\n        description: \"view tracking information for a specific order\",\\n        inputSchema: z.object({ orderId: z.string() }),\\n        execute: async ({ orderId }) =>\\n          `Here is the tracking information for ${orderId}`,\\n      }),\\n    },\\n    stopWhen: stepCountIs(5),\\n  });\\n  return text;\\n}, {\\n  name: \"wrapper\",\\n});\\n\\nawait wrapper(\"What are my orders and where are they? My user ID is 123.\");\\n\\nThe resulting trace will look like this.\\n\\u200bTracing in serverless environments\\nWhen tracing in serverless environments, you must wait for all runs to flush before your environment\\nshuts down. To do this, you can pass a LangSmith Client instance when wrapping the AI SDK method,\\nthen call await client.awaitPendingTraceBatches().\\nMake sure to also pass it into any traceable wrappers you create as well:\\nCopyimport * as ai from \"ai\";\\nimport { tool, stepCountIs } from \"ai\";\\nimport { openai } from \"@ai-sdk/openai\";\\nimport { z } from \"zod\";\\n\\nimport { Client } from \"langsmith\";\\nimport { traceable } from \"langsmith/traceable\";\\nimport { wrapAISDK } from \"langsmith/experimental/vercel\";\\n\\nconst client = new Client();\\n\\nconst { generateText, streamText, generateObject, streamObject } =\\n  wrapAISDK(ai, { client });\\n\\nconst wrapper = traceable(async (input: string) => {\\n  const { text } = await generateText({\\n    model: openai(\"gpt-5-nano\"),\\n    messages: [\\n      {\\n        role: \"user\",\\n        content: input,\\n      },\\n    ],\\n    tools: {\\n      listOrders: tool({\\n        description: \"list all orders\",\\n        inputSchema: z.object({ userId: z.string() }),\\n        execute: async ({ userId }) =>\\n          `User ${userId} has the following orders: 1`,\\n      }),\\n      viewTrackingInformation: tool({\\n        description: \"view tracking information for a specific order\",\\n        inputSchema: z.object({ orderId: z.string() }),\\n        execute: async ({ orderId }) =>\\n          `Here is the tracking information for ${orderId}`,\\n      }),\\n    },\\n    stopWhen: stepCountIs(5),\\n  });\\n  return text;\\n}, {\\n  name: \"wrapper\",\\n  client,\\n});\\n\\ntry {\\n  await wrapper(\"What are my orders and where are they? My user ID is 123.\");\\n} finally {\\n  await client.awaitPendingTraceBatches();\\n}\\n\\nIf you are using Next.js, there is a convenient after hook\\nwhere you can put this logic:\\nCopyimport { after } from \"next/server\"\\nimport { Client } from \"langsmith\";\\n\\n\\nexport async function POST(request: Request) {\\n  const client = new Client();\\n\\n  ...\\n\\n  after(async () => {\\n    await client.awaitPendingTraceBatches();\\n  });\\n\\n  return new Response(JSON.stringify({ ... }), {\\n    status: 200,\\n    headers: { \"Content-Type\": \"application/json\" },\\n  });\\n};\\n\\nSee this page for more detail, including information\\naround managing rate limits in serverless environments.\\n\\u200bPassing LangSmith config\\nYou can pass LangSmith-specific config to your wrapper both when initially wrapping your\\nAI SDK methods and while running them via providerOptions.langsmith.\\nThis includes metadata (which you can later use to filter runs in LangSmith), top-level run name,\\ntags, custom client instances, and more.\\nConfig passed while wrapping will apply to all future calls you make with the wrapped method:\\nCopyimport { openai } from \"@ai-sdk/openai\";\\nimport * as ai from \"ai\";\\n\\nimport { wrapAISDK } from \"langsmith/experimental/vercel\";\\n\\nconst { generateText, streamText, generateObject, streamObject } =\\n  wrapAISDK(ai, {\\n    metadata: {\\n      key_for_all_runs: \"value\",\\n    },\\n    tags: [\"myrun\"],\\n  });\\n\\nawait generateText({\\n  model: openai(\"gpt-5-nano\"),\\n  prompt: \"Write a vegetarian lasagna recipe for 4 people.\",\\n});\\n\\nWhile passing config at runtime via providerOptions.langsmith will apply only to that run.\\nWe suggest importing and wrapping your config in createLangSmithProviderOptions to ensure\\nproper typing:\\nCopyimport { openai } from \"@ai-sdk/openai\";\\nimport * as ai from \"ai\";\\n\\nimport {\\n  wrapAISDK,\\n  createLangSmithProviderOptions,\\n} from \"langsmith/experimental/vercel\";\\n\\nconst { generateText, streamText, generateObject, streamObject } =\\n  wrapAISDK(ai);\\n\\nconst lsConfig = createLangSmithProviderOptions({\\n  metadata: {\\n    individual_key: \"value\",\\n  },\\n  name: \"my_individual_run\",\\n});\\n\\nawait generateText({\\n  model: openai(\"gpt-5-nano\"),\\n  prompt: \"Write a vegetarian lasagna recipe for 4 people.\",\\n  providerOptions: {\\n    langsmith: lsConfig,\\n  },\\n});\\n\\n\\u200bRedacting data\\nYou can customize what inputs and outputs the AI SDK sends to LangSmith by specifying custom input/output\\nprocessing functions. This is useful if you are dealing with sensitive data that you would like to\\navoid sending to LangSmith.\\nBecause output formats vary depending on which AI SDK method you are using, we suggest defining and passing config\\nindividually into wrapped methods. You will also need to provide separate functions for child LLM runs within\\nAI SDK calls, since calling generateText at top level calls the LLM internally and can do so multiple times.\\nWe also suggest passing a generic parameter into createLangSmithProviderOptions to get proper types for inputs and outputs.\\nHereâ€™s an example for generateText:\\nCopyimport {\\n  wrapAISDK,\\n  createLangSmithProviderOptions,\\n} from \"langsmith/experimental/vercel\";\\nimport * as ai from \"ai\";\\nimport { openai } from \"@ai-sdk/openai\";\\n\\nconst { generateText } = wrapAISDK(ai);\\n\\nconst lsConfig = createLangSmithProviderOptions<typeof generateText>({\\n  processInputs: (inputs) => {\\n    const { messages } = inputs;\\n    return {\\n      messages: messages?.map((message) => ({\\n        providerMetadata: message.providerOptions,\\n        role: \"assistant\",\\n        content: \"REDACTED\",\\n      })),\\n      prompt: \"REDACTED\",\\n    };\\n  },\\n  processOutputs: (outputs) => {\\n    return {\\n      providerMetadata: outputs.providerMetadata,\\n      role: \"assistant\",\\n      content: \"REDACTED\",\\n    };\\n  },\\n  processChildLLMRunInputs: (inputs) => {\\n    const { prompt } = inputs;\\n    return {\\n      messages: prompt.map((message) => ({\\n        ...message,\\n        content: \"REDACTED CHILD INPUTS\",\\n      })),\\n    };\\n  },\\n  processChildLLMRunOutputs: (outputs) => {\\n    return {\\n      providerMetadata: outputs.providerMetadata,\\n      content: \"REDACTED CHILD OUTPUTS\",\\n      role: \"assistant\",\\n    };\\n  },\\n});\\n\\nconst { text } = await generateText({\\n  model: openai(\"gpt-5-nano\"),\\n  prompt: \"What is the capital of France?\",\\n  providerOptions: {\\n    langsmith: lsConfig,\\n  },\\n});\\n\\n// Paris.\\nconsole.log(text);\\n\\nThe actual return value will contain the original, non-redacted result but the trace in LangSmith\\nwill be redacted. Hereâ€™s an example.\\nFor redacting tool input/output, wrap your execute method in a traceable like this:\\nCopyimport * as ai from \"ai\";\\nimport { tool, stepCountIs } from \"ai\";\\nimport { openai } from \"@ai-sdk/openai\";\\nimport { z } from \"zod\";\\n\\nimport { Client } from \"langsmith\";\\nimport { traceable } from \"langsmith/traceable\";\\nimport { wrapAISDK } from \"langsmith/experimental/vercel\";\\n\\nconst client = new Client();\\n\\nconst { generateText, streamText, generateObject, streamObject } =\\n  wrapAISDK(ai, { client });\\n\\nconst { text } = await generateText({\\n  model: openai(\"gpt-5-nano\"),\\n  messages: [\\n    {\\n      role: \"user\",\\n      content: \"What are my orders? My user ID is 123.\",\\n    },\\n  ],\\n  tools: {\\n    listOrders: tool({\\n      description: \"list all orders\",\\n      inputSchema: z.object({ userId: z.string() }),\\n      execute: traceable(\\n        async ({ userId }) => {\\n          return `User ${userId} has the following orders: 1`;\\n        },\\n        {\\n          processInputs: (input) => ({ text: \"REDACTED\" }),\\n          processOutputs: (outputs) => ({ text: \"REDACTED\" }),\\n          run_type: \"tool\",\\n          name: \"listOrders\",\\n        }\\n      ) as (input: { userId: string }) => Promise<string>,\\n    }),\\n  },\\n  stopWhen: stepCountIs(5),\\n});\\n\\nThe traceable return type is complex, which makes the cast\\xa0necessary. You may also omit the AI SDK tool wrapper function\\nif you wish to avoid the cast.Was this page helpful?YesNoSuggest editsSemantic KernelCustom instrumentationâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk_legacy', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk_legacy', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Trace with the Vercel AI SDK (Legacy) - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewSetupInstall on KubernetesInstall on DockerInteract with an installationUpgrade an installationConfigure egress for subscription metricsView trace counts across an organizationLangSmith-managed ClickHouseConfigurationConfigure for scaleEnable TTL & data retentionCreate an Ingress for installations (Kubernetes)Mirror images for your installationUse environment variables for model providersTroubleshootingAuthentication & access controlSet up basic authenticationSet up SSO with OAuth2.0 & OIDCCustomize user managementConfigure custom TLS certificatesUse an existing secret for your installation (Kubernetes)Connect external servicesEnable blob storageConnect to an external ClickHouse databaseConnect to an external PostgreSQL databaseConnect to an external Redis databaseScriptsDelete workspacesDelete organizationsDelete tracesGenerate ClickHouse StatsGenerate query statsRun support queries against PostgreSQLRun support queries against ClickHouseObservabilityExport LangSmith telemetry to your observability backendConfigure your collector for telemetryDeploy an observability stackOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationTrace with the Vercel AI SDK (Legacy)Get startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this page0. Installation1. Configure your environment2. Log a traceNode.jsWith traceableNext.jsSentryAdd other metadataCustomize run nameTrace with the Vercel AI SDK (Legacy)Copy pageCopy pageThis page documents an older method of tracing AI SDK runs. For a simpler and more general method that does not require OTEL setup, see the new guide.\\nYou can use LangSmith to trace runs from the Vercel AI SDK using OpenTelemetry (OTEL). This guide will walk through an example.\\nMany popular OpenTelemetry implementations in JavaScript are currently experimental,\\nand may behave erratically in production, especially when instrumenting LangSmith alongside other providers. If you are on AI SDK 5,\\nwe strongly suggest using our recommended approach for tracing AI SDK runs.\\n\\u200b0. Installation\\nInstall the Vercel AI SDK and required OTEL packages. We use their OpenAI integration for the code snippets below, but you can use any of their other options as well.\\nnpmyarnpnpmCopynpm install ai @ai-sdk/openai zod\\n\\nnpmyarnpnpmCopynpm install @opentelemetry/sdk-trace-base @opentelemetry/exporter-trace-otlp-proto @opentelemetry/context-async-hooks\\n\\n\\u200b1. Configure your environment\\nCopyexport LANGSMITH_TRACING=true\\nexport LANGSMITH_API_KEY=<your-api-key>\\nexport LANGSMITH_OTEL_ENABLED=true\\n\\n# This example uses OpenAI, but you can use any LLM provider of choice\\nexport OPENAI_API_KEY=<your-openai-api-key>\\n\\n\\u200b2. Log a trace\\n\\u200bNode.js\\nTo start tracing, you will need to import and call the initializeOTEL method at the start of your code:\\nCopyimport { initializeOTEL } from \"langsmith/experimental/otel/setup\";\\n\\nconst { DEFAULT_LANGSMITH_SPAN_PROCESSOR } = initializeOTEL();\\n\\nAfterwards, add the experimental_telemetry argument to your AI SDK calls that you want to trace.\\nDo not forget to call await DEFAULT_LANGSMITH_SPAN_PROCESSOR.shutdown(); before your application shuts down in order to flush any remaining traces to LangSmith.\\nCopyimport { generateText } from \"ai\";\\nimport { openai } from \"@ai-sdk/openai\";\\n\\nlet result;\\ntry {\\n  result = await generateText({\\n    model: openai(\"gpt-4.1-nano\"),\\n    prompt: \"Write a vegetarian lasagna recipe for 4 people.\",\\n    experimental_telemetry: {\\n      isEnabled: true,\\n    },\\n  });\\n} finally {\\n  await DEFAULT_LANGSMITH_SPAN_PROCESSOR.shutdown();\\n}\\n\\nYou should see a trace in your LangSmith dashboard like this one.\\nYou can also trace runs with tool calls:\\nCopyimport { generateText, tool } from \"ai\";\\nimport { openai } from \"@ai-sdk/openai\";\\nimport { z } from \"zod\";\\n\\nawait generateText({\\n  model: openai(\"gpt-4.1-nano\"),\\n  messages: [\\n    {\\n      role: \"user\",\\n      content: \"What are my orders and where are they? My user ID is 123\",\\n    },\\n  ],\\n  tools: {\\n    listOrders: tool({\\n      description: \"list all orders\",\\n      parameters: z.object({ userId: z.string() }),\\n      execute: async ({ userId }) =>\\n        `User ${userId} has the following orders: 1`,\\n    }),\\n    viewTrackingInformation: tool({\\n      description: \"view tracking information for a specific order\",\\n      parameters: z.object({ orderId: z.string() }),\\n      execute: async ({ orderId }) =>\\n        `Here is the tracking information for ${orderId}`,\\n    }),\\n  },\\n  experimental_telemetry: {\\n    isEnabled: true,\\n  },\\n  maxSteps: 10,\\n});\\n\\nWhich results in a trace like this one.\\n\\u200bWith traceable\\nYou can wrap traceable calls around or within AI SDK tool calls. If you do so, we recommend you initialize a LangSmith client instance that you pass into each traceable, then call client.awaitPendingTraceBatches(); to ensure all traces flush. If you do this, you do not need to manually call shutdown() or forceFlush() on the DEFAULT_LANGSMITH_SPAN_PROCESSOR. Hereâ€™s an example:\\nCopyimport { initializeOTEL } from \"langsmith/experimental/otel/setup\";\\n\\ninitializeOTEL();\\n\\nimport { Client } from \"langsmith\";\\nimport { traceable } from \"langsmith/traceable\";\\nimport { generateText, tool } from \"ai\";\\nimport { openai } from \"@ai-sdk/openai\";\\nimport { z } from \"zod\";\\n\\nconst client = new Client();\\n\\nconst wrappedText = traceable(\\n  async (content: string) => {\\n    const { text } = await generateText({\\n      model: openai(\"gpt-4.1-nano\"),\\n      messages: [{ role: \"user\", content }],\\n      tools: {\\n        listOrders: tool({\\n          description: \"list all orders\",\\n          parameters: z.object({ userId: z.string() }),\\n          execute: async ({ userId }) => {\\n            const getOrderNumber = traceable(\\n              async () => {\\n                return \"1234\";\\n              },\\n              { name: \"getOrderNumber\" }\\n            );\\n            const orderNumber = await getOrderNumber();\\n            return `User ${userId} has the following order: ${orderNumber}`;\\n          },\\n        }),\\n      },\\n      experimental_telemetry: {\\n        isEnabled: true,\\n      },\\n      maxSteps: 10,\\n    });\\n    return { text };\\n  },\\n  { name: \"parentTraceable\", client }\\n);\\n\\nlet result;\\ntry {\\n  result = await wrappedText(\"What are my orders?\");\\n} finally {\\n  await client.awaitPendingTraceBatches();\\n}\\n\\nThe resulting trace will look like this.\\n\\u200bNext.js\\nFirst, install the @vercel/otel package:\\nnpmyarnpnpmCopynpm install @vercel/otel\\n\\nThen, set up an instrumentation.ts file in your root directory.\\nCall initializeOTEL and pass the resulting DEFAULT_LANGSMITH_SPAN_PROCESSOR into the spanProcessors field into your registerOTEL(...) call.\\nIt should look something like this:\\nCopyimport { registerOTel } from \"@vercel/otel\";\\nimport { initializeOTEL } from \"langsmith/experimental/otel/setup\";\\n\\nconst { DEFAULT_LANGSMITH_SPAN_PROCESSOR } = initializeOTEL({});\\n\\nexport function register() {\\n  registerOTel({\\n    serviceName: \"your-project-name\",\\n    spanProcessors: [DEFAULT_LANGSMITH_SPAN_PROCESSOR],\\n  });\\n}\\n\\nAnd finally, in your API routes, call initializeOTEL as well and add an experimental_telemetry field to your AI SDK calls:\\nCopyimport { generateText } from \"ai\";\\nimport { openai } from \"@ai-sdk/openai\";\\n\\nimport { initializeOTEL } from \"langsmith/experimental/otel/setup\";\\n\\ninitializeOTEL();\\n\\nexport async function GET() {\\n  const { text } = await generateText({\\n    model: openai(\"gpt-4.1-nano\"),\\n    messages: [{ role: \"user\", content: \"Why is the sky blue?\" }],\\n    experimental_telemetry: {\\n      isEnabled: true,\\n    },\\n  });\\n\\n  return new Response(text);\\n}\\n\\nYou can also wrap parts of your code in traceables for more granularity.\\n\\u200bSentry\\nIf youâ€™re using Sentry, you can attach the LangSmith trace exporter to Sentryâ€™s default OpenTelemetry instrumentation as show in the example below.\\nAt time of writing, Sentry only supports OTEL v1 packages. LangSmith supports both v1 and v2, but you must make sure you install OTEL v1 packages in order to make instrumentation work.npmyarnpnpmCopynpm install @opentelemetry/sdk-trace-base@1.30.1 @opentelemetry/exporter-trace-otlp-proto@0.57.2 @opentelemetry/context-async-hooks@1.30.1\\n\\nCopyimport { initializeOTEL } from \"langsmith/experimental/otel/setup\";\\nimport { LangSmithOTLPTraceExporter } from \"langsmith/experimental/otel/exporter\";\\nimport { BatchSpanProcessor } from \"@opentelemetry/sdk-trace-base\";\\nimport { traceable } from \"langsmith/traceable\";\\nimport { generateText, tool } from \"ai\";\\nimport { openai } from \"@ai-sdk/openai\";\\nimport { z } from \"zod\";\\nimport * as Sentry from \"@sentry/node\";\\nimport { Client } from \"langsmith\";\\n\\nconst exporter = new LangSmithOTLPTraceExporter();\\nconst spanProcessor = new BatchSpanProcessor(exporter);\\n\\nconst sentry = Sentry.init({\\n  dsn: \"...\",\\n  tracesSampleRate: 1.0,\\n  openTelemetrySpanProcessors: [spanProcessor],\\n});\\n\\ninitializeOTEL({\\n  globalTracerProvider: sentry?.traceProvider,\\n});\\n\\nconst wrappedText = traceable(\\n  async (content: string) => {\\n    const { text } = await generateText({\\n      model: openai(\"gpt-4.1-nano\"),\\n      messages: [{ role: \"user\", content }],\\n      experimental_telemetry: {\\n        isEnabled: true,\\n      },\\n      maxSteps: 10,\\n    });\\n    return { text };\\n  },\\n  { name: \"parentTraceable\" }\\n);\\n\\nlet result;\\ntry {\\n  result = await wrappedText(\"What color is the sky?\");\\n} finally {\\n  await sentry?.traceProvider?.shutdown();\\n}\\n\\n\\u200bAdd other metadata\\nYou can add other metadata to your traces to help organize and filter them in the LangSmith UI:\\nCopyimport { generateText } from \"ai\";\\nimport { openai } from \"@ai-sdk/openai\";\\n\\nawait generateText({\\n  model: openai(\"gpt-4.1-nano\"),\\n  prompt: \"Write a vegetarian lasagna recipe for 4 people.\",\\n  experimental_telemetry: {\\n    isEnabled: true,\\n    metadata: { userId: \"123\", language: \"english\" },\\n  },\\n});\\n\\nMetadata will be visible in your LangSmith dashboard and can be used to filter and search for specific traces.\\nNote that AI SDK propagates metadata on internal child spans as well.\\n\\u200bCustomize run name\\nYou can customize the run name by passing a metadata key named ls_run_name into experimental_telemetry.\\nCopyimport { generateText } from \"ai\";\\nimport { openai } from \"@ai-sdk/openai\";\\n\\nawait generateText({\\n  model: openai(\"gpt-4o-mini\"),\\n  prompt: \"Write a vegetarian lasagna recipe for 4 people.\",\\n  experimental_telemetry: {\\n    isEnabled: true,\\n    // highlight-start\\n    metadata: {\\n      ls_run_name: \"my-custom-run-name\",\\n    },\\n    // highlight-end\\n  },\\n});\\nWas this page helpful?YesNoSuggest editsâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Trace without setting environment variables - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsManual instrumentationConfiguration & troubleshootingProject & environment settingsLog traces to a specific projectTrace without env varsSet a sampling rate for tracesAdvanced tracing techniquesData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationProject & environment settingsTrace without setting environment variablesGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumConfiguration & troubleshootingProject & environment settingsTrace without setting environment variablesCopy pageCopy pageAs mentioned in other guides, the following environment variables allow you to configure tracing enabled, the api endpoint, the api key, and the tracing project:\\n\\nLANGSMITH_TRACING\\nLANGSMITH_API_KEY\\nLANGSMITH_ENDPOINT\\nLANGSMITH_PROJECT\\n\\nIf you need to trace runs with a custom configuration, are working in an environment that doesnâ€™t support typical environment variables (such as Cloudflare Workers), or would simply prefer not to rely on environment variables, LangSmith allows you to configure tracing programmatically.\\nDue to a number of asks for finer-grained control of tracing using the trace context manager, we changed the behavior of with trace to honor the LANGSMITH_TRACING environment variable in version 0.1.95 of the Python SDK. You can find more details in the release notes. The recommended way to disable/enable tracing without setting environment variables is to use the with tracing_context context manager, as shown in the example below.\\n\\nPython: The recommended way to do this in Python is to use the tracing_context context manager. This works for both code annotated with traceable and code within the trace context manager.\\nTypeScript: You can pass in both the client and the tracingEnabled flag to the traceable decorator.\\n\\nPythonTypeScriptCopyimport openai\\nfrom langsmith import Client, tracing_context, traceable\\nfrom langsmith.wrappers import wrap_openai\\n\\nlangsmith_client = Client(\\n  api_key=\"YOUR_LANGSMITH_API_KEY\",  # This can be retrieved from a secrets manager\\n  api_url=\"https://api.smith.langchain.com\",  # Update appropriately for self-hosted installations or the EU region\\n  workspace_id=\"YOUR_WORKSPACE_ID\", # Must be specified for API keys scoped to multiple workspaces\\n)\\n\\nclient = wrap_openai(openai.Client())\\n\\n@traceable(run_type=\"tool\", name=\"Retrieve Context\")\\ndef my_tool(question: str) -> str:\\n  return \"During this morning\\'s meeting, we solved all world conflict.\"\\n\\n@traceable\\ndef chat_pipeline(question: str):\\n  context = my_tool(question)\\n  messages = [\\n      { \"role\": \"system\", \"content\": \"You are a helpful assistant. Please respond to the user\\'s request only based on the given context.\" },\\n      { \"role\": \"user\", \"content\": f\"Question: {question}\\\\nContext: {context}\"}\\n  ]\\n  chat_completion = client.chat.completions.create(\\n      model=\"gpt-4o-mini\", messages=messages\\n  )\\n  return chat_completion.choices[0].message.content\\n\\n# Can set to False to disable tracing here without changing code structure\\nwith tracing_context(enabled=True):\\n  # Use langsmith_extra to pass in a custom client\\n  chat_pipeline(\"Can you summarize this morning\\'s meetings?\", langsmith_extra={\"client\": langsmith_client})\\n\\nIf you prefer a video tutorial, check out the Alternative Ways to Trace video from the Introduction to LangSmith Course.Was this page helpful?YesNoSuggest editsLog traces to a specific projectSet a sampling rate for tracesâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Upload files with traces - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsManual instrumentationConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyAdd metadata and tags to tracesPrevent logging of sensitive data in tracesUpload files with tracesTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationData & privacyUpload files with tracesGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pagePythonTypeScriptConfiguration & troubleshootingData & privacyUpload files with tracesCopy pageCopy pageBefore diving into this content, it would be helpful to read the following guides:\\nTrace with LangSmith using the traceable decorator or wrapper\\n\\nThe following features are available in the following SDK versions:\\nPython SDK: >=0.1.141\\nJS/TS SDK: >=0.2.5\\n\\nLangSmith supports uploading binary files (such as images, audio, videos, PDFs, and CSVs) with your traces. This is particularly useful when working with LLM pipelines using multimodal inputs or outputs.\\nIn both the Python and TypeScript SDKs, attachments can be added to your traces by specifying the MIME type and binary content of each file. This guide explains how to define and trace attachments using the Attachment type in Python and Uint8Array / ArrayBuffer in TypeScript.\\n\\u200bPython\\nIn the Python SDK, you can use the Attachment type to add files to your traces. Each Attachment requires:\\n\\nmime_type (str): The MIME type of the file (e.g., \"image/png\").\\ndata (bytes | Path): The binary content of the file, or the file path.\\n\\nYou can also define an attachment with a tuple tuple of the form (mime_type, data) for convenience.\\nSimply decorate a function with @traceable and include your Attachment instances as arguments. Note that to use the file path instead of the raw bytes, you need to set the dangerously_allow_filesystem flag to True in your traceable decorator.\\nPythonCopyfrom langsmith import traceable\\nfrom langsmith.schemas import Attachment\\nfrom pathlib import Path\\nimport os\\n\\n# Must set dangerously_allow_filesystem to True if you want to use file paths\\n@traceable(dangerously_allow_filesystem=True)\\ndef trace_with_attachments(\\n    val: int,\\n    text: str,\\n    image: Attachment,\\n    audio: Attachment,\\n    video: Attachment,\\n    pdf: Attachment,\\n    csv: Attachment,\\n):\\n    return f\"Processed: {val}, {text}, {len(image.data)}, {len(audio.data)}, {len(video.data)}, {len(pdf.data), {len(csv.data)}}\"\\n\\n# Helper function to load files as bytes\\ndef load_file(file_path: str) -> bytes:\\n    with open(file_path, \"rb\") as f:\\n        return f.read()\\n\\n# Load files and create attachments\\nimage_data = load_file(\"my_image.png\")\\naudio_data = load_file(\"my_mp3.mp3\")\\nvideo_data = load_file(\"my_video.mp4\")\\npdf_data = load_file(\"my_document.pdf\")\\n\\nimage_attachment = Attachment(mime_type=\"image/png\", data=image_data)\\naudio_attachment = Attachment(mime_type=\"audio/mpeg\", data=audio_data)\\nvideo_attachment = Attachment(mime_type=\"video/mp4\", data=video_data)\\npdf_attachment = (\"application/pdf\", pdf_data) # Can just define as tuple of (mime_type, data)\\ncsv_attachment = Attachment(mime_type=\"text/csv\", data=Path(os.getcwd()) / \"my_csv.csv\")\\n\\n# Define other parameters\\nval = 42\\ntext = \"Hello, world!\"\\n\\n# Call the function with traced attachments\\nresult = trace_with_attachments(\\n    val=val,\\n    text=text,\\n    image=image_attachment,\\n    audio=audio_attachment,\\n    video=video_attachment,\\n    pdf=pdf_attachment,\\n    csv=csv_attachment,\\n)\\n\\n\\u200bTypeScript\\nIn the TypeScript SDK, you can add attachments to traces by using Uint8Array or ArrayBuffer as data types. Each attachmentâ€™s MIME type is specified within extractAttachments:\\n\\nUint8Array: Useful for handling binary data directly.\\nArrayBuffer: Represents fixed-length binary data, which can be converted to Uint8Array as needed.\\n\\nWrap your function with traceable and include your attachments within the extractAttachments option.\\nIn the TypeScript SDK, the extractAttachments function is an optional parameter in the traceable configuration. When the traceable-wrapped function is invoked, it extracts binary data (e.g., images, audio files) from your inputs and logs them alongside other trace data, specifying their MIME types.\\nNote that you cannot directly pass in a file path in the TypeScript SDK, as accessing local files is not supported in all runtime environments.\\nTypeScriptCopytype AttachmentData = Uint8Array | ArrayBuffer;\\ntype Attachments = Record<string, [string, AttachmentData]>;\\n\\nextractAttachments?: (\\n    ...args: Parameters<Func>\\n) => [Attachments | undefined, KVMap];\\n\\nTypeScriptCopyimport { traceable } from \"langsmith/traceable\";\\n\\nconst traceableWithAttachments = traceable(\\n    (\\n        val: number,\\n        text: string,\\n        attachment: Uint8Array,\\n        attachment2: ArrayBuffer,\\n        attachment3: Uint8Array,\\n        attachment4: ArrayBuffer,\\n        attachment5: Uint8Array,\\n    ) =>\\n        `Processed: ${val}, ${text}, ${attachment.length}, ${attachment2.byteLength}, ${attachment3.length}, ${attachment4.byteLength}, ${attachment5.byteLength}`,\\n    {\\n        name: \"traceWithAttachments\",\\n        extractAttachments: (\\n            val: number,\\n            text: string,\\n            attachment: Uint8Array,\\n            attachment2: ArrayBuffer,\\n            attachment3: Uint8Array,\\n            attachment4: ArrayBuffer,\\n            attachment5: Uint8Array,\\n        ) => [\\n            {\\n                \"image inputs\": [\"image/png\", attachment],\\n                \"mp3 inputs\": [\"audio/mpeg\", new Uint8Array(attachment2)],\\n                \"video inputs\": [\"video/mp4\", attachment3],\\n                \"pdf inputs\": [\"application/pdf\", new Uint8Array(attachment4)],\\n                \"csv inputs\": [\"text/csv\", new Uint8Array(attachment5)],\\n            },\\n            { val, text },\\n        ],\\n    }\\n);\\n\\nconst fs = Deno // or Node.js fs module\\nconst image = await fs.readFile(\"my_image.png\"); // Uint8Array\\nconst mp3Buffer = await fs.readFile(\"my_mp3.mp3\");\\nconst mp3ArrayBuffer = mp3Buffer.buffer; // Convert to ArrayBuffer\\nconst video = await fs.readFile(\"my_video.mp4\"); // Uint8Array\\nconst pdfBuffer = await fs.readFile(\"my_document.pdf\");\\nconst pdfArrayBuffer = pdfBuffer.buffer; // Convert to ArrayBuffer\\nconst csv = await fs.readFile(\"test-vals.csv\"); // Uint8Array\\n\\n// Define example parameters\\nconst val = 42;\\nconst text = \"Hello, world!\";\\n\\n// Call traceableWithAttachments with the files\\nconst result = await traceableWithAttachments(\\n    val, text, image, mp3ArrayBuffer, video, pdfArrayBuffer, csv\\n);\\n\\nHere is how the above would look in the LangSmith UI. You can expand each attachment to view its contents.\\nWas this page helpful?YesNoSuggest editsPrevent logging of sensitive data in tracesTroubleshoot trace nestingâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/webhooks', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/webhooks', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Configure webhook notifications for rules - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsManual instrumentationConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationAutomationsConfigure webhook notifications for rulesGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageWebhook payloadSecurityWebhook custom HTTP headersWebhook DeliveryExample with ModalSetupSecretsServiceHooking it upAutomationsConfigure webhook notifications for rulesCopy pageCopy pageWhen you add a webhook URL on an automation action, we will make a POST request to your webhook endpoint any time the rules you defined match any new runs.\\n\\n\\u200bWebhook payload\\nThe payload we send to your webhook endpoint contains:\\n\\n\"rule_id\": this is the ID of the automation that sent this payload\\n\"start_time\" and \"end_time\": these are the time boundaries where we found matching runs\\n\"runs\": this is an array of runs, where each run is a dictionary. If you need more information about each run we suggest using our SDK in your endpoint to fetch it from our API.\\n\"feedback_stats\": this is a dictionary with the feedback statistics for the runs. An example payload for this field is shown below.\\n\\nCopy\"feedback_stats\": {\\n    \"about_langchain\": {\\n        \"n\": 1,\\n        \"avg\": 0.0,\\n        \"show_feedback_arrow\": true,\\n        \"values\": {}\\n    },\\n    \"category\": {\\n        \"n\": 0,\\n        \"avg\": null,\\n        \"show_feedback_arrow\": true,\\n        \"values\": {\\n            \"CONCEPTUAL\": 1\\n        }\\n    },\\n    \"user_score\": {\\n        \"n\": 2,\\n        \"avg\": 0.0,\\n        \"show_feedback_arrow\": false,\\n        \"values\": {}\\n    },\\n    \"vagueness\": {\\n        \"n\": 1,\\n        \"avg\": 0.0,\\n        \"show_feedback_arrow\": true,\\n        \"values\": {}\\n    }\\n}\\n\\nfetching from S3 URLsDepending on how recent your runs are, the inputs_s3_urls and outputs_s3_urls fields may contain S3 URLs to the actual data instead of the data itself.The inputs and outputs can be fetched by the ROOT.presigned_url provided in inputs_s3_urls and outputs_s3_urls respectively.\\nThis is an example of the entire payload we send to your webhook endpoint:\\nCopy{\\n  \"rule_id\": \"d75d7417-0c57-4655-88fe-1db3cda3a47a\",\\n  \"start_time\": \"2024-04-05T01:28:54.734491+00:00\",\\n  \"end_time\": \"2024-04-05T01:28:56.492563+00:00\",\\n  \"runs\": [\\n    {\\n      \"status\": \"success\",\\n      \"is_root\": true,\\n      \"trace_id\": \"6ab80f10-d79c-4fa2-b441-922ed6feb630\",\\n      \"dotted_order\": \"20230505T051324571809Z6ab80f10-d79c-4fa2-b441-922ed6feb630\",\\n      \"run_type\": \"tool\",\\n      \"modified_at\": \"2024-04-05T01:28:54.145062\",\\n      \"tenant_id\": \"2ebda79f-2946-4491-a9ad-d642f49e0815\",\\n      \"end_time\": \"2024-04-05T01:28:54.085649\",\\n      \"name\": \"Search\",\\n      \"start_time\": \"2024-04-05T01:28:54.085646\",\\n      \"id\": \"6ab80f10-d79c-4fa2-b441-922ed6feb630\",\\n      \"session_id\": \"6a3be6a2-9a8c-4fc8-b4c6-a8983b286cc5\",\\n      \"parent_run_ids\": [],\\n      \"child_run_ids\": null,\\n      \"direct_child_run_ids\": null,\\n      \"total_tokens\": 0,\\n      \"completion_tokens\": 0,\\n      \"prompt_tokens\": 0,\\n      \"total_cost\": null,\\n      \"completion_cost\": null,\\n      \"prompt_cost\": null,\\n      \"first_token_time\": null,\\n      \"app_path\": \"/o/2ebda79f-2946-4491-a9ad-d642f49e0815/projects/p/6a3be6a2-9a8c-4fc8-b4c6-a8983b286cc5/r/6ab80f10-d79c-4fa2-b441-922ed6feb630?trace_id=6ab80f10-d79c-4fa2-b441-922ed6feb630&start_time=2023-05-05T05:13:24.571809\",\\n      \"in_dataset\": false,\\n      \"last_queued_at\": null,\\n      \"inputs\": null,\\n      \"inputs_s3_urls\": null,\\n      \"outputs\": null,\\n      \"outputs_s3_urls\": null,\\n      \"extra\": null,\\n      \"events\": null,\\n      \"feedback_stats\": null,\\n      \"serialized\": null,\\n      \"share_token\": null\\n    }\\n  ]\\n}\\n\\n\\u200bSecurity\\nWe strongly recommend you add a secret query string parameter to the webhook URL, and verify it on any incoming request. This ensures that if someone discovers your webhook URL you can distinguish those calls from authentic webhook notifications.\\nAn example would be\\nCopyhttps://api.example.com/langsmith_webhook?secret=38ee77617c3a489ab6e871fbeb2ec87d\\n\\n\\u200bWebhook custom HTTP headers\\nIf youâ€™d like to send any specific headers with your webhook, this can be configured per URL. To set this up, click on the Headers option next to the URL field and add your headers.\\nHeaders are stored in encrypted format.\\n\\n\\u200bWebhook Delivery\\nWhen delivering events to your webhook endpoint we follow these guidelines\\n\\nIf we fail to connect to your endpoint, we retry the transport connection up to 2 times, before declaring the delivery failed.\\nIf your endpoint takes longer than 5 seconds to reply we declare the delivery failed and do not .\\nIf your endpoint returns a 5xx status code in less than 5 seconds we retry up to 2 times with exponential backoff.\\nIf your endpoint returns a 4xx status code, we declare the delivery failed and do not retry.\\nAnything your endpoint returns in the body will be ignored\\n\\n\\u200bExample with Modal\\n\\u200bSetup\\nFor an example of how to set this up, we will use Modal. Modal provides autoscaling GPUs for inference and fine-tuning, secure containerization for code agents, and serverless Python web endpoints. Weâ€™ll focus on the web endpoints here.\\nFirst, create a Modal account. Then, locally install the Modal SDK:\\npipuvCopypip install modal\\n\\nTo finish setting up your account, run the command:\\nCopymodal setup\\n\\nand follow the instructions\\n\\u200bSecrets\\nNext, you will need to set up some secrets in Modal.\\nFirst, LangSmith will need to authenticate to Modal by passing in a secret.\\nThe easiest way to do this is to pass in a secret in the query parameters.\\nTo validate this secret, we will need to add a secret in Modal to validate it.\\nWe will do that by creating a Modal secret.\\nYou can see instructions for secrets here.\\nFor this purpose, letâ€™s call our secret ls-webhook and have it set an environment variable with the name LS_WEBHOOK.\\nWe can also set up a LangSmith secret - luckily there is already an integration template for this!\\n\\n\\u200bService\\nAfter that, you can create a Python file that will serve as your endpoint.\\nAn example is below, with comments explaining what is going on:\\nCopyfrom fastapi import HTTPException, status, Request, Query\\nfrom modal import Secret, Stub, web_endpoint, Image\\n\\nstub = Stub(\"auth-example\", image=Image.debian_slim().pip_install(\"langsmith\"))\\n\\n\\n@stub.function(\\n    secrets=[Secret.from_name(\"ls-webhook\"), Secret.from_name(\"my-langsmith-secret\")]\\n)\\n# We want this to be a `POST` endpoint since we will post data here\\n@web_endpoint(method=\"POST\")\\n# We set up a `secret` query parameter\\ndef f(data: dict, secret: str = Query(...)):\\n    # You can import dependencies you don\\'t have locally inside Modal functions\\n    from langsmith import Client\\n\\n    # First, we validate the secret key we pass\\n    import os\\n\\n    if secret != os.environ[\"LS_WEBHOOK\"]:\\n        raise HTTPException(\\n            status_code=status.HTTP_401_UNAUTHORIZED,\\n            detail=\"Incorrect bearer token\",\\n            headers={\"WWW-Authenticate\": \"Bearer\"},\\n        )\\n\\n    # This is where we put the logic for what should happen inside this webhook\\n    ls_client = Client()\\n    runs = data[\"runs\"]\\n    ids = [r[\"id\"] for r in runs]\\n    feedback = list(ls_client.list_feedback(run_ids=ids))\\n    for r, f in zip(runs, feedback):\\n        try:\\n            ls_client.create_example(\\n                inputs=r[\"inputs\"],\\n                outputs={\"output\": f.correction},\\n                dataset_name=\"classifier-github-issues\",\\n            )\\n        except Exception:\\n            raise ValueError(f\"{r} and {f}\")\\n    # Function body\\n    return \"success!\"\\n\\nWe can now deploy this easily with modal deploy ... (see docs here).\\nYou should now get something like:\\nCopyâœ“ Created objects.\\nâ”œâ”€â”€ ðŸ”¨ Created mount /Users/harrisonchase/workplace/langsmith-docs/example-webhook.py\\nâ”œâ”€â”€ ðŸ”¨ Created mount PythonPackage:langsmith\\nâ””â”€â”€ ðŸ”¨ Created f => https://hwchase17--auth-example-f.modal.run\\nâœ“ App deployed! ðŸŽ‰\\n\\nView Deployment: https://modal.com/apps/hwchase17/auth-example\\n\\nThe important thing to remember is https://hwchase17--auth-example-f.modal.run - the function we created to run.\\nNOTE: this is NOT the final deployment URL, make sure not to accidentally use that.\\n\\u200bHooking it up\\nWe can now take the function URL we create above and add it as a webhook.\\nWe have to remember to also pass in the secret key as a query parameter.\\nPutting it all together, it should look something like:\\nCopyhttps://hwchase17--auth-example-f-dev.modal.run?secret={SECRET}\\n\\nReplace {SECRET} with the secret key you created to access the Modal service.Was this page helpful?YesNoSuggest editsSet up automation rulesLog user feedback using the SDKâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/tutorials', 'loc': 'https://docs.smith.langchain.com/observability/tutorials', 'changefreq': 'weekly', 'priority': '0.5'}, page_content=\"Page Not FoundSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewSetupInstall on KubernetesInstall on DockerInteract with an installationUpgrade an installationConfigure egress for subscription metricsView trace counts across an organizationLangSmith-managed ClickHouseConfigurationConfigure for scaleEnable TTL & data retentionCreate an Ingress for installations (Kubernetes)Mirror images for your installationUse environment variables for model providersTroubleshootingAuthentication & access controlSet up basic authenticationSet up SSO with OAuth2.0 & OIDCCustomize user managementConfigure custom TLS certificatesUse an existing secret for your installation (Kubernetes)Connect external servicesEnable blob storageConnect to an external ClickHouse databaseConnect to an external PostgreSQL databaseConnect to an external Redis databaseScriptsDelete workspacesDelete organizationsDelete tracesGenerate ClickHouse StatsGenerate query statsRun support queries against PostgreSQLRun support queries against ClickHouseObservabilityExport LangSmith telemetry to your observability backendConfigure your collector for telemetryDeploy an observability stackOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationPage Not FoundGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForum404Page Not FoundWe couldn't find the page you were looking for. Maybe you were looking for?Deploy an observability stack for your LangSmith deploymentObservabilityObservabilityAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/tutorials/observability', 'loc': 'https://docs.smith.langchain.com/observability/tutorials/observability', 'changefreq': 'weekly', 'priority': '0.5'}, page_content=\"Page Not FoundSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewSetupInstall on KubernetesInstall on DockerInteract with an installationUpgrade an installationConfigure egress for subscription metricsView trace counts across an organizationLangSmith-managed ClickHouseConfigurationConfigure for scaleEnable TTL & data retentionCreate an Ingress for installations (Kubernetes)Mirror images for your installationUse environment variables for model providersTroubleshootingAuthentication & access controlSet up basic authenticationSet up SSO with OAuth2.0 & OIDCCustomize user managementConfigure custom TLS certificatesUse an existing secret for your installation (Kubernetes)Connect external servicesEnable blob storageConnect to an external ClickHouse databaseConnect to an external PostgreSQL databaseConnect to an external Redis databaseScriptsDelete workspacesDelete organizationsDelete tracesGenerate ClickHouse StatsGenerate query statsRun support queries against PostgreSQLRun support queries against ClickHouseObservabilityExport LangSmith telemetry to your observability backendConfigure your collector for telemetryDeploy an observability stackOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationPage Not FoundGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForum404Page Not FoundWe couldn't find the page you were looking for. Maybe you were looking for?Deploy an observability stack for your LangSmith deploymentObservabilityObservabilityAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/pricing', 'loc': 'https://docs.smith.langchain.com/pricing', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Plans and Pricing - LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nProducts\\n\\nFrameworksLangGraphLangChainPlatformsLangSmithLangGraph PlatformResources\\n\\nGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocs\\n\\nPythonLangGraphLangSmithLangChainJavaScriptLangGraphLangSmithLangChainCompany\\n\\nAboutCareersPricingGet a demoSign upPlans for teams of\\xa0any\\xa0sizeGet all the LangChain products -- pay for what you useDeveloperFor hobbyist projects by solo devs.Starting at$0 / monththen pay as you goFirst 5k traces included, starting at $0.50 per 1k base \\xa0traces thereafter.\\n\\n\\n\\nStart for freeGet started with:\\n\\nTracing to debug agent execution\\n\\nContinuous evals, both online and offline\\n\\nPrompt Hub, Playground, and Canvas for auto improving prompts\\n\\nAnnotation queues for human feedback\\n\\nMonitoring and alertingPlusBatteries-included tooling for building reliable agents fast.Starting at$39 / monththen pay as you goFirst 10k traces included, starting at $0.50 per 1k base \\xa0traces thereafter.\\n\\n\\n\\nSign upEverything in the Developer plan, and:\\n\\nUp to 10 seats\\n\\nHigher rate limits for LangSmith traces\\n\\n1 dev-sized LangGraph Platform deployment included\\n\\nEmail supportEnterpriseFor teams with advanced deployment, security, and support needs.\\xa0CustomContact salesEverything in the Plus plan, and:\\n\\nAlternative LangSmith & LangGraph Platform deployment options, including hybrid and self-hosted so data doesnâ€™t leave your VPC\\n\\nCustom SSO\\xa0and RBAC\\n\\nAcccess to deployed engineering team\\n\\nSupport SLA\\n\\nTeam trainings & architectural guidanceLangSmith for Startups and Education.  Seed stage startups and educational institutions, reach out for starter pricing and get shipping today.Reach out to learn about startup pricing for a period of time.Startups\\n\\n\\nEducation\\n\\n\\nEnquire about startup pricing\\n\\n\\nLangSmith for Startups and EducationSeed stage startups and educational institutions, reach out for starter pricing and get shipping today.Startups\\n\\n\\nEducation\\n\\n\\nCompare plansFeaturesDeveloperPlusEnterpriseTeamUsersMaximum of 1 seat(free)Up to 10 seats$39\\xa0per\\xa0seat/monthCustomWorkspaces\\u200d(A workspace separates and restricts access across teams)N/AUp to 3 WorkspacesCustomLangSmith:\\xa0For unified observability &\\xa0evalsTracing\\n\\n\\n\\n\\n\\n\\n\\n\\nMonitoring\\n\\n\\n\\n\\n\\n\\n\\n\\nOnline and Offline Evals\\n\\n\\n\\n\\n\\n\\n\\n\\nDataset collection\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnotation queue (human feedback)\\n\\n\\n\\n\\n\\n\\n\\n\\nPrompt Hub and Playground\\n\\n\\n\\n\\n\\n\\n\\n\\nBulk data export\\n\\n\\n\\n\\n\\n\\n\\nTrace volumePrices for traces vary depending on the data retention period you\\'ve set. \\n\\n\\n\\n 5k base traces / mo included Pay as you go thereafter:\\xa0\\u2028â€¢\\xa0$0.50 per 1k base traces â€¢ $4.50 per 1k extended traces 10k base traces / mo included Pay as you go thereafter:\\xa0\\u2028â€¢\\xa0$0.50 per 1k base traces â€¢ $4.50 per 1k extended tracesCustomBase trace retentionPrices for traces vary depending on the data retention period you\\'ve set. \\n\\n\\n\\n14 days14 daysCustomExtended trace retentionPrices for traces vary depending on the data retention period you\\'ve set. \\n\\n\\n\\n400 days400 daysCustomMax ingested events / hour50k / 250k (with payment on file) 500kCustomTotal trace size stored / hour500MB / 2.5GB (with payment on file)5GBCustomDeployment Options for LangSmithDeployment type(s)CloudCloudCloud, Hybrid, or Self-HostedInfraFully managed by LangChainFully managed by LangChainCloud: Fully managed by LangChainHybrid: SaaS control plane, Self-hosted data plane\\xa0Self-Hosted: Fully self-managedData locationLangChain\\'s Cloud (US\\xa0or EU)LangChain\\'s Cloud (US or EU)Cloud:\\xa0LangChain\\'s Cloud (US\\xa0or EU)Hybrid:\\xa0Your VPCSelf-Hosted:\\xa0Your VPCLangGraph Platform:\\xa0Infrastructure to deploy &\\xa0manage long-running, stateful agentsLangGraph Studio\\n\\n\\n\\n\\n\\n\\n\\n\\nExpose agent as MCP server\\n\\n\\n\\n\\n\\n\\n\\n\\nReal-time streaming of intermediary steps and final output\\n\\n\\n\\n\\n\\n\\n\\n\\n1-Click Deploy\\n\\n\\n\\n\\n\\n\\n\\nHorizontally-scalable service for production-sized deployments\\n\\n\\n\\n\\n\\n\\n\\nAssistants API\\n\\n\\n\\n\\n\\n\\n\\n30+\\xa0API\\xa0endpoints including state and memory\\n\\n\\n\\n\\n\\n\\n\\nCron scheduling\\n\\n\\n\\n\\n\\n\\n\\nAuthentication &\\xa0authorization for LangGraph APIs\\n\\n\\n\\n\\n\\n\\n\\nDeployment for LangGraph PlatformDeployment type(s)N/ACloudCloud, Hybrid, or Self-HostedNode execution costN/A1 free Dev deployment with unlimited node executions included.For additional deployments:\\xa0$0.001/node executionCustomUptime costN/A$0.0007 / min per Development deployment$0.0036 / min per Production deploymentCustomInfraN/AFully managed by LangChainCloud: Fully managed by LangChainHybrid: SaaS control plane, Self-hosted data plane\\xa0Self-Hosted: Fully self-managedData locationN/ALangChain\\'s Cloud (US or EU)Cloud:\\xa0LangChain\\'s Cloud (US\\xa0or EU)Hybrid:\\xa0Your VPCSelf-Hosted:\\xa0Your VPCSecurity ControlsSSO\\n\\nGoogle, GitHubCustom SSORole-Based Access Control\\n\\n\\n\\n\\n\\n\\nOrganization Roles (User and Admin)\\n\\n\\n\\n\\n\\n\\n\\nSupportCommunity Slack\\n\\n\\n\\n\\n\\n\\n\\n\\nEmail Support\\n\\n\\n\\n\\n\\n\\n\\nTeam trainings\\n\\n\\n\\n\\n\\n\\n Architectural guidance for your applications\\n\\n\\n\\n\\n\\n\\nAccess to deployed engineers\\n\\n\\n\\n\\n\\n\\nSLA\\n\\n\\n\\n\\n\\n\\nProcurementBillingMonthly, self-serveMonthly, self-serveAnnual invoiceCustom Terms\\n\\n\\n\\n\\n\\n\\nInfosec Review\\n\\n\\n\\n\\n\\n\\nFAQsGeneral QuestionsWhich plan is right for me?\\n\\nOur Developer plan is a great choice for personal projects. You will have 1 free seat with access to LangSmith (5k base traces/month included).\\xa0\\xa0\\u200dThe Plus plan is for teams that want to self-serve with moderate usage and collaboration needs. You can purchase up to 10 seats with access to LangSmith (10k base traces/month included). You will be able to ship agents with our managed LangGraph Platform Cloud service, with 1 free dev-sized deployment included.\\xa0\\u200dThe Enterprise plan is for teams that need more advanced administration, security, support, or deployment options. Contact our sales team to learn more.Do you offer a plan for startups?\\n\\nYes! We offer a Startup Plan for LangSmith, designed for early-stage companies building agentic applications. Youâ€™ll get discounted rates and generous free trace allotments to build with confidence from day one.\\u200d\\u200dApply here to get started with startup pricing. Customers can stay on the Startup Plan for 2 years before graduating to the Plus Plan.When will I be billed?\\n\\nFor the Developer or Plus Plan, seats are billed monthly on the 1st or pro-rated if added mid-month (no credit for removed seats); traces are billed monthly in arrears for your usage. Enterprise plans are invoiced annually upfront.Will you train on the data that I send to LangSmith or LangGraph Platform?\\n\\nWe will not train on your data, and you own all rights to your data. See our Terms of Service for more information.LangSmith QuestionsWhat is a trace? Can it contain multiple events?\\n\\nA trace represents a single execution of your applicationâ€”whether itâ€™s an agent, evaluator, or playground session. It can include many individual steps, such as LLM calls and other tracked events. Here\\'s an example of a single trace.What is the difference between a base trace and an extended trace?\\n\\nBase traces have a shorter retention period of 14 days and cost $0.50 per 1k traces. Extended traces have a longer retention period of 400 days and cost $5.00 per 1k traces. You can \"upgrade\" base traces to extended traces at $4.50 per 1k traces.Why might I want to upgrade a base trace to an extended trace?\\n\\nBase traces are short-lived (14-day retention) and ideal for quick debugging or ad-hoc analysis. Theyâ€™re priced for volume and short-term utility.\\u200dExtended traces, on the other hand, are retained for 400 days and often include valuable feedbackâ€”whether from users, evaluators, or human labelers. This feedback makes them essential for ongoing improvement and model tuning (higher utility), which comes with a higher price point.\\u200dLangSmith lets you choose the right retention for each trace, helping you balance cost and value. Why would I upgrade a base trace to an extended trace?\\n\\nBase traces are short-lived (14-day retention) and ideal for quick debugging or ad-hoc analysis. Theyâ€™re priced for volume and short-term utility.\\u200dExtended traces, on the other hand, are retained for 400 days and often include valuable feedbackâ€”whether from users, evaluators, or human labelers. This feedback makes them essential for ongoing improvement and model tuning (higher utility), which comes with a higher price point.\\u200dLangSmith lets you choose the right retention for each trace, helping you balance cost and value. Iâ€™ve exhausted my free trace allocation. What can I do?\\n\\nIf youâ€™ve used up your free traces, you can input your credit card details on the Developer or Plus plans to continue sending traces to LangSmith. If youâ€™ve hit the performance usage limits on your tier, you can upgrade to a higher plan to get better limits, or reach out to support@langchain.dev with questions.LangGraph Platform QuestionsDoes LangGraph Platform include any free deployments?\\n\\nPlus plans include 1 free dev-sized deployment. If you spin up additional dev-sized or production-sized deployments, youâ€™ll be charged by usage (nodes executed and uptime).For my free Dev deployment for LangGraph Platform, is there a cap on the number of nodes executed?\\n\\nIf youâ€™re on the Plus plan, you get 1 free dev-sized deployment â€“ all usage in this deployment will be free no matter how many node executions are run.How do you define nodes executed?\\n\\nNodes Executed is the aggregate number of nodes in a LangGraph application that are called and completed successfully during an invocation of the application. If a node in the graph is not called during execution or ends in an error state, these nodes will not be counted. If a node is called and completes successfully multiple times, each occurrence will be counted.What does uptime mean for LangGraph Platform usage?\\n\\nUptime is the duration your deploymentâ€™s database is live and persisting state. Uptime will be tracked as soon as your deployment is live and ends when you shut it down. Dev deployments are typically short-lived (used during iteration, then deleted) â€“ whereas Production deployments stay live and are updated via revisions (rather than being deleted).\\xa0When should I use a dev-sized deployment vs. a production-sized deployment?\\n\\nWe recommend using the production-sized deployment for any customer-facing agent. Dev-sized deployments are intended for testing and do not support horizontal scaling, backups, or performance optimizations needed in production.Ready to start shipping \\u2028reliable agents faster?Get started with tools from the LangChain product suite for every step of the agent development lifecycle.Get a demoSign up for freeProductsLangChainLangSmithLangGraphResourcesGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocsCompanyAboutCareersXLinkedInYouTubeMarketing AssetsSecuritySign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service\\n\\n\\n\\n\\n\\n\\n\\n'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/pricing/faq', 'loc': 'https://docs.smith.langchain.com/pricing/faq', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Plans and Pricing - LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nProducts\\n\\nFrameworksLangGraphLangChainPlatformsLangSmithLangGraph PlatformResources\\n\\nGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocs\\n\\nPythonLangGraphLangSmithLangChainJavaScriptLangGraphLangSmithLangChainCompany\\n\\nAboutCareersPricingGet a demoSign upPlans for teams of\\xa0any\\xa0sizeGet all the LangChain products -- pay for what you useDeveloperFor hobbyist projects by solo devs.Starting at$0 / monththen pay as you goFirst 5k traces included, starting at $0.50 per 1k base \\xa0traces thereafter.\\n\\n\\n\\nStart for freeGet started with:\\n\\nTracing to debug agent execution\\n\\nContinuous evals, both online and offline\\n\\nPrompt Hub, Playground, and Canvas for auto improving prompts\\n\\nAnnotation queues for human feedback\\n\\nMonitoring and alertingPlusBatteries-included tooling for building reliable agents fast.Starting at$39 / monththen pay as you goFirst 10k traces included, starting at $0.50 per 1k base \\xa0traces thereafter.\\n\\n\\n\\nSign upEverything in the Developer plan, and:\\n\\nUp to 10 seats\\n\\nHigher rate limits for LangSmith traces\\n\\n1 dev-sized LangGraph Platform deployment included\\n\\nEmail supportEnterpriseFor teams with advanced deployment, security, and support needs.\\xa0CustomContact salesEverything in the Plus plan, and:\\n\\nAlternative LangSmith & LangGraph Platform deployment options, including hybrid and self-hosted so data doesnâ€™t leave your VPC\\n\\nCustom SSO\\xa0and RBAC\\n\\nAcccess to deployed engineering team\\n\\nSupport SLA\\n\\nTeam trainings & architectural guidanceLangSmith for Startups and Education.  Seed stage startups and educational institutions, reach out for starter pricing and get shipping today.Reach out to learn about startup pricing for a period of time.Startups\\n\\n\\nEducation\\n\\n\\nEnquire about startup pricing\\n\\n\\nLangSmith for Startups and EducationSeed stage startups and educational institutions, reach out for starter pricing and get shipping today.Startups\\n\\n\\nEducation\\n\\n\\nCompare plansFeaturesDeveloperPlusEnterpriseTeamUsersMaximum of 1 seat(free)Up to 10 seats$39\\xa0per\\xa0seat/monthCustomWorkspaces\\u200d(A workspace separates and restricts access across teams)N/AUp to 3 WorkspacesCustomLangSmith:\\xa0For unified observability &\\xa0evalsTracing\\n\\n\\n\\n\\n\\n\\n\\n\\nMonitoring\\n\\n\\n\\n\\n\\n\\n\\n\\nOnline and Offline Evals\\n\\n\\n\\n\\n\\n\\n\\n\\nDataset collection\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnotation queue (human feedback)\\n\\n\\n\\n\\n\\n\\n\\n\\nPrompt Hub and Playground\\n\\n\\n\\n\\n\\n\\n\\n\\nBulk data export\\n\\n\\n\\n\\n\\n\\n\\nTrace volumePrices for traces vary depending on the data retention period you\\'ve set. \\n\\n\\n\\n 5k base traces / mo included Pay as you go thereafter:\\xa0\\u2028â€¢\\xa0$0.50 per 1k base traces â€¢ $4.50 per 1k extended traces 10k base traces / mo included Pay as you go thereafter:\\xa0\\u2028â€¢\\xa0$0.50 per 1k base traces â€¢ $4.50 per 1k extended tracesCustomBase trace retentionPrices for traces vary depending on the data retention period you\\'ve set. \\n\\n\\n\\n14 days14 daysCustomExtended trace retentionPrices for traces vary depending on the data retention period you\\'ve set. \\n\\n\\n\\n400 days400 daysCustomMax ingested events / hour50k / 250k (with payment on file) 500kCustomTotal trace size stored / hour500MB / 2.5GB (with payment on file)5GBCustomDeployment Options for LangSmithDeployment type(s)CloudCloudCloud, Hybrid, or Self-HostedInfraFully managed by LangChainFully managed by LangChainCloud: Fully managed by LangChainHybrid: SaaS control plane, Self-hosted data plane\\xa0Self-Hosted: Fully self-managedData locationLangChain\\'s Cloud (US\\xa0or EU)LangChain\\'s Cloud (US or EU)Cloud:\\xa0LangChain\\'s Cloud (US\\xa0or EU)Hybrid:\\xa0Your VPCSelf-Hosted:\\xa0Your VPCLangGraph Platform:\\xa0Infrastructure to deploy &\\xa0manage long-running, stateful agentsLangGraph Studio\\n\\n\\n\\n\\n\\n\\n\\n\\nExpose agent as MCP server\\n\\n\\n\\n\\n\\n\\n\\n\\nReal-time streaming of intermediary steps and final output\\n\\n\\n\\n\\n\\n\\n\\n\\n1-Click Deploy\\n\\n\\n\\n\\n\\n\\n\\nHorizontally-scalable service for production-sized deployments\\n\\n\\n\\n\\n\\n\\n\\nAssistants API\\n\\n\\n\\n\\n\\n\\n\\n30+\\xa0API\\xa0endpoints including state and memory\\n\\n\\n\\n\\n\\n\\n\\nCron scheduling\\n\\n\\n\\n\\n\\n\\n\\nAuthentication &\\xa0authorization for LangGraph APIs\\n\\n\\n\\n\\n\\n\\n\\nDeployment for LangGraph PlatformDeployment type(s)N/ACloudCloud, Hybrid, or Self-HostedNode execution costN/A1 free Dev deployment with unlimited node executions included.For additional deployments:\\xa0$0.001/node executionCustomUptime costN/A$0.0007 / min per Development deployment$0.0036 / min per Production deploymentCustomInfraN/AFully managed by LangChainCloud: Fully managed by LangChainHybrid: SaaS control plane, Self-hosted data plane\\xa0Self-Hosted: Fully self-managedData locationN/ALangChain\\'s Cloud (US or EU)Cloud:\\xa0LangChain\\'s Cloud (US\\xa0or EU)Hybrid:\\xa0Your VPCSelf-Hosted:\\xa0Your VPCSecurity ControlsSSO\\n\\nGoogle, GitHubCustom SSORole-Based Access Control\\n\\n\\n\\n\\n\\n\\nOrganization Roles (User and Admin)\\n\\n\\n\\n\\n\\n\\n\\nSupportCommunity Slack\\n\\n\\n\\n\\n\\n\\n\\n\\nEmail Support\\n\\n\\n\\n\\n\\n\\n\\nTeam trainings\\n\\n\\n\\n\\n\\n\\n Architectural guidance for your applications\\n\\n\\n\\n\\n\\n\\nAccess to deployed engineers\\n\\n\\n\\n\\n\\n\\nSLA\\n\\n\\n\\n\\n\\n\\nProcurementBillingMonthly, self-serveMonthly, self-serveAnnual invoiceCustom Terms\\n\\n\\n\\n\\n\\n\\nInfosec Review\\n\\n\\n\\n\\n\\n\\nFAQsGeneral QuestionsWhich plan is right for me?\\n\\nOur Developer plan is a great choice for personal projects. You will have 1 free seat with access to LangSmith (5k base traces/month included).\\xa0\\xa0\\u200dThe Plus plan is for teams that want to self-serve with moderate usage and collaboration needs. You can purchase up to 10 seats with access to LangSmith (10k base traces/month included). You will be able to ship agents with our managed LangGraph Platform Cloud service, with 1 free dev-sized deployment included.\\xa0\\u200dThe Enterprise plan is for teams that need more advanced administration, security, support, or deployment options. Contact our sales team to learn more.Do you offer a plan for startups?\\n\\nYes! We offer a Startup Plan for LangSmith, designed for early-stage companies building agentic applications. Youâ€™ll get discounted rates and generous free trace allotments to build with confidence from day one.\\u200d\\u200dApply here to get started with startup pricing. Customers can stay on the Startup Plan for 2 years before graduating to the Plus Plan.When will I be billed?\\n\\nFor the Developer or Plus Plan, seats are billed monthly on the 1st or pro-rated if added mid-month (no credit for removed seats); traces are billed monthly in arrears for your usage. Enterprise plans are invoiced annually upfront.Will you train on the data that I send to LangSmith or LangGraph Platform?\\n\\nWe will not train on your data, and you own all rights to your data. See our Terms of Service for more information.LangSmith QuestionsWhat is a trace? Can it contain multiple events?\\n\\nA trace represents a single execution of your applicationâ€”whether itâ€™s an agent, evaluator, or playground session. It can include many individual steps, such as LLM calls and other tracked events. Here\\'s an example of a single trace.What is the difference between a base trace and an extended trace?\\n\\nBase traces have a shorter retention period of 14 days and cost $0.50 per 1k traces. Extended traces have a longer retention period of 400 days and cost $5.00 per 1k traces. You can \"upgrade\" base traces to extended traces at $4.50 per 1k traces.Why might I want to upgrade a base trace to an extended trace?\\n\\nBase traces are short-lived (14-day retention) and ideal for quick debugging or ad-hoc analysis. Theyâ€™re priced for volume and short-term utility.\\u200dExtended traces, on the other hand, are retained for 400 days and often include valuable feedbackâ€”whether from users, evaluators, or human labelers. This feedback makes them essential for ongoing improvement and model tuning (higher utility), which comes with a higher price point.\\u200dLangSmith lets you choose the right retention for each trace, helping you balance cost and value. Why would I upgrade a base trace to an extended trace?\\n\\nBase traces are short-lived (14-day retention) and ideal for quick debugging or ad-hoc analysis. Theyâ€™re priced for volume and short-term utility.\\u200dExtended traces, on the other hand, are retained for 400 days and often include valuable feedbackâ€”whether from users, evaluators, or human labelers. This feedback makes them essential for ongoing improvement and model tuning (higher utility), which comes with a higher price point.\\u200dLangSmith lets you choose the right retention for each trace, helping you balance cost and value. Iâ€™ve exhausted my free trace allocation. What can I do?\\n\\nIf youâ€™ve used up your free traces, you can input your credit card details on the Developer or Plus plans to continue sending traces to LangSmith. If youâ€™ve hit the performance usage limits on your tier, you can upgrade to a higher plan to get better limits, or reach out to support@langchain.dev with questions.LangGraph Platform QuestionsDoes LangGraph Platform include any free deployments?\\n\\nPlus plans include 1 free dev-sized deployment. If you spin up additional dev-sized or production-sized deployments, youâ€™ll be charged by usage (nodes executed and uptime).For my free Dev deployment for LangGraph Platform, is there a cap on the number of nodes executed?\\n\\nIf youâ€™re on the Plus plan, you get 1 free dev-sized deployment â€“ all usage in this deployment will be free no matter how many node executions are run.How do you define nodes executed?\\n\\nNodes Executed is the aggregate number of nodes in a LangGraph application that are called and completed successfully during an invocation of the application. If a node in the graph is not called during execution or ends in an error state, these nodes will not be counted. If a node is called and completes successfully multiple times, each occurrence will be counted.What does uptime mean for LangGraph Platform usage?\\n\\nUptime is the duration your deploymentâ€™s database is live and persisting state. Uptime will be tracked as soon as your deployment is live and ends when you shut it down. Dev deployments are typically short-lived (used during iteration, then deleted) â€“ whereas Production deployments stay live and are updated via revisions (rather than being deleted).\\xa0When should I use a dev-sized deployment vs. a production-sized deployment?\\n\\nWe recommend using the production-sized deployment for any customer-facing agent. Dev-sized deployments are intended for testing and do not support horizontal scaling, backups, or performance optimizations needed in production.Ready to start shipping \\u2028reliable agents faster?Get started with tools from the LangChain product suite for every step of the agent development lifecycle.Get a demoSign up for freeProductsLangChainLangSmithLangGraphResourcesGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocsCompanyAboutCareersXLinkedInYouTubeMarketing AssetsSecuritySign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service\\n\\n\\n\\n\\n\\n\\n\\n'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/pricing/plans', 'loc': 'https://docs.smith.langchain.com/pricing/plans', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Plans and Pricing - LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nProducts\\n\\nFrameworksLangGraphLangChainPlatformsLangSmithLangGraph PlatformResources\\n\\nGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocs\\n\\nPythonLangGraphLangSmithLangChainJavaScriptLangGraphLangSmithLangChainCompany\\n\\nAboutCareersPricingGet a demoSign upPlans for teams of\\xa0any\\xa0sizeGet all the LangChain products -- pay for what you useDeveloperFor hobbyist projects by solo devs.Starting at$0 / monththen pay as you goFirst 5k traces included, starting at $0.50 per 1k base \\xa0traces thereafter.\\n\\n\\n\\nStart for freeGet started with:\\n\\nTracing to debug agent execution\\n\\nContinuous evals, both online and offline\\n\\nPrompt Hub, Playground, and Canvas for auto improving prompts\\n\\nAnnotation queues for human feedback\\n\\nMonitoring and alertingPlusBatteries-included tooling for building reliable agents fast.Starting at$39 / monththen pay as you goFirst 10k traces included, starting at $0.50 per 1k base \\xa0traces thereafter.\\n\\n\\n\\nSign upEverything in the Developer plan, and:\\n\\nUp to 10 seats\\n\\nHigher rate limits for LangSmith traces\\n\\n1 dev-sized LangGraph Platform deployment included\\n\\nEmail supportEnterpriseFor teams with advanced deployment, security, and support needs.\\xa0CustomContact salesEverything in the Plus plan, and:\\n\\nAlternative LangSmith & LangGraph Platform deployment options, including hybrid and self-hosted so data doesnâ€™t leave your VPC\\n\\nCustom SSO\\xa0and RBAC\\n\\nAcccess to deployed engineering team\\n\\nSupport SLA\\n\\nTeam trainings & architectural guidanceLangSmith for Startups and Education.  Seed stage startups and educational institutions, reach out for starter pricing and get shipping today.Reach out to learn about startup pricing for a period of time.Startups\\n\\n\\nEducation\\n\\n\\nEnquire about startup pricing\\n\\n\\nLangSmith for Startups and EducationSeed stage startups and educational institutions, reach out for starter pricing and get shipping today.Startups\\n\\n\\nEducation\\n\\n\\nCompare plansFeaturesDeveloperPlusEnterpriseTeamUsersMaximum of 1 seat(free)Up to 10 seats$39\\xa0per\\xa0seat/monthCustomWorkspaces\\u200d(A workspace separates and restricts access across teams)N/AUp to 3 WorkspacesCustomLangSmith:\\xa0For unified observability &\\xa0evalsTracing\\n\\n\\n\\n\\n\\n\\n\\n\\nMonitoring\\n\\n\\n\\n\\n\\n\\n\\n\\nOnline and Offline Evals\\n\\n\\n\\n\\n\\n\\n\\n\\nDataset collection\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnotation queue (human feedback)\\n\\n\\n\\n\\n\\n\\n\\n\\nPrompt Hub and Playground\\n\\n\\n\\n\\n\\n\\n\\n\\nBulk data export\\n\\n\\n\\n\\n\\n\\n\\nTrace volumePrices for traces vary depending on the data retention period you\\'ve set. \\n\\n\\n\\n 5k base traces / mo included Pay as you go thereafter:\\xa0\\u2028â€¢\\xa0$0.50 per 1k base traces â€¢ $4.50 per 1k extended traces 10k base traces / mo included Pay as you go thereafter:\\xa0\\u2028â€¢\\xa0$0.50 per 1k base traces â€¢ $4.50 per 1k extended tracesCustomBase trace retentionPrices for traces vary depending on the data retention period you\\'ve set. \\n\\n\\n\\n14 days14 daysCustomExtended trace retentionPrices for traces vary depending on the data retention period you\\'ve set. \\n\\n\\n\\n400 days400 daysCustomMax ingested events / hour50k / 250k (with payment on file) 500kCustomTotal trace size stored / hour500MB / 2.5GB (with payment on file)5GBCustomDeployment Options for LangSmithDeployment type(s)CloudCloudCloud, Hybrid, or Self-HostedInfraFully managed by LangChainFully managed by LangChainCloud: Fully managed by LangChainHybrid: SaaS control plane, Self-hosted data plane\\xa0Self-Hosted: Fully self-managedData locationLangChain\\'s Cloud (US\\xa0or EU)LangChain\\'s Cloud (US or EU)Cloud:\\xa0LangChain\\'s Cloud (US\\xa0or EU)Hybrid:\\xa0Your VPCSelf-Hosted:\\xa0Your VPCLangGraph Platform:\\xa0Infrastructure to deploy &\\xa0manage long-running, stateful agentsLangGraph Studio\\n\\n\\n\\n\\n\\n\\n\\n\\nExpose agent as MCP server\\n\\n\\n\\n\\n\\n\\n\\n\\nReal-time streaming of intermediary steps and final output\\n\\n\\n\\n\\n\\n\\n\\n\\n1-Click Deploy\\n\\n\\n\\n\\n\\n\\n\\nHorizontally-scalable service for production-sized deployments\\n\\n\\n\\n\\n\\n\\n\\nAssistants API\\n\\n\\n\\n\\n\\n\\n\\n30+\\xa0API\\xa0endpoints including state and memory\\n\\n\\n\\n\\n\\n\\n\\nCron scheduling\\n\\n\\n\\n\\n\\n\\n\\nAuthentication &\\xa0authorization for LangGraph APIs\\n\\n\\n\\n\\n\\n\\n\\nDeployment for LangGraph PlatformDeployment type(s)N/ACloudCloud, Hybrid, or Self-HostedNode execution costN/A1 free Dev deployment with unlimited node executions included.For additional deployments:\\xa0$0.001/node executionCustomUptime costN/A$0.0007 / min per Development deployment$0.0036 / min per Production deploymentCustomInfraN/AFully managed by LangChainCloud: Fully managed by LangChainHybrid: SaaS control plane, Self-hosted data plane\\xa0Self-Hosted: Fully self-managedData locationN/ALangChain\\'s Cloud (US or EU)Cloud:\\xa0LangChain\\'s Cloud (US\\xa0or EU)Hybrid:\\xa0Your VPCSelf-Hosted:\\xa0Your VPCSecurity ControlsSSO\\n\\nGoogle, GitHubCustom SSORole-Based Access Control\\n\\n\\n\\n\\n\\n\\nOrganization Roles (User and Admin)\\n\\n\\n\\n\\n\\n\\n\\nSupportCommunity Slack\\n\\n\\n\\n\\n\\n\\n\\n\\nEmail Support\\n\\n\\n\\n\\n\\n\\n\\nTeam trainings\\n\\n\\n\\n\\n\\n\\n Architectural guidance for your applications\\n\\n\\n\\n\\n\\n\\nAccess to deployed engineers\\n\\n\\n\\n\\n\\n\\nSLA\\n\\n\\n\\n\\n\\n\\nProcurementBillingMonthly, self-serveMonthly, self-serveAnnual invoiceCustom Terms\\n\\n\\n\\n\\n\\n\\nInfosec Review\\n\\n\\n\\n\\n\\n\\nFAQsGeneral QuestionsWhich plan is right for me?\\n\\nOur Developer plan is a great choice for personal projects. You will have 1 free seat with access to LangSmith (5k base traces/month included).\\xa0\\xa0\\u200dThe Plus plan is for teams that want to self-serve with moderate usage and collaboration needs. You can purchase up to 10 seats with access to LangSmith (10k base traces/month included). You will be able to ship agents with our managed LangGraph Platform Cloud service, with 1 free dev-sized deployment included.\\xa0\\u200dThe Enterprise plan is for teams that need more advanced administration, security, support, or deployment options. Contact our sales team to learn more.Do you offer a plan for startups?\\n\\nYes! We offer a Startup Plan for LangSmith, designed for early-stage companies building agentic applications. Youâ€™ll get discounted rates and generous free trace allotments to build with confidence from day one.\\u200d\\u200dApply here to get started with startup pricing. Customers can stay on the Startup Plan for 2 years before graduating to the Plus Plan.When will I be billed?\\n\\nFor the Developer or Plus Plan, seats are billed monthly on the 1st or pro-rated if added mid-month (no credit for removed seats); traces are billed monthly in arrears for your usage. Enterprise plans are invoiced annually upfront.Will you train on the data that I send to LangSmith or LangGraph Platform?\\n\\nWe will not train on your data, and you own all rights to your data. See our Terms of Service for more information.LangSmith QuestionsWhat is a trace? Can it contain multiple events?\\n\\nA trace represents a single execution of your applicationâ€”whether itâ€™s an agent, evaluator, or playground session. It can include many individual steps, such as LLM calls and other tracked events. Here\\'s an example of a single trace.What is the difference between a base trace and an extended trace?\\n\\nBase traces have a shorter retention period of 14 days and cost $0.50 per 1k traces. Extended traces have a longer retention period of 400 days and cost $5.00 per 1k traces. You can \"upgrade\" base traces to extended traces at $4.50 per 1k traces.Why might I want to upgrade a base trace to an extended trace?\\n\\nBase traces are short-lived (14-day retention) and ideal for quick debugging or ad-hoc analysis. Theyâ€™re priced for volume and short-term utility.\\u200dExtended traces, on the other hand, are retained for 400 days and often include valuable feedbackâ€”whether from users, evaluators, or human labelers. This feedback makes them essential for ongoing improvement and model tuning (higher utility), which comes with a higher price point.\\u200dLangSmith lets you choose the right retention for each trace, helping you balance cost and value. Why would I upgrade a base trace to an extended trace?\\n\\nBase traces are short-lived (14-day retention) and ideal for quick debugging or ad-hoc analysis. Theyâ€™re priced for volume and short-term utility.\\u200dExtended traces, on the other hand, are retained for 400 days and often include valuable feedbackâ€”whether from users, evaluators, or human labelers. This feedback makes them essential for ongoing improvement and model tuning (higher utility), which comes with a higher price point.\\u200dLangSmith lets you choose the right retention for each trace, helping you balance cost and value. Iâ€™ve exhausted my free trace allocation. What can I do?\\n\\nIf youâ€™ve used up your free traces, you can input your credit card details on the Developer or Plus plans to continue sending traces to LangSmith. If youâ€™ve hit the performance usage limits on your tier, you can upgrade to a higher plan to get better limits, or reach out to support@langchain.dev with questions.LangGraph Platform QuestionsDoes LangGraph Platform include any free deployments?\\n\\nPlus plans include 1 free dev-sized deployment. If you spin up additional dev-sized or production-sized deployments, youâ€™ll be charged by usage (nodes executed and uptime).For my free Dev deployment for LangGraph Platform, is there a cap on the number of nodes executed?\\n\\nIf youâ€™re on the Plus plan, you get 1 free dev-sized deployment â€“ all usage in this deployment will be free no matter how many node executions are run.How do you define nodes executed?\\n\\nNodes Executed is the aggregate number of nodes in a LangGraph application that are called and completed successfully during an invocation of the application. If a node in the graph is not called during execution or ends in an error state, these nodes will not be counted. If a node is called and completes successfully multiple times, each occurrence will be counted.What does uptime mean for LangGraph Platform usage?\\n\\nUptime is the duration your deploymentâ€™s database is live and persisting state. Uptime will be tracked as soon as your deployment is live and ends when you shut it down. Dev deployments are typically short-lived (used during iteration, then deleted) â€“ whereas Production deployments stay live and are updated via revisions (rather than being deleted).\\xa0When should I use a dev-sized deployment vs. a production-sized deployment?\\n\\nWe recommend using the production-sized deployment for any customer-facing agent. Dev-sized deployments are intended for testing and do not support horizontal scaling, backups, or performance optimizations needed in production.Ready to start shipping \\u2028reliable agents faster?Get started with tools from the LangChain product suite for every step of the agent development lifecycle.Get a demoSign up for freeProductsLangChainLangSmithLangGraphResourcesGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocsCompanyAboutCareersXLinkedInYouTubeMarketing AssetsSecuritySign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service\\n\\n\\n\\n\\n\\n\\n\\n'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'loc': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Prompt Engineering Concepts - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsCreate and update promptsCreate a promptManage promptsManage prompts programmaticallyConfigure prompt settingsUse tools in a promptInclude multimodal content in a promptWrite your prompt with AIConnect to modelsTutorialsOptimize a classifierSync prompts with GitHubTest multi-turn conversationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationTest your promptsPrompt Engineering ConceptsGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageWhy prompt engineering?Prompts vs. prompt templatesPrompts in LangSmithChat vs CompletionF-string vs. mustacheToolsStructured outputModelPrompt versioningCommitsTagsPrompt playgroundTesting multiple promptsTesting over a datasetVideo guideTest your promptsPrompt Engineering ConceptsCopy pageCopy pageWhile traditional software applications are built by writing code, AI applications often derive their logic from prompts.\\nThis guide will walk through the key concepts of prompt engineering in LangSmith.\\n\\u200bWhy prompt engineering?\\nA prompt sets the stage for the model, like an audience member at an improv show directing the actorâ€™s next performance - it guides the modelâ€™s behavior without changing its underlying capabilities. Just as telling an actor to â€œbe a pirateâ€ determines how they act, a prompt provides instructions, examples, and context that shape how the model responds.\\nPrompt engineering is important because it allows you to change the way the model behaves. While there are other ways to change the modelâ€™s behavior (like fine-tuning), prompt engineering is usually the simplest to get started with and often provides the highest ROI.\\nWe often see that prompt engineering is multi-disciplinary. Sometimes the best prompt engineer is not the software engineer who is building the application, but rather the product manager or another domain expert. It is important to have the proper tooling and infrastructure to support this cross-disciplinary building.\\n\\u200bPrompts vs. prompt templates\\nAlthough we often use these terms interchangably, it is important to understand the difference between â€œpromptsâ€ and â€œprompt templatesâ€.\\nPrompts refer to the messages that are passed into the language model.\\nPrompt Templates refer to a way of formatting information to get that prompt to hold the information that you want. Prompt templates can include variables for few shot examples, outside context, or any other external data that is needed in your prompt.\\n\\n\\u200bPrompts in LangSmith\\nYou can store and version prompts templates in LangSmith. There are few key aspects of a prompt template to understand.\\n\\u200bChat vs Completion\\nThere are two different types of prompts: chat style prompts and completion style prompts.\\nChat style prompts are a list of messages. This is the prompting style supported by most model APIs these days, and so this should generally be preferred.\\nCompletion style prompts are just a string. This is an older style of prompting, and so mostly exists for legacy reasons.\\n\\u200bF-string vs. mustache\\nYou can format your prompt with input variables using either f-string or mustache format. Here is an example prompt with f-string format:\\nCopyHello, {name}!\\n\\nAnd here is one with mustache:\\nCopyHello, {{name}}!\\n\\nTo add a conditional mustache prompt:\\nCopy{{#is_logged_in}}  Welcome back, {{name}}!{{else}}  Please log in.{{/is_logged_in}}\\n\\n\\nThe playground UI will pick up is_logged_in variable, but any nested variables youâ€™ll need to specify yourself. Paste the following into inputs to ensure the above conditional prompt works:\\n\\nCopy{  \"name\": \"Alice\"}\\n\\nThe LangSmith Playground uses f-string as the default template format, but you can switch to mustache format in the prompt settings/template format section. mustache gives you more flexibility around conditional variables, loops, and nested keys. For conditional variables, youâ€™ll need to manually add json variables in the â€˜inputsâ€™ section. Read the documentation\\n\\u200bTools\\nTools are interfaces the LLM can use to interact with the outside world. Tools consist of a name, description, and JSON schema of arguments used to call the tool.\\n\\u200bStructured output\\nStructured output is a feature of most state of the art LLMs, wherein instead of producing raw text as output they stick to a specified schema. This may or may not use Tools under the hood.\\nStructured outputs are similar to tools, but different in a few key ways. With tools, the LLM choose which tool to call (or may choose not to call any); with structured output, the LLM always responds in this format. With tools, the LLM may select multiple tools; with structured output, only one response is generate.\\n\\u200bModel\\nOptionally, you can store a model configuration alongside a prompt template. This includes the name of the model and any other parameters (temperature, etc).\\n\\u200bPrompt versioning\\nVerisioning is a key part of iterating and collaborating on your different prompts.\\n\\u200bCommits\\nEvery saved update to a prompt creates a new commit. You can view previous commits, making it easy to review earlier prompt versions or revert to a previous state if needed. In the SDK, you can access a specific commit of a prompt by specifying the commit hash along with the prompt name (e.g. prompt_name:commit_hash).\\nIn the UI, you can compare a commit with its previous version by toggling the â€œdiffâ€ button in the top-right corner of the Commits tab.\\n\\n\\u200bTags\\nYou may want to tag prompt commits with a human-readable tag so that you can refer to it even as new commits are added. Common use cases include tagging a prompt with dev or prod tags. This allows you to track which versions of prompts are used where.\\n\\u200bPrompt playground\\nThe prompt playground makes the process of iterating and testing your prompts seamless. You can enter the playground from the sidebar or directly from a saved prompt.\\nIn the playground you can:\\n\\nChange the model being used\\nChange prompt template being used\\nChange the output schema\\nChange the tools available\\nEnter the input variables to run through the prompt template\\nRun the prompt through the model\\nObserve the outputs\\n\\n\\u200bTesting multiple prompts\\nYou can add more prompts to your playground to easily compare outputs and decide which version is better:\\n\\n\\u200bTesting over a dataset\\nTo test over a dataset, you simply select the dataset from the top right and press Start. You can modify whether the results are streamed back as well as how many repitions there are in the test.\\n\\nYou can click on the â€œView Experimentâ€ button to dive deeper into the results of the test.\\n\\u200bVideo guide\\nWas this page helpful?YesNoSuggest editsOverviewCreate a promptâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides', 'loc': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Create a prompt - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsCreate and update promptsCreate a promptManage promptsManage prompts programmaticallyConfigure prompt settingsUse tools in a promptInclude multimodal content in a promptWrite your prompt with AIConnect to modelsTutorialsOptimize a classifierSync prompts with GitHubTest multi-turn conversationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationCreate and update promptsCreate a promptGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageCompose your promptTemplate formatAdd a template variableStructured outputToolsRun the promptSave your promptView your promptsAdd metadataNext stepsCreate and update promptsCreate a promptCopy pageCopy pageNavigate to the  in the left-hand sidebar or from the application homepage.\\n\\n\\u200bCompose your prompt\\nOn the left is an editable view of the prompt.\\nThe prompt is made up of messages, each of which has a â€œroleâ€ - including system, human, and ai.\\n\\u200bTemplate format\\nThe default template format is f-string, but you can change the prompt template format to mustache by clicking on the settings icon next to the model -> prompt format -> template format. Learn more about template formats here.\\n\\n\\u200bAdd a template variable\\nThe power of prompts comes from the ability to use variables in your prompt. You can use variables to add dynamic content to your prompt. Add a template variable in one of two ways:\\n\\n\\nAdd {{variable_name}} to your prompt (with one curly brace on each side for f-string and two for mustache). \\n\\n\\nHighlight text you want to templatize and click the tooltip button that shows up. Enter a name for your variable, and convert. \\n\\n\\nWhen we add a variable, we see a place to enter sample inputs for our prompt variables. Fill these in with values to test the prompt. \\n\\u200bStructured output\\nAdding an output schema to your prompt will get output in a structured format. Learn more about structured output here. \\n\\u200bTools\\nYou can also add a tool by clicking the + Tool button at the bottom of the prompt editor. See here for more information on how to use tools.\\n\\u200bRun the prompt\\nClick â€œStartâ€ to run the prompt.\\n\\n\\u200bSave your prompt\\nTo save your prompt, click the â€œSaveâ€ button, name your prompt, and decide if you want it to be â€œprivateâ€ or â€œpublicâ€. Private prompts are only visible to your workspace, while public prompts are discoverable to anyone.\\nThe model and configuration you select in the Playground settings will be saved with the prompt. When you reopen the prompt, the model and configuration will automatically load from the saved version. \\nThe first time you create a public prompt, youâ€™ll be asked to set a LangChain Hub handle. All your public prompts will be linked to this handle. In a shared workspace, this handle will be set for the whole workspace.\\n\\n\\u200bView your prompts\\nYouâ€™ve just created your first prompt! View a table of your prompts in the prompts tab.\\n\\n\\u200bAdd metadata\\nTo add metadata to your prompt, click the prompt and then click the â€œEditâ€ pencil icon next to the name. This brings you to where you can add additional information about the prompt, including a description, a README, and use cases. For public prompts this information will be visible to anyone who views your prompt in the LangChain Hub.\\n\\n\\n\\u200bNext steps\\nNow that youâ€™ve created a prompt, you can use it in your application code. See how to pull a prompt programmatically.Was this page helpful?YesNoSuggest editsConceptsManage promptsâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/create_a_prompt', 'loc': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/create_a_prompt', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Create a prompt - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsCreate and update promptsCreate a promptManage promptsManage prompts programmaticallyConfigure prompt settingsUse tools in a promptInclude multimodal content in a promptWrite your prompt with AIConnect to modelsTutorialsOptimize a classifierSync prompts with GitHubTest multi-turn conversationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationCreate and update promptsCreate a promptGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageCompose your promptTemplate formatAdd a template variableStructured outputToolsRun the promptSave your promptView your promptsAdd metadataNext stepsCreate and update promptsCreate a promptCopy pageCopy pageNavigate to the  in the left-hand sidebar or from the application homepage.\\n\\n\\u200bCompose your prompt\\nOn the left is an editable view of the prompt.\\nThe prompt is made up of messages, each of which has a â€œroleâ€ - including system, human, and ai.\\n\\u200bTemplate format\\nThe default template format is f-string, but you can change the prompt template format to mustache by clicking on the settings icon next to the model -> prompt format -> template format. Learn more about template formats here.\\n\\n\\u200bAdd a template variable\\nThe power of prompts comes from the ability to use variables in your prompt. You can use variables to add dynamic content to your prompt. Add a template variable in one of two ways:\\n\\n\\nAdd {{variable_name}} to your prompt (with one curly brace on each side for f-string and two for mustache). \\n\\n\\nHighlight text you want to templatize and click the tooltip button that shows up. Enter a name for your variable, and convert. \\n\\n\\nWhen we add a variable, we see a place to enter sample inputs for our prompt variables. Fill these in with values to test the prompt. \\n\\u200bStructured output\\nAdding an output schema to your prompt will get output in a structured format. Learn more about structured output here. \\n\\u200bTools\\nYou can also add a tool by clicking the + Tool button at the bottom of the prompt editor. See here for more information on how to use tools.\\n\\u200bRun the prompt\\nClick â€œStartâ€ to run the prompt.\\n\\n\\u200bSave your prompt\\nTo save your prompt, click the â€œSaveâ€ button, name your prompt, and decide if you want it to be â€œprivateâ€ or â€œpublicâ€. Private prompts are only visible to your workspace, while public prompts are discoverable to anyone.\\nThe model and configuration you select in the Playground settings will be saved with the prompt. When you reopen the prompt, the model and configuration will automatically load from the saved version. \\nThe first time you create a public prompt, youâ€™ll be asked to set a LangChain Hub handle. All your public prompts will be linked to this handle. In a shared workspace, this handle will be set for the whole workspace.\\n\\n\\u200bView your prompts\\nYouâ€™ve just created your first prompt! View a table of your prompts in the prompts tab.\\n\\n\\u200bAdd metadata\\nTo add metadata to your prompt, click the prompt and then click the â€œEditâ€ pencil icon next to the name. This brings you to where you can add additional information about the prompt, including a description, a README, and use cases. For public prompts this information will be visible to anyone who views your prompt in the LangChain Hub.\\n\\n\\n\\u200bNext steps\\nNow that youâ€™ve created a prompt, you can use it in your application code. See how to pull a prompt programmatically.Was this page helpful?YesNoSuggest editsConceptsManage promptsâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_endpoint', 'loc': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_endpoint', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Connect to a custom model - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsCreate and update promptsCreate a promptManage promptsManage prompts programmaticallyConfigure prompt settingsUse tools in a promptInclude multimodal content in a promptWrite your prompt with AIConnect to modelsOpenAI-compliant model provider/proxyCustom modelTutorialsOptimize a classifierSync prompts with GitHubTest multi-turn conversationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationConnect to modelsConnect to a custom modelGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageDeploy a custom model serverAdding configurable fieldsUse the model in the LangSmith PlaygroundCreate and update promptsConnect to modelsConnect to a custom modelCopy pageCopy pageThe LangSmith playground allows you to use your own custom models. You can deploy a model server that exposes your modelâ€™s API via , an open source library for serving LangChain applications. Behind the scenes, the playground will interact with your model server to generate responses.\\n\\u200bDeploy a custom model server\\nFor your convenience, we have provided a sample model server that you can use as a reference. You can find the sample model server here We highly recommend using the sample model server as a starting point.\\nDepending on your model is an instruct-style or chat-style model, you will need to implement either custom_model.py or custom_chat_model.py respectively.\\n\\u200bAdding configurable fields\\nIt is often useful to configure your model with different parameters. These might include temperature, model_name, max_tokens, etc.\\nTo make your model configurable in the LangSmith playground, you need to add configurable fields to your model server. These fields can be used to change model parameters from the playground.\\nYou can add configurable fields by implementing the with_configurable_fields function in the config.py file. You can\\nCopydef with_configurable_fields(self) -> Runnable:\\n    \"\"\"Expose fields you want to be configurable in the playground. We will automatically expose these to the\\n    playground. If you don\\'t want to expose any fields, you can remove this method.\"\"\"\\n    return self.configurable_fields(n=ConfigurableField(\\n        id=\"n\",\\n        name=\"Num Characters\",\\n        description=\"Number of characters to return from the input prompt.\",\\n    ))\\n\\n\\u200bUse the model in the LangSmith Playground\\nOnce you have deployed a model server, you can use it in the LangSmith Playground. Enter the playground and select either the ChatCustomModel or the CustomModel provider for chat-style model or instruct-style models.\\nEnter the URL. The playground will automatically detect the available endpoints and configurable fields. You can then invoke the model with the desired parameters.\\n\\nIf everything is set up correctly, you should see the modelâ€™s response in the playground as well as the configurable fields specified in the with_configurable_fields.\\nSee how to store your model configuration for later use here.Was this page helpful?YesNoSuggest editsOpenAI-compliant model provider/proxyOptimize a classifierâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_openai_compliant_model', 'loc': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_openai_compliant_model', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Connect to an OpenAI compliant model provider/proxy - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsCreate and update promptsCreate a promptManage promptsManage prompts programmaticallyConfigure prompt settingsUse tools in a promptInclude multimodal content in a promptWrite your prompt with AIConnect to modelsOpenAI-compliant model provider/proxyCustom modelTutorialsOptimize a classifierSync prompts with GitHubTest multi-turn conversationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationConnect to modelsConnect to an OpenAI compliant model provider/proxyGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageDeploy an OpenAI compliant modelUse the model in the LangSmith PlaygroundCreate and update promptsConnect to modelsConnect to an OpenAI compliant model provider/proxyCopy pageCopy pageThe LangSmith playground allows you to use any model that is compliant with the OpenAI API. You can utilize your model by setting the Proxy Provider for  in the playground.\\n\\u200bDeploy an OpenAI compliant model\\nMany providers offer OpenAI compliant models or proxy services. Some examples of this include:\\n\\nLiteLLM Proxy\\nOllama\\n\\nYou can use these providers to deploy your model and get an API endpoint that is compliant with the OpenAI API.\\nTake a look at the full specification for more information.\\n\\u200bUse the model in the LangSmith Playground\\nOnce you have deployed a model server, you can use it in the LangSmith Playground.\\nTo access the Prompt Settings menu:\\n\\n\\nUnder the Prompts heading select the gear  icon next to the model name.\\n\\n\\nIn the Model Configuration tab, select the model to edit in the dropdown.\\n\\n\\nFor the Provider dropdown, select OpenAI Compatible Endpoint.\\n\\n\\nAdd your OpenAI Compatible Endpoint to the Base URL input.\\n\\n\\n\\nIf everything is set up correctly, you should see the modelâ€™s response in the playground. You can also use this functionality to invoke downstream pipelines as well.\\nFor information on how to store your model configuration , refer to Configure prompt settings.Was this page helpful?YesNoSuggest editsWrite your prompt with AICustom modelâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/langchain_hub', 'loc': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/langchain_hub', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Manage prompts - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsCreate and update promptsCreate a promptManage promptsManage prompts programmaticallyConfigure prompt settingsUse tools in a promptInclude multimodal content in a promptWrite your prompt with AIConnect to modelsTutorialsOptimize a classifierSync prompts with GitHubTest multi-turn conversationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationCreate and update promptsManage promptsGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageCommit tagsCreate a tagMove a tagDelete a tagUse tags in codeTrigger a webhook on prompt commitConfigure a webhookTrigger the webhookUse the PlaygroundUsing the APIPublic prompt hubCreate and update promptsManage promptsCopy pageCopy pageLangSmith provides several tools to help you manage your prompts effectively. This page describes the following features:\\n\\nCommit tags for version control and environment management.\\nWebhook triggers for automating workflows when prompts are updated.\\nPublic prompt hub for discovering and using community-created prompts.\\n\\n\\u200bCommit tags\\nCommit tags are labels that reference a specific commit in your promptâ€™s version history. They help you mark significant versions and control which versions run in different environments. By referencing tags rather than commit IDs in your code, you can update which version is being used without modifying the code itself.\\nEach tag references exactly one commit, though you can reassign a tag to point to a different commit.\\nUse cases for commit tags can include:\\n\\nEnvironment-specific tags: Mark commits for production or staging environments, which allows you to switch between different versions without changing your code.\\nVersion control: Mark stable versions of your prompts, for example, v1, v2, which lets you reference specific versions in your code and track changes over time.\\nCollaboration: Mark versions ready for review, which enables you to share specific versions with collaborators and get feedback.\\n\\n\\u200bCreate a tag\\nTo create a tag, navigate to the Commits tab for a prompt. Click on the tag icon next to the commit you want to tag. Click New Tag and enter a name for the tag.\\n\\n\\n\\u200bMove a tag\\nTo point a tag to a different commit, click on the tag icon next to the destination commit, and select the tag you want to move. This will automatically update the tag to point to the new commit.\\n\\n\\u200bDelete a tag\\nTo delete a tag, click on the delete icon next to the tag you want to delete. This will delete the tag altogether and it will no longer be associated with any commit.\\n\\u200bUse tags in code\\nTags provide a stable way to reference specific versions of your prompts in code. Instead of using commit hashes directly, you can reference tags that can be updated without changing your code.\\nHere is an example of pulling a prompt by tag in Python:\\nCopyprompt = client.pull_prompt(\"joke-generator:prod\")\\n# If prod tag points to commit a1b2c3d4, this is equivalent to:\\nprompt = client.pull_prompt(\"joke-generator:a1b2c3d4\")\\n\\nFor more information on how to use prompts in code, refer to Managing prompts programmatically.\\n\\u200bTrigger a webhook on prompt commit\\nYou can configure a webhook to be triggered whenever a commit is made to a prompt.\\nSome common use cases of this include:\\n\\nTriggering a CI/CD pipeline when prompts are updated.\\nSynchronizing prompts with a GitHub repository.\\nNotifying team members about prompt modifications.\\n\\n\\u200bConfigure a webhook\\nNavigate to the Prompts section in the left-hand sidebar or from the application homepage. In the top right corner, click on the + Webhook button.\\nAdd a webhook URL and any required headers.\\nYou can only configure one webhook per workspace. If you want to configure multiple per workspace or set up a different webhook for each prompt, let us know in the LangChain Forum.\\nTo test out your webhook, click the Send test notification button. This will send a test notification to the webhook URL you provided with a sample payload.\\nThe sample payload is a JSON object with the following fields:\\n\\nprompt_id: The ID of the prompt that was committed.\\nprompt_name: The name of the prompt that was committed.\\ncommit_hash: The commit hash of the prompt.\\ncreated_at: The date of the commit.\\ncreated_by: The author of the commit.\\nmanifest: The manifest of the prompt.\\n\\n\\u200bTrigger the webhook\\nCommit to a prompt to trigger the webhook youâ€™ve configured.\\n\\u200bUse the Playground\\nIf you do this in the Playground, youâ€™ll be prompted to deselect the webhooks youâ€™d like to avoid triggering.\\n\\n\\u200bUsing the API\\nIf you commit via the API, you can specify to skip triggering the webhook by setting the skip_webhooks parameter to true or to an array of webhook ids to ignore. Refer to the API docs for more information.\\n\\u200bPublic prompt hub\\nLangSmithâ€™s public prompt hub is a collection of prompts that have been created by the LangChain community that you can use for reference.\\nNote that prompts are user-generated and unverified. LangChain does not review or endorse public prompts, use these at your own risk. Use of Prompt Hub is subject to our Terms of Service.\\nNavigate to the Prompts section of the left-hand sidebar and click on Browse all Public Prompts in the LangChain Hub.\\nHere youâ€™ll find all of the publicly listed prompts in the LangChain Hub. You can search for prompts by name, handle, use cases, descriptions, or models. You can fork prompts to your personal organization, view the promptâ€™s details, and run the prompt in the Playground. You can pull any public prompt into your code using the SDK.\\nTo view prompts tied to your workspace, visit the Prompts tab in the sidebar.\\nWas this page helpful?YesNoSuggest editsCreate a promptManage prompts programmaticallyâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically', 'loc': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Manage prompts programmatically - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsCreate and update promptsCreate a promptManage promptsManage prompts programmaticallyConfigure prompt settingsUse tools in a promptInclude multimodal content in a promptWrite your prompt with AIConnect to modelsTutorialsOptimize a classifierSync prompts with GitHubTest multi-turn conversationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationCreate and update promptsManage prompts programmaticallyGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageInstall packagesConfigure environment variablesPush a promptPull a promptUse a prompt without LangChainOpenAIAnthropicList, delete, and like promptsCreate and update promptsManage prompts programmaticallyCopy pageCopy pageYou can use the LangSmith Python and TypeScript SDK to manage prompts programmatically.\\nPreviously this functionality lived in the langchainhub package which is now deprecated. All functionality going forward will live in the langsmith package.\\n\\u200bInstall packages\\nIn Python, you can directly use the LangSmith SDK (recommended, full functionality) or you can use through the LangChain package (limited to pushing and pulling prompts).\\nIn TypeScript, you must use the LangChain npm package for pulling prompts (it also allows pushing). For all other functionality, use the LangSmith package.\\npipuvTypeScriptCopypip install -U langsmith # version >= 0.1.99\\npip install -U langchain langsmith # langsmith version >= 0.1.99 and langchain >= 0.2.13\\n\\n\\u200bConfigure environment variables\\nIf you already have LANGSMITH_API_KEY set to your current workspaceâ€™s api key from LangSmith, you can skip this step.\\nOtherwise, get an API key for your workspace by navigating to Settings > API Keys > Create API Key in LangSmith.\\nSet your environment variable.\\nCopyexport LANGSMITH_API_KEY=\"lsv2_...\"\\n\\nWhat we refer to as â€œpromptsâ€ used to be called â€œreposâ€, so any references to â€œrepoâ€ in the code are referring to a prompt.\\n\\u200bPush a prompt\\nTo create a new prompt or update an existing prompt, you can use the push prompt method.\\nPythonLangChain (Python)TypeScriptCopyfrom langsmith import Client\\nfrom langchain_core.prompts import ChatPromptTemplate\\n\\nclient = Client()\\nprompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\\nurl = client.push_prompt(\"joke-generator\", object=prompt)\\n# url is a link to the prompt in the UI\\nprint(url)\\n\\nYou can also push a prompt as a RunnableSequence of a prompt and a model. This is useful for storing the model configuration you want to use with this prompt. The provider must be supported by the LangSmith playground. (see settings here: Supported Providers)\\nPythonLangChain (Python)TypeScriptCopyfrom langsmith import Client\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_openai import ChatOpenAI\\n\\nclient = Client()\\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\\nprompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\\nchain = prompt | model\\nclient.push_prompt(\"joke-generator-with-model\", object=chain)\\n\\n\\u200bPull a prompt\\nTo pull a prompt, you can use the pull prompt method, which returns a the prompt as a langchain PromptTemplate.\\nTo pull a private prompt you do not need to specify the owner handle (though you can, if you have one set).\\nTo pull a public prompt from the LangChain Hub, you need to specify the handle of the promptâ€™s author.\\nPythonLangChain (Python)TypeScriptCopyfrom langsmith import Client\\nfrom langchain_openai import ChatOpenAI\\n\\nclient = Client()\\nprompt = client.pull_prompt(\"joke-generator\")\\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\\nchain = prompt | model\\nchain.invoke({\"topic\": \"cats\"})\\n\\nSimilar to pushing a prompt, you can also pull a prompt as a RunnableSequence of a prompt and a model. Just specify include_model when pulling the prompt. If the stored prompt includes a model, it will be returned as a RunnableSequence. Make sure you have the proper environment variables set for the model you are using.\\nPythonLangChain (Python)TypeScriptCopyfrom langsmith import Client\\n\\nclient = Client()\\nchain = client.pull_prompt(\"joke-generator-with-model\", include_model=True)\\nchain.invoke({\"topic\": \"cats\"})\\n\\nWhen pulling a prompt, you can also specify a specific commit hash or commit tag to pull a specific version of the prompt.\\nPythonLangChain (Python)TypeScriptCopyprompt = client.pull_prompt(\"joke-generator:12344e88\")\\n\\nTo pull a public prompt from the LangChain Hub, you need to specify the handle of the promptâ€™s author.\\nPythonLangChain (Python)TypeScriptCopyprompt = client.pull_prompt(\"efriis/my-first-prompt\")\\n\\nFor pulling prompts, if you are using Node.js or an environment that supports dynamic imports, we recommend using the langchain/hub/node entrypoint, as it handles deserialization of models associated with your prompt configuration automatically.If you are in a non-Node environment, â€œincludeModelâ€ is not supported for non-OpenAI models and you should use the base langchain/hub entrypoint.\\n\\u200bUse a prompt without LangChain\\nIf you want to store your prompts in LangSmith but use them directly with a model providerâ€™s API, you can use our conversion methods. These convert your prompt into the payload required for the OpenAI or Anthropic API.\\nThese conversion methods rely on logic from within LangChain integration packages, and you will need to install the appropriate package as a dependency in addition to your official SDK of choice. Here are some examples:\\n\\u200bOpenAI\\nPythonTypeScriptCopypip install -U langchain_openai\\n\\nPythonTypeScriptCopyfrom openai import OpenAI\\nfrom langsmith.client import Client, convert_prompt_to_openai_format\\n\\n# langsmith client\\nclient = Client()\\n# openai client\\noai_client = OpenAI()\\n\\n# pull prompt and invoke to populate the variables\\nprompt = client.pull_prompt(\"joke-generator\")\\nprompt_value = prompt.invoke({\"topic\": \"cats\"})\\nopenai_payload = convert_prompt_to_openai_format(prompt_value)\\nopenai_response = oai_client.chat.completions.create(**openai_payload)\\n\\n\\u200bAnthropic\\nPythonTypeScriptCopypip install -U langchain_anthropic\\n\\nPythonTypeScriptCopyfrom anthropic import Anthropic\\nfrom langsmith.client import Client, convert_prompt_to_anthropic_format\\n\\n# langsmith client\\nclient = Client()\\n# anthropic client\\nanthropic_client = Anthropic()\\n\\n# pull prompt and invoke to populate the variables\\nprompt = client.pull_prompt(\"joke-generator\")\\nprompt_value = prompt.invoke({\"topic\": \"cats\"})\\nanthropic_payload = convert_prompt_to_anthropic_format(prompt_value)\\nanthropic_response = anthropic_client.messages.create(**anthropic_payload)\\n\\n\\u200bList, delete, and like prompts\\nYou can also list, delete, and like/unlike prompts using the list prompts, delete prompt, like prompt and unlike prompt methods. See the LangSmith SDK client for extensive documentation on these methods.\\nPythonTypeScriptCopy# List all prompts in my workspace\\nprompts = client.list_prompts()\\n\\n# List my private prompts that include \"joke\"\\nprompts = client.list_prompts(query=\"joke\", is_public=False)\\n\\n# Delete a prompt\\nclient.delete_prompt(\"joke-generator\")\\n\\n# Like a prompt\\nclient.like_prompt(\"efriis/my-first-prompt\")\\n\\n# Unlike a prompt\\nclient.unlike_prompt(\"efriis/my-first-prompt\")\\nWas this page helpful?YesNoSuggest editsManage promptsConfigure prompt settingsâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/managing_model_configurations', 'loc': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/managing_model_configurations', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Configure prompt settings - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsCreate and update promptsCreate a promptManage promptsManage prompts programmaticallyConfigure prompt settingsUse tools in a promptInclude multimodal content in a promptWrite your prompt with AIConnect to modelsTutorialsOptimize a classifierSync prompts with GitHubTest multi-turn conversationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationCreate and update promptsConfigure prompt settingsGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageModel configurationsCreate saved configurationsEdit configurationsDelete configurationsExtra parametersTool settingsPrompt formattingCreate and update promptsConfigure prompt settingsCopy pageCopy pageThe LangSmith playground enables you to control various settings for your prompts. The Prompt Settings window contains:\\n\\nModel configuration\\nTool settings\\nPrompt formatting\\n\\nTo access Prompt Settings:\\n\\n\\nNavigate to the Playground in the left sidebar.\\n\\n\\nUnder the Prompts heading select the gear  icon next to the model name, which will launch the Prompt Settings window.\\n\\n\\n\\n\\u200bModel configurations\\nModel configurations define the parameters your prompt runs against. In the LangSmith Playground, you can save and manage these configurations, which allows you to reuse your preferred settings across prompts and sessions. For details on specific settings, refer to your model providerâ€™s documentation (for example, Anthropic, OpenAI).\\n\\u200bCreate saved configurations\\n\\nIn the Model Configurations tab, adjust the model configuration as neededâ€”you can select a saved configuration to edit.\\nClick the Save As button in the top bar.\\nEnter a name and optional description for your configuration and confirm.\\nNow that youâ€™ve saved the configuration, anyone in your organizationâ€™s workspace can access it. All saved configurations are available in the Model Configuration dropdown.\\nOnce you have created a saved configuration, you can set it as your default, so any new prompt you create will automatically use this configuration. To set a configuration as your default, click the Set as default  icon next to the model name in the dropdown.\\n\\n\\u200bEdit configurations\\n\\nTo rename a saved configuration, or update the description, select the configuration name or description and make the necessary changes.\\nUpdate the current configurationâ€™s parameters as needed and click the Save button at the top.\\n\\n\\u200bDelete configurations\\n\\nSelect the configuration you want to remove.\\nClick the trash  icon to delete it.\\n\\n\\u200bExtra parameters\\nThe Extra Parameters field allows you to pass additional model parameters that arenâ€™t directly supported in the LangSmith interface. This is particularly useful in two scenarios:\\n\\n\\nWhen model providers release new parameters that havenâ€™t yet been integrated into the LangSmith interface. You can specify these parameters in JSON format to use them right away. For example:\\nCopy{\\n    \"reasoning_effort\": \"medium\"\\n}\\n\\n\\n\\nWhen troubleshooting parameter-related errors in the playground, such as:\\nCopyTypeError: AsyncCompletions.create() got an unexpected keyword argument \\'max_concurrency\\'\\n\\nIf you receive an error about unnecessary parameters (which is more common when using LangChain JS for run tracing), you can use this field to remove the extra parameters.\\n\\n\\n\\u200bTool settings\\nTools enable your LLM to perform tasks like searching the web, looking up information, and so on. In the Tools Settings tab, you can manage the ways your LLM uses and accesses the tools you have defined in your prompt, including:\\n\\nParallel Tool Calls: Calling multiple tools in parallel when appropriate. This allows the model to gather information from different sources simultaneously. (Dependent on model support for parallel execution.)\\nTool Choice: Select the tools that the model can access. For more details, refer to Use tools in a prompt.\\n\\n\\u200bPrompt formatting\\nThe Prompt Format tab allows you to specify:\\n\\nThe Prompt type. For details on chat and completion prompts, refer to Prompt engineering concepts.\\nThe Template format. For details on prompt templating and using variables, refer to F-string vs. mustache.\\nWas this page helpful?YesNoSuggest editsManage prompts programmaticallyUse tools in a promptâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/multimodal_content', 'loc': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/multimodal_content', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Include multimodal content in a prompt - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsCreate and update promptsCreate a promptManage promptsManage prompts programmaticallyConfigure prompt settingsUse tools in a promptInclude multimodal content in a promptWrite your prompt with AIConnect to modelsTutorialsOptimize a classifierSync prompts with GitHubTest multi-turn conversationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationCreate and update promptsInclude multimodal content in a promptGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageInline contentTemplate variablesPopulate the template variableRun an evaluationCreate and update promptsInclude multimodal content in a promptCopy pageCopy pageSome applications are based around multimodal content, like a chatbot that can answer questions about a PDF or image. In these cases, youâ€™ll want to include multimodal content in your prompt and test the modelâ€™s ability to answer questions about the content.\\nThe LangSmith Playground supports two methods for incorporating multimodal content in your prompts:\\n\\n\\nInline content: Embed static files (images, PDFs, audio) directly in your prompt. This is ideal when you want to consistently include the same multimodal content across all uses of the prompt. For example, you might include a reference image that helps ground the modelâ€™s responses.\\n\\n\\nTemplate variables: Create dynamic placeholders for attachments that can be populated with different content each time. This approach offers more flexibility, allowing you to:\\n\\nTest how the model handles different inputs\\nCreate reusable prompts that work with varying content\\n\\n\\n\\nNot all models support multimodal content. Before using multimodal features in the playground, make sure your selected model supports the file types you want to use.\\n\\u200bInline content\\nClick the file icon in the message where you want to add multimodal content. Under the Upload content tab, you can upload a file and include it inline in the prompt.\\n\\n\\u200bTemplate variables\\nClick the file icon in the message where you want to add multimodal content. Under the Template variables tab, you can create a template variable for a specific attachment type. Currently, only images, PDFs, and audio files (.wav, .mp3) are supported.\\n\\n\\u200bPopulate the template variable\\nOnce youâ€™ve added a template variable, you can provide content for it using the panel on the right side of the screen. Simply click the + button to upload or select content that will be used to populate the template variable.\\n\\n\\u200bRun an evaluation\\nAfter testing out your prompt manually, you can run an evaluation to see how the prompt performs over a golden dataset of examples.Was this page helpful?YesNoSuggest editsUse tools in a promptWrite your prompt with AIâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/multiple_messages', 'loc': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/multiple_messages', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Test multi-turn conversations - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsCreate and update promptsCreate a promptManage promptsManage prompts programmaticallyConfigure prompt settingsUse tools in a promptInclude multimodal content in a promptWrite your prompt with AIConnect to modelsTutorialsOptimize a classifierSync prompts with GitHubTest multi-turn conversationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationTutorialsTest multi-turn conversationsGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageFrom an existing runFrom a datasetManuallyNext StepsTutorialsTest multi-turn conversationsCopy pageCopy pageThis how-to guide walks you through the various ways you can set up the playground for multi-turn conversations, which will allow you to test different tool configurations and system prompts against longer threads of messages.\\n\\n\\u200bFrom an existing run\\nFirst, ensure you have properly traced a multi-turn conversation, and then navigate to your tracing project. Once you get to your tracing project simply open the run, select the LLM call, and open it in the playground as follows:\\n\\nYou can then edit the system prompt, tweak the tools and/or output schema and observe how the output of the multi-turn conversation changes.\\n\\u200bFrom a dataset\\nBefore starting, make sure you have set up your dataset. Since you want to evaluate multi-turn conversations, make sure there is a key in your inputs that contains a list of messages.\\nOnce you have created your dataset, head to the playground and load your dataset to evaluate.\\nThen, add a messages list variable to your prompt, making sure to name it the same as the key in your inputs that contains the list of messages:\\n\\nWhen you run your prompt, the messages from each example will be added as a list in place of the â€˜Messages Listâ€™ variable.\\n\\u200bManually\\nThere are two ways to manually create multi-turn conversations. The first way is by simply appending messages to the prompt:\\n\\nThis is helpful for quick iteration, but is rigid since the multi-turn conversation is hardcoded. Instead, if you want your prompt to work with any multi-turn conversation you can add a â€˜Messages Listâ€™ variable and add your multi-turn conversation there:\\n\\nThis allows you to just tweak the system prompt or the tools, while allowing any multi-turn conversation to take the place of the Messages List variable, allowing you to reuse this prompt across various runs.\\n\\u200bNext Steps\\nNow that you know how to set up the playground for multi-turn interactions, you can either manually inspect and judge the outputs, or you can add evaluators to classify results.\\nYou can also read these how-to guides to learn more about how to use the playground to run evaluations.Was this page helpful?YesNoSuggest editsSync prompts with GitHubâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/open_a_prompt_from_a_trace', 'loc': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/open_a_prompt_from_a_trace', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Get started with LangSmith - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewQuickstartsTrace an applicationEvaluate an applicationTest promptsAPI & SDKsAPI referencePython SDKJS/TS SDKPricingPlansPricing FAQOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationGet started with LangSmithGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumGet started with LangSmithCopy pageCopy pageLangSmith is a platform for building production-grade LLM applications. Monitor and evaluate your application, so you can ship quickly and with confidence.\\nLangSmith is framework agnostic â€”\\xa0you can use it with or without LangChainâ€™s open source frameworks langchain and langgraph.\\n\\nStart tracingGain visibility into each step your application takes when handling a request to debug faster.Learn moreEvaluate your applicationMeasure quality of your applications over time to build more reliable AI applications.Learn moreTest your promptsIterate on prompts, with automatic version control and collaboration features.Learn moreSet up your workspaceSet up your workspace, configure admin settings, and invite your team to collaborate.Learn moreWas this page helpful?YesNoSuggest editsTrace an applicationâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_canvas', 'loc': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_canvas', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Write your prompt with AI - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsCreate and update promptsCreate a promptManage promptsManage prompts programmaticallyConfigure prompt settingsUse tools in a promptInclude multimodal content in a promptWrite your prompt with AIConnect to modelsTutorialsOptimize a classifierSync prompts with GitHubTest multi-turn conversationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationCreate and update promptsWrite your prompt with AIGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageChat sidebarQuick actionsCustom quick actionsDiffingSaving and using promptsCreate and update promptsWrite your prompt with AICopy pageCopy pageThe prompt canvas makes it easy to edit a prompt with the help of an LLM. This allows you to iterate faster on long prompts and also makes it easier to make overarching stylisting or tonal changes to your prompt. You can enter the promp canvas by clicking the glowing wand over any message in your prompt:\\n\\n\\u200bChat sidebar\\nYou can use the chat sidebar to ask questions about your prompt, or to give instructions in natural language to the LLM for how to rewrite your prompt.\\n\\nYou can also edit the prompt directly - you donâ€™t need to use the LLM. This is useful if you know what edits you want to make and just want to make them directly\\n\\u200bQuick actions\\nThere are quick actions to change the reading level or length of the prompt with a single mouse click:\\n\\n\\u200bCustom quick actions\\nYou can also save your own custom quick actions, for ease of use across all the prompts you are working on in LangSmith:\\n\\n\\u200bDiffing\\nYou can also see the specific differences between each version of your prompt by selecting the diff slider in the top right of the canvas:\\n\\n\\u200bSaving and using prompts\\nLastly, you can save the prompt you have created in the canvas by clicking the â€œUse this Versionâ€ button in the bottom right:\\nWas this page helpful?YesNoSuggest editsInclude multimodal content in a promptOpenAI-compliant model provider/proxyâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_tags', 'loc': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_tags', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Manage prompts - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsCreate and update promptsCreate a promptManage promptsManage prompts programmaticallyConfigure prompt settingsUse tools in a promptInclude multimodal content in a promptWrite your prompt with AIConnect to modelsTutorialsOptimize a classifierSync prompts with GitHubTest multi-turn conversationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationCreate and update promptsManage promptsGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageCommit tagsCreate a tagMove a tagDelete a tagUse tags in codeTrigger a webhook on prompt commitConfigure a webhookTrigger the webhookUse the PlaygroundUsing the APIPublic prompt hubCreate and update promptsManage promptsCopy pageCopy pageLangSmith provides several tools to help you manage your prompts effectively. This page describes the following features:\\n\\nCommit tags for version control and environment management.\\nWebhook triggers for automating workflows when prompts are updated.\\nPublic prompt hub for discovering and using community-created prompts.\\n\\n\\u200bCommit tags\\nCommit tags are labels that reference a specific commit in your promptâ€™s version history. They help you mark significant versions and control which versions run in different environments. By referencing tags rather than commit IDs in your code, you can update which version is being used without modifying the code itself.\\nEach tag references exactly one commit, though you can reassign a tag to point to a different commit.\\nUse cases for commit tags can include:\\n\\nEnvironment-specific tags: Mark commits for production or staging environments, which allows you to switch between different versions without changing your code.\\nVersion control: Mark stable versions of your prompts, for example, v1, v2, which lets you reference specific versions in your code and track changes over time.\\nCollaboration: Mark versions ready for review, which enables you to share specific versions with collaborators and get feedback.\\n\\n\\u200bCreate a tag\\nTo create a tag, navigate to the Commits tab for a prompt. Click on the tag icon next to the commit you want to tag. Click New Tag and enter a name for the tag.\\n\\n\\n\\u200bMove a tag\\nTo point a tag to a different commit, click on the tag icon next to the destination commit, and select the tag you want to move. This will automatically update the tag to point to the new commit.\\n\\n\\u200bDelete a tag\\nTo delete a tag, click on the delete icon next to the tag you want to delete. This will delete the tag altogether and it will no longer be associated with any commit.\\n\\u200bUse tags in code\\nTags provide a stable way to reference specific versions of your prompts in code. Instead of using commit hashes directly, you can reference tags that can be updated without changing your code.\\nHere is an example of pulling a prompt by tag in Python:\\nCopyprompt = client.pull_prompt(\"joke-generator:prod\")\\n# If prod tag points to commit a1b2c3d4, this is equivalent to:\\nprompt = client.pull_prompt(\"joke-generator:a1b2c3d4\")\\n\\nFor more information on how to use prompts in code, refer to Managing prompts programmatically.\\n\\u200bTrigger a webhook on prompt commit\\nYou can configure a webhook to be triggered whenever a commit is made to a prompt.\\nSome common use cases of this include:\\n\\nTriggering a CI/CD pipeline when prompts are updated.\\nSynchronizing prompts with a GitHub repository.\\nNotifying team members about prompt modifications.\\n\\n\\u200bConfigure a webhook\\nNavigate to the Prompts section in the left-hand sidebar or from the application homepage. In the top right corner, click on the + Webhook button.\\nAdd a webhook URL and any required headers.\\nYou can only configure one webhook per workspace. If you want to configure multiple per workspace or set up a different webhook for each prompt, let us know in the LangChain Forum.\\nTo test out your webhook, click the Send test notification button. This will send a test notification to the webhook URL you provided with a sample payload.\\nThe sample payload is a JSON object with the following fields:\\n\\nprompt_id: The ID of the prompt that was committed.\\nprompt_name: The name of the prompt that was committed.\\ncommit_hash: The commit hash of the prompt.\\ncreated_at: The date of the commit.\\ncreated_by: The author of the commit.\\nmanifest: The manifest of the prompt.\\n\\n\\u200bTrigger the webhook\\nCommit to a prompt to trigger the webhook youâ€™ve configured.\\n\\u200bUse the Playground\\nIf you do this in the Playground, youâ€™ll be prompted to deselect the webhooks youâ€™d like to avoid triggering.\\n\\n\\u200bUsing the API\\nIf you commit via the API, you can specify to skip triggering the webhook by setting the skip_webhooks parameter to true or to an array of webhook ids to ignore. Refer to the API docs for more information.\\n\\u200bPublic prompt hub\\nLangSmithâ€™s public prompt hub is a collection of prompts that have been created by the LangChain community that you can use for reference.\\nNote that prompts are user-generated and unverified. LangChain does not review or endorse public prompts, use these at your own risk. Use of Prompt Hub is subject to our Terms of Service.\\nNavigate to the Prompts section of the left-hand sidebar and click on Browse all Public Prompts in the LangChain Hub.\\nHere youâ€™ll find all of the publicly listed prompts in the LangChain Hub. You can search for prompts by name, handle, use cases, descriptions, or models. You can fork prompts to your personal organization, view the promptâ€™s details, and run the prompt in the Playground. You can pull any public prompt into your code using the SDK.\\nTo view prompts tied to your workspace, visit the Prompts tab in the sidebar.\\nWas this page helpful?YesNoSuggest editsCreate a promptManage prompts programmaticallyâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/trigger_webhook', 'loc': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/trigger_webhook', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Manage prompts - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsCreate and update promptsCreate a promptManage promptsManage prompts programmaticallyConfigure prompt settingsUse tools in a promptInclude multimodal content in a promptWrite your prompt with AIConnect to modelsTutorialsOptimize a classifierSync prompts with GitHubTest multi-turn conversationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationCreate and update promptsManage promptsGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageCommit tagsCreate a tagMove a tagDelete a tagUse tags in codeTrigger a webhook on prompt commitConfigure a webhookTrigger the webhookUse the PlaygroundUsing the APIPublic prompt hubCreate and update promptsManage promptsCopy pageCopy pageLangSmith provides several tools to help you manage your prompts effectively. This page describes the following features:\\n\\nCommit tags for version control and environment management.\\nWebhook triggers for automating workflows when prompts are updated.\\nPublic prompt hub for discovering and using community-created prompts.\\n\\n\\u200bCommit tags\\nCommit tags are labels that reference a specific commit in your promptâ€™s version history. They help you mark significant versions and control which versions run in different environments. By referencing tags rather than commit IDs in your code, you can update which version is being used without modifying the code itself.\\nEach tag references exactly one commit, though you can reassign a tag to point to a different commit.\\nUse cases for commit tags can include:\\n\\nEnvironment-specific tags: Mark commits for production or staging environments, which allows you to switch between different versions without changing your code.\\nVersion control: Mark stable versions of your prompts, for example, v1, v2, which lets you reference specific versions in your code and track changes over time.\\nCollaboration: Mark versions ready for review, which enables you to share specific versions with collaborators and get feedback.\\n\\n\\u200bCreate a tag\\nTo create a tag, navigate to the Commits tab for a prompt. Click on the tag icon next to the commit you want to tag. Click New Tag and enter a name for the tag.\\n\\n\\n\\u200bMove a tag\\nTo point a tag to a different commit, click on the tag icon next to the destination commit, and select the tag you want to move. This will automatically update the tag to point to the new commit.\\n\\n\\u200bDelete a tag\\nTo delete a tag, click on the delete icon next to the tag you want to delete. This will delete the tag altogether and it will no longer be associated with any commit.\\n\\u200bUse tags in code\\nTags provide a stable way to reference specific versions of your prompts in code. Instead of using commit hashes directly, you can reference tags that can be updated without changing your code.\\nHere is an example of pulling a prompt by tag in Python:\\nCopyprompt = client.pull_prompt(\"joke-generator:prod\")\\n# If prod tag points to commit a1b2c3d4, this is equivalent to:\\nprompt = client.pull_prompt(\"joke-generator:a1b2c3d4\")\\n\\nFor more information on how to use prompts in code, refer to Managing prompts programmatically.\\n\\u200bTrigger a webhook on prompt commit\\nYou can configure a webhook to be triggered whenever a commit is made to a prompt.\\nSome common use cases of this include:\\n\\nTriggering a CI/CD pipeline when prompts are updated.\\nSynchronizing prompts with a GitHub repository.\\nNotifying team members about prompt modifications.\\n\\n\\u200bConfigure a webhook\\nNavigate to the Prompts section in the left-hand sidebar or from the application homepage. In the top right corner, click on the + Webhook button.\\nAdd a webhook URL and any required headers.\\nYou can only configure one webhook per workspace. If you want to configure multiple per workspace or set up a different webhook for each prompt, let us know in the LangChain Forum.\\nTo test out your webhook, click the Send test notification button. This will send a test notification to the webhook URL you provided with a sample payload.\\nThe sample payload is a JSON object with the following fields:\\n\\nprompt_id: The ID of the prompt that was committed.\\nprompt_name: The name of the prompt that was committed.\\ncommit_hash: The commit hash of the prompt.\\ncreated_at: The date of the commit.\\ncreated_by: The author of the commit.\\nmanifest: The manifest of the prompt.\\n\\n\\u200bTrigger the webhook\\nCommit to a prompt to trigger the webhook youâ€™ve configured.\\n\\u200bUse the Playground\\nIf you do this in the Playground, youâ€™ll be prompted to deselect the webhooks youâ€™d like to avoid triggering.\\n\\n\\u200bUsing the API\\nIf you commit via the API, you can specify to skip triggering the webhook by setting the skip_webhooks parameter to true or to an array of webhook ids to ignore. Refer to the API docs for more information.\\n\\u200bPublic prompt hub\\nLangSmithâ€™s public prompt hub is a collection of prompts that have been created by the LangChain community that you can use for reference.\\nNote that prompts are user-generated and unverified. LangChain does not review or endorse public prompts, use these at your own risk. Use of Prompt Hub is subject to our Terms of Service.\\nNavigate to the Prompts section of the left-hand sidebar and click on Browse all Public Prompts in the LangChain Hub.\\nHere youâ€™ll find all of the publicly listed prompts in the LangChain Hub. You can search for prompts by name, handle, use cases, descriptions, or models. You can fork prompts to your personal organization, view the promptâ€™s details, and run the prompt in the Playground. You can pull any public prompt into your code using the SDK.\\nTo view prompts tied to your workspace, visit the Prompts tab in the sidebar.\\nWas this page helpful?YesNoSuggest editsCreate a promptManage prompts programmaticallyâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/update_a_prompt', 'loc': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/update_a_prompt', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Update a prompt - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationUpdate a promptGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageUpdate metadataUpdate the prompt contentVersion a promptUpdate a promptCopy pageCopy pageNavigate to the  section in the left-hand sidebar or from the application homepage and click on the prompt you want to edit.\\n\\u200bUpdate metadata\\nTo update the prompt metadata (description, use cases, etc.) click the â€œEditâ€ pencil icon.\\n\\nYour prompt metadata will be updated upon save.\\n\\n\\u200bUpdate the prompt content\\nTo update the prompt content itself, you need to enter the prompt playground. Click â€œEdit in playgroundâ€. Now you can make changes to the prompt and test it with different inputs. When youâ€™re happy with the prompt, click â€œCommitâ€ to save it.\\n\\n\\n\\u200bVersion a prompt\\nWhen you add a commit to a prompt, a new version of the prompt is created. You can view all historical versions by clicking the â€œCommitsâ€ tab in the prompt view.\\nWas this page helpful?YesNoSuggest editsâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/use_tools', 'loc': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/use_tools', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Use tools in a prompt - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsCreate and update promptsCreate a promptManage promptsManage prompts programmaticallyConfigure prompt settingsUse tools in a promptInclude multimodal content in a promptWrite your prompt with AIConnect to modelsTutorialsOptimize a classifierSync prompts with GitHubTest multi-turn conversationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationCreate and update promptsUse tools in a promptGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageWhen to use toolsBuilt-in toolsOpenAI ToolsAnthropic ToolsAdding and using toolsAdd a toolUse a built-in toolCreate a custom toolTool choice settingsCreate and update promptsUse tools in a promptCopy pageCopy pageTools allow language models to interact with external systems and perform actions beyond just generating text. In the LangSmith playground, you can use two types of tools:\\n\\n\\nBuilt-in tools: Pre-configured tools provided by model providers (like OpenAI and Anthropic) that are ready to use. These include capabilities like web search, code interpretation, and more.\\n\\n\\nCustom tools: Functions you define to perform specific tasks. These are useful when you need to integrate with your own systems or create specialized functionality. When you define custom tools within the LangSmith Playground, you can verify that the model correctly identifies and calls these tools with the correct arguments. Soon we plan to support executing these custom tool calls directly.\\n\\n\\n\\u200bWhen to use tools\\n\\n\\nUse built-in tools when you need common capabilities like web search or code interpretation. These are built and maintained by the model providers.\\n\\n\\nUse custom tools when you want to test and validate your own tool designs, including:\\n\\nValidating which tools the model chooses to use and seeing the specific arguments it provides in tool calls\\nSimulating tool interactions\\n\\n\\n\\n\\u200bBuilt-in tools\\nThe LangSmith Playground has native support for a variety of tools from OpenAI and Anthropic. If you want to use a tool that isnâ€™t explicitly listed in the Playground, you can still add it by manually specifying its type and any required arguments.\\n\\u200bOpenAI Tools\\n\\nWeb search: Search the web for real-time information\\nImage generation: Generate images based on a text prompt\\nMCP: Gives the model access to tools hosted on a remote MCP server\\nView all OpenAI tools\\n\\n\\u200bAnthropic Tools\\n\\nWeb search: Search the web for up-to-date information\\nView all Anthropic tools\\n\\n\\u200bAdding and using tools\\n\\u200bAdd a tool\\nTo add a tool to your prompt, click the + Tool button at the bottom of the prompt editor. \\n\\u200bUse a built-in tool\\n\\nIn the tool section, select the built-in tool you want to use. Youâ€™ll only see the tools that are compatible with the provider and model youâ€™ve chosen.\\nWhen the model calls the tool, the playground will display the response\\n\\n\\n\\u200bCreate a custom tool\\nTo create a custom tool, youâ€™ll need to provide:\\n\\nName: A descriptive name for your tool\\nDescription: Clear explanation of what the tool does\\nArguments: The inputs your tool requires\\n\\n\\nNote: When running a custom tool in the playground, the model will respond with a JSON object containing the tool name and the tool call. Currently, thereâ€™s no way to connect this to a hosted tool via MCP.\\n\\n\\u200bTool choice settings\\nSome models provide control over which tools are called. To configure this:\\n\\nGo to prompt settings\\nNavigate to tool settings\\nSelect tool choice\\n\\nTo understand the available tool choice options, check the documentation for your specific provider. For example, OpenAIâ€™s documentation on tool choice.\\nWas this page helpful?YesNoSuggest editsConfigure prompt settingsInclude multimodal content in a promptâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_sdk', 'loc': 'https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_sdk', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Prompt engineering quickstart - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewQuickstartsTrace an applicationEvaluate an applicationTest promptsAPI & SDKsAPI referencePython SDKJS/TS SDKPricingPlansPricing FAQOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationQuickstartsPrompt engineering quickstartGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pagePrerequisitesNext stepsVideo guideQuickstartsPrompt engineering quickstartCopy pageCopy pagePrompts guide the behavior of large language models (LLM). Prompt engineering is the process of crafting, testing, and refining the instructions you give to an LLM so it produces reliable and useful responses.\\nLangSmith provides tools to create, version, test, and collaborate on prompts. Youâ€™ll also encounter common concepts like prompt templates, which let you reuse structured prompts, and variables, which allow you to dynamically insert values (such as a userâ€™s question) into a prompt.\\nIn this quickstart, youâ€™ll create, test, and improve prompts using either the UI or the SDK. This quickstart will use OpenAI as the example LLM provider, but the same workflow applies across other providers.\\nIf you prefer to watch a video on getting started with prompt engineering, refer to the quickstart Video guide.\\n\\u200bPrerequisites\\nBefore you begin, make sure you have:\\n\\nA LangSmith account: Sign up or log in at smith.langchain.com.\\nA LangSmith API key: Follow the Create an API key guide.\\nAn OpenAI API key: Generate this from the OpenAI dashboard.\\n\\nSelect the tab for UI or SDK workflows:\\n UI SDK\\u200b1. Set workspace secretIn the LangSmith UI for your workspace, ensure that your OpenAI API key is set as a workspace.\\nNavigate to  Settings and then move to the Secrets tab.\\nOn the Workspace Secrets page, select Add secret and enter the OPENAI_API_KEY and your API key as the Value.\\nSelect Save secret.\\n When adding workspace secrets in the LangSmith UI, make sure the secret keys match the environment variable names expected by your model provider.\\u200b2. Create a prompt\\nIn the LangSmith UI, navigate to the Prompts section in the left-hand menu.\\nClick on + Prompt to create a prompt.\\nModify the prompt by editing or adding prompts and input variables as needed.\\n\\u200b3. Test a prompt\\n\\nUnder the Prompts heading select the gear  icon next to the model name, which will launch the Prompt Settings window on the Model Configuration tab.\\n\\n\\nSet the model configuration you want to use. The Provider and Model you select will determine the parameters that are configurable on this configuration page. Once set, click Save as.\\n\\n\\n\\nSpecify the input variables you would like to test in the Inputs box and then click  Start.\\n\\nTo learn about more options for configuring your prompt in the Playground, refer to Configure prompt settings.\\n\\n\\nAfter testing and refining your prompt, click Save to store it for future use.\\n\\n\\u200b4. Iterate on a promptLangSmith allows for team-based prompt iteration. Workspace members can experiment with prompts in the playground and save their changes as a new commit when ready.To improve your prompts:\\n\\nReference the documentation provided by your model provider for best practices in prompt creation, such as:\\n\\nBest practices for prompt engineering with the OpenAI API\\nGeminiâ€™s Introduction to prompt design\\n\\n\\n\\nBuild and refine your prompts with the Prompt Canvasâ€”an interactive tool in LangSmith. Learn more in the Prompt Canvas guide.\\n\\n\\nTag specific commits to mark important moments in your commit history.\\n\\nTo create a commit, navigate to the Playground and select Commit. Choose the prompt to commit changes to and then Commit.\\nNavigate to Prompts in the left-hand menu. Select the prompt. Once on the promptâ€™s detail page, move to the Commits tab. Find the tag icon  to Add a Commit Tag.\\n\\n\\n\\n\\n\\u200bNext steps\\n\\nLearn more about how to store and manage prompts using the Prompt Hub in the Create a prompt guide.\\nLearn how to set up the Playground to Test multi-turn conversations in this tutorial.\\nLearn how to test your promptâ€™s performance over a dataset instead of individual examples, refer to Run an evaluation from the Prompt Playground.\\n\\n\\u200bVideo guide\\nWas this page helpful?YesNoSuggest editsEvaluate an applicationAPI referenceâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui', 'loc': 'https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Prompt engineering quickstart - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewQuickstartsTrace an applicationEvaluate an applicationTest promptsAPI & SDKsAPI referencePython SDKJS/TS SDKPricingPlansPricing FAQOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationQuickstartsPrompt engineering quickstartGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pagePrerequisitesNext stepsVideo guideQuickstartsPrompt engineering quickstartCopy pageCopy pagePrompts guide the behavior of large language models (LLM). Prompt engineering is the process of crafting, testing, and refining the instructions you give to an LLM so it produces reliable and useful responses.\\nLangSmith provides tools to create, version, test, and collaborate on prompts. Youâ€™ll also encounter common concepts like prompt templates, which let you reuse structured prompts, and variables, which allow you to dynamically insert values (such as a userâ€™s question) into a prompt.\\nIn this quickstart, youâ€™ll create, test, and improve prompts using either the UI or the SDK. This quickstart will use OpenAI as the example LLM provider, but the same workflow applies across other providers.\\nIf you prefer to watch a video on getting started with prompt engineering, refer to the quickstart Video guide.\\n\\u200bPrerequisites\\nBefore you begin, make sure you have:\\n\\nA LangSmith account: Sign up or log in at smith.langchain.com.\\nA LangSmith API key: Follow the Create an API key guide.\\nAn OpenAI API key: Generate this from the OpenAI dashboard.\\n\\nSelect the tab for UI or SDK workflows:\\n UI SDK\\u200b1. Set workspace secretIn the LangSmith UI for your workspace, ensure that your OpenAI API key is set as a workspace.\\nNavigate to  Settings and then move to the Secrets tab.\\nOn the Workspace Secrets page, select Add secret and enter the OPENAI_API_KEY and your API key as the Value.\\nSelect Save secret.\\n When adding workspace secrets in the LangSmith UI, make sure the secret keys match the environment variable names expected by your model provider.\\u200b2. Create a prompt\\nIn the LangSmith UI, navigate to the Prompts section in the left-hand menu.\\nClick on + Prompt to create a prompt.\\nModify the prompt by editing or adding prompts and input variables as needed.\\n\\u200b3. Test a prompt\\n\\nUnder the Prompts heading select the gear  icon next to the model name, which will launch the Prompt Settings window on the Model Configuration tab.\\n\\n\\nSet the model configuration you want to use. The Provider and Model you select will determine the parameters that are configurable on this configuration page. Once set, click Save as.\\n\\n\\n\\nSpecify the input variables you would like to test in the Inputs box and then click  Start.\\n\\nTo learn about more options for configuring your prompt in the Playground, refer to Configure prompt settings.\\n\\n\\nAfter testing and refining your prompt, click Save to store it for future use.\\n\\n\\u200b4. Iterate on a promptLangSmith allows for team-based prompt iteration. Workspace members can experiment with prompts in the playground and save their changes as a new commit when ready.To improve your prompts:\\n\\nReference the documentation provided by your model provider for best practices in prompt creation, such as:\\n\\nBest practices for prompt engineering with the OpenAI API\\nGeminiâ€™s Introduction to prompt design\\n\\n\\n\\nBuild and refine your prompts with the Prompt Canvasâ€”an interactive tool in LangSmith. Learn more in the Prompt Canvas guide.\\n\\n\\nTag specific commits to mark important moments in your commit history.\\n\\nTo create a commit, navigate to the Playground and select Commit. Choose the prompt to commit changes to and then Commit.\\nNavigate to Prompts in the left-hand menu. Select the prompt. Once on the promptâ€™s detail page, move to the Commits tab. Find the tag icon  to Add a Commit Tag.\\n\\n\\n\\n\\n\\u200bNext steps\\n\\nLearn more about how to store and manage prompts using the Prompt Hub in the Create a prompt guide.\\nLearn how to set up the Playground to Test multi-turn conversations in this tutorial.\\nLearn how to test your promptâ€™s performance over a dataset instead of individual examples, refer to Run an evaluation from the Prompt Playground.\\n\\n\\u200bVideo guide\\nWas this page helpful?YesNoSuggest editsEvaluate an applicationAPI referenceâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/tutorials', 'loc': 'https://docs.smith.langchain.com/prompt_engineering/tutorials', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Optimize a classifier - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsCreate and update promptsCreate a promptManage promptsManage prompts programmaticallyConfigure prompt settingsUse tools in a promptInclude multimodal content in a promptWrite your prompt with AIConnect to modelsTutorialsOptimize a classifierSync prompts with GitHubTest multi-turn conversationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationTutorialsOptimize a classifierGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageThe objectiveGetting startedSet up automationsUpdate the applicationSemantic search over examplesTutorialsOptimize a classifierCopy pageCopy pageThis tutorial walks through optimizing a classifier based on user a feedback. Classifiers are great to optimize because its generally pretty simple to collect the desired output, which makes it easy to create few shot examples based on user feedback. That is exactly what we will do in this example.\\n\\u200bThe objective\\nIn this example, we will build a bot that classify GitHub issues based on their title. It will take in a title and classify it into one of many different classes. Then, we will start to collect user feedback and use that to shape how this classifier performs.\\n\\u200bGetting started\\nTo get started, we will first set it up so that we send all traces to a specific project. We can do this by setting an environment variable:\\nCopyimport os\\nos.environ[\"LANGSMITH_PROJECT\"] = \"classifier\"\\n\\nWe can then create our initial application. This will be a really simple function that just takes in a GitHub issue title and tries to label it.\\nCopyimport openai\\nfrom langsmith import traceable, Client\\nimport uuid\\n\\nclient = openai.Client()\\n\\navailable_topics = [\\n    \"bug\",\\n    \"improvement\",\\n    \"new_feature\",\\n    \"documentation\",\\n    \"integration\",\\n]\\n\\nprompt_template = \"\"\"Classify the type of the issue as one of {topics}.\\nIssue: {text}\"\"\"\\n\\n@traceable(\\n    run_type=\"chain\",\\n    name=\"Classifier\",\\n)\\ndef topic_classifier(\\n    topic: str):\\n    return client.chat.completions.create(\\n        model=\"gpt-4o-mini\",\\n        temperature=0,\\n        messages=[\\n            {\\n                \"role\": \"user\",\\n                \"content\": prompt_template.format(\\n                    topics=\\',\\'.join(available_topics),\\n                    text=topic,\\n                )\\n            }\\n        ],\\n    ).choices[0].message.content\\n\\nWe can then start to interact with it. When interacting with it, we will generate the LangSmith run id ahead of time and pass that into this function. We do this so we can attach feedback later on.\\nHereâ€™s how we can invoke the application:\\nCopyrun_id = uuid.uuid4()\\ntopic_classifier(\\n    \"fix bug in LCEL\",\\n    langsmith_extra={\"run_id\": run_id})\\n\\nHereâ€™s how we can attach feedback after. We can collect feedback in two forms.\\nFirst, we can collect â€œpositiveâ€ feedback - this is for examples that the model got right.\\nCopyls_client = Client()\\nrun_id = uuid.uuid4()\\ntopic_classifier(\\n    \"fix bug in LCEL\",\\n    langsmith_extra={\"run_id\": run_id})\\nls_client.create_feedback(\\n    run_id,\\n    key=\"user-score\",\\n    score=1.0,\\n)\\n\\nNext, we can focus on collecting feedback that corresponds to a â€œcorrectionâ€ to the generation. In this example the model will classify it as a bug, whereas I really want this to be classified as documentation.\\nCopyls_client = Client()\\nrun_id = uuid.uuid4()\\ntopic_classifier(\\n    \"fix bug in documentation\",\\n    langsmith_extra={\"run_id\": run_id})\\nls_client.create_feedback(\\n    run_id,\\n    key=\"correction\",\\n    correction=\"documentation\")\\n\\n\\u200bSet up automations\\nWe can now set up automations to move examples with feedback of some form into a dataset. We will set up two automations, one for positive feedback and the other for negative feedback.\\nThe first will take all runs with positive feedback and automatically add them to a dataset. The logic behind this is that any run with positive feedback we can use as a good example in future iterations. Letâ€™s create a dataset called classifier-github-issues to add this data to.\\n\\nThe second will take all runs with a correction and use a webhook to add them to a dataset. When creating this webhook, we will select the option to â€œUse Correctionsâ€. This option will make it so that when creating a dataset from a run, rather than using the output of the run as the gold-truth output of the datapoint, it will use the correction.\\n\\n\\u200bUpdate the application\\nWe can now update our code to pull down the dataset we are sending runs to. Once we pull it down, we can create a string with the examples in it. We can then put this string as part of the prompt!\\nCopy### NEW CODE ###\\n# Initialize the LangSmith Client so we can use to get the dataset\\nls_client = Client()\\n\\n# Create a function that will take in a list of examples and format them into a string\\ndef create_example_string(examples):\\n    final_strings = []\\n    for e in examples:\\n        final_strings.append(f\"Input: {e.inputs[\\'topic\\']}\\\\n> {e.outputs[\\'output\\']}\")\\n    return \"\\\\n\\\\n\".join(final_strings)\\n### NEW CODE ###\\n\\nclient = openai.Client()\\n\\navailable_topics = [\\n    \"bug\",\\n    \"improvement\",\\n    \"new_feature\",\\n    \"documentation\",\\n    \"integration\",\\n]\\n\\nprompt_template = \"\"\"Classify the type of the issue as one of {topics}.\\n\\nHere are some examples:\\n{examples}\\n\\nBegin!\\nIssue: {text}\\n>\"\"\"\\n\\n@traceable(\\n    run_type=\"chain\",\\n    name=\"Classifier\",\\n)\\ndef topic_classifier(\\n    topic: str):\\n    # We can now pull down the examples from the dataset\\n    # We do this inside the function so it always get the most up-to-date examples,\\n    # But this can be done outside and cached for speed if desired\\n    examples = list(ls_client.list_examples(dataset_name=\"classifier-github-issues\"))  # <- New Code\\n    example_string = create_example_string(examples)\\n    return client.chat.completions.create(\\n        model=\"gpt-4o-mini\",\\n        temperature=0,\\n        messages=[\\n            {\\n                \"role\": \"user\",\\n                \"content\": prompt_template.format(\\n                    topics=\\',\\'.join(available_topics),\\n                    text=topic,\\n                    examples=example_string,\\n                )\\n            }\\n        ],\\n    ).choices[0].message.content\\n\\nIf now run the application with a similar input as before, we can see that it correctly learns that anything related to docs (even if a bug) should be classified as documentation\\nCopyls_client = Client()\\nrun_id = uuid.uuid4()\\ntopic_classifier(\\n    \"address bug in documentation\",\\n    langsmith_extra={\"run_id\": run_id})\\n\\n\\u200bSemantic search over examples\\nOne additional thing we can do is only use the most semantically similar examples. This is useful when you start to build up a lot of examples.\\nIn order to do this, we can first define an example to find the k most similar examples:\\nCopyimport numpy as np\\n\\ndef find_similar(examples, topic, k=5):\\n    inputs = [e.inputs[\\'topic\\'] for e in examples] + [topic]\\n    vectors = client.embeddings.create(input=inputs, model=\"text-embedding-3-small\")\\n    vectors = [e.embedding for e in vectors.data]\\n    vectors = np.array(vectors)\\n    args = np.argsort(-vectors.dot(vectors[-1])[:-1])[:5]\\n    examples = [examples[i] for i in args]\\n    return examples\\n\\nWe can then use that in the application\\nCopyls_client = Client()\\n\\ndef create_example_string(examples):\\n    final_strings = []\\n    for e in examples:\\n        final_strings.append(f\"Input: {e.inputs[\\'topic\\']}\\\\n> {e.outputs[\\'output\\']}\")\\n    return \"\\\\n\\\\n\".join(final_strings)\\n\\nclient = openai.Client()\\n\\navailable_topics = [\\n    \"bug\",\\n    \"improvement\",\\n    \"new_feature\",\\n    \"documentation\",\\n    \"integration\",\\n]\\n\\nprompt_template = \"\"\"Classify the type of the issue as one of {topics}.\\n\\nHere are some examples:\\n{examples}\\n\\nBegin!\\nIssue: {text}\\n>\"\"\"\\n\\n@traceable(\\n    run_type=\"chain\",\\n    name=\"Classifier\",\\n)\\ndef topic_classifier(\\n    topic: str):\\n    examples = list(ls_client.list_examples(dataset_name=\"classifier-github-issues\"))\\n    examples = find_similar(examples, topic)\\n    example_string = create_example_string(examples)\\n    return client.chat.completions.create(\\n        model=\"gpt-4o-mini\",\\n        temperature=0,\\n        messages=[\\n            {\\n                \"role\": \"user\",\\n                \"content\": prompt_template.format(\\n                    topics=\\',\\'.join(available_topics),\\n                    text=topic,\\n                    examples=example_string,\\n                )\\n            }\\n        ],\\n    ).choices[0].message.content\\nWas this page helpful?YesNoSuggest editsCustom modelSync prompts with GitHubâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/tutorials/optimize_classifier', 'loc': 'https://docs.smith.langchain.com/prompt_engineering/tutorials/optimize_classifier', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Optimize a classifier - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsCreate and update promptsCreate a promptManage promptsManage prompts programmaticallyConfigure prompt settingsUse tools in a promptInclude multimodal content in a promptWrite your prompt with AIConnect to modelsTutorialsOptimize a classifierSync prompts with GitHubTest multi-turn conversationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationTutorialsOptimize a classifierGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageThe objectiveGetting startedSet up automationsUpdate the applicationSemantic search over examplesTutorialsOptimize a classifierCopy pageCopy pageThis tutorial walks through optimizing a classifier based on user a feedback. Classifiers are great to optimize because its generally pretty simple to collect the desired output, which makes it easy to create few shot examples based on user feedback. That is exactly what we will do in this example.\\n\\u200bThe objective\\nIn this example, we will build a bot that classify GitHub issues based on their title. It will take in a title and classify it into one of many different classes. Then, we will start to collect user feedback and use that to shape how this classifier performs.\\n\\u200bGetting started\\nTo get started, we will first set it up so that we send all traces to a specific project. We can do this by setting an environment variable:\\nCopyimport os\\nos.environ[\"LANGSMITH_PROJECT\"] = \"classifier\"\\n\\nWe can then create our initial application. This will be a really simple function that just takes in a GitHub issue title and tries to label it.\\nCopyimport openai\\nfrom langsmith import traceable, Client\\nimport uuid\\n\\nclient = openai.Client()\\n\\navailable_topics = [\\n    \"bug\",\\n    \"improvement\",\\n    \"new_feature\",\\n    \"documentation\",\\n    \"integration\",\\n]\\n\\nprompt_template = \"\"\"Classify the type of the issue as one of {topics}.\\nIssue: {text}\"\"\"\\n\\n@traceable(\\n    run_type=\"chain\",\\n    name=\"Classifier\",\\n)\\ndef topic_classifier(\\n    topic: str):\\n    return client.chat.completions.create(\\n        model=\"gpt-4o-mini\",\\n        temperature=0,\\n        messages=[\\n            {\\n                \"role\": \"user\",\\n                \"content\": prompt_template.format(\\n                    topics=\\',\\'.join(available_topics),\\n                    text=topic,\\n                )\\n            }\\n        ],\\n    ).choices[0].message.content\\n\\nWe can then start to interact with it. When interacting with it, we will generate the LangSmith run id ahead of time and pass that into this function. We do this so we can attach feedback later on.\\nHereâ€™s how we can invoke the application:\\nCopyrun_id = uuid.uuid4()\\ntopic_classifier(\\n    \"fix bug in LCEL\",\\n    langsmith_extra={\"run_id\": run_id})\\n\\nHereâ€™s how we can attach feedback after. We can collect feedback in two forms.\\nFirst, we can collect â€œpositiveâ€ feedback - this is for examples that the model got right.\\nCopyls_client = Client()\\nrun_id = uuid.uuid4()\\ntopic_classifier(\\n    \"fix bug in LCEL\",\\n    langsmith_extra={\"run_id\": run_id})\\nls_client.create_feedback(\\n    run_id,\\n    key=\"user-score\",\\n    score=1.0,\\n)\\n\\nNext, we can focus on collecting feedback that corresponds to a â€œcorrectionâ€ to the generation. In this example the model will classify it as a bug, whereas I really want this to be classified as documentation.\\nCopyls_client = Client()\\nrun_id = uuid.uuid4()\\ntopic_classifier(\\n    \"fix bug in documentation\",\\n    langsmith_extra={\"run_id\": run_id})\\nls_client.create_feedback(\\n    run_id,\\n    key=\"correction\",\\n    correction=\"documentation\")\\n\\n\\u200bSet up automations\\nWe can now set up automations to move examples with feedback of some form into a dataset. We will set up two automations, one for positive feedback and the other for negative feedback.\\nThe first will take all runs with positive feedback and automatically add them to a dataset. The logic behind this is that any run with positive feedback we can use as a good example in future iterations. Letâ€™s create a dataset called classifier-github-issues to add this data to.\\n\\nThe second will take all runs with a correction and use a webhook to add them to a dataset. When creating this webhook, we will select the option to â€œUse Correctionsâ€. This option will make it so that when creating a dataset from a run, rather than using the output of the run as the gold-truth output of the datapoint, it will use the correction.\\n\\n\\u200bUpdate the application\\nWe can now update our code to pull down the dataset we are sending runs to. Once we pull it down, we can create a string with the examples in it. We can then put this string as part of the prompt!\\nCopy### NEW CODE ###\\n# Initialize the LangSmith Client so we can use to get the dataset\\nls_client = Client()\\n\\n# Create a function that will take in a list of examples and format them into a string\\ndef create_example_string(examples):\\n    final_strings = []\\n    for e in examples:\\n        final_strings.append(f\"Input: {e.inputs[\\'topic\\']}\\\\n> {e.outputs[\\'output\\']}\")\\n    return \"\\\\n\\\\n\".join(final_strings)\\n### NEW CODE ###\\n\\nclient = openai.Client()\\n\\navailable_topics = [\\n    \"bug\",\\n    \"improvement\",\\n    \"new_feature\",\\n    \"documentation\",\\n    \"integration\",\\n]\\n\\nprompt_template = \"\"\"Classify the type of the issue as one of {topics}.\\n\\nHere are some examples:\\n{examples}\\n\\nBegin!\\nIssue: {text}\\n>\"\"\"\\n\\n@traceable(\\n    run_type=\"chain\",\\n    name=\"Classifier\",\\n)\\ndef topic_classifier(\\n    topic: str):\\n    # We can now pull down the examples from the dataset\\n    # We do this inside the function so it always get the most up-to-date examples,\\n    # But this can be done outside and cached for speed if desired\\n    examples = list(ls_client.list_examples(dataset_name=\"classifier-github-issues\"))  # <- New Code\\n    example_string = create_example_string(examples)\\n    return client.chat.completions.create(\\n        model=\"gpt-4o-mini\",\\n        temperature=0,\\n        messages=[\\n            {\\n                \"role\": \"user\",\\n                \"content\": prompt_template.format(\\n                    topics=\\',\\'.join(available_topics),\\n                    text=topic,\\n                    examples=example_string,\\n                )\\n            }\\n        ],\\n    ).choices[0].message.content\\n\\nIf now run the application with a similar input as before, we can see that it correctly learns that anything related to docs (even if a bug) should be classified as documentation\\nCopyls_client = Client()\\nrun_id = uuid.uuid4()\\ntopic_classifier(\\n    \"address bug in documentation\",\\n    langsmith_extra={\"run_id\": run_id})\\n\\n\\u200bSemantic search over examples\\nOne additional thing we can do is only use the most semantically similar examples. This is useful when you start to build up a lot of examples.\\nIn order to do this, we can first define an example to find the k most similar examples:\\nCopyimport numpy as np\\n\\ndef find_similar(examples, topic, k=5):\\n    inputs = [e.inputs[\\'topic\\'] for e in examples] + [topic]\\n    vectors = client.embeddings.create(input=inputs, model=\"text-embedding-3-small\")\\n    vectors = [e.embedding for e in vectors.data]\\n    vectors = np.array(vectors)\\n    args = np.argsort(-vectors.dot(vectors[-1])[:-1])[:5]\\n    examples = [examples[i] for i in args]\\n    return examples\\n\\nWe can then use that in the application\\nCopyls_client = Client()\\n\\ndef create_example_string(examples):\\n    final_strings = []\\n    for e in examples:\\n        final_strings.append(f\"Input: {e.inputs[\\'topic\\']}\\\\n> {e.outputs[\\'output\\']}\")\\n    return \"\\\\n\\\\n\".join(final_strings)\\n\\nclient = openai.Client()\\n\\navailable_topics = [\\n    \"bug\",\\n    \"improvement\",\\n    \"new_feature\",\\n    \"documentation\",\\n    \"integration\",\\n]\\n\\nprompt_template = \"\"\"Classify the type of the issue as one of {topics}.\\n\\nHere are some examples:\\n{examples}\\n\\nBegin!\\nIssue: {text}\\n>\"\"\"\\n\\n@traceable(\\n    run_type=\"chain\",\\n    name=\"Classifier\",\\n)\\ndef topic_classifier(\\n    topic: str):\\n    examples = list(ls_client.list_examples(dataset_name=\"classifier-github-issues\"))\\n    examples = find_similar(examples, topic)\\n    example_string = create_example_string(examples)\\n    return client.chat.completions.create(\\n        model=\"gpt-4o-mini\",\\n        temperature=0,\\n        messages=[\\n            {\\n                \"role\": \"user\",\\n                \"content\": prompt_template.format(\\n                    topics=\\',\\'.join(available_topics),\\n                    text=topic,\\n                    examples=example_string,\\n                )\\n            }\\n        ],\\n    ).choices[0].message.content\\nWas this page helpful?YesNoSuggest editsCustom modelSync prompts with GitHubâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/tutorials/prompt_commit', 'loc': 'https://docs.smith.langchain.com/prompt_engineering/tutorials/prompt_commit', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='How to sync prompts with GitHub - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsCreate and update promptsCreate a promptManage promptsManage prompts programmaticallyConfigure prompt settingsUse tools in a promptInclude multimodal content in a promptWrite your prompt with AIConnect to modelsTutorialsOptimize a classifierSync prompts with GitHubTest multi-turn conversationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationTutorialsHow to sync prompts with GitHubGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pagePrerequisitesUnderstanding LangSmith â€œPrompt Commitsâ€ and WebhooksImplementing a FastAPI Server for Webhook ReceptionConfiguring the Webhook in LangSmithThe Workflow in ActionBeyond a Simple CommitTutorialsHow to sync prompts with GitHubCopy pageCopy pageLangSmith provides a collaborative interface to create, test, and iterate on prompts.\\nWhile you can dynamically fetch prompts from LangSmith into your application at runtime, you may prefer to sync prompts with your own database or version control system. To support this workflow, LangSmith allows you to receive notifications of prompt updates via webhooks.\\nWhy sync prompts with GitHub?\\n\\nVersion Control: Keep your prompts versioned alongside your application code in a familiar system.\\nCI/CD Integration: Trigger automated staging or production deployments when critical prompts change.\\n\\n\\n\\u200bPrerequisites\\nBefore we begin, ensure you have the following set up:\\n\\n\\nGitHub Account: A standard GitHub account.\\n\\n\\nGitHub Repository: Create a new (or choose an existing) repository where your LangSmith prompt manifests will be stored. This could be the same repository as your application code or a dedicated one for prompts.\\n\\n\\nGitHub Personal Access Token (PAT):\\n\\nLangSmith webhooks donâ€™t directly interact with GitHubâ€”they call an intermediary server that you create.\\nThis server requires a GitHub PAT to authenticate and make commits to your repository.\\nMust include the repo scope (public_repo is sufficient for public repositories).\\nGo to GitHub > Settings > Developer settings > Personal access tokens > Tokens (classic).\\nClick Generate new token (classic).\\nName it (e.g., â€œLangSmith Prompt Syncâ€), set an expiration, and select the required scopes.\\nClick Generate token and copy it immediately â€” it wonâ€™t be shown again.\\nStore the token securely and provide it as an environment variable to your server.\\n\\n\\n\\n\\u200bUnderstanding LangSmith â€œPrompt Commitsâ€ and Webhooks\\nIn LangSmith, when you save changes to a prompt, youâ€™re essentially creating a new version or a â€œPrompt Commit.â€ These commits are what can trigger webhooks.\\nThe webhook will send a JSON payload containing the new prompt manifest.\\nSample Webhook PayloadCopy{\\n  \"prompt_id\": \"f33dcb51-eb17-47a5-83ca-64ac8a027a29\",\\n  \"prompt_name\": \"My Prompt\",\\n  \"commit_hash\": \"commit_hash_1234567890\",\\n  \"created_at\": \"2021-01-01T00:00:00Z\",\\n  \"created_by\": \"Jane Doe\",\\n  \"manifest\": {\\n    \"lc\": 1,\\n    \"type\": \"constructor\",\\n    \"id\": [\"langchain\", \"schema\", \"runnable\", \"RunnableSequence\"],\\n    \"kwargs\": {\\n      \"first\": {\\n        \"lc\": 1,\\n        \"type\": \"constructor\",\\n        \"id\": [\"langchain\", \"prompts\", \"chat\", \"ChatPromptTemplate\"],\\n        \"kwargs\": {\\n          \"messages\": [\\n            {\\n              \"lc\": 1,\\n              \"type\": \"constructor\",\\n              \"id\": [\\n                \"langchain_core\",\\n                \"prompts\",\\n                \"chat\",\\n                \"SystemMessagePromptTemplate\"\\n              ],\\n              \"kwargs\": {\\n                \"prompt\": {\\n                  \"lc\": 1,\\n                  \"type\": \"constructor\",\\n                  \"id\": [\\n                    \"langchain_core\",\\n                    \"prompts\",\\n                    \"prompt\",\\n                    \"PromptTemplate\"\\n                  ],\\n                  \"kwargs\": {\\n                    \"input_variables\": [],\\n                    \"template_format\": \"mustache\",\\n                    \"template\": \"You are a chatbot.\"\\n                  }\\n                }\\n              }\\n            },\\n            {\\n              \"lc\": 1,\\n              \"type\": \"constructor\",\\n              \"id\": [\\n                \"langchain_core\",\\n                \"prompts\",\\n                \"chat\",\\n                \"HumanMessagePromptTemplate\"\\n              ],\\n              \"kwargs\": {\\n                \"prompt\": {\\n                  \"lc\": 1,\\n                  \"type\": \"constructor\",\\n                  \"id\": [\\n                    \"langchain_core\",\\n                    \"prompts\",\\n                    \"prompt\",\\n                    \"PromptTemplate\"\\n                  ],\\n                  \"kwargs\": {\\n                    \"input_variables\": [\"question\"],\\n                    \"template_format\": \"mustache\",\\n                    \"template\": \"{{question}}\"\\n                  }\\n                }\\n              }\\n            }\\n          ],\\n          \"input_variables\": [\"question\"]\\n        }\\n      },\\n      \"last\": {\\n        \"lc\": 1,\\n        \"type\": \"constructor\",\\n        \"id\": [\"langchain\", \"schema\", \"runnable\", \"RunnableBinding\"],\\n        \"kwargs\": {\\n          \"bound\": {\\n            \"lc\": 1,\\n            \"type\": \"constructor\",\\n            \"id\": [\"langchain\", \"chat_models\", \"openai\", \"ChatOpenAI\"],\\n            \"kwargs\": {\\n              \"temperature\": 1,\\n              \"top_p\": 1,\\n              \"presence_penalty\": 0,\\n              \"frequency_penalty\": 0,\\n              \"model\": \"gpt-4.1-mini\",\\n              \"extra_headers\": {},\\n              \"openai_api_key\": {\\n                \"id\": [\"OPENAI_API_KEY\"],\\n                \"lc\": 1,\\n                \"type\": \"secret\"\\n              }\\n            }\\n          },\\n          \"kwargs\": {}\\n        }\\n      }\\n    }\\n  }\\n}\\n\\nItâ€™s important to understand that LangSmith webhooks for prompt commits are generally triggered at the workspace level. This means if any prompt within your LangSmith workspace is modified and a â€œprompt commitâ€ is saved, the webhook will fire and send the updated manifest of the prompt. The payloads are identifiable by prompt id. Your receiving server should be designed with this in mind.\\n\\u200bImplementing a FastAPI Server for Webhook Reception\\nTo effectively process webhook notifications from LangSmith when prompts are updated, an intermediary server application is necessary. This server will act as the receiver for HTTP POST requests sent by LangSmith. For demonstration purposes in this guide, we will outline the creation of a simple FastAPI application to fulfill this role.\\nThis publicly accessible server will be responsible for:\\n\\nReceiving Webhook Requests: Listening for incoming HTTP POST requests.\\nParsing Payloads: Extracting and interpreting the JSON-formatted prompt manifest from the request body.\\nCommitting to GitHub: Programmatically creating a new commit in your specified GitHub repository, containing the updated prompt manifest. This ensures your prompts remain version-controlled and synchronized with changes made in LangSmith.\\n\\nFor deployment, platforms like Render.com (offering a suitable free tier), Vercel, Fly.io, or other cloud providers (AWS, GCP, Azure) can be utilized to host the FastAPI application and obtain a public URL.\\nThe serverâ€™s core functionality will include an endpoint for webhook reception, logic for parsing the manifest, and integration with the GitHub API (using a Personal Access Token for authentication) to manage commits.\\nMinimal FastAPI Server Code ()main.pyThis server will listen for incoming webhooks from LangSmith and commit the received prompt manifest to your GitHub repository.Copyimport base64\\nimport json\\nimport uuid\\nfrom typing import Any, Dict\\nimport httpx\\nfrom fastapi import FastAPI, HTTPException, Body\\nfrom pydantic import BaseModel, Field\\nfrom pydantic_settings import BaseSettings, SettingsConfigDict\\n\\n# --- Configuration ---\\nclass AppConfig(BaseSettings):\\n    \"\"\"\\n    Application configuration model.\\n    Loads settings from environment variables.\\n    \"\"\"\\n    GITHUB_TOKEN: str\\n    GITHUB_REPO_OWNER: str\\n    GITHUB_REPO_NAME: str\\n    GITHUB_FILE_PATH: str = \"prompt_manifest.json\"\\n    GITHUB_BRANCH: str = \"main\"\\n    model_config = SettingsConfigDict(\\n        env_file=\".env\",\\n        env_file_encoding=\\'utf-8\\',\\n        extra=\\'ignore\\'\\n    )\\n\\nsettings = AppConfig()\\n\\n# --- Pydantic Models ---\\nclass WebhookPayload(BaseModel):\\n    \"\"\"\\n    Defines the expected structure of the incoming webhook payload.\\n    \"\"\"\\n    prompt_id: UUID = Field(\\n        ...,\\n        description=\"The unique identifier for the prompt.\"\\n    )\\n    prompt_name: str = Field(\\n        ...,\\n        description=\"The name/title of the prompt.\"\\n    )\\n    commit_hash: str = Field(\\n        ...,\\n        description=\"An identifier for the commit event that triggered the webhook.\"\\n    )\\n    created_at: str = Field(\\n        ...,\\n        description=\"Timestamp indicating when the event was created (ISO format preferred).\"\\n    )\\n    created_by: str = Field(\\n        ...,\\n        description=\"The name of the user who created the event.\"\\n    )\\n    manifest: Dict[str, Any] = Field(\\n        ...,\\n        description=\"The main content or configuration data to be committed to GitHub.\"\\n    )\\n\\n# --- GitHub Helper Function ---\\nasync def commit_manifest_to_github(payload: WebhookPayload) -> Dict[str, Any]:\\n    \"\"\"\\n    Helper function to commit the manifest directly to the configured branch.\\n    \"\"\"\\n    github_api_base_url = \"https://api.github.com\"\\n    repo_file_url = (\\n        f\"{github_api_base_url}/repos/{settings.GITHUB_REPO_OWNER}/\"\\n        f\"{settings.GITHUB_REPO_NAME}/contents/{settings.GITHUB_FILE_PATH}\"\\n    )\\n    headers = {\\n        \"Authorization\": f\"Bearer {settings.GITHUB_TOKEN}\",\\n        \"Accept\": \"application/vnd.github.v3+json\",\\n        \"X-GitHub-Api-Version\": \"2022-11-28\",\\n    }\\n    manifest_json_string = json.dumps(payload.manifest, indent=2)\\n    content_base64 = base64.b64encode(manifest_json_string.encode(\\'utf-8\\')).decode(\\'utf-8\\')\\n    commit_message = f\"feat: Update {settings.GITHUB_FILE_PATH} via webhook - commit {payload.commit_hash}\"\\n    data_to_commit = {\\n        \"message\": commit_message,\\n        \"content\": content_base64,\\n        \"branch\": settings.GITHUB_BRANCH,\\n    }\\n    async with httpx.AsyncClient() as client:\\n        current_file_sha = None\\n        try:\\n            params_get = {\"ref\": settings.GITHUB_BRANCH}\\n            response_get = await client.get(repo_file_url, headers=headers, params=params_get)\\n            if response_get.status_code == 200:\\n                current_file_sha = response_get.json().get(\"sha\")\\n            elif response_get.status_code != 404: # If not 404 (not found), it\\'s an unexpected error\\n                response_get.raise_for_status()\\n        except httpx.HTTPStatusError as e:\\n            error_detail = f\"GitHub API error (GET file SHA): {e.response.status_code} - {e.response.text}\"\\n            print(f\"[ERROR] {error_detail}\")\\n            raise HTTPException(status_code=e.response.status_code, detail=error_detail)\\n        except httpx.RequestError as e:\\n            error_detail = f\"Network error connecting to GitHub (GET file SHA): {str(e)}\"\\n            print(f\"[ERROR] {error_detail}\")\\n            raise HTTPException(status_code=503, detail=error_detail)\\n        if current_file_sha:\\n            data_to_commit[\"sha\"] = current_file_sha\\n        try:\\n            response_put = await client.put(repo_file_url, headers=headers, json=data_to_commit)\\n            response_put.raise_for_status()\\n            return response_put.json()\\n        except httpx.HTTPStatusError as e:\\n            error_detail = f\"GitHub API error (PUT content): {e.response.status_code} - {e.response.text}\"\\n            if e.response.status_code == 409: # Conflict\\n                error_detail = (\\n                    f\"GitHub API conflict (PUT content): {e.response.text}. \"\\n                    \"This might be due to an outdated SHA or branch protection rules.\"\\n                )\\n            elif e.response.status_code == 422: # Unprocessable Entity\\n                error_detail = (\\n                    f\"GitHub API Unprocessable Entity (PUT content): {e.response.text}. \"\\n                    f\"Ensure the branch \\'{settings.GITHUB_BRANCH}\\' exists and the payload is correctly formatted.\"\\n                )\\n            print(f\"[ERROR] {error_detail}\")\\n            raise HTTPException(status_code=e.response.status_code, detail=error_detail)\\n        except httpx.RequestError as e:\\n            error_detail = f\"Network error connecting to GitHub (PUT content): {str(e)}\"\\n            print(f\"[ERROR] {error_detail}\")\\n            raise HTTPException(status_code=503, detail=error_detail)\\n\\n# --- FastAPI Application ---\\napp = FastAPI(\\n    title=\"Minimal Webhook to GitHub Commit Service\",\\n    description=\"Receives a webhook and commits its \\'manifest\\' part directly to a GitHub repository.\",\\n    version=\"0.1.0\",\\n)\\n\\n@app.post(\"/webhook/commit\", status_code=201, tags=[\"GitHub Webhooks\"])\\nasync def handle_webhook_direct_commit(payload: WebhookPayload = Body(...)):\\n    \"\"\"\\n    Webhook endpoint to receive events and commit DIRECTLY to the configured branch.\\n    \"\"\"\\n    try:\\n        github_response = await commit_manifest_to_github(payload)\\n        return {\\n            \"message\": \"Webhook received and manifest committed directly to GitHub successfully.\",\\n            \"github_commit_details\": github_response.get(\"commit\", {}),\\n            \"github_content_details\": github_response.get(\"content\", {})\\n        }\\n    except HTTPException:\\n        raise # Re-raise if it\\'s an HTTPException from the helper\\n    except Exception as e:\\n        error_message = f\"An unexpected error occurred: {str(e)}\"\\n        print(f\"[ERROR] {error_message}\")\\n        raise HTTPException(status_code=500, detail=\"An internal server error occurred.\")\\n\\n@app.get(\"/health\", status_code=200, tags=[\"Health\"])\\nasync def health_check():\\n    \"\"\"\\n    A simple health check endpoint.\\n    \"\"\"\\n    return {\"status\": \"ok\", \"message\": \"Service is running.\"}\\n\\n# To run this server (save as main.py):\\n# 1. Install dependencies: pip install fastapi uvicorn pydantic pydantic-settings httpx python-dotenv\\n# 2. Create a .env file with your GitHub token and repo details.\\n# 3. Run with Uvicorn: uvicorn main:app --reload\\n# 4. Deploy to a public platform like Render.com.\\nKey aspects of this server:\\nConfiguration (.env): It expects a .env file with your GITHUB_TOKEN, GITHUB_REPO_OWNER, and GITHUB_REPO_NAME. You can also customize GITHUB_FILE_PATH (default: LangSmith_prompt_manifest.json) and GITHUB_BRANCH (default: main).\\nGitHub Interaction: The commit_manifest_to_github function handles the logic of fetching the current fileâ€™s SHA (to update it) and then committing the new manifest content.\\nWebhook Endpoint (/webhook/commit): This is the URL path your LangSmith webhook will target.\\nError Handling: Basic error handling for GitHub API interactions is included.\\nDeploy this server to your chosen platform (e.g., Render) and note down its public URL (e.g., https://prompt-commit-webhook.onrender.com).\\n\\u200bConfiguring the Webhook in LangSmith\\nOnce your FastAPI server is deployed and you have its public URL, you can configure the webhook in LangSmith:\\n\\n\\nNavigate to your LangSmith workspace.\\n\\n\\nGo to the Prompts section. Here youâ€™ll see a list of your prompts.\\n\\n\\n\\nOn the top right of the Prompts page, click the + Webhook button.\\n\\n\\nYouâ€™ll be presented with a form to configure your webhook:\\n\\n\\nWebhook URL: Enter the full public URL of your deployed FastAPI serverâ€™s endpoint. For our example server, this would be https://prompt-commit-webhook.onrender.com/webhook/commit.\\nHeaders (Optional):\\n\\nYou can add custom headers that LangSmith will send with each webhook request.\\n\\n\\n\\n\\n\\nTest the Webhook: LangSmith provides a â€œSend Test Notificationâ€ button. Use this to send a sample payload to your server. Check your server logs (e.g., on Render) to ensure it receives the request and processes it successfully (or to debug any issues).\\n\\n\\nSave the webhook configuration.\\n\\n\\n\\u200bThe Workflow in Action\\n\\nNow, with everything set up, hereâ€™s what happens:\\n\\n\\nPrompt Modification: A user (developer or non-technical team member) modifies a prompt in the LangSmith UI and saves it, creating a new â€œprompt commit.â€\\n\\n\\nWebhook Trigger: LangSmith detects this new prompt commit and triggers the configured webhook.\\n\\n\\nHTTP Request: LangSmith sends an HTTP POST request to the public URL of your FastAPI server (e.g., https://prompt-commit-webhook.onrender.com/webhook/commit). The body of this request contains the JSON prompt manifest for the entire workspace.\\n\\n\\nServer Receives Payload: Your FastAPI serverâ€™s endpoint receives the request.\\n\\n\\nGitHub Commit: The server parses the JSON manifest from the request body. It then uses the configured GitHub Personal Access Token, repository owner, repository name, file path, and branch to:\\n\\nCheck if the manifest file already exists in the repository on the specified branch to get its SHA (this is necessary for updating an existing file).\\nCreate a new commit with the latest prompt manifest, either creating the file or updating it if it already exists. The commit message will indicate that itâ€™s an update from LangSmith.\\n\\n\\n\\nConfirmation: You should see the new commit appear in your GitHub repository.\\n\\n\\n\\nYouâ€™ve now successfully synced your LangSmith prompts with GitHub!\\n\\u200bBeyond a Simple Commit\\nOur example FastAPI server performs a direct commit of the entire prompt manifest. However, this is just the starting point. You can extend the serverâ€™s functionality to perform more sophisticated actions:\\n\\nGranular Commits: Parse the manifest and commit changes to individual prompt files if you prefer a more granular structure in your repository.\\nTrigger CI/CD: Instead of (or in addition to) committing, have the server trigger a CI/CD pipeline (e.g., Jenkins, GitHub Actions, GitLab CI) to deploy a staging environment, run tests, or build new application versions.\\nUpdate Databases/Caches: If your application loads prompts from a database or cache, update these stores directly.\\nNotifications: Send notifications to Slack, email, or other communication channels about prompt changes.\\nSelective Processing: Based on metadata within the LangSmith payload (if available, e.g., which specific prompt changed or by whom), you could apply different logic.\\nWas this page helpful?YesNoSuggest editsOptimize a classifierTest multi-turn conversationsâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/reference', 'loc': 'https://docs.smith.langchain.com/reference', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Get started with LangSmith - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewQuickstartsTrace an applicationEvaluate an applicationTest promptsAPI & SDKsAPI referencePython SDKJS/TS SDKPricingPlansPricing FAQOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationGet started with LangSmithGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumGet started with LangSmithCopy pageCopy pageLangSmith is a platform for building production-grade LLM applications. Monitor and evaluate your application, so you can ship quickly and with confidence.\\nLangSmith is framework agnostic â€”\\xa0you can use it with or without LangChainâ€™s open source frameworks langchain and langgraph.\\n\\nStart tracingGain visibility into each step your application takes when handling a request to debug faster.Learn moreEvaluate your applicationMeasure quality of your applications over time to build more reliable AI applications.Learn moreTest your promptsIterate on prompts, with automatic version control and collaboration features.Learn moreSet up your workspaceSet up your workspace, configure admin settings, and invite your team to collaborate.Learn moreWas this page helpful?YesNoSuggest editsTrace an applicationâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods', 'loc': 'https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Authentication methods - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationSetupOverviewCreate an account and API keySet up a workspaceManage organizations using the APIManage billingSet up resource tagsUser managementAdditional resourcesFAQsCloud architecture and scalabilityRegions FAQAuthentication methodsData purging for complianceRelease versionsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationAdditional resourcesAuthentication methodsGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageCloudEmail/PasswordSocial ProvidersSAML SSOSelf-HostedSSO with OAuth 2.0 and OIDCEmail/Password a.k.a. basic authNoneAdditional resourcesAuthentication methodsCopy pageCopy pageLangSmith supports multiple authentication methods for easy sign-up and login.\\n\\u200bCloud\\n\\u200bEmail/Password\\nUsers can use an email address and password to sign up and login to LangSmith.\\n\\u200bSocial Providers\\nUsers can alternatively use their credentials from GitHub or Google.\\n\\u200bSAML SSO\\nEnterprise customers can configure SAML SSO and SCIM\\n\\u200bSelf-Hosted\\nSelf-hosted customers have more control over how their users can login to LangSmith. For more in-depth coverage of configuration options, see the self-hosting docs and Helm chart.\\n\\u200bSSO with OAuth 2.0 and OIDC\\nProduction installations should configure SSO in order to use an external identity provider. This enables users to login through an identity platform like Auth0/Okta. LangSmith supports almost any OIDC-compliant provider. Learn more about configuring SSO in the SSO configuration guide\\n\\u200bEmail/Password a.k.a. basic auth\\nThis auth method requires very little configuration as it does not require an external identity provider. It is most appropriate to use for self-hosted trials. Learn more in the basic auth configuration guide\\n\\u200bNone\\nThis authentication mode will be removed after the launch of Basic Auth.\\nIf zero authentication methods are enabled, a self-hosted installation does not require any login/sign-up. This configuration should only be used for verifying installation at the infrastructure level, as the feature set supported in this mode is restricted with only a single organization and workspace.Was this page helpful?YesNoSuggest editsRegions FAQData purging for complianceâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability', 'loc': 'https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Cloud architecture and scalability - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationSetupOverviewCreate an account and API keySet up a workspaceManage organizations using the APIManage billingSet up resource tagsUser managementAdditional resourcesFAQsCloud architecture and scalabilityRegions FAQAuthentication methodsData purging for complianceRelease versionsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationAdditional resourcesCloud architecture and scalabilityGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageArchitectureRegional storageRegion-independent storageGCP servicesAllowlisting IP addressesEgress from LangChain SaaSIngress into LangChain SaaSAdditional resourcesCloud architecture and scalabilityCopy pageCopy pageThis section is only relevant for the cloud-managed LangSmith services available at https://smith.langchain.com and https://eu.smith.langchain.com.For information on the self-hosted LangSmith solution, please refer to the self-hosted documentation.\\nLangSmith is deployed on Google Cloud Platform (GCP) and is designed to be highly scalable. Many customers run production workloads on LangSmith for both LLM application observability and evaluation.\\n\\u200bArchitecture\\nThe US-based LangSmith service is deployed in the us-central1 (Iowa) region of GCP.\\nNOTE: The EU-based LangSmith service is now available (as of mid-July 2024) and is deployed in the europe-west4 (Netherlands) region of GCP. If you are interested in an enterprise plan in this region, contact our sales team.\\n\\u200bRegional storage\\nThe resources and services in this table are stored in the location corresponding to the URL where sign-up occurred (either the US or EU). Cloud-managed LangSmith uses Supabase for authentication/authorization and ClickHouse Cloud for data warehouse.\\nUSEUURLhttps://smith.langchain.comhttps://eu.smith.langchain.comAPI URLhttps://api.smith.langchain.comhttps://eu.api.smith.langchain.comGCPus-central1 (Iowa)europe-west4 (Netherlands)SupabaseAWS us-east-1 (N. Virginia)AWS eu-central-1 (Germany)ClickHouse Cloudus-central1 (Iowa)europe-west4 (Netherlands)LangGraph Cloudus-central1 (Iowa)europe-west4 (Netherlands)\\nSee the Regions FAQ for more information.\\n\\u200bRegion-independent storage\\nData listed here is stored exclusively in the US:\\n\\nPayment and billing information with Stripe and Metronome\\n\\n\\u200bGCP services\\nLangSmith is composed of the following services, all deployed on Google Kubernetes Engine (GKE):\\n\\nLangSmith Frontend: serves the LangSmith UI.\\nLangSmith Backend: serves the LangSmith API.\\nLangSmith Platform Backend: handles authentication and other high-volume tasks. (Internal service)\\nLangSmith Playground: handles forwarding requests to various LLM providers for the Playground feature.\\nLangSmith Queue: handles processing of asynchronous tasks. (Internal service)\\n\\nLangSmith uses the following GCP storage services:\\n\\nGoogle Cloud Storage (GCS) for runs inputs and outputs.\\nGoogle Cloud SQL PostgreSQL for transactional workloads.\\nGoogle Cloud Memorystore for Redis for queuing and caching.\\nClickhouse Cloud on GCP for trace ingestion and analytics. Our services connect to Clickhouse Cloud, which is hosted in the same GCP region, via a private endpoint.\\n\\nSome additional GCP services we use include:\\n\\nGoogle Cloud Load Balancer for routing traffic to the LangSmith services.\\nGoogle Cloud CDN for caching static assets.\\nGoogle Cloud Armor for security and rate limits. For more information on rate limits we enforce, please refer to this guide.\\n\\n\\n\\u200bAllowlisting IP addresses\\n\\u200bEgress from LangChain SaaS\\nAll traffic leaving LangSmith services will be routed through a NAT gateway. All traffic will appear to originate from the following IP addresses:\\nUSEU34.59.65.9734.13.192.6734.67.51.22134.147.105.6434.46.212.3734.90.22.16634.132.150.8834.147.36.21335.188.222.20134.32.137.11334.58.194.12734.91.238.18434.59.97.17335.204.101.241104.198.162.5535.204.48.32\\nIt may be helpful to allowlist these IP addresses if connecting to your own AzureOpenAI service or other endpoints that may be required by the Playground or Online Evaluation.\\n\\u200bIngress into LangChain SaaS\\nThe langchain endpoints map to the following static IP addresses:\\nUSEU34.8.121.3934.95.92.21434.107.251.23434.13.73.122\\nYou may need to allowlist these to enable traffic from your private network to LangSmith SaaS endpoints (api.smith.langchain.com, smith.langchain.com, beacon.langchain.com, eu.api.smith.langchain.com, eu.smith.langchain.com, eu.beacon.langchain.com).Was this page helpful?YesNoSuggest editsFAQsRegions FAQâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/reference/data_formats/dataset_json_types', 'loc': 'https://docs.smith.langchain.com/reference/data_formats/dataset_json_types', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Dataset prebuilt JSON schema types - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationCommon data typesDataset prebuilt JSON schema typesGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumCommon data typesDataset prebuilt JSON schema typesCopy pageCopy pageLangSmith recommends that you set a schema on the inputs and outputs of your dataset schemas to ensure data consistency and that your examples are in the right format for downstream processing, like running evals.\\nIn order to better support LLM workflows, LangSmith has support for a few different predefined prebuilt types. These schemas are hosted publicly by the LangSmith API, and can be defined in your dataset schemas using JSON Schema references. The table of available schemas can be seen below\\nTypeJSON Schema Reference LinkUsageMessagehttps://api.smith.langchain.com/public/schemas/v1/message.jsonRepresents messages sent to a chat model, following the OpenAI standard format.Toolhttps://api.smith.langchain.com/public/schemas/v1/tooldef.jsonTool definitions available to chat models for function calling, defined in OpenAIâ€™s JSON Schema inspired function format.\\nLangSmith lets you define a series of transformations that collect the above prebuilt types from your traces and add them to your dataset. For more info on available transformations, see our referenceWas this page helpful?YesNoSuggest editsExample data formatDataset transformationsâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/reference/data_formats/example_data_format', 'loc': 'https://docs.smith.langchain.com/reference/data_formats/example_data_format', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Example data format - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationCommon data typesExample data formatGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumCommon data typesExample data formatCopy pageCopy pageBefore diving into this content, it might be helpful to read the following:\\nConceptual guide on evaluation\\n\\nLangSmith stores examples in datasets as follows:\\nField NameTypeDescriptionidUUIDUnique identifier for the example.namestringThe name of the example.created_atdatetimeThe time this example was createdmodified_atdatetimeThe last time this example was modifiedinputsobjectA map of inputs for the example.outputsobjectA map or set of outputs generated by the run.dataset_idUUIDThe dataset the example belongs tosource_run_idUUIDIf this example was created from a LangSmith Run, the ID of said runmetadataobjectA map of additional, user or SDK defined information that can be stored on an example.\\nTo learn more about how examples are used in evaluation, read our how-to guide on evaluating LLM applications.Was this page helpful?YesNoSuggest editsAudit evaluator scoresDataset prebuilt JSON schema typesâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/reference/data_formats/feedback_data_format', 'loc': 'https://docs.smith.langchain.com/reference/data_formats/feedback_data_format', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Feedback data format - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsManual instrumentationConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationData type referenceFeedback data formatGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumData type referenceFeedback data formatCopy pageCopy pageBefore diving into this content, it might be helpful to read the following:\\nConceptual guide on tracing and feedback\\n\\nFeedback is LangSmithâ€™s way of storing the criteria and scores from evaluation on a particular trace or intermediate run (span). Feedback can be produced from a variety of ways, such as:\\n\\nSent up along with a trace from the LLM application\\nGenerated by a user in the app inline or in an annotation queue\\nGenerated by an automatic evaluator during offline evaluation\\nGenerated by an online evaluator\\n\\nFeedback is stored in a simple format with the following fields:\\nField NameTypeDescriptionidUUIDUnique identifier for the record itselfcreated_atdatetimeTimestamp when the record was createdmodified_atdatetimeTimestamp when the record was last modifiedsession_idUUIDUnique identifier for the experiment or tracing project the run was a part ofrun_idUUIDUnique identifier for a specific run within a sessionkeystringA key describing the criteria of the feedback, eg â€œcorrectnessâ€scorenumberNumerical score associated with the feedback keyvaluestringReserved for storing a value associated with the score. Useful for categorical feedback.commentstringAny comment or annotation associated with the record. This can be a justification for the score given.correctionobjectReserved for storing correction details, if anyfeedback_sourceobjectObject containing information about the feedback sourcefeedback_source.typestringThe type of source where the feedback originated, eg â€œapiâ€, â€œappâ€, â€œevaluatorâ€feedback_source.metadataobjectReserved for additional metadata, currentlyfeedback_source.user_idUUIDUnique identifier for the user providing feedback\\nHere is an example JSON representation of a feedback record in the above format:\\nCopy{\\n  \"created_at\": \"2024-05-05T23:23:11.077838\",\\n  \"modified_at\": \"2024-05-05T23:23:11.232962\",\\n  \"session_id\": \"c919298b-0af2-4517-97a2-0f98ed4a48f8\",\\n  \"run_id\": \"e26174e5-2190-4566-b970-7c3d9a621baa\",\\n  \"key\": \"correctness\",\\n  \"score\": 1.0,\\n  \"value\": null,\\n  \"comment\": \"I gave this score because the answer was correct.\",\\n  \"correction\": null,\\n  \"id\": \"62104630-c7f5-41dc-8ee2-0acee5c14224\",\\n  \"feedback_source\": {\\n    \"type\": \"app\",\\n    \"metadata\": null,\\n    \"user_id\": \"ad52b092-1346-42f4-a934-6e5521562fab\"\\n  }\\n}\\nWas this page helpful?YesNoSuggest editsRun (span) data formatTrace query syntaxâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/reference/data_formats/run_data_format', 'loc': 'https://docs.smith.langchain.com/reference/data_formats/run_data_format', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Run (span) data format - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsManual instrumentationConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationData type referenceRun (span) data formatGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageWhat is dotted_order?Data type referenceRun (span) data formatCopy pageCopy pageBefore diving into this content, it might be helpful to read the following:\\nConceptual guide on tracing and runs\\n\\nLangSmith stores and processes trace data in a simple format that is easy to export and import.\\nMany of these fields are optional or not important to know about but are included for completeness. The bolded fields are the most important ones to know about.\\nField NameTypeDescriptionidUUIDUnique identifier for the span.namestringThe name associated with the run.inputsobjectA map or set of inputs provided to the run.run_typestringType of run, e.g., â€œllmâ€, â€œchainâ€, â€œtoolâ€.start_timedatetimeStart time of the run.end_timedatetimeEnd time of the run.extraobjectAny extra information run.errorstringError message if the run encountered an error.outputsobjectA map or set of outputs generated by the run.eventsarray of objectsA list of event objects associated with the run. This is relevant for runs executed with streaming.tagsarray of stringsTags or labels associated with the run.trace_idUUIDUnique identifier for the trace the run is a part of. This is also the id field of the root run of the tracedotted_orderstringOrdering string, hierarchical. Format: run_start_timeZrun_uuid.child_run_start_timeZchild_run_uuidâ€¦statusstringCurrent status of the run execution, e.g., â€œerrorâ€, â€œpendingâ€, â€œsuccessâ€child_run_idsarray of UUIDsList of IDs for all child runs.direct_child_run_idsarray of UUIDsList of IDs for direct children of this run.parent_run_idsarray of UUIDsList of IDs for all parent runs.feedback_statsobjectAggregations of feedback statistics for this runreference_example_idUUIDID of a reference example associated with the run. This is usually only present for evaluation runs.total_tokensintegerTotal number of tokens processed by the run.prompt_tokensintegerNumber of tokens in the prompt of the run.completion_tokensintegerNumber of tokens in the completion of the run.total_coststringTotal cost associated with processing the run.prompt_coststringCost associated with the prompt part of the run.completion_coststringCost associated with the completion of the run.first_token_timedatetimeTime when the first token of a model output was generated. Only applies for runs with run_type=\"llm\" and streaming enabled.session_idstringSession identifier for the run, also known as the tracing project ID.in_datasetbooleanIndicates whether the run is included in a dataset.parent_run_idUUIDUnique identifier of the parent run.execution_order (deprecated)integerThe order in which this run was executed within the trace.serializedobjectSerialized state of the object executing the run if applicable.manifest_id (deprecated)UUIDIdentifier for a manifest associated with the span.manifest_s3_idUUIDS3 identifier for the manifest.inputs_s3_urlsobjectS3 URLs for the inputs.outputs_s3_urlsobjectS3 URLs for the outputs.price_model_idUUIDIdentifier for the pricing model applied to the run.app_pathstringApplication (UI) path for this run.last_queued_atdatetimeLast time the span was queued.share_tokenstringToken for sharing access to the runâ€™s data.\\nHere is an example of a JSON representation of a run in the above format:\\nCopy{\\n  \"id\": \"497f6eca-6276-4993-bfeb-53cbbbba6f08\",\\n  \"name\": \"string\",\\n  \"inputs\": {},\\n  \"run_type\": \"llm\",\\n  \"start_time\": \"2024-04-29T00:49:12.090000\",\\n  \"end_time\": \"2024-04-29T00:49:12.459000\",\\n  \"extra\": {},\\n  \"error\": \"string\",\\n  \"execution_order\": 1,\\n  \"serialized\": {},\\n  \"outputs\": {},\\n  \"parent_run_id\": \"f8faf8c1-9778-49a4-9004-628cdb0047e5\",\\n  \"manifest_id\": \"82825e8e-31fc-47d5-83ce-cd926068341e\",\\n  \"manifest_s3_id\": \"0454f93b-7eb6-4b9d-a203-f1261e686840\",\\n  \"events\": [{}],\\n  \"tags\": [\"foo\"],\\n  \"inputs_s3_urls\": {},\\n  \"outputs_s3_urls\": {},\\n  \"trace_id\": \"df570c03-5a03-4cea-8df0-c162d05127ac\",\\n  \"dotted_order\": \"20240429T004912090000Z497f6eca-6276-4993-bfeb-53cbbbba6f08\",\\n  \"status\": \"string\",\\n  \"child_run_ids\": [\"497f6eca-6276-4993-bfeb-53cbbbba6f08\"],\\n  \"direct_child_run_ids\": [\"497f6eca-6276-4993-bfeb-53cbbbba6f08\"],\\n  \"parent_run_ids\": [\"f8faf8c1-9778-49a4-9004-628cdb0047e5\"],\\n  \"feedback_stats\": {\\n    \"correctness\": {\\n      \"n\": 1,\\n      \"avg\": 1.0\\n    }\\n  },\\n  \"reference_example_id\": \"9fb06aaa-105f-4c87-845f-47d62ffd7ee6\",\\n  \"total_tokens\": 0,\\n  \"prompt_tokens\": 0,\\n  \"completion_tokens\": 0,\\n  \"total_cost\": \"string\",\\n  \"prompt_cost\": \"string\",\\n  \"completion_cost\": \"string\",\\n  \"price_model_id\": \"0b5d9575-bec3-4256-b43a-05893b8b8440\",\\n  \"first_token_time\": null,\\n  \"session_id\": \"1ffd059c-17ea-40a8-8aef-70fd0307db82\",\\n  \"app_path\": \"string\",\\n  \"last_queued_at\": null,\\n  \"in_dataset\": true,\\n  \"share_token\": \"d0430ac3-04a1-4e32-a7ea-57776ad22c1c\"\\n}\\n\\n\\u200bWhat is dotted_order?\\nA runâ€™s dotted order is a sortable key that fully specifies its location within the tracing hierarchy.\\nTake the following example:\\nCopyimport langsmith as ls\\n\\n@ls.traceable\\ndef grandchild():\\n    p(\"grandchild\")\\n\\n@ls.traceable\\ndef child():\\n    grandchild()\\n\\n@ls.traceable\\ndef parent():\\n    child()\\n\\nIf you print out the IDs at each stage, you may get the following:\\nCopyparent\\trun_id=0e01bf50-474d-4536-810f-67d3ee7ea3e7\\ttrace_id=0e01bf50-474d-4536-810f-67d3ee7ea3e7  parent_run_id=null\\tdotted_order=20240919T171648521691Z0e01bf50-474d-4536-810f-67d3ee7ea3e7\\nchild\\trun_id=a8024e23-5b82-47fd-970e-f6a5ba3f5097\\ttrace_id=0e01bf50-474d-4536-810f-67d3ee7ea3e7  parent_run_id=0e01bf50-474d-4536-810f-67d3ee7ea3e7\\tdotted_order=20240919T171648521691Z0e01bf50-474d-4536-810f-67d3ee7ea3e7.20240919T171648523407Za8024e23-5b82-47fd-970e-f6a5ba3f5097\\ngrandchild\\trun_id=0ec6b845-18b9-4aa1-8f1b-6ba3f9fdefd6\\ttrace_id=0e01bf50-474d-4536-810f-67d3ee7ea3e7  parent_run_id=a8024e23-5b82-47fd-970e-f6a5ba3f5097\\tdotted_order=20240919T171648521691Z0e01bf50-474d-4536-810f-67d3ee7ea3e7.20240919T171648523407Za8024e23-5b82-47fd-970e-f6a5ba3f5097.20240919T171648523563Z0ec6b845-18b9-4aa1-8f1b-6ba3f9fdefd6\\n\\nNote a few invariants:\\n\\nThe â€œidâ€ is equal to the last 36 characters of the dotted order (the suffix after the final â€œZâ€). See 0ec6b845-18b9-4aa1-8f1b-6ba3f9fdefd6 for example in the grandchild.\\nThe â€œtrace_idâ€ is equal to the first UUID in the dotted order (i.e., dotted_order.split(\\'.\\')[0].split(\\'Z\\')[1])\\nIf â€œparent_run_idâ€ exists, it is the penultimate UUID in the dotted order. See a8024e23-5b82-47fd-970e-f6a5ba3f5097 in the grandchild, for an example.\\nIf you split the dotted_order on the dots, each segment is formatted as (<run_start_time>Z<run_id>)\\nWas this page helpful?YesNoSuggest editsInsights (Beta)Feedback data formatâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/reference/data_formats/trace_query_syntax', 'loc': 'https://docs.smith.langchain.com/reference/data_formats/trace_query_syntax', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Trace query syntax - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsManual instrumentationConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationData type referenceTrace query syntaxGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageFilter argumentsFilter query languageData type referenceTrace query syntaxCopy pageCopy pageUsing the method in the SDK or endpoint in the API, you can filter runs to analyze and export.\\n\\u200bFilter arguments\\nKeysDescriptionproject_id / project_nameThe project(s) to fetch runs from - can be a single project or a list of projects.trace_idFetch runs that are part of a specific trace.run_typeThe type of run to get, such as llm, chain, tool, retriever, etc.dataset_name / dataset_idFetch runs that are associated with an example row in the specified dataset. This is useful for comparing prompts or models over a given dataset.reference_example_idFetch runs that are associated with a specific example row. This is useful for comparing prompts or models on a given input.parent_run_idFetch runs that are children of a given run. This is useful for fetching runs grouped together using the context manager or for fetching an agent trajectory.errorFetch runs that errored or did not error.run_idsFetch runs with a given list of run ids. Note: This will ignore all other filtering arguments.filterFetch runs that match a given structured filter statement. See the guide below for more information.trace_filterFilter to apply to the ROOT run in the trace tree. This is meant to be used in conjunction with the regular filter parameter to let you filter runs by attributes of the root run within a trace.tree_filterFilter to apply to OTHER runs in the trace tree, including sibling and child runs. This is meant to be used in conjunction with the regular filter parameter to let you filter runs by attributes of any run within a trace.is_rootOnly return root runs.selectSelect the fields to return in the response. By default, all fields are returned.query (experimental)Natural language query, which translates your query into a filter statement.\\n\\u200bFilter query language\\nLangSmith supports powerful filtering capabilities with a filter query language to permit complex filtering operations when fetching runs.\\nThe filtering grammar is based on common comparators on fields in the run object. Supported comparators include:\\n\\ngte (greater than or equal to)\\ngt (greater than)\\nlte (less than or equal to)\\nlt (less than)\\neq (equal to)\\nneq (not equal to)\\nhas (check if run contains a tag or metadata json blob)\\nsearch (search for a substring in a string field)\\n\\nAdditionally, you can combine multiple comparisons through the and operator.\\nThese can be applied on fields of the run object, such as its id, name, run_type, start_time / end_time, latency, total_tokens, error, execution_order, tags, and any associated feedback through feedback_key and feedback_score.Was this page helpful?YesNoSuggest editsFeedback data formatâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/reference/evaluation/dataset_transformations', 'loc': 'https://docs.smith.langchain.com/reference/evaluation/dataset_transformations', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Dataset transformations - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationCommon data typesDataset transformationsGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageTransformation typesChat Model prebuilt schemaCompatibilityEnablementSpecsInput SchemaOutput SchemaTransformationsCommon data typesDataset transformationsCopy pageCopy pageLangSmith allows you to attach transformations to fields in your datasetâ€™s schema that apply to your data before it is added to your dataset, whether that be from UI, API, or run rules.\\nCoupled with LangSmithâ€™s prebuilt JSON schema types, these allow you to do easy preprocessing of your data before saving it into your datasets.\\n\\u200bTransformation types\\nTransformation TypeTarget TypesFunctionalityremove_system_messagesArray[Message]Filters a list of messages to remove any system messages.convert_to_openai_messageMessage Array[Message]Converts any incoming data from LangChainâ€™s internal serialization format to OpenAIâ€™s standard message format using langchainâ€™s convert_to_openai_messages. If the target field is marked as required, and no matching message is found upon entry, it will attempt to extract a message (or list of messages) from several well-known LangSmith tracing formats (e.g., any traced LangChain BaseChatModel run or traced run from the LangSmith OpenAI wrapper), and remove the original key containing the message.convert_to_openai_toolArray[Tool] Only available on top level fields in the inputs dictionary.Converts any incoming data into OpenAI standard tool formats here using langchainâ€™s convert_to_openai_tool Will extract tool definitions from a runâ€™s invocation parameters if present / no tools are found at the specified key. This is useful because LangChain chat models trace tool definitions to the extra.invocation_params field of the run rather than inputs.remove_extra_fieldsObjectRemoves any field not defined in the schema for this target object.\\n\\u200bChat Model prebuilt schema\\nThe main use case for transformations is to simplify collecting production traces into datasets in a format that can be standardized across model providers for usage in evaluations / few shot prompting / etc downstream.\\nTo simplify setup of transformations for our end users, LangSmith offers a pre-defined schema that will do the following:\\n\\nExtract messages from your collected runs and transform them into the openai standard format, which makes them compatible all LangChain ChatModels and most model providersâ€™ SDK for downstream evaluation and experimentation\\nExtract any tools used by your LLM and add them to your exampleâ€™s input to be used for reproducability in downstream evaluation\\n\\nUsers who want to iterate on their system prompts often also add the Remove System Messages transformation on their input messages when using our Chat Model schema, which will prevent you from saving the system prompt to your dataset.\\n\\u200bCompatibility\\nThe LLM run collection schema is built to collect data from LangChain BaseChatModel runs or traced runs from the LangSmith OpenAI wrapper.\\nPlease reach out to support@langchain.dev if you have an LLM run you are tracing that is not compatible and we can extend support.\\nIf you want to apply transformations to other sorts of runs (for example, representing LangGraph state with message history), please define your schema directly and manually add the relevant transformations.\\n\\u200bEnablement\\nWhen adding a run from a tracing project or annotation queue to a dataset, if it has the LLM run type, we will apply the Chat Model schema by default.\\nFor enablement on new datasets, see our dataset management how-to guide.\\n\\u200bSpecs\\nFor the full API specs of the prebuilt schema, see the below sections:\\n\\u200bInput Schema\\nCopy{\\n  \"type\": \"object\",\\n  \"properties\": {\\n    \"messages\": {\\n      \"type\": \"array\",\\n      \"items\": {\\n        \"$ref\": \"https://api.smith.langchain.com/public/schemas/v1/message.json\"\\n      }\\n    },\\n    \"tools\": {\\n      \"type\": \"array\",\\n      \"items\": {\\n        \"$ref\": \"https://api.smith.langchain.com/public/schemas/v1/tooldef.json\"\\n      }\\n    }\\n  },\\n  \"required\": [\"messages\"]\\n}\\n\\n\\u200bOutput Schema\\nCopy{\\n  \"type\": \"object\",\\n  \"properties\": {\\n    \"message\": {\\n      \"$ref\": \"https://api.smith.langchain.com/public/schemas/v1/message.json\"\\n    }\\n  },\\n  \"required\": [\"message\"]\\n}\\n\\n\\u200bTransformations\\nAnd the transformations look as follows:\\nCopy[\\n  {\\n    \"path\": [\"inputs\"],\\n    \"transformation_type\": \"remove_extra_fields\"\\n  },\\n  {\\n    \"path\": [\"inputs\", \"messages\"],\\n    \"transformation_type\": \"convert_to_openai_message\"\\n  },\\n  {\\n    \"path\": [\"inputs\", \"tools\"],\\n    \"transformation_type\": \"convert_to_openai_tool\"\\n  },\\n  {\\n    \"path\": [\"outputs\"],\\n    \"transformation_type\": \"remove_extra_fields\"\\n  },\\n  {\\n    \"path\": [\"outputs\", \"message\"],\\n    \"transformation_type\": \"convert_to_openai_message\"\\n  }\\n]\\nWas this page helpful?YesNoSuggest editsDataset prebuilt JSON schema typesâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/reference/regions_faq', 'loc': 'https://docs.smith.langchain.com/reference/regions_faq', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Regions FAQ - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationSetupOverviewCreate an account and API keySet up a workspaceManage organizations using the APIManage billingSet up resource tagsUser managementAdditional resourcesFAQsCloud architecture and scalabilityRegions FAQAuthentication methodsData purging for complianceRelease versionsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationAdditional resourcesRegions FAQGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageLegal and complianceWhat privacy and data protection frameworks does LangSmith, including its EU instance, comply with?My company isnâ€™t based in the EU, can I still have my data hosted there?Do you have a legal entity in the EU that we can contract with?Do different legal terms apply if I choose the EU region?FeaturesHow do I use the EU instance?Are there any functional differences between US and EU cloud-managed LangSmith?Can an organization have workspaces in different regions?Can I connect an EU organization to a US organization and share billing?What data will be stored in my selected region?How can I see my organizationâ€™s region?Can I switch my organization from the US to EU or vice versa?Plans and pricingIs the EU region available on all LangSmith plans?Is pricing different for the EU region compared to the US region?What currency is used for payment if I use the EU region?Additional resourcesRegions FAQCopy pageCopy pageSee the cloud architecture reference for additional details.\\n\\u200bLegal and compliance\\n\\u200bWhat privacy and data protection frameworks does LangSmith, including its EU instance, comply with?\\nLangSmith complies with the General Data Protection Regulation (GDPR) and other laws and regulations applicable to the LangSmith service. We are also SOC 2 Type 2 certified and are HIPAA compliant. You can request more information about our security policies and posture at trust.langchain.com. If you would like to sign a Data Processing Addendum (DPA) with us, please reach out to support@langchain.dev. Please note we only enter into Business Associate Agreements (BAAs) with customers on our Enterprise plan.\\n\\u200bMy company isnâ€™t based in the EU, can I still have my data hosted there?\\nYes, you can host your LangSmith data in the EU instance independent of your location.\\n\\u200bDo you have a legal entity in the EU that we can contract with?\\nWe do not have a legal entity in the EU for customer contracting today.\\n\\u200bDo different legal terms apply if I choose the EU region?\\nThe terms are the same for the EU and US regions.\\n\\u200bFeatures\\n\\u200bHow do I use the EU instance?\\nFollow the instructions here to create an account and an API key (make sure to change the region to EU in the dropdown)\\n\\u200bAre there any functional differences between US and EU cloud-managed LangSmith?\\nThere may be a small delay between launches to each region depending on the feature. Besides that, they are functionally equivalent - all features supported in the US are supported in the EU and vice versa.\\n\\u200bCan an organization have workspaces in different regions?\\nLangSmith does not support this at the moment, but if you are interested, please contact support@langchain.dev and share your use case.\\n\\u200bCan I connect an EU organization to a US organization and share billing?\\nLangSmith does not support this at the moment, but if you are interested, please contact support@langchain.dev and share your use case.\\n\\u200bWhat data will be stored in my selected region?\\nSee the cloud architecture reference for details.\\n\\u200bHow can I see my organizationâ€™s region?\\nCheck your URL - any organizations on https://eu.smith.langchain.com are in the EU, and any on https://smith.langchain.com are in the US.\\n\\u200bCan I switch my organization from the US to EU or vice versa?\\nWe do not support migration between regions at this time, but if you are interested in this feature, please reach out to support@langchain.dev.\\n\\u200bPlans and pricing\\n\\u200bIs the EU region available on all LangSmith plans?\\nYes, you can sign up for the EU region on all plans including free plans.\\n\\u200bIs pricing different for the EU region compared to the US region?\\nNo, pricing is the same for the EU and US regions.\\n\\u200bWhat currency is used for payment if I use the EU region?\\nAll LangSmith plans are paid in USD.Was this page helpful?YesNoSuggest editsCloud architecture and scalabilityAuthentication methodsâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators', 'loc': 'https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='\\n\\n\\n\\n\\nLangChain off-the-shelf evaluators | ðŸ¦œï¸ðŸ› ï¸ LangSmith\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentLangSmith docs have moved! Find the LangSmith docs at the new LangChain Docs site.API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceLangChain off-the-shelf evaluatorsReferencesdk_referenceLangChain off-the-shelf evaluatorsLangChain off-the-shelf evaluators\\nLangChain\\'s evaluation module provides evaluators you can use as-is for common evaluation scenarios.\\nTo learn how to use these evaluators, please refer to the following guide.\\nnoteWe currently support off-the-shelf evaluators in LangChain for Python only.\\nnoteMost of these evaluators are useful but imperfect! We recommend against blind trust of any single automated metric and to always incorporate them as a part of a holistic testing and evaluation strategy.\\nMany of the LLM-based evaluators return a binary score for a given datapoint, so measuring differences in prompt or model performance are most reliable in aggregate over a larger dataset.\\nThe following table enumerates the off-the-shelf evaluators available in LangSmith, along with their output keys and a simple code sample.\\nEvaluator nameOutput KeySimple Code ExampleQ&AcorrectnessLangChainStringEvaluator(\"qa\")Contextual Q&Acontextual accuracyLangChainStringEvaluator(\"context_qa\")Chain of Thought Q&Acot contextual accuracyLangChainStringEvaluator(\"cot_qa\")CriteriaDepends on criteria keyLangChainStringEvaluator(\"criteria\", config={ \"criteria\": <criterion> })criterion may be one of the default implemented criteria: conciseness, relevance, correctness, coherence, harmfulness, maliciousness, helpfulness, controversiality, misogyny, and criminality.Or, you may define your own criteria in a custom dict as follows:{ \"criterion_key\": \"criterion description\" }Labeled CriteriaDepends on criteria keyLangChainStringEvaluator(\"labeled_criteria\", config={ \"criteria\": <criterion> })criterion may be one of the default implemented criteria: conciseness, relevance, correctness, coherence, harmfulness, maliciousness, helpfulness, controversiality, misogyny, and criminality.Or, you may define your own criteria in a custom dict as follows:{ \"criterion_key\": \"criterion description\" }ScoreDepends on criteria keyLangChainStringEvaluator(\"score_string\", config={ \"criteria\": <criterion>, \"normalize_by\": 10 })criterion may be one of the default implemented criteria: conciseness, relevance, correctness, coherence, harmfulness, maliciousness, helpfulness, controversiality, misogyny, and criminality.Or, you may define your own criteria in a custom dict as follows:{ \"criterion_key\": \"criterion description\" }. Scores are out of 10, so normalize_by will cast this to a score from 0 to 1.Labeled ScoreDepends on criteria keyLangChainStringEvaluator(\"labeled_score_string\", config={ \"criteria\": <criterion>, \"normalize_by\": 10 })criterion may be one of the default implemented criteria: conciseness, relevance, correctness, coherence, harmfulness, maliciousness, helpfulness, controversiality, misogyny, and criminality.Or, you may define your own criteria in a custom dict as follows:{ \"criterion_key\": \"criterion description\" }. Scores are out of 10, so normalize_by will cast this to a score from 0 to 1.Embedding distanceembedding_cosine_distanceLangChainStringEvaluator(\"embedding_distance\")String Distancestring_distanceLangChainStringEvaluator(\"string_distance\", config={\"distance\": \"damerau_levenshtein\" }) distance defines the string difference metric to be applied, such as levenshtein or jaro_winkler.Exact Matchexact_matchLangChainStringEvaluator(\"exact_match\")Regex Matchregex_matchLangChainStringEvaluator(\"regex_match\")Json Validityjson_validityLangChainStringEvaluator(\"json_validity\")Json Equalityjson_equalityLangChainStringEvaluator(\"json_equality\")Json Edit Distancejson_edit_distanceLangChainStringEvaluator(\"json_edit_distance\")Json Schemajson_schemaLangChainStringEvaluator(\"json_schema\")Was this page helpful?You can leave detailed feedback on GitHub.PreviousRegions FAQCommunityLangChain ForumTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright Â© 2025 LangChain, Inc.\\n\\n'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/self_hosting', 'loc': 'https://docs.smith.langchain.com/self_hosting', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Architectural overview - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewSetupInstall on KubernetesInstall on DockerInteract with an installationUpgrade an installationConfigure egress for subscription metricsView trace counts across an organizationLangSmith-managed ClickHouseConfigurationConfigure for scaleEnable TTL & data retentionCreate an Ingress for installations (Kubernetes)Mirror images for your installationUse environment variables for model providersTroubleshootingAuthentication & access controlSet up basic authenticationSet up SSO with OAuth2.0 & OIDCCustomize user managementConfigure custom TLS certificatesUse an existing secret for your installation (Kubernetes)Connect external servicesEnable blob storageConnect to an external ClickHouse databaseConnect to an external PostgreSQL databaseConnect to an external Redis databaseScriptsDelete workspacesDelete organizationsDelete tracesGenerate ClickHouse StatsGenerate query statsRun support queries against PostgreSQLRun support queries against ClickHouseObservabilityExport LangSmith telemetry to your observability backendConfigure your collector for telemetryDeploy an observability stackOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationArchitectural overviewGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageStorage ServicesClickHousePostgreSQLRedisBlob storageServicesLangSmith frontendLangSmith backendLangSmith queueLangSmith platform backendLangSmith playgroundLangSmith ACE (Arbitrary Code Execution) backendArchitectural overviewCopy pageCopy pageSelf-hosted LangSmith is an add-on to the Enterprise Plan designed for our largest, most security-conscious customers. See our pricing page for more detail, and contact our sales team if you want to get a license key to trial LangSmith in your environment.\\nYou can run LangSmith in Kubernetes (recommended) or Docker in a cloud environment that you control. The LangSmith application consists of several components including LangSmith servers and stateful services:\\n\\nLangSmith frontend\\nLangSmith backend\\nLangSmith platform backend\\nLangSmith Playground\\nLangSmith queue\\nLangSmith ACE (Arbitrary Code Execution) backend\\nClickHouse\\nPostgreSQL\\nRedis\\nBlob storage (Optional, but recommended)\\n\\n\\nTo access the LangSmith UI and send API requests, you will need to expose the LangSmith frontend service. Depending on your installation method, this can be a load balancer or a port exposed on the host machine.\\n\\u200bStorage Services\\nLangSmith Self-Hosted will bundle all storage services by default. You can configure LangSmith to use external versions of all storage services. In a production setting, we strongly recommend using external storage services.\\n\\u200bClickHouse\\nClickHouse is a high-performance, column-oriented SQL database management system (DBMS) for online analytical processing (OLAP).\\nLangSmith uses ClickHouse as the primary data store for traces and feedback (high-volume data).\\n\\u200bPostgreSQL\\nPostgreSQL is a powerful, open source object-relational database system that uses and extends the SQL language combined with many features that safely store and scale the most complicated data workloads\\nLangSmith uses PostgreSQL as the primary data store for transactional workloads and operational data (almost everything besides traces and feedback).\\n\\u200bRedis\\nRedis is a powerful in-memory key-value database that persists on disk. By holding data in memory, Redis offers high performance for operations like caching.\\nLangSmith uses Redis to back queuing and caching operations.\\n\\u200bBlob storage\\nLangSmith supports several blob storage providers, including AWS S3, Azure Blob Storage, and Google Cloud Storage.\\nLangSmith uses blob storage to store large files, such as trace artifacts, feedback attachments, and other large data objects. Blob storage is optional, but highly recommended for production deployments.\\n\\u200bServices\\n\\u200bLangSmith frontend\\nThe frontend uses Nginx to serve the LangSmith UI and route API requests to the other servers. This serves as the entrypoint for the application and is the only component that must be exposed to users.\\n\\u200bLangSmith backend\\nThe backend is the main entrypoint for CRUD API requests and handles the majority of the business logic for the application. This includes handling requests from the frontend and SDK, preparing traces for ingestion, and supporting the hub API.\\n\\u200bLangSmith queue\\nThe queue handles incoming traces and feedback to ensure that they are ingested and persisted into the traces and feedback datastore asynchronously, handling checks for data integrity and ensuring successful insert into the datastore, handling retries in situations such as database errors or the temporary inability to connect to the database.\\n\\u200bLangSmith platform backend\\nThe platform backend is another critical service that primarily handles authentication, run ingestion, and other high-volume tasks.\\n\\u200bLangSmith playground\\nThe playground is a service that handles forwarding requests to various LLM APIs to support the LangSmith Playground feature. This can also be used to connect to your own custom model servers.\\n\\u200bLangSmith ACE (Arbitrary Code Execution) backend\\nThe ACE backend is a service that handles executing arbitrary code in a secure environment. This is used to support running custom code within LangSmith.Was this page helpful?YesNoSuggest editsInstall on KubernetesâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/self_hosting/architectural_overview', 'loc': 'https://docs.smith.langchain.com/self_hosting/architectural_overview', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Architectural overview - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewSetupInstall on KubernetesInstall on DockerInteract with an installationUpgrade an installationConfigure egress for subscription metricsView trace counts across an organizationLangSmith-managed ClickHouseConfigurationConfigure for scaleEnable TTL & data retentionCreate an Ingress for installations (Kubernetes)Mirror images for your installationUse environment variables for model providersTroubleshootingAuthentication & access controlSet up basic authenticationSet up SSO with OAuth2.0 & OIDCCustomize user managementConfigure custom TLS certificatesUse an existing secret for your installation (Kubernetes)Connect external servicesEnable blob storageConnect to an external ClickHouse databaseConnect to an external PostgreSQL databaseConnect to an external Redis databaseScriptsDelete workspacesDelete organizationsDelete tracesGenerate ClickHouse StatsGenerate query statsRun support queries against PostgreSQLRun support queries against ClickHouseObservabilityExport LangSmith telemetry to your observability backendConfigure your collector for telemetryDeploy an observability stackOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationArchitectural overviewGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageStorage ServicesClickHousePostgreSQLRedisBlob storageServicesLangSmith frontendLangSmith backendLangSmith queueLangSmith platform backendLangSmith playgroundLangSmith ACE (Arbitrary Code Execution) backendArchitectural overviewCopy pageCopy pageSelf-hosted LangSmith is an add-on to the Enterprise Plan designed for our largest, most security-conscious customers. See our pricing page for more detail, and contact our sales team if you want to get a license key to trial LangSmith in your environment.\\nYou can run LangSmith in Kubernetes (recommended) or Docker in a cloud environment that you control. The LangSmith application consists of several components including LangSmith servers and stateful services:\\n\\nLangSmith frontend\\nLangSmith backend\\nLangSmith platform backend\\nLangSmith Playground\\nLangSmith queue\\nLangSmith ACE (Arbitrary Code Execution) backend\\nClickHouse\\nPostgreSQL\\nRedis\\nBlob storage (Optional, but recommended)\\n\\n\\nTo access the LangSmith UI and send API requests, you will need to expose the LangSmith frontend service. Depending on your installation method, this can be a load balancer or a port exposed on the host machine.\\n\\u200bStorage Services\\nLangSmith Self-Hosted will bundle all storage services by default. You can configure LangSmith to use external versions of all storage services. In a production setting, we strongly recommend using external storage services.\\n\\u200bClickHouse\\nClickHouse is a high-performance, column-oriented SQL database management system (DBMS) for online analytical processing (OLAP).\\nLangSmith uses ClickHouse as the primary data store for traces and feedback (high-volume data).\\n\\u200bPostgreSQL\\nPostgreSQL is a powerful, open source object-relational database system that uses and extends the SQL language combined with many features that safely store and scale the most complicated data workloads\\nLangSmith uses PostgreSQL as the primary data store for transactional workloads and operational data (almost everything besides traces and feedback).\\n\\u200bRedis\\nRedis is a powerful in-memory key-value database that persists on disk. By holding data in memory, Redis offers high performance for operations like caching.\\nLangSmith uses Redis to back queuing and caching operations.\\n\\u200bBlob storage\\nLangSmith supports several blob storage providers, including AWS S3, Azure Blob Storage, and Google Cloud Storage.\\nLangSmith uses blob storage to store large files, such as trace artifacts, feedback attachments, and other large data objects. Blob storage is optional, but highly recommended for production deployments.\\n\\u200bServices\\n\\u200bLangSmith frontend\\nThe frontend uses Nginx to serve the LangSmith UI and route API requests to the other servers. This serves as the entrypoint for the application and is the only component that must be exposed to users.\\n\\u200bLangSmith backend\\nThe backend is the main entrypoint for CRUD API requests and handles the majority of the business logic for the application. This includes handling requests from the frontend and SDK, preparing traces for ingestion, and supporting the hub API.\\n\\u200bLangSmith queue\\nThe queue handles incoming traces and feedback to ensure that they are ingested and persisted into the traces and feedback datastore asynchronously, handling checks for data integrity and ensuring successful insert into the datastore, handling retries in situations such as database errors or the temporary inability to connect to the database.\\n\\u200bLangSmith platform backend\\nThe platform backend is another critical service that primarily handles authentication, run ingestion, and other high-volume tasks.\\n\\u200bLangSmith playground\\nThe playground is a service that handles forwarding requests to various LLM APIs to support the LangSmith Playground feature. This can also be used to connect to your own custom model servers.\\n\\u200bLangSmith ACE (Arbitrary Code Execution) backend\\nThe ACE backend is a service that handles executing arbitrary code in a secure environment. This is used to support running custom code within LangSmith.Was this page helpful?YesNoSuggest editsInstall on KubernetesâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/self_hosting/configuration', 'loc': 'https://docs.smith.langchain.com/self_hosting/configuration', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Configure LangSmith for scale - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewSetupInstall on KubernetesInstall on DockerInteract with an installationUpgrade an installationConfigure egress for subscription metricsView trace counts across an organizationLangSmith-managed ClickHouseConfigurationConfigure for scaleEnable TTL & data retentionCreate an Ingress for installations (Kubernetes)Mirror images for your installationUse environment variables for model providersTroubleshootingAuthentication & access controlSet up basic authenticationSet up SSO with OAuth2.0 & OIDCCustomize user managementConfigure custom TLS certificatesUse an existing secret for your installation (Kubernetes)Connect external servicesEnable blob storageConnect to an external ClickHouse databaseConnect to an external PostgreSQL databaseConnect to an external Redis databaseScriptsDelete workspacesDelete organizationsDelete tracesGenerate ClickHouse StatsGenerate query statsRun support queries against PostgreSQLRun support queries against ClickHouseObservabilityExport LangSmith telemetry to your observability backendConfigure your collector for telemetryDeploy an observability stackOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationConfigurationConfigure LangSmith for scaleGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageSummaryTrace ingestion (write path)Trace querying (read path)Example LangSmith configurations for scaleLow reads, low writes Low reads, high writes High reads, low writes Medium reads, medium writes High reads, high writes ConfigurationConfigure LangSmith for scaleCopy pageCopy pageA self-hosted LangSmith instance can handle a large number of traces and users. The default configuration for the self-hosted deployment can handle substantial load, and you can configure your deployment to be able to achieve higher scale. This page describes scaling considerations and provides some examples to help configure your self-hosted instance.\\nFor example configurations, refer to Example LangSmith configurations for scale.\\n\\u200bSummary\\nThe table below provides an overview comparing different LangSmith configurations for various load patterns (reads / writes):\\nLow / lowLow / highHigh / lowMedium / mediumHigh / highConcurrent frontend users55502050Traces submitted per second101000101001000Frontend replicas1 (default)4224Platform backend replicas3 (default)203 (default)3 (default)20Queue replicas3 (default)160610160Backend replicas2 (default)5401650Redis resources8 Gi (default)200 Gi external8 Gi (default)13Gi external200 Gi externalClickHouse resources4 CPU16 Gi (default)10 CPU32Gi memory8 CPU16 Gi per replica16 CPU24Gi memory14 CPU24 Gi per replicaClickHouse setupSingle instanceSingle instance3-node replicated clusterSingle instance3-node replicated clusterPostgres resources2 CPU8 GB memory10GB storage (external)2 CPU8 GB memory10GB storage (external)2 CPU8 GB memory10GB storage (external)2 CPU8 GB memory10GB storage (external)2 CPU8 GB memory10GB storage (external)Blob storageDisabledEnabledEnabledEnabledEnabled\\nBelow we go into more details about the read and write paths as well as provide a values.yaml snippet for you to start with for your self-hosted LangSmith instance.\\n\\u200bTrace ingestion (write path)\\nCommon usage that put load on the write path:\\n\\nIngesting traces via the Python or JavaScript LangSmith SDK\\nIngesting traces via the @traceable wrapper\\nSubmitting traces via the /runs/multipart endpoint\\n\\nServices that play a large role in trace ingestion:\\n\\nPlatform backend service: Receives initial request to ingest traces and places traces on a Redis queue\\nRedis cache: Used to queue traces that need to be persisted\\nQueue service: Persists traces for querying\\nClickHouse: Persistent storage used for traces\\n\\nWhen scaling up the write path (trace ingestion), it is helpful to monitor the four services/resources listed above. Here are some typical changes that can help increase performance of trace ingestion:\\n\\nGive ClickHouse more resources (CPU and memory) if it is approaching resource limits.\\nIncrease the number of platform-backend pods if ingest requests are taking long to respond.\\nIncrease queue service pod replicas if traces are not being processed from Redis fast enough.\\nUse a larger Redis cache if you notice that the current Redis instance is reaching resource limits. This could also be a reason why ingest requests take a long time.\\n\\n\\u200bTrace querying (read path)\\nCommon usage that puts load on the read path:\\n\\nUsers on the frontend looking at tracing projects or individual traces\\nScripts used to query for trace info\\nHitting either the /runs/query or /runs/<run-id> api endpoints\\n\\nServices that play a large role in querying traces:\\n\\nBackend service: Receives the request and submits a query to ClickHouse to then respond to the request\\nClickHouse: Persistent storage for traces. This is the main database that is queried when requesting trace info.\\n\\nWhen scaling up the read path (trace querying), it is helpful to monitor the two services/resources listed above. Here are some typical changes that can help improve performance of trace querying:\\n\\nIncrease the number of backend service pods. This would be most impactful if backend service pods are reaching 1 core CPU usage.\\nGive ClickHouse more resources (CPU or Memory). ClickHouse can be very resource intensive, but it should lead to better performance.\\nMove to a replicated ClickHouse cluster. Adding replicas of ClickHouse helps with read performance, but we recommend staying below 5 replicas (start with 3).\\n\\nFor more precise guidance on how this translates to helm chart values, refer to the examples the following section. If you are unsure why your LangSmith instance cannot handle a certain load pattern, contact the LangChain team.\\n\\u200bExample LangSmith configurations for scale\\nBelow we provide some example LangSmith configurations based on expected read and write loads.\\nFor read load (trace querying):\\n\\nLow means roughly 5 users looking at traces at a time (about 10 requests per second)\\nMedium means roughly 20 users looking at traces at a time (about 40 requests per second)\\nHigh means roughly 50 users looking at traces at a time (about 100 requests per second)\\n\\nFor write load (trace ingestion):\\n\\nLow means up to 10 traces submitted per second\\nMedium means up to 100 traces submitted per second\\nHigh means up to 1000 traces submitted per second\\n\\nThe exact optimal configuration depends on your usage and trace payloads. Use the examples below in combination with the information above and your specific usage to update your LangSmith configuration as you see fit. If you have any questions, please reach out to the LangChain team.\\n\\u200bLow reads, low writes \\nThe default LangSmith configuration will handle this load. No custom resource configuration is needed here.\\n\\u200bLow reads, high writes \\nYou have a very high scale of trace ingestions, but single digit number of users on the frontend querying traces at any one time.\\nFor this, we recommend a configuration like this:\\nCopyconfig:\\n  blobStorage:\\n    # Please also set the other keys to connect to your blob storage. See configuration section.\\n    enabled: true\\n  settings:\\n    redisRunsExpirySeconds: \"3600\"\\n# ttl:\\n#   enabled: true\\n#   ttl_period_seconds:\\n#     longlived: \"7776000\"  # 90 days (default is 400 days)\\n#     shortlived: \"604800\"  # 7 days (default is 14 days)\\n\\nfrontend:\\n  deployment:\\n    replicas: 4 # OR enable autoscaling to this level (example below)\\n# autoscaling:\\n#   enabled: true\\n#   maxReplicas: 4\\n#   minReplicas: 2\\n\\nplatformBackend:\\n  deployment:\\n    replicas: 20 # OR enable autoscaling to this level (example below)\\n# autoscaling:\\n#   enabled: true\\n#   maxReplicas: 20\\n#   minReplicas: 8\\n\\n## Note that we are actively working on improving performance of this service to reduce the number of replicas.\\nqueue:\\n  deployment:\\n    replicas: 160 # OR enable autoscaling to this level (example below)\\n# autoscaling:\\n#   enabled: true\\n#   maxReplicas: 160\\n#   minReplicas: 40\\n\\nbackend:\\n  deployment:\\n    replicas: 5 # OR enable autoscaling to this level (example below)\\n# autoscaling:\\n#   enabled: true\\n#   maxReplicas: 5\\n#   minReplicas: 3\\n\\n## Ensure your Redis cache is at least 200 GB\\nredis:\\n  external:\\n    enabled: true\\n    existingSecretName: langsmith-redis-secret # Set the connection url for your external Redis instance (200+ GB)\\n\\nclickhouse:\\n  statefulSet:\\n    persistence:\\n      # This may depend on your configured TTL (see config section).\\n      # We recommend 600Gi for every shortlived TTL day if operating at this scale constantly.\\n      size: 4200Gi # This assumes 7 days TTL and operating a this scale constantly.\\n    resources:\\n      requests:\\n        cpu: \"10\"\\n        memory: \"32Gi\"\\n      limits:\\n        cpu: \"16\"\\n        memory: \"48Gi\"\\n\\ncommonEnv:\\n  - name: \"CLICKHOUSE_ASYNC_INSERT_WAIT_PCT_FLOAT\"\\n    value: \"0\"\\n\\n\\u200bHigh reads, low writes \\nYou have a relatively low scale of trace ingestions, but many frontend users querying traces and/or have scripts that hit the /runs/query or /runs/<run-id> endpoints frequently.\\nFor this, we strongly recommend setting up a replicated ClickHouse cluster to enable high read scale at low latency. See our external ClickHouse doc for more guidance on how to setup a replicated ClickHouse cluster. For this load pattern, we recommend using a 3 node replicated setup, where each replica in the cluster should have resource requests of 8+ cores and 16+ GB memory, and resource limit of 12 cores and 32 GB memory.\\nFor this, we recommend a configuration like this:\\nCopyconfig:\\n  blobStorage:\\n    # Please also set the other keys to connect to your blob storage. See configuration section.\\n    enabled: true\\n\\nfrontend:\\n  deployment:\\n    replicas: 2\\n\\nqueue:\\n  deployment:\\n    replicas: 6 # OR enable autoscaling to this level (example below)\\n# autoscaling:\\n#   enabled: true\\n#   maxReplicas: 6\\n#   minReplicas: 4\\n\\nbackend:\\n  deployment:\\n    replicas: 40 # OR enable autoscaling to this level (example below)\\n# autoscaling:\\n#   enabled: true\\n#   maxReplicas: 40\\n#   minReplicas: 16\\n\\n# We strongly recommend setting up a replicated clickhouse cluster for this load.\\n# Update these values as needed to connect to your replicated clickhouse cluster.\\nclickhouse:\\n  external:\\n    # If using a 3 node replicated setup, each replica in the cluster should have resource requests of 8+ cores and 16+ GB memory, and resource limit of 12 cores and 32 GB memory.\\n    enabled: true\\n    host: langsmith-ch-clickhouse-replicated.default.svc.cluster.local\\n    port: \"8123\"\\n    nativePort: \"9000\"\\n    user: \"default\"\\n    password: \"password\"\\n    database: \"default\"\\n    cluster: \"replicated\"\\n\\n\\u200bMedium reads, medium writes \\nThis is a good all around configuration that should be able to handle most usage patterns of LangSmith. In internal testing, this configuration allowed us to scale to 100 traces ingested per second and 40 read requests per second.\\nFor this, we recommend a configuration like this:\\nCopyconfig:\\n  blobStorage:\\n    # Please also set the other keys to connect to your blob storage. See configuration section.\\n    enabled: true\\n  settings:\\n    redisRunsExpirySeconds: \"3600\"\\n\\nfrontend:\\n  deployment:\\n    replicas: 2\\n\\nqueue:\\n  deployment:\\n    replicas: 10 # OR enable autoscaling to this level (example below)\\n# autoscaling:\\n#   enabled: true\\n#   maxReplicas: 10\\n#   minReplicas: 5\\n\\nbackend:\\n  deployment:\\n    replicas: 16 # OR enable autoscaling to this level (example below)\\n# autoscaling:\\n#   enabled: true\\n#   maxReplicas: 16\\n#   minReplicas: 8\\n\\nredis:\\n  statefulSet:\\n    resources:\\n      requests:\\n        memory: 13Gi\\n      limits:\\n        memory: 13Gi\\n\\n  # -- For external redis instead use something like below --\\n  # external:\\n  #   enabled: true\\n  #   connectionUrl: \"<URL>\" OR existingSecretName: \"<SECRET-NAME>\"\\n\\nclickhouse:\\n  statefulSet:\\n    persistence:\\n      # This may depend on your configured TTL.\\n      # We recommend 60Gi for every shortlived TTL day if operating at this scale constantly.\\n      size: 420Gi # This assumes 7 days TTL and operating a this scale constantly.\\n    resources:\\n      requests:\\n        cpu: \"16\"\\n        memory: \"24Gi\"\\n      limits:\\n        cpu: \"28\"\\n        memory: \"40Gi\"\\n\\ncommonEnv:\\n  - name: \"CLICKHOUSE_ASYNC_INSERT_WAIT_PCT_FLOAT\"\\n    value: \"0\"\\n\\nIf you still notice slow reads with the above configuration, we recommend moving to a replicated Clickhouse cluster setup\\n\\u200bHigh reads, high writes \\nYou have a very high rate of trace ingestion (approaching 1000 traces submitted per second) and also have many users querying traces on the frontend (over 50 users) and/or scripts that are consistently making requests to /runs/query or /runs/<run-id> endpoints.\\nFor this, we very strongly recommend setting up a replicated ClickHouse cluster to prevent degraded read performance at high write scale. See our external ClickHouse doc for more guidance on how to set up a replicated ClickHouse cluster. For this load pattern, we recommend using a 3 node replicated setup, where each replica in the cluster should have resource requests of 14+ cores and 24+ GB memory, and resource limit of 20 cores and 48 GB memory. We also recommend that each node/instance of ClickHouse has 600 Gi of volume storage for each day of TTL that you enable (as per the configuration below).\\nOverall, we recommend a configuration like this:\\nCopyconfig:\\n  blobStorage:\\n    # Please also set the other keys to connect to your blob storage. See configuration section.\\n    enabled: true\\n  settings:\\n    redisRunsExpirySeconds: \"3600\"\\n# ttl:\\n#   enabled: true\\n#   ttl_period_seconds:\\n#     longlived: \"7776000\"  # 90 days (default is 400 days)\\n#     shortlived: \"604800\"  # 7 days (default is 14 days)\\n\\nfrontend:\\n  deployment:\\n    replicas: 4 # OR enable autoscaling to this level (example below)\\n# autoscaling:\\n#   enabled: true\\n#   maxReplicas: 4\\n#   minReplicas: 2\\n\\nplatformBackend:\\n  deployment:\\n    replicas: 20 # OR enable autoscaling to this level (example below)\\n# autoscaling:\\n#   enabled: true\\n#   maxReplicas: 20\\n#   minReplicas: 8\\n\\n## Note that we are actively working on improving performance of this service to reduce the number of replicas.\\nqueue:\\n  deployment:\\n    replicas: 160 # OR enable autoscaling to this level (example below)\\n# autoscaling:\\n#   enabled: true\\n#   maxReplicas: 160\\n#   minReplicas: 40\\n\\nbackend:\\n  deployment:\\n    replicas: 50 # OR enable autoscaling to this level (example below)\\n# autoscaling:\\n#   enabled: true\\n#   maxReplicas: 50\\n#   minReplicas: 20\\n\\n## Ensure your Redis cache is at least 200 GB\\nredis:\\n  external:\\n    enabled: true\\n    existingSecretName: langsmith-redis-secret # Set the connection url for your external Redis instance (200+ GB)\\n\\n# We strongly recommend setting up a replicated clickhouse cluster for this load.\\n# Update these values as needed to connect to your replicated clickhouse cluster.\\nclickhouse:\\n  external:\\n    # If using a 3 node replicated setup, each replica in the cluster should have resource requests of 14+ cores and 24+ GB memory, and resource limit of 20 cores and 48 GB memory.\\n    enabled: true\\n    host: langsmith-ch-clickhouse-replicated.default.svc.cluster.local\\n    port: \"8123\"\\n    nativePort: \"9000\"\\n    user: \"default\"\\n    password: \"password\"\\n    database: \"default\"\\n    cluster: \"replicated\"\\n\\ncommonEnv:\\n  - name: \"CLICKHOUSE_ASYNC_INSERT_WAIT_PCT_FLOAT\"\\n    value: \"0\"\\n\\nEnsure that the Kubernetes cluster is configured with sufficient resources to scale to the recommended size. After deployment, all of the pods in the Kubernetes cluster should be in a Running state. Pods stuck in Pending may indicate that you are reaching node pool limits or need larger nodes.Also, ensure that any ingress controller deployed on the cluster is able to handle the desired load to prevent bottlenecks.Was this page helpful?YesNoSuggest editsLangSmith-managed ClickHouseEnable TTL & data retentionâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/self_hosting/configuration/basic_auth', 'loc': 'https://docs.smith.langchain.com/self_hosting/configuration/basic_auth', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Basic authentication with email and password - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewSetupInstall on KubernetesInstall on DockerInteract with an installationUpgrade an installationConfigure egress for subscription metricsView trace counts across an organizationLangSmith-managed ClickHouseConfigurationConfigure for scaleEnable TTL & data retentionCreate an Ingress for installations (Kubernetes)Mirror images for your installationUse environment variables for model providersTroubleshootingAuthentication & access controlSet up basic authenticationSet up SSO with OAuth2.0 & OIDCCustomize user managementConfigure custom TLS certificatesUse an existing secret for your installation (Kubernetes)Connect external servicesEnable blob storageConnect to an external ClickHouse databaseConnect to an external PostgreSQL databaseConnect to an external Redis databaseScriptsDelete workspacesDelete organizationsDelete tracesGenerate ClickHouse StatsGenerate query statsRun support queries against PostgreSQLRun support queries against ClickHouseObservabilityExport LangSmith telemetry to your observability backendConfigure your collector for telemetryDeploy an observability stackOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationAuthentication & access controlBasic authentication with email and passwordGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageRequirements and featuresMigrating from None authConfigurationAuthentication & access controlBasic authentication with email and passwordCopy pageCopy pageLangSmith supports login via username/password with a few limitations:\\n\\nYou cannot change an existing installation from basic auth mode to OAuth with PKCE (deprecated) or vice versa - installations must be either one or the other. A basic auth installation requires a completely fresh installation including a separate PostgreSQL database/schema, unless migrating from an existing None type installation (see below).\\nUsers must be given their initial auto-generated password once they are invited. This password may be changed later by any Organization Admin.\\nYou cannot use both basic auth and OAuth with client secret at the same time.\\n\\n\\u200bRequirements and features\\n\\nThere is a single Default organization that is provisioned during initial installation, and creating additional organizations is not supported\\nYour initial password (configured below) must be least 12 characters long and have at least one lowercase, uppercase, and symbol\\nThere are no strict requirements for the secret used for signing JWTs, but we recommend securely generating a string of at least 32 characters. For example: openssl rand -base64 32\\n\\n\\u200bMigrating from None auth\\nOnly supported in versions 0.7 and above.\\nMigrating an installation from None auth mode replaces the single â€œdefaultâ€ user with a user with the configured credentials and keeps all existing resources. The single pre-existing workspace ID post-migration remains 00000000-0000-0000-0000-000000000000, but everything else about the migrated installation is standard for a basic auth installation.\\nTo migrate, simply update your configuration as shown below and run helm upgrade (or docker-compose up) as usual.\\n\\u200bConfiguration\\nChanging the JWT secret will log out your users\\nHelmDockerCopyconfig:\\n  authType: mixed\\n  basicAuth:\\n    enabled: true\\n    initialOrgAdminEmail: <YOUR EMAIL ADDRESS>\\n    initialOrgAdminPassword: <PASSWORD> # Must be at least 12 characters long and have at least one lowercase, uppercase, and symbol\\n    jwtSecret: <SECRET>\\n\\nAdditionally, in docker-compose you will need to run the bootstrap command to create the initial organization and user:\\nCopydocker-compose exec langchain-backend python hooks/auth_bootstrap.pyc\\n\\nOnce configured, you will see a login screen like the one below. You should be able to login with the initialOrgAdminEmail and initialOrgAdminPassword values, and your user will be auto-provisioned with role Organization Admin. See the admin guide for more details on organization roles.\\nWas this page helpful?YesNoSuggest editsTroubleshootingSet up SSO with OAuth2.0 & OIDCâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/self_hosting/configuration/blob_storage', 'loc': 'https://docs.smith.langchain.com/self_hosting/configuration/blob_storage', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Enable blob storage - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewSetupInstall on KubernetesInstall on DockerInteract with an installationUpgrade an installationConfigure egress for subscription metricsView trace counts across an organizationLangSmith-managed ClickHouseConfigurationConfigure for scaleEnable TTL & data retentionCreate an Ingress for installations (Kubernetes)Mirror images for your installationUse environment variables for model providersTroubleshootingAuthentication & access controlSet up basic authenticationSet up SSO with OAuth2.0 & OIDCCustomize user managementConfigure custom TLS certificatesUse an existing secret for your installation (Kubernetes)Connect external servicesEnable blob storageConnect to an external ClickHouse databaseConnect to an external PostgreSQL databaseConnect to an external Redis databaseScriptsDelete workspacesDelete organizationsDelete tracesGenerate ClickHouse StatsGenerate query statsRun support queries against PostgreSQLRun support queries against ClickHouseObservabilityExport LangSmith telemetry to your observability backendConfigure your collector for telemetryDeploy an observability stackOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationConnect external servicesEnable blob storageGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageRequirementsAuthenticationAmazon S3Google Cloud StorageAzure Blob StorageCH SearchConfigurationTTL ConfigurationAmazon S3Google Cloud StorageAzure blob storageConnect external servicesEnable blob storageCopy pageCopy pageBy default, LangSmith stores run inputs, outputs, errors, manifests, extras, and events in ClickHouse. If you so choose, you can instead store this information in blob storage, which has a couple of notable benefits. For the best results in production deployments, we strongly recommend using blob storage, which offers the following benefits:\\n\\nIn high trace environments, inputs, outputs, errors, manifests, extras, and events may balloon the size of your databases.\\nIf using LangSmith Managed ClickHouse, you may want sensitive information in blob storage that resides in your environment. To alleviate this, LangSmith supports storing run inputs, outputs, errors, manifests, extras, events, and attachments in an external blob storage system.\\n\\n\\u200bRequirements\\nAzure blob storage is available in Helm chart versions 0.8.9 and greater. Deleting trace projects is supported in Azure starting in Helm chart version 0.10.43.\\n\\n\\nAccess to a valid blob storage service\\n\\nAmazon S3\\n\\nGoogle Cloud Storage (GCS)\\n\\n\\nAzure Blob Storage\\n\\n\\n\\nA bucket/directory in your blob storage to store the data. We highly recommend creating a separate bucket/directory for LangSmith data.\\n\\nIf you are using TTLs, you will need to set up a lifecycle policy to delete old data. You can find more information on configuring TTLs here. These policies should mirror the TTLs you have set in your LangSmith configuration, or you may experience data loss. See here on how to setup the lifecycle rules for TTLs for blob storage.\\n\\n\\n\\nCredentials to permit LangSmith Services to access the bucket/directory\\n\\nYou will need to provide your LangSmith instance with the necessary credentials to access the bucket/directory. Read the authentication section below for more information.\\n\\n\\n\\nIf using S3 or GCS, an API url for your blob storage service\\n\\nThis will be the URL that LangSmith uses to access your blob storage system\\nFor Amazon S3, this will be the URL of the S3 endpoint. Something like: https://s3.amazonaws.com or https://s3.us-west-1.amazonaws.com if using a regional endpoint.\\nFor Google Cloud Storage, this will be the URL of the GCS endpoint. Something like: https://storage.googleapis.com\\n\\n\\n\\n\\u200bAuthentication\\n\\u200bAmazon S3\\nTo authenticate to Amazon S3, you will need to create an IAM policy granting the following permissions on your bucket.\\nCopy{\\n  \"Version\": \"2012-10-17\",\\n  \"Statement\": [\\n    {\\n      \"Effect\": \"Allow\",\\n      \"Action\": [\\n        \"s3:GetObject\",\\n        \"s3:PutObject\",\\n        \"s3:DeleteObject\",\\n        \"s3:ListBucket\"\\n      ],\\n      \"Resource\": [\\n        \"arn:aws:s3:::your-bucket-name\",\\n        \"arn:aws:s3:::your-bucket-name/*\"\\n      ]\\n    }\\n  ]\\n}\\n\\nOnce you have the correct policy, there are three ways to authenticate with Amazon S3:\\n\\n\\n(Recommended) IAM Role for Service Account: You can create an IAM role for your LangSmith instance and attach the policy to that role. You can then provide the role to LangSmith. This is the recommended way to authenticate with Amazon S3 in production.\\n\\nYou will need to create an IAM role with the policy attached.\\nYou will need to allow LangSmith service accounts to assume the role. The langsmith-queue, langsmith-backend, and langsmith-platform-backend service accounts will need to be able to assume the role.\\nThe service account names will be different if you are using a custom release name. You can find the service account names by running kubectl get serviceaccounts in your cluster.\\n\\nYou will need to provide the role ARN to LangSmith. You can do this by adding the eks.amazonaws.com/role-arn: \"<role_arn>\" annotation to the queue, backend, and platform-backend services in your Helm Chart installation.\\n\\n\\n\\nAccess Key and Secret Key: You can provide LangSmith with an access key and secret key. This is the simplest way to authenticate with Amazon S3. However, it is not recommended for production use as it is less secure.\\n\\nYou will need to create a user with the policy attached. Then you can provision an access key and secret key for that user.\\n\\n\\n\\nVPC Endpoint Access: You can enable access to your S3 bucket via a VPC endpoint, which allows traffic to flow securely from your VPC to your S3 bucket.\\n\\nYouâ€™ll need to provision a VPC endpoint and configure it to allow access to your S3 bucket.\\nYou can refer to our public Terraform modules for guidance and an example of configuring this.\\n\\n\\n\\n\\u200bGoogle Cloud Storage\\nTo authenticate with Google Cloud Storage, you will need to create a service account with the necessary permissions to access your bucket.\\nYour service account will need the Storage Admin role or a custom role with equivalent permissions. This can be scoped to the bucket that LangSmith will be using.\\nOnce you have a provisioned service account, you will need to generate a HMAC key for that service account. This key and secret will be used to authenticate with Google Cloud Storage.\\n\\u200bAzure Blob Storage\\nTo authenticate with Azure Blob Storage, you will need to use one of the following methods to grant LangSmith workloads permission to access your container (listed in order of precedence):\\n\\nStorage account and access key\\nConnection string\\nWorkload identity (recommended), managed identity, or environment variables supported by DefaultAzureCredential. This is the default authentication method when configuration for either option above is not present.\\n\\nTo use workload identity, add the label azure.workload.identity/use: true to the queue, backend, and platform-backend deployments. Additionally, add the azure.workload.identity/client-id annotation to the corresponding service accounts, which should be an existing Azure AD Applicationâ€™s client ID or user-assigned managed identityâ€™s client ID. See Azureâ€™s documentation for additional details.\\n\\n\\n\\nSome deployments may need further customization of the connection configuration using a Service URL Override instead of the default service URL (https://<storage_account_name>.blob.core.windows.net/). For example, this override is necessary in order to use a different blob storage domain (e.g. government or china).\\n\\u200bCH Search\\nBy default, LangSmith will still store tokens for search in ClickHouse. If you are using LangSmith Managed Clickhouse, you may want to disable this feature to avoid sending potentially sensitive information to ClickHouse. You can do this in your blob storage configuration.\\n\\u200bConfiguration\\nAfter creating your bucket and obtaining the necessary credentials, you can configure LangSmith to use your blob storage system.\\nHelmDockerCopyconfig:\\n  blobStorage:\\n    enabled: true\\n    engine: \"S3\" # Or \"Azure\". This is case-sensitive.\\n    chSearchEnabled: true # Set to false if you want to disable CH search (Recommended for LangSmith Managed Clickhouse)\\n    bucketName: \"your-bucket-name\"\\n    apiURL: \"Your connection url\"\\n    accessKey: \"Your access key\" # Optional. Only required if using S3 access key and secret key\\n    accessKeySecret: \"Your access key secret\" # Optional. Only required if using access key and secret key\\n    # The following blob storage configuration values are for Azure and require blobStorage.engine = \"Azure\". Omit otherwise.\\n    azureStorageAccountName: \"Your storage account name\" # Optional. Only required if using storage account and access key.\\n    azureStorageAccountKey: \"Your storage account access key\" # Optional. Only required if using storage account and access key.\\n    azureStorageContainerName: \"your-container-name\" # Required\\n    azureStorageConnectionString: \"\" # Optional.\\n    azureStorageServiceUrlOverride: \"\" # Optional\\n  backend: # Optional, only required if using IAM role for service account on AWS or workload identity on AKS\\n    deployment: # Azure only\\n      labels:\\n        azure.workload.identity/use: true\\n    serviceAccount:\\n      annotations:\\n        azure.workload.identity/client-id: \"<client_id>\" # Azure only\\n        eks.amazonaws.com/role-arn: \"<role_arn>\" # AWS only\\n  platformBackend: # Optional, only required if using IAM role for service account on AWS or workload identity on AKS\\n    deployment: # Azure only\\n      labels:\\n        azure.workload.identity/use: true\\n    serviceAccount:\\n      annotations:\\n        azure.workload.identity/client-id: \"<client_id>\" # Azure only\\n        eks.amazonaws.com/role-arn: \"<role_arn>\" # AWS only\\n  queue: # Optional, only required if using IAM role for service account on AWS or workload identity on AKS\\n    deployment: # Azure only\\n      labels:\\n        azure.workload.identity/use: true\\n    serviceAccount:\\n      annotations:\\n        azure.workload.identity/client-id: \"<client_id>\" # Azure only\\n        eks.amazonaws.com/role-arn: \"<role_arn>\" # AWS only\\n\\nIf using an access key and secret, you can also provide an existing Kubernetes secret that contains the authentication information. This is recommended over providing the access key and secret key directly in your config. See the generated secret template for the expected secret keys.\\n\\u200bTTL Configuration\\nIf using the TTL feature with LangSmith, youâ€™ll also have to configure TTL rules for your blob storage. Trace information stored on blob storage is stored on a particular prefix path, which determines the TTL for the data. When a traceâ€™s retention is extended, its corresponding blob storage path changes to ensure that it matches the new extended retention.\\nThe following TTL prefix are used:\\n\\nttl_s/: Short term TTL, configured for 14 days.\\nttl_l/: Long term TTL, configured for 400 days.\\n\\nIf you have customized the TTLs in your LangSmith configuration, you will need to adjust the TTLs in your blob storage configuration to match.\\n\\u200bAmazon S3\\nIf using S3 for your blob storage, you will need to setup a filter lifecycle configuration that matches the prefixes above. You can find information for this in the Amazon Documentation.\\nAs an example, if you are using Terraform to manage your S3 bucket, you would setup something like this:\\nCopy  rule {\\n    id      = \"short-term-ttl\"\\n    prefix  = \"ttl_s/\"\\n    enabled = true\\n    expiration {\\n      days = 14\\n    }\\n  }\\n  rule {\\n    id      = \"long-term-ttl\"\\n    prefix  = \"ttl_l/\"\\n    enabled = true\\n    expiration {\\n      days = 400\\n    }\\n  }\\n\\n\\u200bGoogle Cloud Storage\\nYou will need to setup lifecycle conditions for your GCS buckets that you are using. You can find information for this in the Google Documentation, specifically using matchesPrefix.\\nAs an example, if you are using Terraform to manage your GCS bucket, you would setup something like this:\\nCopy  lifecycle_rule {\\n    condition {\\n      age            = 14\\n      matches_prefix = [\"ttl_s\"]\\n    }\\n    action {\\n      type = \"Delete\"\\n    }\\n  }\\n  lifecycle_rule {\\n    condition {\\n      age            = 400\\n      matches_prefix = [\"ttl_l\"]\\n    }\\n    action {\\n      type = \"Delete\"\\n    }\\n  }\\n\\n\\u200bAzure blob storage\\nYou will need to configure a lifecycle management policy on the container in order to expire objects matching the prefixes above.\\nAs an example, if you are using Terraform to manage your blob storage container, you would setup something like this:\\nCopyresource \"azurerm_storage_management_policy\" \"example\" {\\n  storage_account_id = \"my-storage-account-id\"\\n  rule {\\n    name = \"base\"\\n    enabled = true\\n    type = \"Lifecycle\"\\n    filters {\\n      prefix_match = [\"my-container/ttl_s\"]\\n      blob_types = [\"blockBlob\"]\\n    }\\n    actions {\\n      base_blob {\\n        delete_after_days_since_creation_greater_than = 14\\n      }\\n      snapshot {\\n        delete_after_days_since_creation_greater_than = 14\\n      }\\n      version {\\n        delete_after_days_since_creation_greater_than = 14\\n      }\\n    }\\n  }\\n  rule {\\n    name = \"extended\"\\n    enabled = true\\n    type = \"Lifecycle\"\\n    filters {\\n      prefix_match = [\"my-container/ttl_l\"]\\n      blob_types = [\"blockBlob\"]\\n    }\\n    actions {\\n      base_blob {\\n        delete_after_days_since_creation_greater_than = 400\\n      }\\n      snapshot {\\n        delete_after_days_since_creation_greater_than = 400\\n      }\\n      version {\\n        delete_after_days_since_creation_greater_than = 400\\n      }\\n    }\\n  }\\n}\\nWas this page helpful?YesNoSuggest editsUse an existing secret for your installation (Kubernetes)Connect to an external ClickHouse databaseâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/self_hosting/configuration/custom_tls_certificates', 'loc': 'https://docs.smith.langchain.com/self_hosting/configuration/custom_tls_certificates', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Configure custom TLS certificates - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewSetupInstall on KubernetesInstall on DockerInteract with an installationUpgrade an installationConfigure egress for subscription metricsView trace counts across an organizationLangSmith-managed ClickHouseConfigurationConfigure for scaleEnable TTL & data retentionCreate an Ingress for installations (Kubernetes)Mirror images for your installationUse environment variables for model providersTroubleshootingAuthentication & access controlSet up basic authenticationSet up SSO with OAuth2.0 & OIDCCustomize user managementConfigure custom TLS certificatesUse an existing secret for your installation (Kubernetes)Connect external servicesEnable blob storageConnect to an external ClickHouse databaseConnect to an external PostgreSQL databaseConnect to an external Redis databaseScriptsDelete workspacesDelete organizationsDelete tracesGenerate ClickHouse StatsGenerate query statsRun support queries against PostgreSQLRun support queries against ClickHouseObservabilityExport LangSmith telemetry to your observability backendConfigure your collector for telemetryDeploy an observability stackOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationAuthentication & access controlConfigure custom TLS certificatesGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageUse custom TLS certificates for model providersMount internal CAs for TLSAuthentication & access controlConfigure custom TLS certificatesCopy pageCopy pageUse this guide to configure custom TLS certificates in LangSmith. This is required when connecting securely to model providers or external services, especially if you rely on self-signed certificates or internal certificate authorities. This page describes two related tasks:\\n\\nUsing custom TLS certificates for model providers (such as Azure, OpenAI, or a custom model server)\\nMounting internal certificate authorities (CAs) to enable TLS connections for databases and other external services.\\n\\n\\u200bUse custom TLS certificates for model providers\\nThis feature is currently only available for the following model providers:\\nAzure OpenAI\\nOpenAI\\nCustom (our custom model server). Refer to the custom model server documentation for more information.\\nThese TLS settings will apply to all invocations of the selected model providers including when used through Online Evaluation.\\nYou can use custom TLS certificates to connect to model providers in the LangSmith playground. This is useful if you are using a self-signed certificate, a certificate from a custom certificate authority or mutual TLS authentication.\\nTo use custom TLS certificates, you need to set the following environment variables. See the self hosted deployment section for more information on how to set up application configuration.\\n\\nLANGSMITH_PLAYGROUND_TLS_MODEL_PROVIDERS: A comma-separated list of model providers that require custom TLS certificates. Note that azure_openai, openai and custom are currently the only supported model provider that supports custom TLS certificates, but more providers will be supported in the future.\\nLANGSMITH_PLAYGROUND_TLS_CA: The custom certificate authority (CA) certificate in PEM format. This must be a file path (for a mounted volume).\\n[Optional] LANGSMITH_PLAYGROUND_TLS_KEY: The private key in PEM format. This must be a file path (for a mounted volume). This is usually only necessary for mutual TLS authentication.\\n[Optional] LANGSMITH_PLAYGROUND_TLS_CERT: The certificate in PEM format. This must be a file path (for a mounted volume). This is usually only necessary for mutual TLS authentication.\\n\\nOnce you have set these environment variables, enter the LangSmith Playground Settings page and select the Provider that requires custom TLS certificates. Set your model provider configuration as usual, and the custom TLS certificates will be used when connecting to the model provider.\\n\\u200bMount internal CAs for TLS\\n\\nCreate a file containing all CAs required for TLS with databases and external services. If your deployment is communicating directly to beacon.langchain.com without a proxy, make sure to include a public trusted CA. All certs should be concatenated in this file with an empty line in between.\\n\\nCopy-----BEGIN CERTIFICATE-----\\n<PUBLIC_CA>\\n-----END CERTIFICATE-----\\n\\n-----BEGIN CERTIFICATE-----\\n<INTERNAL_CA>\\n-----END CERTIFICATE-----\\n\\n...\\n\\n\\nCreate a Kubernetes secret with a key containing the contents of this file.\\n\\nCopykubectl create secret generic <SECRET_NAME> --from-file=<SECRET_KEY>=<CA_BUNDLE_FILE_PATH> -n <NAMESPACE>\\n\\n\\nIf using custom CA for TLS with your databases and other external services, provide the following values to your LangSmith helm chart:\\n\\nHelmCopyconfig:\\n  customCa:\\n    secretName: <SECRET_NAME> # The name of the secret created in step 2.\\n    secretKey: <SECRET_KEY> # The key in the secret containing the CA bundle.\\n\\nclickhouse:\\n  external:\\n    tls: true # Only enable if you want TLS for Clickhouse.\\npostgres:\\n  external:\\n    customTls: true # Only enable if you want TLS for Postgres.\\n\\n\\nMake sure to use TLS supported connection strings:\\n\\nPostgres: Add ?sslmode=verify-full&sslrootcert=system to the end.\\nRedis: Use rediss:// instead of redis:// as the prefix.\\n\\n\\nWas this page helpful?YesNoSuggest editsCustomize user managementUse an existing secret for your installation (Kubernetes)âŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/self_hosting/configuration/external_clickhouse', 'loc': 'https://docs.smith.langchain.com/self_hosting/configuration/external_clickhouse', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Connect to an external ClickHouse database - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewSetupInstall on KubernetesInstall on DockerInteract with an installationUpgrade an installationConfigure egress for subscription metricsView trace counts across an organizationLangSmith-managed ClickHouseConfigurationConfigure for scaleEnable TTL & data retentionCreate an Ingress for installations (Kubernetes)Mirror images for your installationUse environment variables for model providersTroubleshootingAuthentication & access controlSet up basic authenticationSet up SSO with OAuth2.0 & OIDCCustomize user managementConfigure custom TLS certificatesUse an existing secret for your installation (Kubernetes)Connect external servicesEnable blob storageConnect to an external ClickHouse databaseConnect to an external PostgreSQL databaseConnect to an external Redis databaseScriptsDelete workspacesDelete organizationsDelete tracesGenerate ClickHouse StatsGenerate query statsRun support queries against PostgreSQLRun support queries against ClickHouseObservabilityExport LangSmith telemetry to your observability backendConfigure your collector for telemetryDeploy an observability stackOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationConnect external servicesConnect to an external ClickHouse databaseGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageRequirementsHA Replicated Clickhouse ClusterLangSmith-managed ClickHouseParametersConfigurationConnect external servicesConnect to an external ClickHouse databaseCopy pageCopy pageClickHouse is a high-performance, column-oriented database system. It allows for fast ingestion of data and is optimized for analytical queries.\\nLangSmith uses ClickHouse as the primary data store for traces and feedback. By default, self-hosted LangSmith will use an internal ClickHouse database that is bundled with the LangSmith instance. This is run as a stateful set in the same Kubernetes cluster as the LangSmith application or as a Docker container on the same host as the LangSmith application.\\nHowever, you can configure LangSmith to use an external ClickHouse database for easier management and scaling. By configuring an external ClickHouse database, you can manage backups, scaling, and other operational tasks for your database. While Clickhouse is not yet a native service in Azure, AWS, or Google Cloud, you can run LangSmith with an external ClickHouse database in the following ways:\\n\\n\\nLangSmith-managed ClickHouse\\n\\n\\nProvision a ClickHouse Cloud either directly or through a cloud provider marketplace:\\n\\nAzure Marketplace\\nGoogle Cloud Marketplace\\nAWS Marketplace\\n\\n\\n\\nOn a VM in your cloud provider\\n\\n\\nUsing the first two options (LangSmith-managed ClickHouse or ClickHouse Cloud) will provision a Clickhouse service OUTSIDE of your VPC. However, both options support private endpoints, meaning that you can direct traffic to the ClickHouse service without exposing it to the public internet (eg via AWS PrivateLink, or GCP Private Service Connect).Additionally, sensitive information can be configured to be not stored in Clickhouse. Please reach out to support@langchain.dev for more information.\\n\\u200bRequirements\\n\\nA provisioned ClickHouse instance that your LangSmith application will have network access to (see above for options).\\nA user with admin access to the ClickHouse database. This user will be used to create the necessary tables, indexes, and views.\\nWe support both standalone ClickHouse and externally managed clustered deployments. For clustered deployments, ensure all nodes are running the same version. Note that clustered setups are not supported with bundled ClickHouse installations.\\nWe only support ClickHouse versions >= 23.9. Use of ClickHouse versions >= 24.2 requires LangSmith v0.6 or later.\\nWe rely on a few configuration parameters to be set on your ClickHouse instance. These are detailed below:\\n\\nCopy<profiles>\\n  <default>\\n      <async_insert>1</async_insert> # Turn on async insert\\n      <async_insert_max_data_size>25000000</async_insert_max_data_size> # Flush data to disk after 25MB. You may need to adjust this based on your workload.\\n      <wait_for_async_insert>0</wait_for_async_insert> # Disable waiting for async insert by default\\n      <parallel_view_processing>1</parallel_view_processing> # Enable parallel view processing\\n      <materialize_ttl_after_modify>0</materialize_ttl_after_modify> # Disable TTL materialization after modify\\n      <wait_for_async_insert_timeout>120</wait_for_async_insert_timeout> # Set the timeout for waiting for async insert\\n      <lightweight_deletes_sync>0</lightweight_deletes_sync> # Disable lightweight deletes sync\\n      <allow_materialized_view_with_bad_select>1</allow_materialized_view_with_bad_select> # Allow materialized views with legacy SELECT statements that cause CH to fail\\n  </default>\\n</profiles>\\n\\nOur system has been tuned to work with the above configuration parameters. Changing these parameters may result in unexpected behavior.\\n\\u200bHA Replicated Clickhouse Cluster\\nBy default, the setup process above will only work with a single node Clickhouse cluster.\\nIf you would like to use a multi-node Clickhouse cluster for HA, we support this with additional required configuration. This setup can use a Clickhouse cluster with multiple nodes where data replicated via Zookeeper or Clickhouse Keeper. For more information on Clickhouse replication, see Clickhouse Data Replication Docs.\\nIn order to setup LangSmith with a replicated multi-node Clickhouse setup:\\n\\nYou need to have a Clickhouse cluster that is setup with Keeper or Zookeeper for data replication and the appropriate settings. See Clickhouse Replication Setup Docs.\\nYou need to set the cluster setting in the LangSmith Configuration section, specifically the cluster settings to match your Clickhouse Cluster name. This will use the Replicated table engines when running the Clickhouse migrations.\\nIf in addition to HA, you would like to load balance among the Clickhouse nodes (to distribute reads or writes), we suggest using a load balancer or DNS load balancing to round robin among your Clickhouse servers.\\nNote: You will need to enable your cluster setting before launching LangSmith for the first time and running the Clickhouse migrations. This is a requirement since the table engine will need to be created as a Replicated table engine vs the non replicated engine type.\\n\\nWhen running migrations with cluster enabled, the migration will create the Replicated table engine flavor. This means that data will be replicated among the servers in the cluster. This is a master-master setup where any server can process reads, writes, or merges.\\nFor an example setup of a replicated ClickHouse cluster, refer to the replicated ClickHouse section in the LangSmith Helm chart repo, under examples.\\n\\u200bLangSmith-managed ClickHouse\\n\\nIf using LangSmith-managed ClickHouse, you will need to set up a VPC peering connection between the LangSmith VPC and the ClickHouse VPC. Please reach out to support@langchain.dev for more information.\\nYou will also need to set up Blob Storage. You can read more about Blob Storage in the Blob Storage documentation.\\n\\nClickHouse installations managed by LangSmith use a SharedMerge engine, which automatically clusters them and separates compute from storage.\\nFor more information, refer to the managed ClickHouse page.\\n\\u200bParameters\\nYou will need to provide several parameters to your LangSmith installation to configure an external ClickHouse database. These parameters include:\\n\\nHost: The hostname or IP address of the ClickHouse database\\nHTTP Port: The port that the ClickHouse database listens on for HTTP connections\\nNative Port: The port that the ClickHouse database listens on for native connections\\nDatabase: The name of the ClickHouse database that LangSmith should use\\nUsername: The username to use to connect to the ClickHouse database\\nPassword: The password to use to connect to the ClickHouse database\\nCluster (Optional): The name of the ClickHouse cluster if using an external Clickhouse cluster. When set, LangSmith will run migrations on the cluster and replicate data across instances.\\n\\nImportant considerations for clustered deployments:\\n\\nClustered setups must be configured on a fresh schema - existing standalone ClickHouse instances cannot be converted to clustered mode.\\n\\n\\nClustering is only supported with externally managed ClickHouse deployments. It is not compatible with bundled ClickHouse installations as these do not include required ZooKeeper configurations.\\n\\n\\nWhen using a clustered deployment, LangSmith will automatically:\\n\\n\\nRun database migrations across all nodes in the cluster\\n\\n\\nConfigure tables for data replication across the cluster\\n\\nNote that while data is replicated across nodes, LangSmith does not configure distributed tables or handle query routing - queries will be directed to the specified host. You will need to handle any load balancing or query distribution at the infrastructure level if desired.\\n\\u200bConfiguration\\nWith these parameters in hand, you can configure your LangSmith instance to use the provisioned ClickHouse database. You can do this by modifying the config.yaml file for your LangSmith Helm Chart installation or the .env file for your Docker installation.\\nHelmDockerCopyclickhouse:\\n  external:\\n    enabled: true\\n    host: \"host\"\\n    port: \"http port\"\\n    nativePort: \"native port\"\\n    user: \"default\"\\n    password: \"password\"\\n    database: \"default\"\\n    tls: false\\n    cluster: \"my_cluster_name\"  # Optional: Set this if using an external Clickhouse cluster\\n\\nOnce configured, you should be able to reinstall your LangSmith instance. If everything is configured correctly, your LangSmith instance should now be using your external ClickHouse database.Was this page helpful?YesNoSuggest editsEnable blob storageConnect to an external PostgreSQL databaseâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/self_hosting/configuration/external_postgres', 'loc': 'https://docs.smith.langchain.com/self_hosting/configuration/external_postgres', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Connect to an external PostgreSQL database - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewSetupInstall on KubernetesInstall on DockerInteract with an installationUpgrade an installationConfigure egress for subscription metricsView trace counts across an organizationLangSmith-managed ClickHouseConfigurationConfigure for scaleEnable TTL & data retentionCreate an Ingress for installations (Kubernetes)Mirror images for your installationUse environment variables for model providersTroubleshootingAuthentication & access controlSet up basic authenticationSet up SSO with OAuth2.0 & OIDCCustomize user managementConfigure custom TLS certificatesUse an existing secret for your installation (Kubernetes)Connect external servicesEnable blob storageConnect to an external ClickHouse databaseConnect to an external PostgreSQL databaseConnect to an external Redis databaseScriptsDelete workspacesDelete organizationsDelete tracesGenerate ClickHouse StatsGenerate query statsRun support queries against PostgreSQLRun support queries against ClickHouseObservabilityExport LangSmith telemetry to your observability backendConfigure your collector for telemetryDeploy an observability stackOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationConnect external servicesConnect to an external PostgreSQL databaseGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageRequirementsConnection StringConfigurationConnect external servicesConnect to an external PostgreSQL databaseCopy pageCopy pageLangSmith uses a PostgreSQL database as the primary data store for transactional workloads and operational data (almost everything besides runs). By default, LangSmith Self-Hosted will use an internal PostgreSQL database. However, you can configure LangSmith to use an external PostgreSQL database (). By configuring an external PostgreSQL database, you can more easily manage backups, scaling, and other operational tasks for your database.\\n\\u200bRequirements\\n\\n\\nA provisioned PostgreSQL database that your LangSmith instance will have network access to. We recommend using a managed PostgreSQL service like:\\n\\nAmazon RDS\\nGoogle Cloud SQL\\nAzure Database for PostgreSQL\\n\\n\\n\\nNote: We only officially support PostgreSQL versions >= 14.\\n\\n\\nA user with admin access to the PostgreSQL database. This user will be used to create the necessary tables, indexes, and schemas.\\n\\n\\nThis user will also need to have the ability to create extensions in the database. We use/will try to install the btree_gin, btree_gist, pgcrypto, citext, and pg_trgm extensions.\\n\\n\\nIf using a schema other than public, ensure that you do not have any other schemas with the extensions enabled, or you must include that in your search path.\\n\\n\\nSupport for pgbouncer and other connection poolers is community-based. Community members have reported that pgbouncer has worked with pool_mode = session and a suitable setting for ignore_startup_parameters (as of writing, search_path and lock_timeout need to be ignored). Care is needed to avoid polluting connection pools; some level of PostgreSQL expertise is advisable. LangChain Inc currently does not have roadmap plans for formal test coverage or commercial support of pgbouncer or amazon rds proxy or any other poolers, but the community is welcome to discuss and collaborate on support through GitHub issues.\\n\\n\\nBy default, we recommend an instance with at least 2 vCPUs and 8GB of memory. However, the actual requirements will depend on your workload and the number of users you have. We recommend monitoring your PostgreSQL instance and scaling up as needed.\\n\\n\\n\\u200bConnection String\\nYou will need to provide a connection string to your PostgreSQL database. This connection string should include the following information:\\n\\nHost\\nPort\\nDatabase\\nUsername\\nPassword(Make sure to url encode this if there are any special characters)\\nURL params\\n\\nThis will take the form of:\\nCopyusername:password@host:port/database?<url_params>\\n\\nAn example connection string might look like:\\nCopymyuser:mypassword@myhost:5432/mydatabase?sslmode=disable\\n\\nWithout url parameters, the connection string would look like:\\nCopymyuser:mypassword@myhost:5432/mydatabase\\n\\n\\u200bConfiguration\\nWith your connection string in hand, you can configure your LangSmith instance to use an external PostgreSQL database. You can do this by modifying the values file for your LangSmith Helm Chart installation or the .env file for your Docker installation.\\nHelmDockerCopypostgres:\\n  external:\\n    enabled: true\\n    connectionUrl: \"Your connection url\"\\n\\nOnce configured, you should be able to reinstall your LangSmith instance. If everything is configured correctly, your LangSmith instance should now be using your external PostgreSQL database.Was this page helpful?YesNoSuggest editsConnect to an external ClickHouse databaseConnect to an external Redis databaseâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/self_hosting/configuration/external_redis', 'loc': 'https://docs.smith.langchain.com/self_hosting/configuration/external_redis', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Connect to an external Redis database - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewSetupInstall on KubernetesInstall on DockerInteract with an installationUpgrade an installationConfigure egress for subscription metricsView trace counts across an organizationLangSmith-managed ClickHouseConfigurationConfigure for scaleEnable TTL & data retentionCreate an Ingress for installations (Kubernetes)Mirror images for your installationUse environment variables for model providersTroubleshootingAuthentication & access controlSet up basic authenticationSet up SSO with OAuth2.0 & OIDCCustomize user managementConfigure custom TLS certificatesUse an existing secret for your installation (Kubernetes)Connect external servicesEnable blob storageConnect to an external ClickHouse databaseConnect to an external PostgreSQL databaseConnect to an external Redis databaseScriptsDelete workspacesDelete organizationsDelete tracesGenerate ClickHouse StatsGenerate query statsRun support queries against PostgreSQLRun support queries against ClickHouseObservabilityExport LangSmith telemetry to your observability backendConfigure your collector for telemetryDeploy an observability stackOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationConnect external servicesConnect to an external Redis databaseGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageRequirementsConnection StringConfigurationConnect external servicesConnect to an external Redis databaseCopy pageCopy pageLangSmith uses Redis to back our queuing/caching operations. By default, LangSmith Self-Hosted will use an internal Redis instance. However, you can configure LangSmith to use an external Redis instance. By configuring an external Redis instance, you can more easily manage backups, scaling, and other operational tasks for your Redis instance.\\n\\u200bRequirements\\n\\n\\nA provisioned Redis instance that your LangSmith instance will have network access to. We recommend using a managed Redis service like:\\n\\nAmazon ElastiCache\\nGoogle Cloud Memorystore\\nAzure Cache for Redis\\n\\n\\n\\nNote: We only officially support Redis versions >= 5.\\n\\n\\nWe do not support Redis Cluster.\\n\\n\\nBy default, we recommend an instance with at least 2 vCPUs and 8GB of memory. However, the actual requirements will depend on your tracing workload. We recommend monitoring your Redis instance and scaling up as needed.\\n\\n\\nCertain tiers of managed Redis services may use Redis Cluster under the hood, but you can point to a single node in the cluster. For example on Azure Cache for Redis, the Premium tier and above use Redis Cluster, so you will need to use a lower tier.\\n\\u200bConnection String\\nWe use redis-py to connect to Redis. This library supports a variety of connection strings. You can find more information on the connection string format here.\\nYou will need to assemble the connection string for your Redis instance. This connection string should include the following information:\\n\\nHost\\nDatabase\\nPort\\nURL params\\n\\nThis will take the form of:\\nCopy\"redis://host:port/db?<url_params>\"\\n\\nAn example connection string might look like:\\nCopy\"redis://langsmith-redis:6379/0\"\\n\\nTo use SSL, you can use the rediss:// prefix. An example connection string with SSL might look like:\\nCopy\"rediss://langsmith-redis:6380/0?password=foo\"\\n\\n\\u200bConfiguration\\nWith your connection string in hand, you can configure your LangSmith instance to use an external Redis instance. You can do this by modifying the values file for your LangSmith Helm Chart installation or the .env file for your Docker installation.\\nHelmDockerCopyredis:\\n  external:\\n    enabled: true\\n    connectionUrl: \"Your connection url\"\\n\\nOnce configured, you should be able to reinstall your LangSmith instance. If everything is configured correctly, your LangSmith instance should now be using your external Redis instance.Was this page helpful?YesNoSuggest editsConnect to an external PostgreSQL databaseDelete workspacesâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/self_hosting/configuration/ingress', 'loc': 'https://docs.smith.langchain.com/self_hosting/configuration/ingress', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Create an Ingress for installations (Kubernetes) - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewSetupInstall on KubernetesInstall on DockerInteract with an installationUpgrade an installationConfigure egress for subscription metricsView trace counts across an organizationLangSmith-managed ClickHouseConfigurationConfigure for scaleEnable TTL & data retentionCreate an Ingress for installations (Kubernetes)Mirror images for your installationUse environment variables for model providersTroubleshootingAuthentication & access controlSet up basic authenticationSet up SSO with OAuth2.0 & OIDCCustomize user managementConfigure custom TLS certificatesUse an existing secret for your installation (Kubernetes)Connect external servicesEnable blob storageConnect to an external ClickHouse databaseConnect to an external PostgreSQL databaseConnect to an external Redis databaseScriptsDelete workspacesDelete organizationsDelete tracesGenerate ClickHouse StatsGenerate query statsRun support queries against PostgreSQLRun support queries against ClickHouseObservabilityExport LangSmith telemetry to your observability backendConfigure your collector for telemetryDeploy an observability stackOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationConfigurationCreate an Ingress for installations (Kubernetes)Get startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageRequirementsParametersConfigurationConfigurationCreate an Ingress for installations (Kubernetes)Copy pageCopy pageBy default, LangSmith will provision a LoadBalancer service for the langsmith-frontend. Depending on your cloud provider, this may result in a public IP address being assigned to the service. If you would like to use a custom domain or have more control over the routing of traffic to your LangSmith installation, you can configure an Ingress.\\n\\u200bRequirements\\n\\nAn existing Kubernetes cluster\\nAn existing Ingress Controller installed in your Kubernetes cluster\\n\\n\\u200bParameters\\nYou may need to provide certain parameters to your LangSmith installation to configure the Ingress. Additionally, we will want to convert the langsmith-frontend service to a ClusterIP service.\\n\\n\\nHostname (optional): The hostname that you would like to use for your LangSmith installation. E.g \"langsmith.example.com\". If you leave this empty, the ingress will serve all traffic to the LangSmith installation.\\n\\n\\nSubdomain (optional): If you would like to serve LangSmith under a URL path, you can specify it here. For example, adding \"langsmith\" will serve the application at \"example.hostname.com/langsmith\". This will apply to UI paths as well as API endpoints.\\n\\n\\nIngressClassName (optional): The name of the Ingress class that you would like to use. If not set, the default Ingress class will be used.\\n\\n\\nAnnotations (optional): Additional annotations to add to the Ingress. Certain providers like AWS may use annotations to control things like TLS termination.\\nFor example, you can add the following annotations using the AWS ALB Ingress Controller to attach an ACM certificate to the Ingress:\\nCopyannotations:\\n  alb.ingress.kubernetes.io/certificate-arn: \"<your-certificate-arn>\"\\n\\n\\n\\nLabels (optional): Additional labels to add to the Ingress.\\n\\n\\nTLS (optional): If you would like to serve LangSmith over HTTPS, you can add TLS configuration here (many Ingress controllers may have other ways of controlling TLS so this is often not needed). This should be an array of TLS configurations. Each TLS configuration should have the following fields:\\n\\n\\nhosts: An array of hosts that the certificate should be valid for. E.g [â€œlangsmith.example.comâ€]\\n\\n\\nsecretName: The name of the Kubernetes secret that contains the certificate and private key. This secret should have the following keys:\\n\\ntls.crt: The certificate\\ntls.key: The private key\\n\\n\\n\\nYou can read more about creating a TLS secret here.\\n\\n\\n\\n\\n\\u200bConfiguration\\nWith these parameters in hand, you can configure your LangSmith instance to use an Ingress. You can do this by modifying the config.yaml file for your LangSmith Helm Chart installation.\\nCopyingress:\\n  enabled: true\\n  hostname: \"\" # Main domain for LangSmith\\n  subdomain: \"\" # If you want to serve langsmith on a subdomain\\n  ingressClassName: \"\" # If not set, the default ingress class will be used\\n  annotations: {} # Add annotations here if needed\\n  labels: {} # Add labels here if needed\\n  tls: [] # Add TLS configuration here if needed\\nfrontend:\\n  service:\\n    type: ClusterIP\\n\\nOnce configured, you will need to update your LangSmith installation. If everything is configured correctly, your LangSmith instance should now be accessible via the Ingress. You can run the following to check the status of your Ingress:\\nCopykubectl get ingress\\n\\nYou should see something like this in the output:\\nCopyNAME                         CLASS   HOSTS    ADDRESS          PORTS     AGE\\nlangsmith-ingress            nginx   <host>   35.227.243.203   80, 443   95d\\n\\nIf you do not have automated DNS setup, you will need to add the IP address to your DNS provider manually.Was this page helpful?YesNoSuggest editsEnable TTL & data retentionMirror images for your installationâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/self_hosting/configuration/mirroring_images', 'loc': 'https://docs.smith.langchain.com/self_hosting/configuration/mirroring_images', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Mirror images for your LangSmith installation - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewSetupInstall on KubernetesInstall on DockerInteract with an installationUpgrade an installationConfigure egress for subscription metricsView trace counts across an organizationLangSmith-managed ClickHouseConfigurationConfigure for scaleEnable TTL & data retentionCreate an Ingress for installations (Kubernetes)Mirror images for your installationUse environment variables for model providersTroubleshootingAuthentication & access controlSet up basic authenticationSet up SSO with OAuth2.0 & OIDCCustomize user managementConfigure custom TLS certificatesUse an existing secret for your installation (Kubernetes)Connect external servicesEnable blob storageConnect to an external ClickHouse databaseConnect to an external PostgreSQL databaseConnect to an external Redis databaseScriptsDelete workspacesDelete organizationsDelete tracesGenerate ClickHouse StatsGenerate query statsRun support queries against PostgreSQLRun support queries against ClickHouseObservabilityExport LangSmith telemetry to your observability backendConfigure your collector for telemetryDeploy an observability stackOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationConfigurationMirror images for your LangSmith installationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageRequirementsMirroring the ImagesConfigurationConfigurationMirror images for your LangSmith installationCopy pageCopy pageBy default, LangSmith will pull images from our public Docker registry. However, if you are running LangSmith in an environment that does not have internet access, or if you would like to use a private Docker registry, you can mirror the images to your own registry and then configure your LangSmith installation to use those images.\\n\\u200bRequirements\\n\\nAuthenticated access to a Docker registry that your Kubernetes cluster/machine has access to.\\nDocker installed on your local machine or a machine that has access to the Docker registry.\\nA Kubernetes cluster or a machine where you can run LangSmith.\\n\\n\\u200bMirroring the Images\\nFor your convenience, we have provided a script that will mirror the images for you. You can find the script in the LangSmith Helm Chart repository\\nTo use the script, you will need to run the script with the following command specifying your registry and platform:\\nCopybash mirror_images.sh <your-registry> [<platform>]\\n\\nWhere <your-registry> is the URL of your Docker registry (e.g. myregistry.com) and <platform> is the platform you are using (e.g. linux/amd64, linux/arm64, etc.). If you do not specify a platform, it will default to linux/amd64.\\nFor example, if your registry is myregistry.com, your platform is linux/arm64, and you want to use the latest version of the images, you would run:\\nCopybash mirror_langsmith_images.sh --registry myregistry --platform linux/arm64 --version 0.10.66\\n\\nNote that this script will assume that you have Docker installed and that you are authenticated to your registry. It will also push the images to the specified registry with the same repository/tag as the original images.\\nAlternatively, you can pull, mirror, and push the images manually. The images that you will need to mirror are found in the values.yaml file of the LangSmith Helm Chart. These can be found here: LangSmith Helm Chart values.yaml\\nHere is an example of how to mirror the images using Docker:\\nCopy# Pull the images from the public registry\\ndocker pull langchain/langsmith-backend:latest\\ndocker tag langchain/langsmith-backend:latest <your-registry>/langsmith-backend:latest\\ndocker push <your-registry>/langsmith-backend:latest\\n\\nYou will need to repeat this for each image that you want to mirror.\\n\\u200bConfiguration\\nOnce the images are mirrored, you will need to configure your LangSmith installation to use the mirrored images. You can do this by modifying the values.yaml file for your LangSmith Helm Chart installation or the .env file for your Docker installation. Replace tag with the version you want to use, e.g. 0.10.66 for the latest version at the time of writing.\\nHelmDockerCopyimages:\\n  imagePullSecrets: [] # Add your image pull secrets here if needed\\n  registry: \"\" # Set this to your registry URL if you mirrored all images to the same registry using our script. Then you can remove the repository prefix from the images below.\\n  aceBackendImage:\\n    repository: \"(your-registry)/langchain/langsmith-ace-backend\"\\n    pullPolicy: IfNotPresent\\n    tag: \"0.10.66\"\\n  backendImage:\\n    repository: \"(your-registry)/langchain/langsmith-backend\"\\n    pullPolicy: IfNotPresent\\n    tag: \"0.10.66\"\\n  frontendImage:\\n    repository: \"(your-registry)/langchain/langsmith-frontend\"\\n    pullPolicy: IfNotPresent\\n    tag: \"0.10.66\"\\n  hostBackendImage:\\n    repository: \"(your-registry)/langchain/hosted-langserve-backend\"\\n    pullPolicy: IfNotPresent\\n    tag: \"0.10.66\"\\n  operatorImage:\\n    repository: \"(your-registry)/langchain/langgraph-operator\"\\n    pullPolicy: IfNotPresent\\n    tag: \"6cc83a8\"\\n  platformBackendImage:\\n    repository: \"(your-registry)/langchain/langsmith-go-backend\"\\n    pullPolicy: IfNotPresent\\n    tag: \"0.10.66\"\\n  playgroundImage:\\n    repository: \"(your-registry)/langchain/langsmith-playground\"\\n    pullPolicy: IfNotPresent\\n    tag: \"0.10.66\"\\n  postgresImage:\\n    repository: \"(your-registry)/postgres\"\\n    pullPolicy: IfNotPresent\\n    tag: \"14.7\"\\n  redisImage:\\n    repository: \"(your-registry)/redis\"\\n    pullPolicy: IfNotPresent\\n    tag: \"7\"\\n  clickhouseImage:\\n    repository: \"(your-registry)/clickhouse/clickhouse-server\"\\n    pullPolicy: Always\\n    tag: \"24.8\"\\n\\nOnce configured, you will need to update your LangSmith installation. You can follow our upgrade guide here: Upgrading LangSmith.If your upgrade is successful, your LangSmith instance should now be using the mirrored images from your Docker registry.Was this page helpful?YesNoSuggest editsCreate an Ingress for installations (Kubernetes)Use environment variables for model providersâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/self_hosting/configuration/playground_environment_settings', 'loc': 'https://docs.smith.langchain.com/self_hosting/configuration/playground_environment_settings', 'changefreq': 'weekly', 'priority': '0.5'}, page_content=\"Page Not FoundSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsCreate and update promptsCreate a promptManage promptsManage prompts programmaticallyConfigure prompt settingsUse tools in a promptInclude multimodal content in a promptWrite your prompt with AIConnect to modelsTutorialsOptimize a classifierSync prompts with GitHubTest multi-turn conversationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationPage Not FoundGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForum404Page Not FoundWe couldn't find the page you were looking for. Maybe you were looking for?Use environment variables for model providersConfigure prompt settingsArchitectural overviewAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/self_hosting/configuration/scale', 'loc': 'https://docs.smith.langchain.com/self_hosting/configuration/scale', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Configure LangSmith for scale - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewSetupInstall on KubernetesInstall on DockerInteract with an installationUpgrade an installationConfigure egress for subscription metricsView trace counts across an organizationLangSmith-managed ClickHouseConfigurationConfigure for scaleEnable TTL & data retentionCreate an Ingress for installations (Kubernetes)Mirror images for your installationUse environment variables for model providersTroubleshootingAuthentication & access controlSet up basic authenticationSet up SSO with OAuth2.0 & OIDCCustomize user managementConfigure custom TLS certificatesUse an existing secret for your installation (Kubernetes)Connect external servicesEnable blob storageConnect to an external ClickHouse databaseConnect to an external PostgreSQL databaseConnect to an external Redis databaseScriptsDelete workspacesDelete organizationsDelete tracesGenerate ClickHouse StatsGenerate query statsRun support queries against PostgreSQLRun support queries against ClickHouseObservabilityExport LangSmith telemetry to your observability backendConfigure your collector for telemetryDeploy an observability stackOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationConfigurationConfigure LangSmith for scaleGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageSummaryTrace ingestion (write path)Trace querying (read path)Example LangSmith configurations for scaleLow reads, low writes Low reads, high writes High reads, low writes Medium reads, medium writes High reads, high writes ConfigurationConfigure LangSmith for scaleCopy pageCopy pageA self-hosted LangSmith instance can handle a large number of traces and users. The default configuration for the self-hosted deployment can handle substantial load, and you can configure your deployment to be able to achieve higher scale. This page describes scaling considerations and provides some examples to help configure your self-hosted instance.\\nFor example configurations, refer to Example LangSmith configurations for scale.\\n\\u200bSummary\\nThe table below provides an overview comparing different LangSmith configurations for various load patterns (reads / writes):\\nLow / lowLow / highHigh / lowMedium / mediumHigh / highConcurrent frontend users55502050Traces submitted per second101000101001000Frontend replicas1 (default)4224Platform backend replicas3 (default)203 (default)3 (default)20Queue replicas3 (default)160610160Backend replicas2 (default)5401650Redis resources8 Gi (default)200 Gi external8 Gi (default)13Gi external200 Gi externalClickHouse resources4 CPU16 Gi (default)10 CPU32Gi memory8 CPU16 Gi per replica16 CPU24Gi memory14 CPU24 Gi per replicaClickHouse setupSingle instanceSingle instance3-node replicated clusterSingle instance3-node replicated clusterPostgres resources2 CPU8 GB memory10GB storage (external)2 CPU8 GB memory10GB storage (external)2 CPU8 GB memory10GB storage (external)2 CPU8 GB memory10GB storage (external)2 CPU8 GB memory10GB storage (external)Blob storageDisabledEnabledEnabledEnabledEnabled\\nBelow we go into more details about the read and write paths as well as provide a values.yaml snippet for you to start with for your self-hosted LangSmith instance.\\n\\u200bTrace ingestion (write path)\\nCommon usage that put load on the write path:\\n\\nIngesting traces via the Python or JavaScript LangSmith SDK\\nIngesting traces via the @traceable wrapper\\nSubmitting traces via the /runs/multipart endpoint\\n\\nServices that play a large role in trace ingestion:\\n\\nPlatform backend service: Receives initial request to ingest traces and places traces on a Redis queue\\nRedis cache: Used to queue traces that need to be persisted\\nQueue service: Persists traces for querying\\nClickHouse: Persistent storage used for traces\\n\\nWhen scaling up the write path (trace ingestion), it is helpful to monitor the four services/resources listed above. Here are some typical changes that can help increase performance of trace ingestion:\\n\\nGive ClickHouse more resources (CPU and memory) if it is approaching resource limits.\\nIncrease the number of platform-backend pods if ingest requests are taking long to respond.\\nIncrease queue service pod replicas if traces are not being processed from Redis fast enough.\\nUse a larger Redis cache if you notice that the current Redis instance is reaching resource limits. This could also be a reason why ingest requests take a long time.\\n\\n\\u200bTrace querying (read path)\\nCommon usage that puts load on the read path:\\n\\nUsers on the frontend looking at tracing projects or individual traces\\nScripts used to query for trace info\\nHitting either the /runs/query or /runs/<run-id> api endpoints\\n\\nServices that play a large role in querying traces:\\n\\nBackend service: Receives the request and submits a query to ClickHouse to then respond to the request\\nClickHouse: Persistent storage for traces. This is the main database that is queried when requesting trace info.\\n\\nWhen scaling up the read path (trace querying), it is helpful to monitor the two services/resources listed above. Here are some typical changes that can help improve performance of trace querying:\\n\\nIncrease the number of backend service pods. This would be most impactful if backend service pods are reaching 1 core CPU usage.\\nGive ClickHouse more resources (CPU or Memory). ClickHouse can be very resource intensive, but it should lead to better performance.\\nMove to a replicated ClickHouse cluster. Adding replicas of ClickHouse helps with read performance, but we recommend staying below 5 replicas (start with 3).\\n\\nFor more precise guidance on how this translates to helm chart values, refer to the examples the following section. If you are unsure why your LangSmith instance cannot handle a certain load pattern, contact the LangChain team.\\n\\u200bExample LangSmith configurations for scale\\nBelow we provide some example LangSmith configurations based on expected read and write loads.\\nFor read load (trace querying):\\n\\nLow means roughly 5 users looking at traces at a time (about 10 requests per second)\\nMedium means roughly 20 users looking at traces at a time (about 40 requests per second)\\nHigh means roughly 50 users looking at traces at a time (about 100 requests per second)\\n\\nFor write load (trace ingestion):\\n\\nLow means up to 10 traces submitted per second\\nMedium means up to 100 traces submitted per second\\nHigh means up to 1000 traces submitted per second\\n\\nThe exact optimal configuration depends on your usage and trace payloads. Use the examples below in combination with the information above and your specific usage to update your LangSmith configuration as you see fit. If you have any questions, please reach out to the LangChain team.\\n\\u200bLow reads, low writes \\nThe default LangSmith configuration will handle this load. No custom resource configuration is needed here.\\n\\u200bLow reads, high writes \\nYou have a very high scale of trace ingestions, but single digit number of users on the frontend querying traces at any one time.\\nFor this, we recommend a configuration like this:\\nCopyconfig:\\n  blobStorage:\\n    # Please also set the other keys to connect to your blob storage. See configuration section.\\n    enabled: true\\n  settings:\\n    redisRunsExpirySeconds: \"3600\"\\n# ttl:\\n#   enabled: true\\n#   ttl_period_seconds:\\n#     longlived: \"7776000\"  # 90 days (default is 400 days)\\n#     shortlived: \"604800\"  # 7 days (default is 14 days)\\n\\nfrontend:\\n  deployment:\\n    replicas: 4 # OR enable autoscaling to this level (example below)\\n# autoscaling:\\n#   enabled: true\\n#   maxReplicas: 4\\n#   minReplicas: 2\\n\\nplatformBackend:\\n  deployment:\\n    replicas: 20 # OR enable autoscaling to this level (example below)\\n# autoscaling:\\n#   enabled: true\\n#   maxReplicas: 20\\n#   minReplicas: 8\\n\\n## Note that we are actively working on improving performance of this service to reduce the number of replicas.\\nqueue:\\n  deployment:\\n    replicas: 160 # OR enable autoscaling to this level (example below)\\n# autoscaling:\\n#   enabled: true\\n#   maxReplicas: 160\\n#   minReplicas: 40\\n\\nbackend:\\n  deployment:\\n    replicas: 5 # OR enable autoscaling to this level (example below)\\n# autoscaling:\\n#   enabled: true\\n#   maxReplicas: 5\\n#   minReplicas: 3\\n\\n## Ensure your Redis cache is at least 200 GB\\nredis:\\n  external:\\n    enabled: true\\n    existingSecretName: langsmith-redis-secret # Set the connection url for your external Redis instance (200+ GB)\\n\\nclickhouse:\\n  statefulSet:\\n    persistence:\\n      # This may depend on your configured TTL (see config section).\\n      # We recommend 600Gi for every shortlived TTL day if operating at this scale constantly.\\n      size: 4200Gi # This assumes 7 days TTL and operating a this scale constantly.\\n    resources:\\n      requests:\\n        cpu: \"10\"\\n        memory: \"32Gi\"\\n      limits:\\n        cpu: \"16\"\\n        memory: \"48Gi\"\\n\\ncommonEnv:\\n  - name: \"CLICKHOUSE_ASYNC_INSERT_WAIT_PCT_FLOAT\"\\n    value: \"0\"\\n\\n\\u200bHigh reads, low writes \\nYou have a relatively low scale of trace ingestions, but many frontend users querying traces and/or have scripts that hit the /runs/query or /runs/<run-id> endpoints frequently.\\nFor this, we strongly recommend setting up a replicated ClickHouse cluster to enable high read scale at low latency. See our external ClickHouse doc for more guidance on how to setup a replicated ClickHouse cluster. For this load pattern, we recommend using a 3 node replicated setup, where each replica in the cluster should have resource requests of 8+ cores and 16+ GB memory, and resource limit of 12 cores and 32 GB memory.\\nFor this, we recommend a configuration like this:\\nCopyconfig:\\n  blobStorage:\\n    # Please also set the other keys to connect to your blob storage. See configuration section.\\n    enabled: true\\n\\nfrontend:\\n  deployment:\\n    replicas: 2\\n\\nqueue:\\n  deployment:\\n    replicas: 6 # OR enable autoscaling to this level (example below)\\n# autoscaling:\\n#   enabled: true\\n#   maxReplicas: 6\\n#   minReplicas: 4\\n\\nbackend:\\n  deployment:\\n    replicas: 40 # OR enable autoscaling to this level (example below)\\n# autoscaling:\\n#   enabled: true\\n#   maxReplicas: 40\\n#   minReplicas: 16\\n\\n# We strongly recommend setting up a replicated clickhouse cluster for this load.\\n# Update these values as needed to connect to your replicated clickhouse cluster.\\nclickhouse:\\n  external:\\n    # If using a 3 node replicated setup, each replica in the cluster should have resource requests of 8+ cores and 16+ GB memory, and resource limit of 12 cores and 32 GB memory.\\n    enabled: true\\n    host: langsmith-ch-clickhouse-replicated.default.svc.cluster.local\\n    port: \"8123\"\\n    nativePort: \"9000\"\\n    user: \"default\"\\n    password: \"password\"\\n    database: \"default\"\\n    cluster: \"replicated\"\\n\\n\\u200bMedium reads, medium writes \\nThis is a good all around configuration that should be able to handle most usage patterns of LangSmith. In internal testing, this configuration allowed us to scale to 100 traces ingested per second and 40 read requests per second.\\nFor this, we recommend a configuration like this:\\nCopyconfig:\\n  blobStorage:\\n    # Please also set the other keys to connect to your blob storage. See configuration section.\\n    enabled: true\\n  settings:\\n    redisRunsExpirySeconds: \"3600\"\\n\\nfrontend:\\n  deployment:\\n    replicas: 2\\n\\nqueue:\\n  deployment:\\n    replicas: 10 # OR enable autoscaling to this level (example below)\\n# autoscaling:\\n#   enabled: true\\n#   maxReplicas: 10\\n#   minReplicas: 5\\n\\nbackend:\\n  deployment:\\n    replicas: 16 # OR enable autoscaling to this level (example below)\\n# autoscaling:\\n#   enabled: true\\n#   maxReplicas: 16\\n#   minReplicas: 8\\n\\nredis:\\n  statefulSet:\\n    resources:\\n      requests:\\n        memory: 13Gi\\n      limits:\\n        memory: 13Gi\\n\\n  # -- For external redis instead use something like below --\\n  # external:\\n  #   enabled: true\\n  #   connectionUrl: \"<URL>\" OR existingSecretName: \"<SECRET-NAME>\"\\n\\nclickhouse:\\n  statefulSet:\\n    persistence:\\n      # This may depend on your configured TTL.\\n      # We recommend 60Gi for every shortlived TTL day if operating at this scale constantly.\\n      size: 420Gi # This assumes 7 days TTL and operating a this scale constantly.\\n    resources:\\n      requests:\\n        cpu: \"16\"\\n        memory: \"24Gi\"\\n      limits:\\n        cpu: \"28\"\\n        memory: \"40Gi\"\\n\\ncommonEnv:\\n  - name: \"CLICKHOUSE_ASYNC_INSERT_WAIT_PCT_FLOAT\"\\n    value: \"0\"\\n\\nIf you still notice slow reads with the above configuration, we recommend moving to a replicated Clickhouse cluster setup\\n\\u200bHigh reads, high writes \\nYou have a very high rate of trace ingestion (approaching 1000 traces submitted per second) and also have many users querying traces on the frontend (over 50 users) and/or scripts that are consistently making requests to /runs/query or /runs/<run-id> endpoints.\\nFor this, we very strongly recommend setting up a replicated ClickHouse cluster to prevent degraded read performance at high write scale. See our external ClickHouse doc for more guidance on how to set up a replicated ClickHouse cluster. For this load pattern, we recommend using a 3 node replicated setup, where each replica in the cluster should have resource requests of 14+ cores and 24+ GB memory, and resource limit of 20 cores and 48 GB memory. We also recommend that each node/instance of ClickHouse has 600 Gi of volume storage for each day of TTL that you enable (as per the configuration below).\\nOverall, we recommend a configuration like this:\\nCopyconfig:\\n  blobStorage:\\n    # Please also set the other keys to connect to your blob storage. See configuration section.\\n    enabled: true\\n  settings:\\n    redisRunsExpirySeconds: \"3600\"\\n# ttl:\\n#   enabled: true\\n#   ttl_period_seconds:\\n#     longlived: \"7776000\"  # 90 days (default is 400 days)\\n#     shortlived: \"604800\"  # 7 days (default is 14 days)\\n\\nfrontend:\\n  deployment:\\n    replicas: 4 # OR enable autoscaling to this level (example below)\\n# autoscaling:\\n#   enabled: true\\n#   maxReplicas: 4\\n#   minReplicas: 2\\n\\nplatformBackend:\\n  deployment:\\n    replicas: 20 # OR enable autoscaling to this level (example below)\\n# autoscaling:\\n#   enabled: true\\n#   maxReplicas: 20\\n#   minReplicas: 8\\n\\n## Note that we are actively working on improving performance of this service to reduce the number of replicas.\\nqueue:\\n  deployment:\\n    replicas: 160 # OR enable autoscaling to this level (example below)\\n# autoscaling:\\n#   enabled: true\\n#   maxReplicas: 160\\n#   minReplicas: 40\\n\\nbackend:\\n  deployment:\\n    replicas: 50 # OR enable autoscaling to this level (example below)\\n# autoscaling:\\n#   enabled: true\\n#   maxReplicas: 50\\n#   minReplicas: 20\\n\\n## Ensure your Redis cache is at least 200 GB\\nredis:\\n  external:\\n    enabled: true\\n    existingSecretName: langsmith-redis-secret # Set the connection url for your external Redis instance (200+ GB)\\n\\n# We strongly recommend setting up a replicated clickhouse cluster for this load.\\n# Update these values as needed to connect to your replicated clickhouse cluster.\\nclickhouse:\\n  external:\\n    # If using a 3 node replicated setup, each replica in the cluster should have resource requests of 14+ cores and 24+ GB memory, and resource limit of 20 cores and 48 GB memory.\\n    enabled: true\\n    host: langsmith-ch-clickhouse-replicated.default.svc.cluster.local\\n    port: \"8123\"\\n    nativePort: \"9000\"\\n    user: \"default\"\\n    password: \"password\"\\n    database: \"default\"\\n    cluster: \"replicated\"\\n\\ncommonEnv:\\n  - name: \"CLICKHOUSE_ASYNC_INSERT_WAIT_PCT_FLOAT\"\\n    value: \"0\"\\n\\nEnsure that the Kubernetes cluster is configured with sufficient resources to scale to the recommended size. After deployment, all of the pods in the Kubernetes cluster should be in a Running state. Pods stuck in Pending may indicate that you are reaching node pool limits or need larger nodes.Also, ensure that any ingress controller deployed on the cluster is able to handle the desired load to prevent bottlenecks.Was this page helpful?YesNoSuggest editsLangSmith-managed ClickHouseEnable TTL & data retentionâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/self_hosting/configuration/sso', 'loc': 'https://docs.smith.langchain.com/self_hosting/configuration/sso', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Set up SSO with OAuth2.0 and OIDC - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewSetupInstall on KubernetesInstall on DockerInteract with an installationUpgrade an installationConfigure egress for subscription metricsView trace counts across an organizationLangSmith-managed ClickHouseConfigurationConfigure for scaleEnable TTL & data retentionCreate an Ingress for installations (Kubernetes)Mirror images for your installationUse environment variables for model providersTroubleshootingAuthentication & access controlSet up basic authenticationSet up SSO with OAuth2.0 & OIDCCustomize user managementConfigure custom TLS certificatesUse an existing secret for your installation (Kubernetes)Connect external servicesEnable blob storageConnect to an external ClickHouse databaseConnect to an external PostgreSQL databaseConnect to an external Redis databaseScriptsDelete workspacesDelete organizationsDelete tracesGenerate ClickHouse StatsGenerate query statsRun support queries against PostgreSQLRun support queries against ClickHouseObservabilityExport LangSmith telemetry to your observability backendConfigure your collector for telemetryDeploy an observability stackOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationAuthentication & access controlSet up SSO with OAuth2.0 and OIDCGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageOverviewWith Client Secret (Recommended)PrerequisitesConfigurationSession length controlsOverride Sub ClaimGoogle Workspace IdP setupOkta IdP setupSupported featuresConfiguration stepsSP-initiated SSOWithout Client Secret (PKCE) (Deprecated)RequirementsAuthentication & access controlSet up SSO with OAuth2.0 and OIDCCopy pageCopy pageLangSmith Self-Hosted provides SSO via OAuth2.0 and OIDC. This will delegate authentication to your Identity Provider (IdP) to manage access to LangSmith.\\nOur implementation supports almost anything that is OIDC compliant, with a few exceptions. Once configured, you will see a login screen like this:\\n\\n\\u200bOverview\\nYou may upgrade a basic auth installation to this mode, but not a none auth installation. In order to upgrade, simply remove the basic auth configuration and add the required configuration parameters as shown below. Users may then login via OAuth only. In order to maintain access post-upgrade, you must have access to login via OAuth using an email address that previously logged in via basic auth.\\nLangSmith does not support moving from SSO to basic auth mode in self-hosted at the moment. We also do not support moving from OAuth Mode with client secret to OAuth mode without a client secret and vice versa. Finally, we do not support having both basic auth and OAuth at the same time. Ensure you disable the basic auth configuration when enabling OAuth.\\n\\u200bWith Client Secret (Recommended)\\nBy default, LangSmith Self-Hosted supports the Authorization Code flow with Client Secret. In this version of the flow, your client secret is stored security in the LangSmith platform (not on the frontend) and used for authentication and establishing auth sessions.\\n\\u200bPrerequisites\\n\\nYou must be self-hosted and on an Enterprise plan.\\nYour IdP must support the Authorization Code flow with Client Secret.\\nYour IdP must support using an external discovery/issuer URL. We will use this to fetch the necessary routes and keys for your IdP.\\nYou must provide the OIDC, email, and profile scopes to LangSmith. We use these to fetch the necessary user information and email for your users.\\n\\nLangSmith SSO is only supported over https.\\n\\u200bConfiguration\\n\\nYou will need to set the callback URL in your IdP to https://<host>/api/v1/oauth/custom-oidc/callback, where host is the domain or IP you have provisioned for your LangSmith instance. This is where your IdP will redirect the user after they have authenticated.\\nYou will need to provide the oauthClientId, oauthClientSecret, hostname, and oauthIssuerUrl in your values.yaml file. This is where you will configure your LangSmith instance.\\nIf you have not already configured Oauth with client secret or if you only have personal orgs, you must provide an email address to assign as the initial org admin for the newly provisioned SSO org. If you are upgrading from basic auth, your existing org will be reused instead.\\n\\nHelmDockerCopyconfig:\\n  authType: mixed\\n  hostname: https://langsmith.example.com\\n  initialOrgAdminEmail: test@email.com # Set this if required\\n  oauth:\\n    enabled: true\\n    oauthClientId: <YOUR CLIENT ID>\\n    oauthClientSecret: <YOUR CLIENT SECRET>\\n    oauthIssuerUrl: <YOUR DISCOVERY URL>\\n    oauthScopes: \"email,profile,openid\"\\n\\n\\u200bSession length controls\\nAll of the environment variables in this section are for the platform-backend service and can be added using platformBackend.deployment.extraEnv in Helm.\\n\\nBy default, session length is controlled by the expiration of the identity token returned by the identity provider\\nMost setups should use refresh tokens to enable session length extension beyond the identity token expiration up to OAUTH_SESSION_MAX_SEC, which may require including the offline_access scope by adding to oauthScopes (Helm) or OAUTH_SCOPES (Docker)\\nOAUTH_SESSION_MAX_SEC (default 1 day) can be overridden to a maximum of one week (604800)\\nFor identity provider setups that donâ€™t support refresh tokens, setting OAUTH_OVERRIDE_TOKEN_EXPIRY=\"true\" will take OAUTH_SESSION_MAX_SEC as the session length, ignoring the identity token expiration\\n\\n\\u200bOverride Sub Claim\\nIn some scenarios, it may be necessary to override which claim is used as the sub claim from your identity provider.\\nFor example, in SCIM, the resolved sub claim and SCIM externalId must match in order for login to succeed.\\nIf there are restrictions on the source attribute of the sub claim and/or the SCIM externalId, set the ISSUER_SUB_CLAIM_OVERRIDES environment variable to select which OIDC JWT claim is used as the sub.\\nIf an issuer URL starts with one of the URLs in this configuration, the sub claim is taken from the field name specified.\\nFor example, with the following configuration, a token with the issuer https://idp.yourdomain.com/application/uuid would use the customClaim value as the sub:\\nCopyISSUER_SUB_CLAIM_OVERRIDES=\\'{\"https://idp.yourdomain.com\": \"customClaim\"}\\'\\n\\nIf unset, the default value for this configuration uses the oid claim when Azure Entra ID is used as the identity provider:\\nCopyISSUER_SUB_CLAIM_OVERRIDES=\\'{\"https://login.microsoftonline.com/\": \"oid\", \"https://sts.windows.net/\": \"oid\"}\\'\\n\\n\\u200bGoogle Workspace IdP setup\\nYou can use Google Workspace as a single sign-on (SSO) provider using OAuth2.0 and OIDC without PKCE.\\nYou must have administrator-level access to your organizationâ€™s Google Cloud Platform (GCP) account to create a new project, or permissions to create and configure OAuth 2.0 credentials for an existing project. We recommend that you create a new project for managing access, since each GCP project has a single OAuth consent screen.\\n\\n\\nCreate a new GCP project, see the Google documentation topic creating and managing projects\\n\\n\\nAfter you have created the project, open the Credentials page in the Google API Console (making sure the project in the top left corner is correct)\\n\\n\\nCreate new credentials: Create Credentials â†’ OAuth client ID\\n\\n\\nChoose Web application as the Application type and enter a name for the application e.g. LangSmith\\n\\n\\nIn Authorized Javascript origins put the domain of your LangSmith instance e.g. https://langsmith.yourdomain.com\\n\\n\\nIn Authorized redirect URIs put the domain of your LangSmith instance followed by /api/v1/oauth/custom-oidc/callback e.g. https://langsmith.yourdomain.com/api/v1/oauth/custom-oidc/callback\\n\\n\\nClick Create, then download the JSON or copy and save the Client ID (ends with .apps.googleusercontent.com) and Client secret somewhere secure. You will be able to access these later if needed.\\n\\n\\nSelect OAuth consent screen from the navigation menu on the left\\n\\nChoose the Application type as Internal. If you select Public, anyone with a Google account can sign in.\\nEnter a descriptive Application name. This name is shown to users on the consent screen when they sign in. For example, use LangSmith or <organization_name> SSO for LangSmith.\\nVerify that the Scopes for Google APIs only lists email, profile, and openid scopes. Only these scopes are required for single sign-on. If you grant additional scopes it increases the risk of exposing sensitive data.\\n\\n\\n\\n(Optional) control who within your organization has access to LangSmith: https://admin.google.com/ac/owl/list?tab=configuredApps. See Googleâ€™s documentation for additional details.\\n\\n\\nConfigure LangSmith to use this OAuth application. For examples, here are the configvalues that would be used for Kubernetes configuration:\\n\\noauthClientId: Client ID (ends with .apps.googleusercontent.com)\\noauthClientSecret: Client secret\\nhostname: the domain of your LangSmith instance e.g. https://langsmith.yourdomain.com (no trailing slash)\\noauthIssuerUrl: https://accounts.google.com\\noauth.enabled: true\\nauthType: mixed\\n\\n\\n\\n\\u200bOkta IdP setup\\n\\u200bSupported features\\n\\nIdP-initiated SSO\\nSP-initiated SSO\\n\\n\\u200bConfiguration steps\\nFor additional information, see Oktaâ€™s documentation.\\nIf you have any questions or issues, please reach out to support@langchain.dev.\\nVia Okta Integration Network (recommended)\\nThis method of configuration is required in order to use SCIM with Okta.\\n\\nSign in to Okta.\\nIn the upper-right corner, select Admin. The button is not visible from the Admin area.\\nSelect Browse App Integration Catalog.\\nFind and select the LangSmith application.\\nOn the application overview page, select Add Integration.\\nFill in ApiUrlBase:\\n\\nYour LangSmith API URL without the protocol (https://) formatted as <langsmith_domain>/api/v1, e.g., langsmith.yourdomain.com/api/v1.\\nIf your installation is configured with a subdomain / path prefix, include that in the URL, e.g., langsmith.yourdomain.com/prefix/api/v1.\\n\\n\\nLeave AuthHost empty.\\n(Optional, if planning to use SCIM as well) Fill in LangSmithUrl: The <langsmith_url> portion from above, e.g., langsmith.yourdomain.com.\\nUnder Application Visibility, keep the box unchecked.\\nSelect Next.\\nSelect OpenID Connect.\\nFill in Sign-On Options:\\n\\nApplication username format: Email.\\nUpdate application username on: Create and update.\\nAllow users to securely see their password: leave unchecked.\\n\\n\\nClick Save.\\nConfigure LangSmith to use this OAuth application (see general configuration section for details about initialOrgAdminEmail):\\n\\nHelmDockerCopyconfig:\\n  authType: mixed\\n  hostname: https://langsmith.example.com # the domain of your instance (note no trailing slash)\\n  initialOrgAdminEmail: test@email.com # Set this if required\\n  oauth:\\n    enabled: true\\n    oauthClientId: \"Client ID\" # (starts with `0o`)\\n    oauthClientSecret: \"Client secret\"\\n    oauthIssuerUrl: \"https://company-7422949.okta.com\" # the URL of your Okta instance\\n    oauthScopes: \"email,profile,openid\"\\n\\nVia Custom App Integration\\nSCIM is not compatible with this method of configuration. Refer to Via Okta Integration Network.\\n\\nLog in to Okta as an administrator, and go to the Okta Admin console.\\nUnder Applications > Applications click Create App Integration.\\nSelect OIDC - OpenID Connect as the Sign-in method and Web Application as the Application type, then click Next.\\nEnter an App integration name (e.g., LangSmith).\\nRecommended: Check Core grants > Refresh Token (see session length controls).\\nIn Sign-in redirect URIs put the domain of your LangSmith instance followed by /api/v1/oauth/custom-oidc/callback, e.g., https://langsmith.yourdomain.com/api/v1/oauth/custom-oidc/callback. If your installation is configured with a subdomain / path prefix, include that in the URL, e.g., https://langsmith.yourdomain.com/prefix/api/v1/oauth/custom-oidc/callback.\\nRemove the default URI under Sign-out redirect URIs.\\nUnder Trusted Origins > Base URIs add your langsmith URL with the protocol, e.g., https://langsmith.yourdomain.com.\\nSelect your desired option under Assignments > Controlled access:\\n\\nAllow everyone in your organization to access.\\nLimit access to selected groups.\\nSkip group assignment for now.\\n\\n\\nClick Save.\\nUnder Sign On > OpenID Connect ID Token set Issuer to Okta URL.\\n(Optional) Under General > Login set Login initiated by to Either Okta or App to enable IdP-initiated login.\\n(Recommended) Under General > Login > Email verification experience fill in the Callback URI with the LangSmith URL, e.g., https://langsmith.yourdomain.com.\\nConfigure LangSmith to use this OAuth application (see general configuration section for details about initialOrgAdminEmail):\\n\\nHelmDockerCopyconfig:\\n  authType: mixed\\n  hostname: https://langsmith.example.com # the domain of your instance (note no trailing slash)\\n  initialOrgAdminEmail: test@email.com # Set this if required\\n  oauth:\\n    enabled: true\\n    oauthClientId: \"Client ID\" # (starts with `0o`)\\n    oauthClientSecret: \"Client secret\"\\n    oauthIssuerUrl: \"https://company-7422949.okta.com\" # the URL of your Okta instance\\n    oauthScopes: \"email,profile,openid\"\\n\\n\\u200bSP-initiated SSO\\nUsers can sign in using the Login via SSO button on the LangSmith homepage.\\n\\u200bWithout Client Secret (PKCE) (Deprecated)\\nWe recommend running with a Client Secret if possible (previously we didnâ€™t support this). However, if your IdP does not support this, you can use the Authorization Code with PKCE flow.\\nThis flow does not require a Client Secret. For the alternative workflow, refer to With client secret.\\n\\u200bRequirements\\nThere are a couple of requirements for using OAuth SSO with LangSmith:\\n\\nYour IdP must support the Authorization Code with PKCE flow (Google does not support this flow for example, but see above for an alternative configuration that Google supports). This is often displayed in your OAuth Provider as configuring a â€œSingle Page Application (SPA)â€\\nYour IdP must support using an external discovery/issuer URL. We will use this to fetch the necessary routes and keys for your IdP.\\nYou must provide the OIDC, email, and profile scopes to LangSmith. We use these to fetch the necessary user information and email for your users.\\nYou will need to set the callback URL in your IdP to http://<host>/oauth-callback, where host is the domain or IP you have provisioned for your LangSmith instance. This is where your IdP will redirect the user after they have authenticated.\\nYou will need to provide the oauthClientId and oauthIssuerUrl in your values.yaml file. This is where you will configure your LangSmith instance.\\n\\nHelmDockerCopyconfig:\\n  oauth:\\n    enabled: true\\n    oauthClientId: <YOUR CLIENT ID>\\n    oauthIssuerUrl: <YOUR DISCOVERY URL>\\nWas this page helpful?YesNoSuggest editsSet up basic authenticationCustomize user managementâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/self_hosting/configuration/ttl', 'loc': 'https://docs.smith.langchain.com/self_hosting/configuration/ttl', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Enable TTL and data retention - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewSetupInstall on KubernetesInstall on DockerInteract with an installationUpgrade an installationConfigure egress for subscription metricsView trace counts across an organizationLangSmith-managed ClickHouseConfigurationConfigure for scaleEnable TTL & data retentionCreate an Ingress for installations (Kubernetes)Mirror images for your installationUse environment variables for model providersTroubleshootingAuthentication & access controlSet up basic authenticationSet up SSO with OAuth2.0 & OIDCCustomize user managementConfigure custom TLS certificatesUse an existing secret for your installation (Kubernetes)Connect external servicesEnable blob storageConnect to an external ClickHouse databaseConnect to an external PostgreSQL databaseConnect to an external Redis databaseScriptsDelete workspacesDelete organizationsDelete tracesGenerate ClickHouse StatsGenerate query statsRun support queries against PostgreSQLRun support queries against ClickHouseObservabilityExport LangSmith telemetry to your observability backendConfigure your collector for telemetryDeploy an observability stackOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationConfigurationEnable TTL and data retentionGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageRequirementsClickHouse TTL Cleanup JobDefault ScheduleDisabling the JobConfiguring the ScheduleConfiguring Minimum Expired Rows Per PartChecking Expired RowsConfiguring Maximum Active MutationsEmergency: Stopping Running MutationsBackups and Data RetentionConfigurationEnable TTL and data retentionCopy pageCopy pageLangSmith Self-Hosted allows enablement of automatic TTL and Data Retention of traces. This can be useful if youâ€™re complying with data privacy regulations, or if you want to have more efficient space usage and auto cleanup of your traces. Traces will also have their data retention period automatically extended based on certain actions or run rule applications.\\n\\u200bRequirements\\nYou can configure retention through helm or environment variable settings. There are a few options that are configurable:\\n\\nEnabled: Whether data retention is enabled or disabled. If enabled, via the UI you can your default organization and project TTL tiers to apply to traces (see data retention guide for details).\\nRetention Periods: You can configure system-wide retention periods for shortlived and longlived traces. Once configured, you can manage the retention level at each project as well as set an organization-wide default for new projects.\\n\\nHelmDockerCopyconfig:\\n  ttl:\\n    enabled: true\\n    ttl_period_seconds:\\n      # -- 400 day longlived and 14 day shortlived\\n      longlived: \"34560000\"\\n      shortlived: \"1209600\"\\n\\n\\u200bClickHouse TTL Cleanup Job\\nAs of version 0.11, a cron job runs on weekends to assist in deleting expired data that may not have been cleaned up by ClickHouseâ€™s built-in TTL mechanism.\\nThis job uses potentially long running mutations (ALTER TABLE DELETE), which are expensive operations that can impact ClickHouseâ€™s performance. We recommend running these operations only during off-peak hours (nights and weekends). During testing with 1 concurrent active mutation (default), we did not observe significant CPU, memory, or latency increases.\\n\\u200bDefault Schedule\\nBy default, the cleanup job runs:\\n\\nSaturday: 8pm and 10pm UTC\\nSunday: 12am, 2am, and 4am UTC\\n\\n\\u200bDisabling the Job\\nTo disable the cleanup job entirely:\\nCopyqueue:\\n  deployment:\\n    extraEnv:\\n      - name: \"ENABLE_CLICKHOUSE_TTL_CLEANUP_CRON\"\\n        value: \"false\"\\n\\n\\u200bConfiguring the Schedule\\nYou can customize when the cleanup job runs by modifying the cron expressions:\\nCopyqueue:\\n  deployment:\\n    extraEnv:\\n      # UTC: Sunday 12am/2am/4am\\n      - name: \"CLICKHOUSE_TTL_CLEANUP_CRON_WEEKEND_MORNING\"\\n        value: \"0 0,2,4 * * 0\"\\n      # UTC: Saturday 8pm/10pm\\n      - name: \"CLICKHOUSE_TTL_CLEANUP_CRON_WEEKEND_EVENING\"\\n        value: \"0 20,22 * * 6\"\\n\\nTo run the job on a single cron schedule, set both CLICKHOUSE_TTL_CLEANUP_CRON_WEEKEND_EVENING and CLICKHOUSE_TTL_CLEANUP_CRON_WEEKEND_MORNING to the same value. Job locking prevents overlapping executions.\\n\\u200bConfiguring Minimum Expired Rows Per Part\\nThe job goes table by table, scanning parts and deleting data from parts containing a minimum number of expired rows. This threshold balances efficiency and thoroughness:\\n\\nToo low: Job scans entire parts to clear minimal data (inefficient)\\nToo high: Job misses parts with significant expired data\\n\\nCopyqueue:\\n  deployment:\\n    extraEnv:\\n      - name: \"CLICKHOUSE_TTL_CRON_MIN_EXPIRED_ROWS_PER_PART\"\\n        value: \"100000\" # 100k expired rows\\n\\n\\u200bChecking Expired Rows\\nUse this query to analyze expired rows in your tables, and tweak your minimum value accordingly:\\nCopy-- Query for Runs table. For other tables, replace \\'ttl_seconds\\' with \\'trace_ttl_seconds\\'\\nSELECT\\n    _part,\\n    count() AS expired_rows\\nFROM runs\\nWHERE trace_first_received_at IS NOT NULL\\nAND ttl_seconds IS NOT NULL\\nAND toDateTime(assumeNotNull(trace_first_received_at) + toIntervalSecond(assumeNotNull(ttl_seconds))) < now()\\nGROUP BY _part\\nORDER BY expired_rows DESC\\n\\n\\u200bConfiguring Maximum Active Mutations\\nDelete operations can be time-consuming (~50 minutes for a 100GB part). You can increase concurrent mutations to speed up the process:\\nCopyqueue:\\n  deployment:\\n    extraEnv:\\n      - name: \"CLICKHOUSE_TTL_CRON_MAX_ACTIVE_MUTATIONS\"\\n        value: \"1\"\\n\\nIncreasing concurrent DELETE operations can severely impact system performance. Monitor your system carefully and only increase this value if you can tolerate potentially slower insert and read latencies.\\n\\u200bEmergency: Stopping Running Mutations\\nIf you experience latency spikes and need to terminate a running mutation:\\n\\n\\nFind active mutations:\\nCopySELECT * FROM system.mutations WHERE is_done = 0;\\n\\nLook for the mutation_id where the command column contains a DELETE statement.\\n\\n\\nKill the mutation:\\nCopyKILL MUTATION WHERE mutation_id = \\'<mutation_id>\\';\\n\\n\\n\\n\\u200bBackups and Data Retention\\nIf disk space does not decrease after running this job, or if it continues to increase, backups may be causing the issue by creating file system hard links. These links prevent ClickHouse from cleaning up the data.\\nTo verify, check the following directories inside your ClickHouse pod:\\n\\n/var/lib/clickhouse/backup\\n/var/lib/clickhouse/shadow\\n\\nIf backups are present, copy them to an external filesystem or blob storage (e.g., S3), then clear the directories. Within a few minutes, you will notice disk space releasing.Was this page helpful?YesNoSuggest editsConfigure for scaleCreate an Ingress for installations (Kubernetes)âŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/self_hosting/configuration/user_management', 'loc': 'https://docs.smith.langchain.com/self_hosting/configuration/user_management', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='User management - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationSetupOverviewCreate an account and API keySet up a workspaceManage organizations using the APIManage billingSet up resource tagsUser managementAdditional resourcesFAQsCloud architecture and scalabilityRegions FAQAuthentication methodsData purging for complianceRelease versionsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationSetupUser managementGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageSet up access controlCreate a roleAssign a role to a userSet up SAML SSO for your organizationJust-in-time (JIT) provisioningLogin methods and accessEnforce SAML SSO onlyPrerequisitesInitial configurationEntra ID (Azure)GoogleOktaSupported featuresConfiguration stepsSP-initiated SSOSet up SCIM for your organizationRequirementsPrerequisitesRole PrecedenceEmail verificationAttributes and MappingGroup Naming ConventionMappingUser AttributesGroup AttributesStep 1: Configure SAML SSO (Cloud only)NameID FormatStep 2: Disable JIT provisioningDisabling JIT for CloudDisabling JIT for Self-HostedStep 3: Generate SCIM bearer tokenStep 4: Configure your Identity ProviderAzure Entra ID configuration stepsOkta configuration stepsOther Identity ProvidersSetupUser managementCopy pageCopy pageThis page covers user management features in LangSmith, including access control, authentication, and automated user provisioning:\\n\\nSet up access control: Configure role-based access control (RBAC) to manage user permissions within workspaces, including creating custom roles and assigning them to users.\\nSAML SSO (Enterprise plan): Set up Single Sign-On authentication for Enterprise customers using SAML 2.0, including configuration for popular identity providers.\\nSCIM User Provisioning (Enterprise plan): Automate user provisioning and deprovisioning between your identity provider and LangSmith using SCIM.\\n\\n\\u200bSet up access control\\nRBAC (Role-Based Access Control) is a feature that is only available to Enterprise customers. If you are interested in this feature, contact our sales team. Other plans default to using the Admin role for all users.\\nYou may find it helpful to read the Administration overview page before setting up access control.\\nLangSmith relies on RBAC to manage user permissions within a workspace. This allows you to control who can access your LangSmith workspace and what they can do within it. Only users with the workspace:manage permission can manage access control settings for a workspace.\\n\\u200bCreate a role\\nBy default, LangSmith comes with a set of system roles:\\n\\nAdmin: has full access to all resources within the workspace.\\nViewer: has read-only access to all resources within the workspace.\\nEditor: has full permissions except for workspace management (adding/removing users, changing roles, configuring service keys).\\n\\nIf these do not fit your access model, Organization Admins can create custom roles to suit your needs.\\nTo create a role, navigate to the Roles tab in the Members and roles section of the Organization settings page. Note that new roles that you create will be usable across all workspaces within your organization.\\nClick on the Create Role button to create a new role. A Create role form will open.\\n\\nAssign permissions for the different LangSmith resources that you want to control access to.\\n\\u200bAssign a role to a user\\nOnce you have your roles set up, you can assign them to users. To assign a role to a user, navigate to the Workspace members tab in the Workspaces section of the Organization settings page\\nEach user will have a Role dropdown that you can use to assign a role to them.\\n\\nYou can also invite new users with a given role.\\n\\n\\u200bSet up SAML SSO for your organization\\nSingle Sign-On (SSO) functionality is available for Enterprise Cloud customers to access LangSmith through a single authentication source. This allows administrators to centrally manage team access and keeps information more secure.\\nLangSmithâ€™s SSO configuration is built using the SAML (Security Assertion Markup Language) 2.0 standard. SAML 2.0 enables connecting an Identity Provider (IdP) to your organization for an easier, more secure login experience.\\nSSO services permit a user to use one set of credentials (for example, a name or email address and password) to access multiple applications. The service authenticates the end user only once for all the applications the user has been given rights to and eliminates further prompts when the user switches applications during the same session. The benefits of SSO include:\\n\\nStreamlines user management across systems for organization owners.\\nEnables organizations to enforce their own security policies (e.g., MFA).\\nRemoves the need for end users to remember and manage multiple passwords. Simplifies the end-user experience, by allowing sign in at one single access point across multiple applications.\\n\\n\\u200bJust-in-time (JIT) provisioning\\nLangSmith supports Just-in-time provisioning when using SAML SSO. This allows someone signing in via SAML SSO to join the organization and selected workspaces automatically as a member.\\nJIT provisioning only runs for new users, that is, users who do not already have access to the organization with the same email address via a different login method.\\n\\u200bLogin methods and access\\nOnce you have completed your configuration of SAML SSO for your organization, users will be able to log in via SAML SSO in addition to other login methods, such as username/password or Google Authenticationâ€:\\n\\nWhen logged in via SAML SSO, users can only access the corresponding organization with SAML SSO configured.\\nUsers with SAML SSO as their only login method do not have personal organizations.\\nWhen logged in via any other method, users can access the organization with SAML SSO configured along with any other organizations they are a part of.\\n\\n\\u200bEnforce SAML SSO only\\nTo ensure users can only access the organization when logged in using SAML SSO and no other method, check the Login via SSO only checkbox and click Save. Once this happens, users accessing the organization that are logged-in via a non-SSO login method are required to log back in using SAML SSO. This setting can be switched back to allow all login methods by unselecting the checkbox and clicking Save.\\nYou must be logged in via SAML SSO in order to update this setting to Only SAML SSO. This is to ensure the SAML settings are valid and avoid locking users out of your organization.\\nFor troubleshooting, refer to the SAML SSO FAQs. If you have issues setting up SAML SSO, reach out to the LangChain support team at support@langchain.dev.\\n\\u200bPrerequisites\\nSAML SSO is available for organizations on the Enterprise plan. Please contact sales to learn more.\\n\\nYour organization must be on an Enterprise plan.\\nYour Identity Provider (IdP) must support the SAML 2.0 standard.\\nOnly Organization Admins can configure SAML SSO.\\n\\nFor instructions on using SCIM along with SAML for user provisioning and deprovisioning, refer to the SCIM setup.\\n\\u200bInitial configuration\\nFor IdP-specific configuration steps, refer to one of the following:\\nEntra ID\\nGoogle\\nOkta\\n\\n\\n\\nIn your IdP: Configure a SAML application with the following details, then copy the metadata URL or XML for step 3.\\nThe following URLs are different for the US and EU regions. Ensure you select the correct link.\\n\\nSingle sign-on URL (or ACS URL):\\n\\nUS: https://auth.langchain.com/auth/v1/sso/saml/acs\\nEU: https://eu.auth.langchain.com/auth/v1/sso/saml/acs\\n\\n\\nAudience URI (or SP Entity ID):\\n\\nUS: https://auth.langchain.com/auth/v1/sso/saml/metadata\\nEU: https://eu.auth.langchain.com/auth/v1/sso/saml/metadata\\n\\n\\nName ID format: email address.\\nApplication username: email address.\\nRequired claims: sub and email.\\n\\n\\n\\nIn LangSmith: Go to Settings -> Members and roles -> SSO Configuration. Fill in the required information and submit to activate SSO login:\\n\\nFill in either the SAML metadata URL or SAML metadata XML.\\nSelect the Default workspace role and Default workspaces. New users logging in via SSO will be added to the specified workspaces with the selected role.\\n\\n\\n\\n\\nDefault workspace role and Default workspaces are editable. The updated settings will apply to new users only, not existing users.\\n(Coming soon) SAML metadata URL and SAML metadata XML are editable. This is usually only necessary when cryptographic keys are rotated/expired or the metadata URL has changed but the same IdP is still used.\\n\\n\\u200bEntra ID (Azure)\\nFor additional information, see Microsoftâ€™s documentation.\\n\\nStep 1: Create a new Entra ID application integration\\n\\n\\nLog in to the Azure portal with a privileged role (e.g., Global Administrator). On the left navigation pane, select the Entra ID service.\\n\\n\\nNavigate to Enterprise Applications and then select All Applications.\\n\\n\\nClick Create your own application.\\n\\n\\nIn the Create your own application window:\\n\\nEnter a name for your application (e.g., LangSmith).\\nSelect *Integrate any other application you donâ€™t find in the gallery (Non-gallery)**.\\n\\n\\n\\nClick Create.\\n\\n\\nStep 2: Configure the Entra ID application and obtain the SAML Metadata\\n\\n\\nOpen the enterprise application that you created.\\n\\n\\nIn the left-side navigation, select Manage > Single sign-on.\\n\\n\\nOn the Single sign-on page, click SAML.\\n\\n\\nUpdate the Basic SAML Configuration:\\n\\nIdentifier (Entity ID):\\n\\nUS: https://auth.langchain.com/auth/v1/sso/saml/metadata\\nEU: https://eu.auth.langchain.com/auth/v1/sso/saml/metadata\\n\\n\\nReply URL (Assertion Consumer Service URL):\\n\\nUS: https://auth.langchain.com/auth/v1/sso/saml/acs\\nEU: https://eu.auth.langchain.com/auth/v1/sso/saml/acs\\n\\n\\nLeave Relay State, Logout Url, and Sign on URL empty.\\nClick Save.\\n\\n\\n\\nEnsure required claims are present with Namespace: http://schemas.xmlsoap.org/ws/2005/05/identity/claims:\\n\\nsub: user.objectid.\\nemailaddress: user.userprincipalname or user.mail (if using the latter, ensure all users have the Email field filled in under Contact Information).\\n(Optional) For SCIM, see the setup documentation for specific instructions about Unique User Identifier (Name ID).\\n\\n\\n\\nOn the SAML-based Sign-on page, under SAML Certificates, copy the App Federation Metadata URL.\\n\\n\\nStep 3: Set up LangSmith SSO Configuration\\nFollow the instructions under initial configuration in the Fill in required information step, using the metadata URL from the previous step.\\nStep 4: Verify the SSO setup\\n\\n\\nAssign the application to users/groups in Entra ID:\\n\\n\\nSelect Manage > Users and groups.\\n\\n\\nClick Add user/group.\\n\\n\\nIn the Add Assignment window:\\n\\nUnder Users, click None Selected.\\nSearch for the user you want to assign to the enterprise application, and then click Select.\\nVerify that the user is selected, and click Assign.\\n\\n\\n\\n\\n\\nHave the user sign in via the unique login URL from the SSO Configuration page, or go to Manage > Single sign-on and select Test single sign-on with (application name).\\n\\n\\n\\u200bGoogle\\nFor additional information, see Googleâ€™s documentation.\\nStep 1: Create and configure the Google Workspace SAML application\\n\\n\\nMake sure youâ€™re signed into an administrator account with the appropriate permissions.\\n\\n\\nIn the Admin console, go to Menu -> Apps -> Web and mobile apps.\\n\\n\\nClick Add App and then Add custom SAML app.\\n\\n\\nEnter the app name and, optionally, upload an icon. Click Continue.\\n\\n\\nOn the Google Identity Provider details page, download the IDP metadata and save it for Step 2. Click Continue.\\n\\n\\nIn the Service Provider Details window, enter:\\n\\nACS URL:\\n\\nUS: https://auth.langchain.com/auth/v1/sso/saml/acs\\nEU: https://eu.auth.langchain.com/auth/v1/sso/saml/acs\\n\\n\\nEntity ID:\\n\\nUS: https://auth.langchain.com/auth/v1/sso/saml/metadata\\nEU: https://eu.auth.langchain.com/auth/v1/sso/saml/metadata\\n\\n\\nLeave Start URL and the Signed response box empty.\\nSet Name ID format to EMAIL and leave Name ID as the default (Basic Information > Primary email).\\nClick Continue.\\n\\n\\n\\nUse Add mapping to ensure required claims are present:\\n\\nBasic Information > Primary email -> email\\n\\n\\n\\nStep 2: Set up LangSmith SSO Configuration\\nFollow the instructions under initial configuration in the Fill in required information step, using the IDP metadata from the previous step as the metadata XML.\\nStep 3: Turn on the SAML app in Google\\n\\n\\nSelect the SAML app under Menu -> Apps -> Web and mobile apps\\n\\n\\nClick User access.\\n\\n\\nTurn on the service:\\n\\n\\nTo turn the service on for everyone in your organization, click On for everyone, and then click Save.\\n\\n\\nTo turn the service on for an organizational unit:\\n\\nAt the left, select the organizational unit then On.\\nIf the Service status is set to Inherited and you want to keep the updated setting, even if the parent setting changes, click Override.\\nIf the Service status is set to Overridden, either click Inherit to revert to the same setting as its parent, or click Save to keep the new setting, even if the parent setting changes.\\n\\n\\n\\nTo turn on a service for a set of users across or within organizational units, select an access group. For details, go to Use groups to customize service access.\\n\\n\\n\\n\\nEnsure that the email addresses your users use to sign in to LangSmith match the email addresses they use to sign in to your Google domain.\\n\\n\\nStep 4: Verify the SSO setup\\nHave a user with access sign in via the unique login URL from the SSO Configuration page, or go to the SAML application page in Google and click TEST SAML LOGIN.\\n\\u200bOkta\\n\\u200bSupported features\\n\\nIdP-initiated SSO (Single Sign-On)\\nSP-initiated SSO\\nJust-In-Time provisioning\\nEnforce SSO only\\n\\n\\u200bConfiguration steps\\nFor additional information, see Oktaâ€™s documentation.\\nStep 1: Create and configure the Okta SAML application\\nVia Okta Integration Network (recommended)\\n\\nSign in to Okta.\\nIn the upper-right corner, select Admin. The button is not visible from the Admin area.\\nSelect Browse App Integration Catalog.\\nFind and select the LangSmith application.\\nOn the application overview page, select Add Integration.\\nLeave ApiUrlBase empty.\\nFill in AuthHost:\\n\\nUS: auth.langchain.com\\nEU: eu.auth.langchain.com\\n\\n\\n(Optional, if planning to use SCIM as well) Fill in LangSmithUrl:\\n\\nUS: api.smith.langchain.com\\nEU: eu.api.smith.langchain.com\\n\\n\\nUnder Application Visibility, keep the box unchecked.\\nSelect Next.\\nSelect SAML 2.0.\\nFill in Sign-On Options:\\n\\nApplication username format: Email\\nUpdate application username on: Create and update\\nAllow users to securely see their password: leave unchecked.\\n\\n\\nCopy the Metadata URL from the Sign On Options page to use in the next step.\\n\\nVia Custom App Integration\\nSCIM is not compatible with this method of configuration. Refer to Via Okta Integration Network.\\n\\n\\nLog in to Okta as an administrator, and go to the Okta Admin console.\\n\\n\\nUnder Applications > Applications click Create App Integration.\\n\\n\\nSelect SAML 2.0.\\n\\n\\nEnter an App name (e.g., LangSmith) and optionally an App logo, then click Next.\\n\\n\\nEnter the following information in the Configure SAML page:\\n\\nSingle sign-on URL (ACS URL). Keep Use this for Recipient URL and Destination URL checked:\\n\\nUS: https://auth.langchain.com/auth/v1/sso/saml/acs\\nEU: https://eu.auth.langchain.com/auth/v1/sso/saml/acs\\n\\n\\nAudience URI (SP Entity ID):\\n\\nUS: https://auth.langchain.com/auth/v1/sso/saml/metadata\\nEU: https://eu.auth.langchain.com/auth/v1/sso/saml/metadata\\n\\n\\nName ID format: Persistent.\\nApplication username: email.\\nLeave the rest of the fields empty or set to their default.\\nClick Next.\\n\\n\\n\\nClick Finish.\\n\\n\\nCopy the Metadata URL from the Sign On page to use in the next step.\\n\\n\\nStep 2: Set up LangSmith SSO Configuration\\nFollow the instructions under initial configuration in the Fill in required information step, using the metadata URL from the previous step.\\nStep 3: Assign users to LangSmith in Okta\\n\\nUnder Applications > Applications, select the SAML application created in Step 1.\\nUnder the Assignments tab, click Assign then either Assign to People or Assign to Groups.\\nMake the desired selection(s), then Assign and Done.\\n\\nStep 4: Verify the SSO setup\\nHave a user with access sign in via the unique login URL from the SSO Configuration page, or have a user select the application from their Okta dashboard.\\n\\u200bSP-initiated SSO\\nOnce service-providerâ€“initiated SSO is configured, users can sign in using a unique login URL. You can find this in the LangSmith UI under Organization members and roles then SSO configuration.\\n\\u200bSet up SCIM for your organization\\nSystem for Cross-domain Identity Management (SCIM) is an open standard that allows for the automation of user provisioning. Using SCIM, you can automatically provision and de-provision users in your LangSmith organization and workspaces, keeping user access synchronized with your organizationâ€™s identity provider.\\nSCIM is available for organizations on the Enterprise plan. Contact sales to learn more.SCIM is available on Helm chart versions 0.10.41 (application version 0.10.108) and later.SCIM support is API-only (see instructions below).\\nSCIM eliminates the need for manual user management and ensures that user access is always up-to-date with your organizationâ€™s identity system. This allows for:\\n\\nAutomated user management: Users are automatically added, updated, and removed from LangSmith based on their status in your IdP.\\nReduced administrative overhead: No need to manage user access manually across multiple systems.\\nImproved security: Users who leave your organization are automatically deprovisioned from LangSmith.\\nConsistent access control: User attributes and group memberships are synchronized between systems.\\nScaling team access control: Efficiently manage large teams with many workspaces and custom roles.\\nRole assignment: Select specific Organization Roles and Workspace Roles for groups of users.\\n\\n\\u200bRequirements\\n\\u200bPrerequisites\\n\\nYour organization must be on an Enterprise plan.\\nYour Identity Provider (IdP) must support SCIM 2.0.\\nOnly Organization Admins can configure SCIM.\\nFor cloud customers: SAML SSO must be configurable for your organization.\\nFor self-hosted customers: OAuth with Client Secret authentication mode must be enabled.\\nFor self-hosted customers, network traffic must be allowed from the identity provider to LangSmith:\\n\\nMicrosoft Entra ID supports allowlisting IP ranges or an agent-based solution to provide connectivity.\\n(details).\\nOkta supports allow-listing IPs or domains (details)\\nor an agent-based solution (details) to provide connectivity.\\n\\n\\n\\n\\u200bRole Precedence\\nWhen a user belongs to multiple groups for the same workspace, the following precedence applies:\\n\\nOrganization Admin groups take highest precedence. Users in these groups will be Admin in all workspaces.\\nMost recently created workspace-specific group takes precedence over other workspace groups.\\n\\nWhen a group is deleted or a user is removed from a group, their access is updated according to their remaining group membership, following the precedence rules.SCIM group membership will override manually assigned roles or roles assigned via Just-in-time (JIT) provisioning. We recommend disabling JIT provisioning to avoid conflicts.\\n\\u200bEmail verification\\nIn cloud only, creating a new user with SCIM triggers an email to the user.\\nThey must verify their email address by clicking the link in this email.\\nThe link expires in 24 hours, and can be resent if needed by removing and re-adding the user via SCIM.\\n\\u200bAttributes and Mapping\\n\\u200bGroup Naming Convention\\nGroup membership maps to LangSmith workspace membership and workspace roles with a specific naming convention:\\nOrganization Admin Groups\\nFormat: <optional_prefix>Organization Admin or <optional_prefix>Organization Admins\\nExamples:\\n\\nLS:Organization Admins\\nGroups-Organization Admins\\nOrganization Admin\\n\\nWorkspace-Specific Groups\\nFormat: <optional_prefix><org_role_name>:<workspace_name>:<workspace_role_name>\\nExamples:\\n\\nLS:Organization User:Production:Annotators\\nGroups-Organization User:Engineering:Developers\\nOrganization User:Marketing:Viewers\\n\\n\\u200bMapping\\nWhile specific instructions depending on the identity provider may vary, these mappings show what is supported by the LangSmith SCIM integration:\\n\\u200bUser Attributes\\nLangSmith App AttributeIdentity Provider AttributeMatching PrecedenceuserName1email addressactive!deactivatedemails[type eq \"work\"].valueemail address2name.formatteddisplayName OR givenName + familyName3givenNamegivenNamefamilyNamefamilyNameexternalIdsub41\\n\\nuserName is not required by LangSmith\\nEmail address is required\\nUse the computed expression if your displayName does not match the format of Firstname Lastname\\nTo avoid inconsistency, this should match the SAML NameID assertion for cloud customers, or the sub OAuth2.0 claim for self-hosted.\\n\\n\\u200bGroup Attributes\\nLangSmith App AttributeIdentity Provider AttributeMatching PrecedencedisplayNamedisplayName11externalIdobjectIdmembersmembers\\n\\nGroups must follow the naming convention described in the Group Naming Convention section.\\nIf your company has a group naming policy, you should instead map from the description identity provider attribute and\\nset the description based on the Group Naming Convention section.\\n\\n\\u200bStep 1: Configure SAML SSO (Cloud only)\\nThere are two scenarios for SAML SSO configuration:\\n\\nIf SAML SSO is already configured for your organization, you should skip the steps to initially add the application (Add application from Okta Integration Network or Create a new Entra ID application integration), as you already have an application configured and just need to enable provisioning.\\nIf you are configuring SAML SSO for the first time alongside SCIM, first follow the instructions to set up SAML SSO, then follow the instructions here to enable SCIM.\\n\\n\\u200bNameID Format\\nLangSmith uses the SAML NameID to identify users. The NameID is a required field in the SAML response and is case-insensitive.\\nThe NameID must:\\n\\nBe unique to each user.\\nBe a persistent value that never changes, such as a randomly generated unique user ID.\\nMatch exactly on each sign-in attempt. It should not rely on user input.\\n\\nThe NameID should not be an email address or username because email addresses and usernames are more likely to change over time and can be case-sensitive.\\nThe NameID format must be Persistent, unless you are using a field, like email, that requires a different format.\\n\\u200bStep 2: Disable JIT provisioning\\nBefore enabling SCIM, disable Just-in-time (JIT) provisioning to prevent conflicts between automatic and manual user provisioning.\\n\\u200bDisabling JIT for Cloud\\nUse the PATCH /orgs/current/info endpoint:\\nCopycurl -X PATCH $LANGCHAIN_ENDPOINT/orgs/current/info \\\\\\n  -H \"X-Api-Key: $LANGCHAIN_API_KEY\" \\\\\\n  -H \"Content-Type: application/json\" \\\\\\n  -d \\'{\"jit_provisioning_enabled\": false}\\'\\n\\n\\u200bDisabling JIT for Self-Hosted\\nAs of LangSmith chart version 0.11.14, you can disable JIT provisioning for your self-hosted organization using SSO. To disable, set the following values:\\nCopycommonEnv:\\n  - name: SELF_HOSTED_JIT_PROVISIONING_ENABLED\\n    value: \"false\"\\n\\n\\u200bStep 3: Generate SCIM bearer token\\nIn self-hosted environments, the full URL below may look like https://langsmith.yourdomain.com/api/v1/platform/orgs/current/scim/tokens (without a subdomain, note the /api/v1 path prefix) or https://langsmith.yourdomain.com/subdomain/api/v1/platform/orgs/current/scim/tokens (with a subdomain) - see the ingress docs for more details.\\nGenerate a SCIM Bearer Token for your organization. This token will be used by your IdP to authenticate SCIM API requests. Ensure env vars are set appropriately, for example:\\nCopycurl -X POST $LANGCHAIN_ENDPOINT/v1/platform/orgs/current/scim/tokens \\\\\\n  -H \"X-Api-Key: $LANGCHAIN_API_KEY\" \\\\\\n  -H \"X-Organization-Id: $LANGCHAIN_ORGANIZATION_ID\" \\\\\\n  -H \"Content-Type: application/json\" \\\\\\n  -d \\'{\"description\": \"Your description here\"}\\'\\n\\nNote that the SCIM Bearer Token value is not available outside of the response to this request. These additional endpoints are present:\\n\\nGET /v1/platform/orgs/current/scim/tokens\\nGET /v1/platform/orgs/current/scim/tokens/{scim_token_id}\\nPATCH /v1/platform/orgs/current/scim/tokens/{scim_token_id} (only the description field is supported)\\nDELETE /v1/platform/orgs/current/scim/tokens/{scim_token_id}\\n\\n\\u200bStep 4: Configure your Identity Provider\\nIf you use Azure Entra ID (formerly Azure AD) or Okta, there are specific instructions for identity provider setup (refer to Azure Entra ID, Okta). The requirements and steps above are applicable for all identity providers.\\n\\u200bAzure Entra ID configuration steps\\nFor additional information, see Microsoftâ€™s documentation.\\nIn self-hosted installations, the oid JWT claim is used as the sub.\\nSee this Microsoft Learn link\\nand the related configuration instructions for additional details.\\nStep 1: Configure SCIM in your Enterprise Application\\n\\nLog in to the Azure portal with a privileged role (e.g., Global Administrator).\\nNavigate to your existing LangSmith Enterprise Application.\\nIn the left-side navigation, select Manage > Provisioning.\\nClick Get started.\\n\\nStep 2: Configure Admin credentials\\n\\n\\nUnder Admin Credentials:\\n\\n\\nTenant URL:\\n\\nUS: https://api.smith.langchain.com/scim/v2\\nEU: https://eu.api.smith.langchain.com/scim/v2\\nSelf-hosted: <langsmith_url>/scim/v2\\n\\n\\n\\nSecret Token: Enter the SCIM Bearer Token generated in Step 3.\\n\\n\\n\\n\\nClick Test Connection to verify the configuration.\\n\\n\\nClick Save.\\n\\n\\nStep 3: Configure Attribute Mappings\\nConfigure the following attribute mappings under Mappings:\\nUser Attributes\\nSet Target Object Actions to Create and Update (start with Delete disabled for safety):\\nLangSmith App AttributeMicrosoft Entra ID AttributeMatching PrecedenceuserNameuserPrincipalNameactiveNot([IsSoftDeleted])emails[type eq \"work\"].valuemail1name.formatteddisplayName OR Join(\" \", [givenName], [surname])2externalIdobjectId31\\n\\nUserâ€™s email address must be present in Entra ID.\\nUse the Join expression if your displayName does not match the format of Firstname Lastname.\\nTo avoid inconsistency, this should match the SAML NameID assertion and the sub OAuth2.0 claim. For SAML SSO in cloud, the Unique User Identifier (Name ID) required claim should be user.objectID and the Name identifier format should be persistent.\\n\\nGroup Attributes\\nSet Target Object Actions to Create and Update only (start with Delete disabled for safety):\\nLangSmith App AttributeMicrosoft Entra ID AttributeMatching PrecedencedisplayNamedisplayName11externalIdobjectIdmembersmembers\\n\\nGroups must follow the naming convention described in the Group Naming Convention section.\\nIf your company has a group naming policy, you should instead map from the description Microsoft Entra ID Attribute and\\nset the description based on the Group Naming Convention section.\\n\\nStep 4: Assign Users and Groups\\n\\nUnder Applications > Applications, select your LangSmith Enterprise Application.\\nUnder the Assignments tab, click Assign then either Assign to People or Assign to Groups.\\nMake the desired selection(s), then Assign and Done.\\n\\nStep 5: Enable Provisioning\\n\\nSet Provisioning Status to On under Provisioning.\\nMonitor the initial sync to ensure users and groups are provisioned correctly.\\nOnce verified, enable Delete actions for both User and Group mappings.\\n\\nFor troubleshooting, refer to the SAML SSO FAQs. If you have issues setting up SCIM, reach out to the LangChain support team at support@langchain.dev.\\n\\u200bOkta configuration steps\\nYou must use the Okta Lifecycle Management product. This product tier is required to use SCIM on Okta.\\nSupported features\\n\\nCreate users\\nUpdate user attributes\\nDeactivate users\\nGroup push\\n\\nStep 1: Add application from Okta Integration Network\\nIf you have already configured SSO login via SAML (cloud) or OAuth2.0 with OIDC (self-hosted), skip this step.\\nSee SAML SSO setup for cloud or OAuth2.0 setup for self-hosted.\\nStep 2: Configure API Integration\\n\\nIn the Provisioning tab, select Configure API integration.\\nSelect Enable API integration.\\nFor Base URL (if present):\\n\\n\\nUS: https://api.smith.langchain.com/scim/v2\\nEU: https://eu.api.smith.langchain.com/scim/v2\\nSelf-hosted: <langsmith_url>/scim/v2 (note there is no /api/v1 path prefix) or if a subdomain is configured <langsmith_url>/subdomain/scim/v2\\n\\n\\nFor API Token, paste the SCIM token you generated above.\\nKeep Import Groups checked.\\nTo verify the configuration, select Test API Credentials.\\nSelect Save.\\nAfter saving the API integration details, new settings tabs appear on the left. Select To App.\\nSelect Edit.\\nSelect the Enable checkbox for Create Users, Update Users, and Deactivate Users.\\nSelect Save.\\nAssign users and/or groups in the Assignments tab. Assigned users are created and managed in your LangSmith group.\\n\\nStep 3: Configure User Provisioning Settings\\n\\nConfigure provisioning: under Provisioning > To App > Provisioning to App, click Edit, then check Create Users, Update User Attributes, and Deactivate Users.\\nUnder <application_name> Attribute Mappings, set the user attribute mappings as shown below, and delete the rest:\\n\\n\\nStep 4: Push Groups\\nOkta does not support group attributes besides the group name itself, so group name must follow the naming convention described in the Group Naming Convention section.\\nFollow Oktaâ€™s Enable Group Push instructions to configure groups to push by name or by rule.\\n\\u200bOther Identity Providers\\nOther identity providers have not been tested but may function depending on their SCIM implementation.Was this page helpful?YesNoSuggest editsSet up resource tagsFAQsâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/self_hosting/configuration/using_an_existing_secret', 'loc': 'https://docs.smith.langchain.com/self_hosting/configuration/using_an_existing_secret', 'changefreq': 'weekly', 'priority': '0.5'}, page_content=\"Page Not FoundSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationSetupOverviewCreate an account and API keySet up a workspaceManage organizations using the APIManage billingSet up resource tagsUser managementAdditional resourcesFAQsCloud architecture and scalabilityRegions FAQAuthentication methodsData purging for complianceRelease versionsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationPage Not FoundGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForum404Page Not FoundWe couldn't find the page you were looking for. Maybe you were looking for?Use an existing secret for your installation (Kubernetes)Get started with LangSmithSelf-host LangSmith with DockerAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/self_hosting/egress', 'loc': 'https://docs.smith.langchain.com/self_hosting/egress', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Egress for subscription metrics and operational metadata - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewSetupInstall on KubernetesInstall on DockerInteract with an installationUpgrade an installationConfigure egress for subscription metricsView trace counts across an organizationLangSmith-managed ClickHouseConfigurationConfigure for scaleEnable TTL & data retentionCreate an Ingress for installations (Kubernetes)Mirror images for your installationUse environment variables for model providersTroubleshootingAuthentication & access controlSet up basic authenticationSet up SSO with OAuth2.0 & OIDCCustomize user managementConfigure custom TLS certificatesUse an existing secret for your installation (Kubernetes)Connect external servicesEnable blob storageConnect to an external ClickHouse databaseConnect to an external PostgreSQL databaseConnect to an external Redis databaseScriptsDelete workspacesDelete organizationsDelete tracesGenerate ClickHouse StatsGenerate query statsRun support queries against PostgreSQLRun support queries against ClickHouseObservabilityExport LangSmith telemetry to your observability backendConfigure your collector for telemetryDeploy an observability stackOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationSetupEgress for subscription metrics and operational metadataGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageLangSmith TelemetryWhat we use it forWhat we collectHow to disableExample payloadsLicense VerificationUsage ReportingTelemetry: MetricsTelemetry: TracesOur CommitmentSetupEgress for subscription metrics and operational metadataCopy pageCopy pageThis section only applies to customers who are not running in offline mode and assumes you are using a self-hosted LangSmith instance serving version 0.9.0 or later. Previous versions of LangSmith did not have this feature.\\nSelf-Hosted LangSmith instances store all information locally and will never send sensitive information outside of your network. We currently only track platform usage for billing purposes according to the entitlements in your order. In order to better remotely support our customers, we do require egress to https://beacon.langchain.com.\\nIn the future, we will be introducing support diagnostics to help us ensure that the LangSmith platform is running at an optimal level within your environment.\\nThis will require egress to https://beacon.langchain.com from your network. Refer to the allowlisting IP section for static IP addresses, if needed.\\nGenerally, data that we send to Beacon can be categorized as follows:\\n\\n\\nSubscription Metrics\\n\\n\\nSubscription metrics are used to determine level of access and utilization of LangSmith. This includes, but are not limited to:\\n\\nNumber of traces\\nSeats allocated per contract\\nSeats in currently use\\n\\n\\n\\n\\n\\nOperational Metadata\\n\\nThis metadata will contain and collect the above subscription metrics to assist with remote support, allowing the LangChain team to diagnose and troubleshoot performance issues more effectively and proactively.\\n\\n\\n\\n\\u200bLangSmith Telemetry\\nAs of version 0.11, LangSmith deployments will by default send telemetry data back to our backend. All telemetry data is associated with an organization and deployment, but never identified with individual users. We do not collect PII (personally identifiable information) in any form.\\n\\u200bWhat we use it for\\n\\nTo provide more proactive support and faster troubleshooting of self-hosted instances.\\nAssisting with performance tuning.\\nUnderstanding real-world usage to prioritize improvements.\\n\\n\\u200bWhat we collect\\n\\nRequest metadata: anonymized request counts, sizes, and durations.\\nDatabase metrics: query durations, error rates, and performance counters.\\nDistributed traces: end-to-end traces with timing and error information for high-latency or failed requests.\\n\\nWe do not collect actual payload contents, database records, or any data that can identify your end users or customers.\\n\\u200bHow to disable\\nSet the following values in your langsmith_config.yaml file:\\nCopyconfig:\\n  telemetry:\\n    metrics: false\\n    traces: false\\n\\n\\u200bExample payloads\\nIn an effort to maximize transparency, we provide sample payloads here:\\n\\u200bLicense Verification\\nEndpoint:\\nPOST beacon.langchain.com/v1/beacon/verify\\nRequest:\\nCopy{\\n  \"license\": \"<YOUR_LICENSE_KEY>\"\\n}\\n\\nResponse:\\nCopy{\\n  \"token\": \"Valid JWT\" //Short-lived JWT token to avoid repeated license checks\\n}\\n\\n\\u200bUsage Reporting\\nEndpoint:\\nPOST beacon.langchain.com/v1/beacon/ingest-traces\\nRequest:\\nCopy{\\n  \"license\": \"<YOUR_LICENSE_KEY>\",\\n  \"trace_transactions\": [\\n    {\\n      \"id\": \"af28dfea-5358-463d-a2dc-37df1da72498\",\\n      \"tenant_id\": \"3a1c2b6f-4430-4b92-8a5b-79b8b567bbc1\",\\n      \"session_id\": \"b26ae531-cdb3-42a5-8bcf-05355199fe27\",\\n      \"trace_count\": 5,\\n      \"start_insertion_time\": \"2025-01-06T10:00:00Z\",\\n      \"end_insertion_time\": \"2025-01-06T11:00:00Z\",\\n      \"start_interval_time\": \"2025-01-06T09:00:00Z\",\\n      \"end_interval_time\": \"2025-01-06T10:00:00Z\",\\n      \"status\": \"completed\",\\n      \"num_failed_send_attempts\": 0,\\n      \"transaction_type\": \"type1\",\\n      \"organization_id\": \"c5b5f53a-4716-4326-8967-d4f7f7799735\"\\n    }\\n  ]\\n}\\n\\nResponse:\\nCopy{\\n  \"inserted_count\": 1 //Number of transactions successfully ingested\\n}\\n\\n\\u200bTelemetry: Metrics\\nEndpoint:\\nPOST beacon.langchain.com/v1/beacon/v1/metrics\\nRequest:\\nCopy{\\n  \"resourceMetrics\": [\\n    {\\n      \"resource\": {\\n        \"attributes\": [\\n          {\\n            \"key\": \"resource.name\",\\n            \"value\": { \"stringValue\": \"langsmith-metrics\" }\\n          },\\n          {\\n            \"key\": \"env\",\\n            \"value\": { \"stringValue\": \"ls_self_hosted\" }\\n          }\\n        ]\\n      },\\n      \"scopeMetrics\": [\\n        {\\n          \"scope\": {\\n            \"name\": \"langsmith.metrics\",\\n            \"version\": \"0.1.0\"\\n          },\\n          \"metrics\": [\\n            {\\n              \"name\": \"langsmith_http_requests_latency\",\\n              \"unit\": \"seconds\",\\n              \"description\": \"Request latency of LangSmith services\",\\n              \"gauge\": {\\n                \"dataPoints\": [\\n                  {\\n                    \"asDouble\": 12.34,\\n                    \"startTimeUnixNano\": 1678886400000000000,\\n                    \"timeUnixNano\": 1678886400000000000,\\n                    \"attributes\": [\\n                      {\\n                        \"key\": \"endpoint\",\\n                        \"value\": { \"stringValue\": \"/sessions\" }\\n                      },\\n                      { \"key\": \"method\", \"value\": { \"stringValue\": \"GET\" } },\\n                      {\\n                        \"key\": \"service_name\",\\n                        \"value\": { \"stringValue\": \"langsmith_backend\" }\\n                      }\\n                    ]\\n                  }\\n                ]\\n              }\\n            },\\n            {\\n              \"name\": \"langsmith_http_requests_failed\",\\n              \"unit\": \"1\",\\n              \"description\": \"Counter of failed requests for LangSmith services\",\\n              \"sum\": {\\n                \"dataPoints\": [\\n                  {\\n                    \"asInt\": 456,\\n                    \"startTimeUnixNano\": 1678886400000000000,\\n                    \"timeUnixNano\": 1678886400000000000,\\n                    \"attributes\": [\\n                      {\\n                        \"key\": \"endpoint\",\\n                        \"value\": { \"stringValue\": \"/info\" }\\n                      },\\n                      { \"key\": \"method\", \"value\": { \"stringValue\": \"POST\" } },\\n                      {\\n                        \"key\": \"service_name\",\\n                        \"value\": { \"stringValue\": \"langsmith_platform_backend\" }\\n                      }\\n                    ],\\n                    \"aggregationTemporality\": 2,\\n                    \"isMonotonic\": true\\n                  }\\n                ]\\n              }\\n            }\\n          ]\\n        }\\n      ]\\n    }\\n  ]\\n}\\n\\n\\u200bTelemetry: Traces\\nEndpoint:\\nPOST beacon.langchain.com/v1/beacon/v1/traces\\nRequest:\\nCopy{\\n  \"resourceSpans\": [\\n    {\\n      \"resource\": {\\n        \"attributes\": [\\n          {\\n            \"key\": \"env\",\\n            \"value\": {\\n              \"stringValue\": \"ls_self_hosted\"\\n            }\\n          },\\n          {\\n            \"key\": \"service.name\",\\n            \"value\": {\\n              \"stringValue\": \"langsmith_backend\"\\n            }\\n          }\\n        ]\\n      },\\n      \"scopeSpans\": [\\n        {\\n          \"scope\": {},\\n          \"spans\": [\\n            {\\n              \"traceId\": \"71699b6fe85982c7c8995ea3d9c95df2\",\\n              \"spanId\": \"3c191d03fa8be0\",\\n              \"parentSpanId\": \"\",\\n              \"name\": \"receive_request\",\\n              \"startTimeUnixNano\": \"1581452772000000321\",\\n              \"endTimeUnixNano\": \"1581452773000000789\",\\n              \"droppedAttributesCount\": 1,\\n              \"events\": [\\n                {\\n                  \"timeUnixNano\": \"1581452773000000123\",\\n                  \"name\": \"parse_request\",\\n                  \"attributes\": [\\n                    {\\n                      \"key\": \"request_size\",\\n                      \"value\": {\\n                        \"stringValue\": \"100\"\\n                      }\\n                    }\\n                  ],\\n                  \"droppedAttributesCount\": 2\\n                },\\n                {\\n                  \"timeUnixNano\": \"1581452773000000123\",\\n                  \"name\": \"event\",\\n                  \"droppedAttributesCount\": 2\\n                }\\n              ],\\n              \"droppedEventsCount\": 1,\\n              \"status\": {\\n                \"message\": \"status-cancelled\",\\n                \"code\": 2\\n              }\\n            },\\n            {\\n              \"traceId\": \"71699b6fe85982c7c8995ea3d9c95df2\",\\n              \"spanId\": \"0932ksdka12345\",\\n              \"parentSpanId\": \"3c191d03fa8be0\",\\n              \"name\": \"process_request\",\\n              \"startTimeUnixNano\": \"1581452772000000321\",\\n              \"endTimeUnixNano\": \"1581452773000000789\",\\n              \"links\": [],\\n              \"droppedLinksCount\": 3,\\n              \"status\": {}\\n            }\\n          ]\\n        }\\n      ]\\n    }\\n  ]\\n}\\n\\n\\u200bOur Commitment\\nLangChain will not store any sensitive information in the Subscription Metrics or Operational Metadata. Any data collected will not be shared with a third party. If you have any concerns about the data being sent, please reach out to your account team.Was this page helpful?YesNoSuggest editsUpgrade an installationView trace counts across an organizationâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/self_hosting/faq', 'loc': 'https://docs.smith.langchain.com/self_hosting/faq', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Frequently asked questions - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationSetupOverviewCreate an account and API keySet up a workspaceManage organizations using the APIManage billingSet up resource tagsUser managementAdditional resourcesFAQsCloud architecture and scalabilityRegions FAQAuthentication methodsData purging for complianceRelease versionsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationAdditional resourcesFrequently asked questionsGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageI canâ€™t create API keys or manage users in the UI, whatâ€™s wrong?How does load balancing/ingress work?How can we authenticate to the application?Can I use external storage services?Does my application need egress to function properly?Resource requirements for the application?SAML SSO FAQsHow do I change a SAML SSO userâ€™s email address?How do I fix â€œ405 method not allowedâ€?SCIM FAQsCan I use SCIM without SAML SSO?What happens if I have both JIT provisioning and SCIM enabled?How do I change a userâ€™s role or workspace access?What happens when a user is removed from all groups?Can I use custom group names?Why is my Okta integration not working?Additional resourcesFrequently asked questionsCopy pageCopy page\\u200bI canâ€™t create API keys or manage users in the UI, whatâ€™s wrong?\\n\\nYou have likely deployed LangSmith without setting up SSO. LangSmith requires SSO to manage users and API keys. You can find more information on setting up SSO in the configuration section.\\n\\n\\u200bHow does load balancing/ingress work?\\n\\nYou will need to expose the frontend container/service to your applications/users. This will handle routing to all downstream services.\\nYou will need to terminate SSL at the ingress level. We recommend using a managed service like AWS ALB, GCP Load Balancer, or Nginx.\\n\\n\\u200bHow can we authenticate to the application?\\n\\nCurrently, our self-hosted solution supports SSO with OAuth2.0 and OIDC as an authn solution. Note, we do offer a no-auth solution but highly recommend setting up oauth before moving into production.\\n\\nYou can find more information on setting up SSO in the configuration section.\\n\\u200bCan I use external storage services?\\n\\nYou can configure LangSmith to use external versions of all storage services. In a production setting, we strongly recommend using external storage services. Check out the configuration section for more information.\\n\\n\\u200bDoes my application need egress to function properly?\\nOur deployment only needs egress for a few things (most of which can reside within your VPC):\\n\\n\\nFetching images (If mirroring your images, this may not be needed)\\n\\n\\nTalking to any LLM endpoints\\n\\n\\nTalking to any external storage services you may have configured\\n\\n\\nFetching OAuth information\\n\\n\\nSubscription Metrics and Operational Metadata (if not running in offline mode)\\n\\nRequires egress to https://beacon.langchain.com\\nSee Egress for more information\\n\\n\\n\\nYour VPC can set up rules to limit any other access. Note: We require the X-Organization-Id and X-Tenant-Id headers to be allowed to be passed through to the backend service. These are used to determine which organization and workspace (previously called â€œtenantâ€) the request is for.\\n\\u200bResource requirements for the application?\\n\\nIn kubernetes, we recommend a minimum helm configuration which can be found in here. For docker, we recommend a minimum of 16GB of RAM and 4 CPUs.\\nFor Postgres, we recommend a minimum of 8GB of RAM and 2 CPUs.\\nFor Redis, we recommend 4GB of RAM and 2 CPUs.\\nFor Clickhouse, we recommend 32GB of RAM and 8 CPUs.\\n\\n\\u200bSAML SSO FAQs\\n\\u200bHow do I change a SAML SSO userâ€™s email address?\\nSome identity providers retain the original User ID through an email change while others do not, so we recommend that you follow these steps to avoid duplicate users in LangSmith:\\n\\nRemove the user from the organization (see here)\\nChange their email address in the IdP\\nHave them login to LangSmith again via SAML SSO - this will trigger the usual JIT provisioning flow with their new email address\\n\\n\\u200bHow do I fix â€œ405 method not allowedâ€?\\nEnsure youâ€™re using the correct ACS URL: https://auth.langchain.com/auth/v1/sso/saml/acs\\n\\u200bSCIM FAQs\\n\\u200bCan I use SCIM without SAML SSO?\\n\\nCloud: No, SAML SSO is required for SCIM in cloud deployments\\nSelf-hosted: Yes, SCIM works with OAuth with Client Secret authentication mode\\n\\n\\u200bWhat happens if I have both JIT provisioning and SCIM enabled?\\nJIT provisioning and SCIM can conflict with each other. We recommend disabling JIT provisioning before enabling SCIM to ensure consistent user provisioning behavior.\\n\\u200bHow do I change a userâ€™s role or workspace access?\\nUpdate the userâ€™s group membership in your IdP. The changes will be synchronized to LangSmith according to the role precedence rules.\\n\\u200bWhat happens when a user is removed from all groups?\\nThe user will be deprovisioned from your LangSmith organization according to your IdPâ€™s deprovisioning settings.\\n\\u200bCan I use custom group names?\\nYes. If your identity provider supports syncing alternate fields to the displayName group attribute, you may use an alternate attribute (like description) as the displayName in LangSmith and retain full customizability of the identity provider group name. Otherwise, groups must follow the specific naming convention described in the Group Naming Convention section to properly map to LangSmith roles and workspaces.\\n\\u200bWhy is my Okta integration not working?\\nSee Oktaâ€™s troubleshooting guide here: https://help.okta.com/en-us/content/topics/users-groups-profiles/usgp-group-push-troubleshoot.htm.Was this page helpful?YesNoSuggest editsUser managementCloud architecture and scalabilityâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/self_hosting/installation', 'loc': 'https://docs.smith.langchain.com/self_hosting/installation', 'changefreq': 'weekly', 'priority': '0.5'}, page_content=\"Page Not FoundSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewSetupInstall on KubernetesInstall on DockerInteract with an installationUpgrade an installationConfigure egress for subscription metricsView trace counts across an organizationLangSmith-managed ClickHouseConfigurationConfigure for scaleEnable TTL & data retentionCreate an Ingress for installations (Kubernetes)Mirror images for your installationUse environment variables for model providersTroubleshootingAuthentication & access controlSet up basic authenticationSet up SSO with OAuth2.0 & OIDCCustomize user managementConfigure custom TLS certificatesUse an existing secret for your installation (Kubernetes)Connect external servicesEnable blob storageConnect to an external ClickHouse databaseConnect to an external PostgreSQL databaseConnect to an external Redis databaseScriptsDelete workspacesDelete organizationsDelete tracesGenerate ClickHouse StatsGenerate query statsRun support queries against PostgreSQLRun support queries against ClickHouseObservabilityExport LangSmith telemetry to your observability backendConfigure your collector for telemetryDeploy an observability stackOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationPage Not FoundGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForum404Page Not FoundWe couldn't find the page you were looking for. Maybe you were looking for?Self-host LangSmith with DockerSelf-host LangSmith on KubernetesInteract with your self-hosted instance of LangSmithAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/self_hosting/installation/docker', 'loc': 'https://docs.smith.langchain.com/self_hosting/installation/docker', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Self-host LangSmith with Docker - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewSetupInstall on KubernetesInstall on DockerInteract with an installationUpgrade an installationConfigure egress for subscription metricsView trace counts across an organizationLangSmith-managed ClickHouseConfigurationConfigure for scaleEnable TTL & data retentionCreate an Ingress for installations (Kubernetes)Mirror images for your installationUse environment variables for model providersTroubleshootingAuthentication & access controlSet up basic authenticationSet up SSO with OAuth2.0 & OIDCCustomize user managementConfigure custom TLS certificatesUse an existing secret for your installation (Kubernetes)Connect external servicesEnable blob storageConnect to an external ClickHouse databaseConnect to an external PostgreSQL databaseConnect to an external Redis databaseScriptsDelete workspacesDelete organizationsDelete tracesGenerate ClickHouse StatsGenerate query statsRun support queries against PostgreSQLRun support queries against ClickHouseObservabilityExport LangSmith telemetry to your observability backendConfigure your collector for telemetryDeploy an observability stackOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationSetupSelf-host LangSmith with DockerGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pagePrerequisitesRunning via Docker Compose1. Fetch the LangSmith docker-compose.yml file2. Configure environment variables3. Start serverValidate your deployment:Checking the logsStopping the serverUsing LangSmithSetupSelf-host LangSmith with DockerCopy pageCopy pageSelf-hosting LangSmith is an add-on to the Enterprise Plan designed for our largest, most security-conscious customers. See our pricing page for more detail, and contact our sales team if you want to get a license key to trial LangSmith in your environment.\\nThis guide provides instructions for installing and setting up your environment to run LangSmith locally using Docker. You can do this either by using the LangSmith SDK or by using Docker Compose directly.\\nNote that use of Docker is limited to local development environments only and does not extend support to container services such as AWS Elastic Container Service, Azure Container Instances, and Google Cloud Run.\\nDo not use this in production.\\n\\u200bPrerequisites\\n\\n\\nEnsure Docker is installed and running on your system. You can verify this by running:\\nCopydocker info\\n\\nIf you donâ€™t see any server information in the output, make sure Docker is installed correctly and launch the Docker daemon.\\n\\nRecommended: At least 4 vCPUs, 16GB Memory available on your machine.\\n\\nYou may need to tune resource requests/limits for all of our different services based off of organization size/usage\\n\\n\\nDisk Space: LangSmith can potentially require a lot of disk space. Ensure you have enough disk space available.\\n\\n\\n\\nLangSmith License Key\\n\\nYou can get this from your LangChain representative. Contact our sales team for more information.\\n\\n\\n\\nApi Key Salt\\n\\nThis is a secret key that you can generate. It should be a random string of characters.\\nYou can generate this using the following command:\\n\\nCopyopenssl rand -base64 32\\n\\n\\n\\nEgress to https://beacon.langchain.com (if not running in offline mode)\\n\\nLangSmith requires egress to https://beacon.langchain.com for license verification and usage reporting. This is required for LangSmith to function properly. You can find more information on egress requirements in the Egress section.\\n\\n\\n\\nConfiguration\\n\\nThere are several configuration options that you can set in the .env file. You can find more information on the available configuration options in the Configuration section.\\n\\n\\n\\n\\u200bRunning via Docker Compose\\nThe following explains how to run the LangSmith using Docker Compose. This is the most flexible way to run LangSmith without Kubernetes. The default configuration for Docker Compose is intended for local testing only and not for instances where any services are exposed to the public internet. In production, we highly recommend using a secured Kubernetes environment.\\n\\u200b1. Fetch the LangSmith docker-compose.yml file\\nYou can find the docker-compose.yml file and related files in the LangSmith SDK repository here: LangSmith Docker Compose File\\nCopy the docker-compose.yml file and all files in that directory from the LangSmith SDK to your project directory.\\n\\nEnsure that you copy the users.xml file as well.\\n\\n\\u200b2. Configure environment variables\\n\\nCopy the .env.example file from the LangSmith SDK to your project directory and rename it to .env.\\nConfigure the appropriate values in the .env file. You can find the available configuration options in the Configuration section.\\n\\nYou can also set these environment variables in the docker-compose.yml file directly or export them in your terminal. We recommend setting them in the .env file.\\n\\u200b3. Start server\\nStart the LangSmith application by executing the following command in your terminal:\\nCopydocker-compose up\\n\\nYou can also run the server in the background by running:\\nCopydocker-compose up -d\\n\\n\\u200bValidate your deployment:\\n\\n\\nCurl the exposed port of the cli-langchain-frontend-1 container:\\nCopycurl localhost:1980/info{\"version\":\"0.5.7\",\"license_expiration_time\":\"2033-05-20T20:08:06\",\"batch_ingest_config\":{\"scale_up_qsize_trigger\":1000,\"scale_up_nthreads_limit\":16,\"scale_down_nempty_trigger\":4,\"size_limit\":100,\"size_limit_bytes\":20971520}}\\n\\n\\n\\nVisit the exposed port of the cli-langchain-frontend-1 container on your browser\\nThe LangSmith UI should be visible/operational at http://localhost:1980\\n\\n\\n\\n\\u200bChecking the logs\\nIf, at any point, you want to check if the server is running and see the logs, run\\nCopydocker-compose logs\\n\\n\\u200bStopping the server\\nCopydocker-compose down\\n\\n\\u200bUsing LangSmith\\nNow that LangSmith is running, you can start using it to trace your code. You can find more information on how to use self-hosted LangSmith in the self-hosted usage guide.\\nYour LangSmith instance is now running but may not be fully setup yet.\\nIf you used one of the basic configs, you may have deployed a no-auth configuration. In this state, there is no authentication or concept of user accounts nor API keys and traces can be submitted directly without an API key so long as the hostname is passed to the LangChain tracer/LangSmith SDK.\\nAs a next step, it is strongly recommended you work with your infrastructure administrators to:\\n\\nSetup DNS for your LangSmith instance to enable easier access\\nConfigure SSL to ensure in-transit encryption of traces submitted to LangSmith\\nConfigure LangSmith for oauth authentication or basic authentication to secure your LangSmith instance\\nSecure access to your Docker environment to limit access to only the LangSmith frontend and API\\nConnect LangSmith to secured Postgres and Redis instances\\nWas this page helpful?YesNoSuggest editsInstall on KubernetesInteract with an installationâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/self_hosting/installation/kubernetes', 'loc': 'https://docs.smith.langchain.com/self_hosting/installation/kubernetes', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Self-host LangSmith on Kubernetes - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewSetupInstall on KubernetesInstall on DockerInteract with an installationUpgrade an installationConfigure egress for subscription metricsView trace counts across an organizationLangSmith-managed ClickHouseConfigurationConfigure for scaleEnable TTL & data retentionCreate an Ingress for installations (Kubernetes)Mirror images for your installationUse environment variables for model providersTroubleshootingAuthentication & access controlSet up basic authenticationSet up SSO with OAuth2.0 & OIDCCustomize user managementConfigure custom TLS certificatesUse an existing secret for your installation (Kubernetes)Connect external servicesEnable blob storageConnect to an external ClickHouse databaseConnect to an external PostgreSQL databaseConnect to an external Redis databaseScriptsDelete workspacesDelete organizationsDelete tracesGenerate ClickHouse StatsGenerate query statsRun support queries against PostgreSQLRun support queries against ClickHouseObservabilityExport LangSmith telemetry to your observability backendConfigure your collector for telemetryDeploy an observability stackOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationSetupSelf-host LangSmith on KubernetesGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pagePrerequisitesDatabasesKubernetes cluster requirementsConfigure your Helm Charts:Deploying to Kubernetes:Validate your deployment:Using LangSmithSetupSelf-host LangSmith on KubernetesCopy pageCopy pageSelf-hosting LangSmith is an add-on to the Enterprise Plan designed for our largest, most security-conscious customers. See our pricing page for more detail, and contact our sales team if you want to get a license key to trial LangSmith in your environment.\\nThis guide will walk you through the process of deploying LangSmith to a Kubernetes cluster. We will use Helm to install LangSmith and its dependencies.\\nWeâ€™ve successfully tested LangSmith on the following Kubernetes distributions:\\n\\nGoogle Kubernetes Engine (GKE)\\nAmazon Elastic Kubernetes Service (EKS)\\nAzure Kubernetes Service (AKS)\\nOpenShift (4.14+)\\nMinikube and Kind (for development purposes)\\n\\nWe have several Terraform modules the help in the provisioning of resources for LangSmith. You can find those in our public Terraform repo.Supported cloud providers include:\\nAWS terraform modules\\nAzure terraform modules\\nYou can click on the links above to see the documentation for each module. These modules are designed to help you quickly set up the necessary infrastructure for LangSmith, including Kubernetes clusters, storage, and networking.\\n\\u200bPrerequisites\\nEnsure you have the following tools/items ready. Some items are marked optional:\\n\\n\\nLangSmith License Key\\n\\nYou can get this from your LangChain representative. Contact our sales team for more information.\\n\\n\\n\\nApi Key Salt\\n\\nThis is a secret key that you can generate. It should be a random string of characters.\\nYou can generate this using the following command:\\n\\nCopyopenssl rand -base64 32\\n\\n\\n\\nJWT Secret (Optional but used for basic auth)\\n\\nThis is a secret key that you can generate. It should be a random string of characters.\\nYou can generate this using the following command:\\n\\nCopyopenssl rand -base64 32\\n\\n\\n\\n\\u200bDatabases\\nLangSmith uses a PostgreSQL database, a Redis cache, and a ClickHouse database to store traces. By default, these services are installed inside your Kubernetes cluster. However, we highly recommend using external databases instead. For PostgreSQL and Redis, the best option is your cloud providerâ€™s managed services.\\nFor more information, refer to the following setup guides for external services:\\n\\nPostgreSQL\\nRedis\\nClickHouse\\n\\n\\u200bKubernetes cluster requirements\\n\\n\\nYou will need a working Kubernetes cluster that you can access via kubectl. Your cluster should have the following minimum requirements:\\n\\n\\nRecommended: At least 16 vCPUs, 64GB Memory available\\n\\nYou may need to tune resource requests/limits for all of our different services based off of organization size/usage. Our recommendations can be found here.\\nWe recommend using a cluster autoscaler to handle scaling up/down of nodes based on resource usage.\\nWe recommend setting up the metrics server so that autoscaling can be turned on.\\nIf you are running Clickhouse in-cluster, you must have a node with at least 4 vCPUs and 16GB of memory allocatable as ClickHouse will request this amount of resources by default.\\n\\n\\n\\nValid Dynamic PV provisioner or PVs available on your cluster (required only if you are running databases in-cluster)\\n\\nTo enable persistence, we will try to provision volumes for any database running in-cluster.\\nIf using PVs in your cluster, we highly recommend setting up backups in a production environment.\\nWe strongly encourage using a storage class backed by SSDs for better performance. We recommend 7000 IOPS and 1000 MiB/s throughput.\\nOn EKS, you may need to ensure you have the ebs-csi-driver installed and configured for dynamic provisioning. Refer to the EBS CSI Driver documentation for more information.\\n\\nYou can verify this by running:\\nCopykubectl get storageclass\\n\\nThe output should show at least one storage class with a provisioner that supports dynamic provisioning. For example:\\nCopyNAME            PROVISIONER                 RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\\ngp2 (default)   ebs.csi.eks.amazonaws.com   Delete          WaitForFirstConsumer   true                   161d\\n\\nWe highly recommend using a storage class that supports volume expansion. This is because traces can potentially require a lot of disk space and your volumes may need to be resized over time.\\nRefer to the Kubernetes documentation for more information on storage classes.\\n\\n\\n\\n\\nHelm\\n\\nTo install helm refer to the Helm documentation\\n\\n\\n\\nEgress to https://beacon.langchain.com (if not running in offline mode)\\n\\nLangSmith requires egress to https://beacon.langchain.com for license verification and usage reporting. This is required for LangSmith to function properly. You can find more information on egress requirements in the Egress section.\\n\\n\\n\\n\\u200bConfigure your Helm Charts:\\n\\n\\nCreate a new file called langsmith_config.yaml with the configuration options from the previous step.\\n\\nThere are several configuration options that you can set in the langsmith_config.yaml file. You can find more information on specific configuration options in the Configuration section.\\nIf you are new to Kubernetes or Helm, weâ€™d recommend starting with one of the example configurations in the examples directory of the Helm Chart repository here: LangSmith helm chart examples.\\nYou can see a full list of configuration options in the values.yaml file in the Helm Chart repository here: LangSmith Helm Chart\\n\\n\\n\\nAt a minimum, you will need to set the following configuration options (using basic auth):\\nCopyconfig:\\n  langsmithLicenseKey: \"<your license key>\"\\n  apiKeySalt: \"<your api key salt>\"\\n  authType: mixed\\n  basicAuth:\\n    enabled: true\\n    initialOrgAdminEmail: \"admin@langchain.dev\" # Change this to your admin email address\\n    initialOrgAdminPassword: \"secure-password\" # Must be at least 12 characters long and have at least one lowercase, uppercase, and symbol\\n    jwtSecret: <your jwt salt> # A random string of characters used to sign JWT tokens for basic auth.\\n\\n\\n\\nYou will also need to specify connection details for any external databases you are using.\\n\\u200bDeploying to Kubernetes:\\n\\n\\nVerify that you can connect to your Kubernetes cluster(note: We highly suggest installing into an empty namespace)\\n\\n\\nRun kubectl get pods\\nOutput should look something like:\\nCopykubectl get pods                                                                                                                                                                     âŽˆ langsmith-eks-2vauP7wf 21:07:46No resources found in default namespace.\\n\\n\\n\\nIf you are using a namespace other than the default namespace, you will need to specify the namespace in the helm and kubectl commands by using the -n <namespace> flag.\\n\\n\\nEnsure you have the LangChain Helm repo added. (skip this step if you are using local charts)\\nCopyhelm repo add langchain https://langchain-ai.github.io/helm/\"langchain\" has been added to your repositories\\n\\n\\n\\nFind the latest version of the chart. You can find the available versions in the Helm Chart repository.\\n\\nWe generally recommend using the latest version.\\nYou can also run helm search repo langchain/langsmith --versions to see the available versions. The output will look something like this:\\n\\nCopylangchain/langsmith     0.10.14         0.10.32         Helm chart to deploy the langsmith application ...\\nlangchain/langsmith     0.10.13         0.10.32         Helm chart to deploy the langsmith application ...\\nlangchain/langsmith     0.10.12         0.10.32         Helm chart to deploy the langsmith application ...\\nlangchain/langsmith     0.10.11         0.10.29         Helm chart to deploy the langsmith application ...\\nlangchain/langsmith     0.10.10         0.10.29         Helm chart to deploy the langsmith application ...\\n\\n\\n\\nRun helm upgrade -i langsmith langchain/langsmith --values langsmith_config.yaml --version <version> -n <namespace> --wait --debug\\n\\nReplace <namespace> with the namespace you want to deploy LangSmith to.\\nReplace <version> with the version of LangSmith you want to install from the previous step. Most users should install the latest version available.\\n\\nOnce the helm install command runs and finishes successfully, you should see output similar to this:\\nCopyNAME: langsmith\\nLAST DEPLOYED: Fri Sep 17 21:08:47 2021\\nNAMESPACE: langsmith\\nSTATUS: deployed\\nREVISION: 1\\nTEST SUITE: None\\n\\nThis may take a few minutes to complete as it will create several Kubernetes resources and run several jobs to initialize the database and other services.\\n\\n\\nRun kubectl get pods Output should now look something like this (note the exact pod names may vary based on the version and configuration you used):\\nCopylangsmith-backend-6ff46c99c4-wz22d       1/1     Running   0          3h2m\\nlangsmith-frontend-6bbb94c5df-8xrlr      1/1     Running   0          3h2m\\nlangsmith-hub-backend-5cc68c888c-vppjj   1/1     Running   0          3h2m\\nlangsmith-playground-6d95fd8dc6-x2d9b    1/1     Running   0          3h2m\\nlangsmith-postgres-0                     1/1     Running   0          9h\\nlangsmith-queue-5898b9d566-tv6q8         1/1     Running   0          3h2m\\nlangsmith-redis-0                        1/1     Running   0          9h\\n\\n\\n\\n\\u200bValidate your deployment:\\n\\n\\nRun kubectl get services\\nOutput should look something like:\\nCopyNAME                    TYPE           CLUSTER-IP       EXTERNAL-IP                                                               PORT(S)        AGE\\nlangsmith-backend       ClusterIP      172.20.140.77    <none>                                                                    1984/TCP       35h\\nlangsmith-frontend      LoadBalancer   172.20.253.251   <external ip>                                                             80:31591/TCP   35h\\nlangsmith-hub-backend   ClusterIP      172.20.112.234   <none>                                                                    1985/TCP       35h\\nlangsmith-playground    ClusterIP      172.20.153.194   <none>                                                                    3001/TCP       9h\\nlangsmith-postgres      ClusterIP      172.20.244.82    <none>                                                                    5432/TCP       35h\\nlangsmith-redis         ClusterIP      172.20.81.217    <none>                                                                    6379/TCP       35h\\n\\n\\n\\nCurl the external ip of the langsmith-frontend service:\\nCopycurl <external ip>/api/tenants\\n\\nExpected output:\\nCopy[{\"id\":\"00000000-0000-0000-0000-000000000000\",\"has_waitlist_access\":true,\"created_at\":\"2023-09-13T18:25:10.488407\",\"display_name\":\"Personal\",\"config\":{\"is_personal\":true,\"max_identities\":1},\"tenant_handle\":\"default\"}]\\n\\n\\n\\nVisit the external ip for the langsmith-frontend service on your browser\\nThe LangSmith UI should be visible/operational\\n\\n\\n\\n\\u200bUsing LangSmith\\nNow that LangSmith is running, you can start using it to trace your code. You can find more information on how to use self-hosted LangSmith in the self-hosted usage guide.\\nYour LangSmith instance is now running but may not be fully setup yet.\\nIf you used one of the basic configs, you will have a default admin user account created for you. You can log in with the email address and password you specified in the langsmith_config.yaml file.\\nAs a next step, it is strongly recommended you work with your infrastructure administrators to:\\n\\nSetup DNS for your LangSmith instance to enable easier access\\nConfigure SSL to ensure in-transit encryption of traces submitted to LangSmith\\nConfigure LangSmith with Single Sign-On to secure your LangSmith instance\\nConnect LangSmith to external Postgres and Redis instances\\nSet up Blob Storage for storing large files\\n\\nReview our configuration section for more information on how to configure these options.Was this page helpful?YesNoSuggest editsOverviewInstall on DockerâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/self_hosting/langsmith_managed_clickhouse', 'loc': 'https://docs.smith.langchain.com/self_hosting/langsmith_managed_clickhouse', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='LangSmith-managed ClickHouse - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewSetupInstall on KubernetesInstall on DockerInteract with an installationUpgrade an installationConfigure egress for subscription metricsView trace counts across an organizationLangSmith-managed ClickHouseConfigurationConfigure for scaleEnable TTL & data retentionCreate an Ingress for installations (Kubernetes)Mirror images for your installationUse environment variables for model providersTroubleshootingAuthentication & access controlSet up basic authenticationSet up SSO with OAuth2.0 & OIDCCustomize user managementConfigure custom TLS certificatesUse an existing secret for your installation (Kubernetes)Connect external servicesEnable blob storageConnect to an external ClickHouse databaseConnect to an external PostgreSQL databaseConnect to an external Redis databaseScriptsDelete workspacesDelete organizationsDelete tracesGenerate ClickHouse StatsGenerate query statsRun support queries against PostgreSQLRun support queries against ClickHouseObservabilityExport LangSmith telemetry to your observability backendConfigure your collector for telemetryDeploy an observability stackOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationSetupLangSmith-managed ClickHouseGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageArchitecture OverviewRequirementsData storageStored feedback data fieldsStored run data fieldsSetupLangSmith-managed ClickHouseCopy pageCopy pagePlease read the LangSmith architectural overview and guide on connecting to external ClickHouse before proceeding with this guide.\\nLangSmith uses ClickHouse as the primary storage engine for traces and feedback. For easier management and scaling, it is recommended to connect a self-hosted LangSmith instance to an external ClickHouse instance. LangSmith-managed ClickHouse is an option that allows you to use a fully managed ClickHouse instance that is monitored and maintained by the LangSmith team.\\n\\u200bArchitecture Overview\\nThe architecture of using LangSmith-managed ClickHouse with your self-hosted LangSmith instance is similar to using a fully self-hosted ClickHouse instance, with a few key differences:\\n\\nYou will need to set up a private network connection between your LangSmith instance and the LangSmith-managed ClickHouse instance. This is to ensure that your data is secure and that you can connect to the ClickHouse instance from your self-hosted LangSmith instance.\\nWith this option, sensitive information (inputs and outputs) of your traces will be stored in cloud object storage (S3 or GCS) within your cloud instead of ClickHouse to ensure that sensitive information doesnâ€™t leave your VPC. For more details on where particular data fields are stored, refer to Data storage.\\nThe LangSmith team will monitor your ClickHouse instance and ensure that it is running smoothly. This allows us to track metrics like run-ingestion delay and query performance.\\n\\nThe overall architecture looks like this:\\n\\n\\u200bRequirements\\n\\nYou must use a supported blob storage option. Read the blob storage guide for more information.\\nTo use private endpoints, ensure that your VPC is in a ClickHouse Cloud supported region. Otherwise, you will need to use a public endpoint we will secure with firewall rules. Your VPC will need to have a NAT gateway to allow us to allowlist your traffic.\\nYou must have a VPC that can connect to the LangSmith-managed ClickHouse service. You will need to work with our team to set up the necessary networking.\\nYou must have a LangSmith self-hosted instance running. You can use our managed ClickHouse service with both Kubernetes and Docker installations.\\n\\n\\u200bData storage\\nClickHouse stores runs and feedback data, specifically:\\n\\nAll feedback data fields.\\nSome run data fields.\\n\\nFor a list of fields, refer to Stored run data fields and Stored feedback data fields.\\nLangChain defines sensitive application data as inputs, outputs, errors, manifests, extras, and events of a run, since these fields may contain LLM prompts and completions. With LangSmith-managed ClickHouse, these sensitive fields are stored in cloud object storage (S3 or GCS) within your cloud, while the rest of the run data is stored in ClickHouse, ensuring sensitive information never leaves your VPC.\\n\\u200bStored feedback data fields\\nBecause all feedback data is stored in ClickHouse, do not send sensitive information in feedback (scores and annotations/comments) or in any other run fields that are mentioned in Stored run data fields.\\nUsing a LangSmith-managed ClickHouse setup, all feedback data fields are stored in ClickHouse:\\nField NameTypeDescriptionidUUIDUnique identifier for the record itselfcreated_atdatetimeTimestamp when the record was createdmodified_atdatetimeTimestamp when the record was last modifiedsession_idUUIDUnique identifier for the experiment or tracing project the run was a part ofrun_idUUIDUnique identifier for a specific run within a sessionkeystringA key describing the criteria of the feedback, eg â€œcorrectnessâ€scorenumberNumerical score associated with the feedback keyvaluestringReserved for storing a value associated with the score. Useful for categorical feedback.commentstringAny comment or annotation associated with the record. This can be a justification for the score given.correctionobjectReserved for storing correction details, if anyfeedback_sourceobjectObject containing information about the feedback sourcefeedback_source.typestringThe type of source where the feedback originated, eg â€œapiâ€, â€œappâ€, â€œevaluatorâ€feedback_source.metadataobjectReserved for additional metadata, currentlyfeedback_source.user_idUUIDUnique identifier for the user providing feedback\\nThis reference doc explains the stored feedback format, which is the LangSmithâ€™s way of representing evaluation scores and annotations on runs.\\n\\u200bStored run data fields\\nRun data fields are split between the managed ClickHouse database and your cloud object storage (e.g., S3 or GCS).\\nFor run fields stored in object storage, only a reference or pointer is kept in ClickHouse. For example, inputs and outputs content are offloaded to S3/GCS, with the ClickHouse record storing corresponding S3 URLs in the inputs_s3_urls and outputs_s3_urls fields.\\nThe table details each run field and where it is stored:\\nFieldStorage LocationidClickHousenameClickHouseinputsObject Storagerun_typeClickHousestart_timeClickHouseend_timeClickHouseextraObject StorageerrorObject StorageoutputsObject StorageeventsObject StoragetagsClickHousetrace_idClickHousedotted_orderClickHousestatusClickHousechild_run_idsClickHousedirect_child_run_idsClickHouseparent_run_idsClickHousefeedback_statsClickHousereference_example_idClickHousetotal_tokensClickHouseprompt_tokensClickHousecompletion_tokensClickHousetotal_costClickHouseprompt_costClickHousecompletion_costClickHousefirst_token_timeClickHousesession_idClickHousein_datasetClickHouseparent_run_idClickHouseexecution_order (deprecated)ClickHouseserializedClickHousemanifest_id (deprecated)ClickHousemanifest_s3_idClickHouseinputs_s3_urlsClickHouseoutputs_s3_urlsClickHouseprice_model_idClickHouseapp_pathClickHouselast_queued_atClickHouseshare_tokenClickHouse\\nThis reference doc explains the format of stored runs (spans), which are the building blocks of traces.Was this page helpful?YesNoSuggest editsView trace counts across an organizationConfigure for scaleâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/self_hosting/observability', 'loc': 'https://docs.smith.langchain.com/self_hosting/observability', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Observability - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsManual instrumentationConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationObservabilityGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumObservabilityCopy pageCopy pageWelcome to the LangSmith Observability documentation. The following sections help you set up and use tracing, monitoring, and observability features:\\n\\n\\nSet up tracing: Configure tracing for your applications with basic configuration, integrations with popular frameworks, and advanced configuration options.\\n\\n\\nView traces: Access and manage your traces through the UI and API, including filtering, exporting, sharing, and comparing traces.\\n\\n\\nMonitoring: Set up dashboards and alerts to monitor your application performance and receive notifications when issues arise.\\n\\n\\nAutomations: Configure rules, webhooks, and online evaluations to automate your observability workflows.\\n\\n\\nHuman Feedback: Collect and manage human feedback on your application outputs through annotation queues and inline annotation.\\n\\n\\nTrace a RAG application: Follow a tutorial to trace a Retrieval-Augmented Generation (RAG) application from start to finish.\\n\\n\\nFor terminology definitions and core concepts, refer to the introduction on observability.Was this page helpful?YesNoSuggest editsConceptsâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/self_hosting/observability/export_backend', 'loc': 'https://docs.smith.langchain.com/self_hosting/observability/export_backend', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Export LangSmith telemetry to your observability backend - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewSetupInstall on KubernetesInstall on DockerInteract with an installationUpgrade an installationConfigure egress for subscription metricsView trace counts across an organizationLangSmith-managed ClickHouseConfigurationConfigure for scaleEnable TTL & data retentionCreate an Ingress for installations (Kubernetes)Mirror images for your installationUse environment variables for model providersTroubleshootingAuthentication & access controlSet up basic authenticationSet up SSO with OAuth2.0 & OIDCCustomize user managementConfigure custom TLS certificatesUse an existing secret for your installation (Kubernetes)Connect external servicesEnable blob storageConnect to an external ClickHouse databaseConnect to an external PostgreSQL databaseConnect to an external Redis databaseScriptsDelete workspacesDelete organizationsDelete tracesGenerate ClickHouse StatsGenerate query statsRun support queries against PostgreSQLRun support queries against ClickHouseObservabilityExport LangSmith telemetry to your observability backendConfigure your collector for telemetryDeploy an observability stackOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationObservabilityExport LangSmith telemetry to your observability backendGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageLogs: OTel ExampleMetrics: OTel ExampleLangSmith ServicesFrontend NginxPostgres + RedisClickhouseTraces: OTel ExampleObservabilityExport LangSmith telemetry to your observability backendCopy pageCopy pageThis section is only applicable for Kubernetes deployments.\\nSelf-Hosted LangSmith instances produce telemetry data in the form of logs, metrics and traces. This section will show you how to access and export that data to an observability collector or backend.\\nThis section assumes that you have monitoring infrastructure set up already, or you will set up this infrastructure and want to know how to configure LangSmith to collect data from it.\\nInfrastructure refers to:\\n\\nCollectors, such as OpenTelemetry, FluentBit or Prometheus.\\nObservability backends, such as Datadog or the Grafana ecosystem.\\n\\n\\u200bLogs: OTel Example\\nAll services that are part of the LangSmith self-hosted deployment write logs to their nodeâ€™s filesystem and to stdout. In order to access these logs, you need to set up your collector to read from either the filesystem or stdout. Most popular collectors support reading logs from filesystems.\\n\\nOpenTelemetry: File Log Receiver\\nFluentBit: Tail Input\\nDatadog: Kubernetes Log Collection\\n\\n\\u200bMetrics: OTel Example\\n\\u200bLangSmith Services\\nThe following LangSmith services expose metrics at an endpoint, in the Prometheus metrics format. The frontend does not currently expose metrics.\\n\\nBackend: http://<langsmith_release_name>-backend.<namespace>.svc.cluster.local:1984/metrics\\nPlatform Backend: http://<langsmith_release_name>-platform-backend.<namespace>.svc.cluster.local:1986/metrics\\nPlayground: http://<langsmith_release_name>-playground.<namespace>.svc.cluster.local:1988/metrics\\n(LangGraph Platform Control Plane only) Host Backend: http://<langsmith_release_name>-host-backend.<namespace>.svc.cluster.local:1985/metrics\\n\\nYou can use a Prometheus or OpenTelemetry collector to scrape the endpoints, and export metrics to the backend of your choice.\\n\\u200bFrontend Nginx\\nThe frontend service exposes its Nginx metrics at the following endpoint: langsmith-frontend.langsmith.svc.cluster.local:80/nginx_status. You can either scrape them yourself, or bring up a Prometheus Nginx exporter using the LangSmith Observability Helm Chart\\nThe following sections apply for in-cluster databases only. If you are using external databases, you will need to configure exposing and fetching metrics.\\n\\u200bPostgres + Redis\\nIf you are using in-cluster Postgres/Redis instances, you can use a Prometheus exporter to expose metrics from your instance. You can deploy your own, or if you would like, you can use the LangSmith Observability Helm Chart to deploy an exporter for you.\\n\\u200bClickhouse\\nThe in-cluster Clickhouse is configured to expose metrics without the need for an exporter. You can use your collector to scrape metrics at http://<langsmith_release_name>-clickhouse.<namespace>.svc.cluster.local:9363/metrics\\n\\u200bTraces: OTel Example\\nThe LangSmith Backend, Platform Backend, Playground and LangSmith Queue deployments have been instrumented to emit Otel traces. Tracing is toggled off by default, and can be enabled for all LangSmith services with the following in your langsmith_config.yaml (or equivalent) file:\\nCopyconfig:\\n  tracing:\\n    enabled: true\\n    endpoint: \"<your_collector_endpoint>\"\\n    useTls: true # / false\\n    env: \"ls_self_hosted\" # This value will be set as an \"env\" attribute in your spans\\n    exporter: \"http\" # must be either http or grpc\\nWas this page helpful?YesNoSuggest editsRun support queries against ClickHouseConfigure your collector for telemetryâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/self_hosting/observability/langsmith_collector', 'loc': 'https://docs.smith.langchain.com/self_hosting/observability/langsmith_collector', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Configure your collector for LangSmith telemetry - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewSetupInstall on KubernetesInstall on DockerInteract with an installationUpgrade an installationConfigure egress for subscription metricsView trace counts across an organizationLangSmith-managed ClickHouseConfigurationConfigure for scaleEnable TTL & data retentionCreate an Ingress for installations (Kubernetes)Mirror images for your installationUse environment variables for model providersTroubleshootingAuthentication & access controlSet up basic authenticationSet up SSO with OAuth2.0 & OIDCCustomize user managementConfigure custom TLS certificatesUse an existing secret for your installation (Kubernetes)Connect external servicesEnable blob storageConnect to an external ClickHouse databaseConnect to an external PostgreSQL databaseConnect to an external Redis databaseScriptsDelete workspacesDelete organizationsDelete tracesGenerate ClickHouse StatsGenerate query statsRun support queries against PostgreSQLRun support queries against ClickHouseObservabilityExport LangSmith telemetry to your observability backendConfigure your collector for telemetryDeploy an observability stackOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationObservabilityConfigure your collector for LangSmith telemetryGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageReceiversLogsMetricsTracesProcessorsRecommended OTEL ProcessorsExportersExample Collector Configuration: Logs SidecarExample Collector Configuration: Metrics and Traces GatewayObservabilityConfigure your collector for LangSmith telemetryCopy pageCopy pageThe various services in a LangSmith deployment emit telemetry data in the form of logs, metrics, and traces. You may already have telemetry collectors set up in your Kubernetes cluster, or would like to deploy one to monitor your application.\\nThis page describes how to configure an OTel Collector to gather telemetry data from LangSmith. Note that all of the concepts discussed below can be translated to other collectors such as Fluentd or FluentBit.\\nThis section is only applicable for Kubernetes deployments.\\n\\u200bReceivers\\n\\u200bLogs\\nThis is an example for a Sidecar collector to read logs from its own pod, excluding logs from non domain-specific containers. A Sidecar configuration is useful here because we require access to every containerâ€™s filesystem. A DaemonSet can also be used.\\nCopyfilelog:\\n  exclude:\\n    - \"**/otc-container/*.log\"\\n  include:\\n    - /var/log/pods/${POD_NAMESPACE}_${POD_NAME}_${POD_UID}/*/*.log\\n  include_file_name: false\\n  include_file_path: true\\n  operators:\\n    - id: container-parser\\n      type: container\\n  retry_on_failure:\\n    enabled: true\\n  start_at: end\\nenv:\\n  - name: POD_NAME\\n    valueFrom:\\n      fieldRef:\\n        fieldPath: metadata.name\\n  - name: POD_NAMESPACE\\n    valueFrom:\\n      fieldRef:\\n        fieldPath: metadata.namespace\\n  - name: POD_UID\\n    valueFrom:\\n      fieldRef:\\n        fieldPath: metadata.uid\\nvolumes:\\n  - name: varlogpods\\n    hostPath:\\n      path: /var/log/pods\\nvolumeMounts:\\n  - name: varlogpods\\n    mountPath: /var/log/pods\\n    readOnly: true\\n\\nThis configuration requires â€˜getâ€™, â€˜listâ€™, and â€˜watchâ€™ permissions on pods in the given namespace.\\n\\u200bMetrics\\nMetrics can be scraped using the Prometheus endpoints. A single instance Gateway collector can be be used to avoid duplication of queries when fetching metrics. The following config scrapes all of the default named LangSmith services:\\nCopyprometheus:\\n  config:\\n    scrape_configs:\\n      - job_name: langsmith-services\\n        metrics_path: /metrics\\n        scrape_interval: 15s\\n        # Only scrape endpoints in the LangSmith namespace\\n        kubernetes_sd_configs:\\n          - role: endpoints\\n            namespaces:\\n              names: [<langsmith-namespace>]\\n        relabel_configs:\\n          # Only scrape services with the name langsmith-.*\\n          - source_labels: [__meta_kubernetes_service_name]\\n            regex: \"langsmith-.*\"\\n            action: keep\\n          # Only scrape ports with the following names\\n          - source_labels: [__meta_kubernetes_endpoint_port_name]\\n            regex: \"(backend|platform|playground|redis-metrics|postgres-metrics|metrics)\"\\n            action: keep\\n          # Promote useful metadata into regular labels\\n          - source_labels: [__meta_kubernetes_service_name]\\n            target_label: k8s_service\\n          - source_labels: [__meta_kubernetes_pod_name]\\n            target_label: k8s_pod\\n          # Replace the default \"host:port\" as Prom\\'s instance label\\n          - source_labels: [__address__]\\n            target_label: instance\\n\\nThis configuration requires â€˜getâ€™, â€˜listâ€™, and â€˜watchâ€™ permissions on pods, services and endpoints in the given namespace.\\n\\u200bTraces\\nFor traces, you need to enable the OTLP receiver. The following configuration can be used to listen to HTTP traces on port 4318, and GRPC on port 4317:\\nCopyotlp:\\n  protocols:\\n    grpc:\\n      endpoint: 0.0.0.0:4317\\n    http:\\n      endpoint: 0.0.0.0:4318\\n\\n\\u200bProcessors\\n\\u200bRecommended OTEL Processors\\nThe following processors are recommended when using the OTel collector:\\n\\nBatch Processor: Groups the data into batches before sending to exporters.\\nMemory Limiter: Prevents the collector from using too much memory and crashing. When the soft limit is crossed, the collector stops accepting new data.\\nKubernetes Attributes Processor: Adds Kubernetes metadata such as pod name into the telemetry data.\\n\\n\\u200bExporters\\nExporters just need to point to an external endpoint of your liking. The following configuration allows you to configure a separate endpoint for logs, metrics and traces:\\nCopyotlphttp/logs:\\n  endpoint: <your_logs_endpoint>\\notlphttp/metrics:\\n  endpoint: <your_metrics_endpoint>\\notlphttp/traces:\\n  endpoint: <your_traces_endpoint>\\n\\nThe OTel Collector also supports exporting directly to a Datadog endpoint.\\n\\u200bExample Collector Configuration: Logs Sidecar\\nCopymode: sidecar\\nimage: otel/opentelemetry-collector-contrib\\nconfig:\\n  receivers:\\n    filelog:\\n      exclude:\\n        - \"**/otc-container/*.log\"\\n      include:\\n        - /var/log/pods/${POD_NAMESPACE}_${POD_NAME}_${POD_UID}/*/*.log\\n      include_file_name: false\\n      include_file_path: true\\n      operators:\\n        - id: container-parser\\n          type: container\\n      retry_on_failure:\\n        enabled: true\\n      start_at: end\\n  processors:\\n    batch:\\n      send_batch_size: 8192\\n      timeout: 10s\\n    memory_limiter:\\n      check_interval: 1m\\n      limit_percentage: 90\\n      spike_limit_percentage: 80\\n  exporters:\\n    otlphttp/logs:\\n      endpoint: <your-endpoint>\\n  service:\\n    pipelines:\\n      logs/langsmith:\\n        receivers: [filelog]\\n        processors: [batch, memory_limiter]\\n        exporters: [otlphttp/logs]\\nenv:\\n  - name: POD_NAME\\n    valueFrom:\\n      fieldRef:\\n        fieldPath: metadata.name\\n  - name: POD_NAMESPACE\\n    valueFrom:\\n      fieldRef:\\n        fieldPath: metadata.namespace\\n  - name: POD_UID\\n    valueFrom:\\n      fieldRef:\\n        fieldPath: metadata.uid\\nvolumes:\\n  - name: varlogpods\\n    hostPath:\\n      path: /var/log/pods\\nvolumeMounts:\\n  - name: varlogpods\\n    mountPath: /var/log/pods\\n    readOnly: true\\n\\n\\u200bExample Collector Configuration: Metrics and Traces Gateway\\nCopymode: deployment\\nimage: otel/opentelemetry-collector-contrib\\nconfig:\\n  receivers:\\n    prometheus:\\n      config:\\n        scrape_configs:\\n          - job_name: langsmith-services\\n            metrics_path: /metrics\\n            scrape_interval: 15s\\n            # Only scrape endpoints in the LangSmith namespace\\n            kubernetes_sd_configs:\\n              - role: endpoints\\n                namespaces:\\n                  names: [<langsmith-namespace>]\\n            relabel_configs:\\n              # Only scrape services with the name langsmith-.*\\n              - source_labels: [__meta_kubernetes_service_name]\\n                regex: \"langsmith-.*\"\\n                action: keep\\n              # Only scrape ports with the following names\\n              - source_labels: [__meta_kubernetes_endpoint_port_name]\\n                regex: \"(backend|platform|playground|redis-metrics|postgres-metrics|metrics)\"\\n                action: keep\\n              # Promote useful metadata into regular labels\\n              - source_labels: [__meta_kubernetes_service_name]\\n                target_label: k8s_service\\n              - source_labels: [__meta_kubernetes_pod_name]\\n                target_label: k8s_pod\\n              # Replace the default \"host:port\" as Prom\\'s instance label\\n              - source_labels: [__address__]\\n                target_label: instance\\n    otlp:\\n      protocols:\\n        grpc:\\n          endpoint: 0.0.0.0:4317\\n        http:\\n          endpoint: 0.0.0.0:4318\\n  processors:\\n    batch:\\n      send_batch_size: 8192\\n      timeout: 10s\\n    memory_limiter:\\n      check_interval: 1m\\n      limit_percentage: 90\\n      spike_limit_percentage: 80\\n  exporters:\\n    otlphttp/metrics:\\n      endpoint: <metrics_endpoint>\\n    otlphttp/traces:\\n      endpoint: <traces_endpoint>\\n  service:\\n    pipelines:\\n      metrics/langsmith:\\n        receivers: [prometheus]\\n        processors: [batch, memory_limiter]\\n        exporters: [otlphttp/metrics]\\n      traces/langsmith:\\n        receivers: [otlp]\\n        processors: [batch, memory_limiter]\\n        exporters: [otlphttp/traces]\\nWas this page helpful?YesNoSuggest editsExport LangSmith telemetry to your observability backendDeploy an observability stackâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/self_hosting/observability/observability_stack', 'loc': 'https://docs.smith.langchain.com/self_hosting/observability/observability_stack', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Deploy an observability stack for your LangSmith deployment - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewSetupInstall on KubernetesInstall on DockerInteract with an installationUpgrade an installationConfigure egress for subscription metricsView trace counts across an organizationLangSmith-managed ClickHouseConfigurationConfigure for scaleEnable TTL & data retentionCreate an Ingress for installations (Kubernetes)Mirror images for your installationUse environment variables for model providersTroubleshootingAuthentication & access controlSet up basic authenticationSet up SSO with OAuth2.0 & OIDCCustomize user managementConfigure custom TLS certificatesUse an existing secret for your installation (Kubernetes)Connect external servicesEnable blob storageConnect to an external ClickHouse databaseConnect to an external PostgreSQL databaseConnect to an external Redis databaseScriptsDelete workspacesDelete organizationsDelete tracesGenerate ClickHouse StatsGenerate query statsRun support queries against PostgreSQLRun support queries against ClickHouseObservabilityExport LangSmith telemetry to your observability backendConfigure your collector for telemetryDeploy an observability stackOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationObservabilityDeploy an observability stack for your LangSmith deploymentGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageSection 1: Prometheus ExportersSection 2: Full Observability StackPrerequisites1. Compute Resources2. Cert-Manager3. OpenTelemetry OperatorInstallationPost-InstallationEnable Logs and Traces in LangSmithGrafana UsageObservabilityDeploy an observability stack for your LangSmith deploymentCopy pageCopy pageThis section is only applicable for Kubernetes deployments.\\nLangSmith applications expose telemetry data that can be sent to the backend of your choice. If you donâ€™t already have an observability stack, or prefer to keep LangSmith telemetry separate from your main application, you can use the LangSmith Observability Helm chart to deploy a basic observability stack.\\n\\u200bSection 1: Prometheus Exporters\\nUse this section if you would like to only deploy metrics exporters for the components in your self hosted deployment, which you can then scrape using your telemetry. If you would like a full observability stack deployed for you, go to the End-to-End Deployment Section.\\nThe helm chart provides a set of Prometheus exporters to expose metrics from Redis, Postgres, Nginx, and Kube state metrics.\\n\\nCreate a local file called langsmith_obs_config.yaml\\nCopy over the values from this file into langsmith_obs_config.yaml, making sure to modify the values to match your LangSmith deployment.\\nFind the latest version of the chart by running helm search repo langchain/langsmith-observability --versions.\\nGrab the latest version number, and run helm install langsmith-observability langchain/langsmith-observability --values langsmith_obs_config.yaml --version <version> -n <namespace> --wait --debug\\n\\nThis will allow you to scrape metrics at the following service endpoints:\\n\\nPostgres: langsmith-observability-postgres-exporter:9187/metrics\\nRedis: langsmith-observability-redis-exporter:9121/metrics\\nNginx: langsmith-observability-nginx-exporter:9113/metrics\\nKubeStateMetrics: langsmith-observability-kube-state-metrics:8080/metrics\\n\\nYou should see the following if the installation went through:\\nCopyRelease \"langsmith-observability\" has been installed. Happy Helming!NAME: langsmith-observabilityLAST DEPLOYED: Wed Jun 25 11:17:34 2025NAMESPACE: langsmith-observabilitySTATUS: deployedREVISION: 1\\n\\nAnd if you run kubectl get pods -n langsmith-observability, you should see:\\nCopylangsmith-observability-kube-state-metrics-b58bb8db4-bm4g5        1/1     Running   0          2m22slangsmith-observability-nginx-exporter-6d686d9d4b-5qw9v           1/1     Running   0          2m22slangsmith-observability-postgres-exporter-67d5db5684-tffbm        1/1     Running   0          2m22slangsmith-observability-redis-exporter-846c4d65cb-vbtwd           1/1     Running   0          2m22s\\n\\n\\u200bSection 2: Full Observability Stack\\nThis is not a production observability stack. Use this to gain quick insight into logs, metrics and traces for your deployment. This is only made to handle a few dozen GB of data per day.\\nThis section will show you how to deploy the end-to-end observability stack for LangSmith, using the Helm Chart.\\nThis chart is built around the open-source LGTM Stack from Grafana. It consists of:\\n\\nLoki for logs.\\nMimir for metrics + alerting.\\nTempo for traces.\\nGrafana for monitoring UI.\\n\\nAs well as OpenTelemetry Collectors for gathering the telemetry data.\\n\\u200bPrerequisites\\n\\u200b1. Compute Resources\\nThe resource requests and limits for each part of the stack can be modified in the helm chart. Here are the current allocations (request/limit):\\n\\nLoki: 2vCPU/3vCPU + 2Gi/4Gi\\nMimir: 1vCPU/2vCPU + 2Gi/4Gi\\nTempo: 1vCPU/2vCPU + 4Gi/6Gi\\n\\nMake sure you have those resources allocated before bringing up the helm chart, or modify the resource values in your helm configuration file.\\n\\u200b2. Cert-Manager\\nThe helm chart uses the OpenTelemetry Operator to provision collectors. The operator require that you have cert-manager installed in your Kubernetes cluster.\\nIf you do not have it installed, you can run the following commands:\\nCopyhelm repo add jetstack https://charts.jetstack.iohelm repo updatehelm install cert-manager jetstack/cert-manager -n cert-manager --create-namespace\\n\\n\\u200b3. OpenTelemetry Operator\\nUse the following to install the OpenTelemetry Operator:\\nCopyhelm repo add open-telemetry https://open-telemetry.github.io/opentelemetry-helm-chartshelm repo updatehelm install opentelemetry-operator open-telemetry/opentelemetry-operator -n <namespace>\\n\\n\\u200bInstallation\\nThe following instructions will bring up OTel collectors, the LGTM stack, Grafana and Prometheus exporters.\\n\\nCreate a local file called langsmith_obs_config.yaml\\nCopy over the values from this file into langsmith_obs_config.yaml, making sure to modify the values to match your LangSmith deployment.\\nFind the latest version of the chart by running helm search repo langchain/langsmith-observability --versions.\\nGrab the latest version number, and run helm install langsmith-observability langchain/langsmith-observability --values langsmith_obs_config.yaml --version <version> -n <namespace> --wait --debug\\n\\nYou can selectively collect logs, metrics or traces by modifying the boolean values under otelCollector in your config file. You can also selectively bring up each respective piece of the backend (Loki, Mimir, Tempo).\\nYou should see the following if the install went through:\\nCopyRelease \"langsmith-observability\" has been installed. Happy Helming!NAME: langsmith-observabilityLAST DEPLOYED: Wed Jun 25 11:17:34 2025NAMESPACE: langsmith-observabilitySTATUS: deployedREVISION: 1\\n\\nAnd if you run kubectl get pods -n langsmith-observability, you should see:\\nCopylangsmith-observability-collector-gateway-collector-7746fb8pzbg   1/1     Running   0          5m26slangsmith-observability-grafana-7c6fc976f9-cdbvr                  1/1     Running   0          2m49slangsmith-observability-kube-state-metrics-b58bb8db4-bm4g5        1/1     Running   0          5m27slangsmith-observability-loki-0                                    2/2     Running   0          5m27slangsmith-observability-loki-chunks-cache-0                       2/2     Running   0          5m27slangsmith-observability-loki-gateway-769fb6fff8-zjsn5             1/1     Running   0          5m27slangsmith-observability-loki-results-cache-0                      2/2     Running   0          5m27slangsmith-observability-mimir-0                                   1/1     Running   0          5m26slangsmith-observability-nginx-exporter-6d686d9d4b-5qw9v           1/1     Running   0          5m27slangsmith-observability-postgres-exporter-67d5db5684-tffbm        1/1     Running   0          5m27slangsmith-observability-redis-exporter-846c4d65cb-vbtwd           1/1     Running   0          5m27slangsmith-observability-tempo-0                                   1/1     Running   0          5m27sopentelemetry-operator-756dff697-vblbn                            2/2     Running   0          12m\\n\\n\\u200bPost-Installation\\n\\u200bEnable Logs and Traces in LangSmith\\nOnce you have installed the observability helm chart, you need to set the following values in your LangSmith helm configuration file to enable collection of logs and traces.\\nCopycommonPodAnnotations:\\n  # E.g.: \"langsmith-observability/langsmith-observability-collector-sidecar\"\\n  sidecar.opentelemetry.io/inject: \"${LANGSMITH_OBS_NAMESPACE}/${LANGSMITH_OTEL_CRD_NAME}\"\\nobservability:\\n  tracing:\\n    enabled: true\\n    # Replace this with the endpoint of your trace collector.\\n    # E.g.: \"http://langsmith-observability-collector-gateway-collector.langsmith-observability.svc.cluster.local:4318/v1/traces\"\\n    endpoint: \"http://${GATEWAY_COLLECTOR_SERVICE_NAME}.${LANGSMITH_OBS_NAMESPACE}.svc.cluster.local:4318/v1/traces\"\\n\\n\\nTo get ${LANGSMITH_OTEL_CRD_NAME}, you can run kubectl get opentelemetrycollectors -n ${LANGSMITH_OBS_NAMESPACE} and select the name of the one with MODE = sidecar\\nTo get ${GATEWAY_COLLECTOR_SERVICE_NAME} name, run kubectl get services -n ${LANGSMITH_OBS_NAMESPACE} and select the one with Ports 4317/4318 AND a ClusterIP set. It should be something like langsmith-observability-collector-gateway-collector\\n\\nNow run helm upgrade langsmith langchain/langsmith --values langsmith_config.yaml -n <langsmith-namespace> --wait --debug\\nOnce upgraded, if you run kubectl get pods -n <langsmith-namespace> you should see the following (note the 2/2 for sidecar collectors):\\nCopylangsmith-ace-backend-7dc85f7dff-xjbkj         2/2     Running     0               7m53slangsmith-backend-566b66979c-rgcfh             2/2     Running     1               7m53slangsmith-clickhouse-0                         2/2     Running     0               7m49slangsmith-frontend-7cf8549885-vpkns            2/2     Running     0               7m53slangsmith-platform-backend-5d46db7d9d-f6gh7    2/2     Running     0               7m52slangsmith-platform-backend-5d46db7d9d-lrr4d    2/2     Running     1               7m41slangsmith-platform-backend-5d46db7d9d-pcp27    2/2     Running     0               7m28slangsmith-playground-65d4c9699c-h656r          2/2     Running     0               7m52slangsmith-postgres-0                           2/2     Running     0               7m51slangsmith-queue-bdcd45bd6-htssd                2/2     Running     0               7m52slangsmith-queue-bdcd45bd6-pwdx4                2/2     Running     0               6m31slangsmith-queue-bdcd45bd6-xqrb8                2/2     Running     0               5m11slangsmith-redis-0                              2/2     Running     0               7m51s\\n\\n\\u200bGrafana Usage\\nOnce everything is installed, do the following: to get your Grafana password:\\nCopykubectl get secret langsmith-observability-grafana -n <langsmith_observability_namespace> -o jsonpath=\"{.data.admin-password}\" | base64 --decode\\n\\nThen port-forward into the langsmith-observability-grafana container at port 3000, and open your browser as localhost:3000. Use the username admin and the password from the secret above to log into Grafana.\\nOnce in Grafana, you can use the UI to monitor logs, metrics and traces. Grafana also comes pre-packaged with sets of dashboards for monitoring the main components of your deployment.\\nWas this page helpful?YesNoSuggest editsConfigure your collector for telemetryâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/self_hosting/organization_charts', 'loc': 'https://docs.smith.langchain.com/self_hosting/organization_charts', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='View trace counts across your organization - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewSetupInstall on KubernetesInstall on DockerInteract with an installationUpgrade an installationConfigure egress for subscription metricsView trace counts across an organizationLangSmith-managed ClickHouseConfigurationConfigure for scaleEnable TTL & data retentionCreate an Ingress for installations (Kubernetes)Mirror images for your installationUse environment variables for model providersTroubleshootingAuthentication & access controlSet up basic authenticationSet up SSO with OAuth2.0 & OIDCCustomize user managementConfigure custom TLS certificatesUse an existing secret for your installation (Kubernetes)Connect external servicesEnable blob storageConnect to an external ClickHouse databaseConnect to an external PostgreSQL databaseConnect to an external Redis databaseScriptsDelete workspacesDelete organizationsDelete tracesGenerate ClickHouse StatsGenerate query statsRun support queries against PostgreSQLRun support queries against ClickHouseObservabilityExport LangSmith telemetry to your observability backendConfigure your collector for telemetryDeploy an observability stackOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationSetupView trace counts across your organizationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageProgrammatically fetch trace countsMethod 1: Use the LangSmith REST APIMethod 2: Use PostgreSQL support queriesSetupView trace counts across your organizationCopy pageCopy pageThis feature is available on Helm chart versions 0.9.5 and later.\\nLangSmith automatically generates and syncs organization usage charts for self-hosted installations.\\nThese charts are available under Settings > Usage and billing > Usage graph:\\n\\nUsage by Workspace: this counts traces (root runs) by workspace\\nOrganization Usage: this counts all traces (root runs) for the organization\\n\\nThe charts are refreshed to include any new workspaces every 5 minutes. Note that the charts are not editable.\\n\\u200bProgrammatically fetch trace counts\\nYou can retrieve trace counts programmatically using two different methods:\\n\\u200bMethod 1: Use the LangSmith REST API\\nIf your self-hosted installation uses an online key, you can use the LangSmith REST API to fetch organization usage data.\\nCopycurl -X GET \"https://your-langsmith-instance.com/api/v1/orgs/current/billing/usage\" \\\\\\n  -H \"Accept: application/json\" \\\\\\n  -H \"X-API-Key: your-api-key\" \\\\\\n  -G \\\\\\n  -d \"starting_on=2025-09-01T00:00:00Z\" \\\\\\n  -d \"ending_before=2025-10-01T00:00:00Z\" \\\\\\n  -d \"on_current_plan=true\"\\n\\n\\u200bMethod 2: Use PostgreSQL support queries\\nFor installations using offline keys or when you need more detailed export capabilities, you can run support queries directly against the PostgreSQL database. All available scripts are in the support queries repository.\\nCopysh run_support_query_pg.sh \"postgres://postgres:postgres@localhost:5432/postgres\" \\\\\\n  --input support_queries/pg_get_trace_counts_daily.sql \\\\\\n  --output trace_counts.csv\\n\\nFor more detailed information about running support queries, see the Run support queries against PostgreSQL guide.Was this page helpful?YesNoSuggest editsConfigure egress for subscription metricsLangSmith-managed ClickHouseâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/self_hosting/release_notes', 'loc': 'https://docs.smith.langchain.com/self_hosting/release_notes', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='\\n\\n\\n\\n\\n\\n\\nLangChain - Changelog\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nProducts\\n\\n\\n\\n\\n\\n\\n\\nLangChain\\nLangSmith\\nLangGraph\\n\\n\\n\\n\\n\\n\\nMethods\\n\\n\\n\\n\\n\\n\\n\\nRetrieval\\nAgents\\nEvaluation\\n\\n\\n\\n\\n\\n\\nResources\\n\\n\\n\\n\\n\\n\\n\\nBlog\\nCase Studies\\nLangChain Academy\\nCommunity\\nExperts\\nChangelog\\n\\n\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\n\\nPython\\n\\nLangChain\\nLangSmith\\nLangGraph\\n\\n\\n\\nJavaScript\\n\\nLangChain\\nLangSmith\\nLangGraph\\n\\n\\n\\n\\n\\n\\nCompany\\n\\n\\n\\n\\n\\n\\n\\nAbout\\nCareers\\n\\n\\n\\n\\n\\nPricing\\n\\n\\nGet a demo\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain Changelog\\n\\n\\n\\n\\n\\nSign up for our newsletter to stay up to date\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPowered by LaunchNotes\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  Loading...\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  Loading...\\n\\n\\n\\n\\n    August 2025\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangSmith Self-Hosted\\n\\n\\n\\n\\nLangSmith Self-Hosted v0.11\\n\\n\\n      This release brings customizable LangGraph Platform deployments, streamlined evaluation via Align Evals, running evals directly from Studio, and operational...\\n    \\n\\nAugust 18, 2025\\n\\n\\n\\n\\n    April 2025\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangSmith Self-Hosted\\n\\n\\n\\n\\nLangSmith Self-Hosted v0.10\\n\\n\\n      This release brings alerting, UIâ€driven experiment workflows, end-to-end OpenTelemetry support and a host of new capabilities alongside several bug fixes....\\n    \\n\\nApril 22, 2025\\n\\n\\n\\n\\n    January 2025\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangSmith Self-Hosted\\n\\n\\n\\n\\nLangSmith Self-Hosted v0.9\\n\\n\\n      This release contains important changes for license key verification and sending subscription metrics and operational metadata to LangChain, beta support for...\\n    \\n\\nJanuary 21, 2025\\n\\n\\n\\n\\n    December 2024\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangSmith Self-Hosted\\n\\n\\n\\n\\nðŸ“ˆ Organization Usage Charts for Self-Hosted Customers\\n\\n\\n      Self-hosted LangSmith customers can now access organization usage charts . Track trace usage across your entire org or by workspace with ease. Whatâ€™s New:...\\n    \\n\\nDecember 10, 2024\\n\\n\\n\\n\\n    October 2024\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangSmith Self-Hosted\\n\\n\\n\\n\\nLangSmith Self-hosted v0.8\\n\\n\\n      The LangSmith Self-hosted v0.8 release adds a new LangSmith home page, new features including support for custom code evaluators and bulk data export, and...\\n    \\n\\nOctober 29, 2024\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJump to Month\\n\\nAugust 2025\\n\\n\\nApril 2025\\n\\n\\nJanuary 2025\\n\\n\\nDecember 2024\\n\\n\\nOctober 2024\\n\\n\\n\\n\\n\\n\\nPowered by LaunchNotes\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSubscribe to updates\\n\\nÃ—\\n\\n\\n\\n\\n\\nEmail\\n\\n\\n\\n\\n\\n\\n\\n\\nSubscribe\\n\\n              By clicking subscribe, you accept our privacy policy and\\n              terms and conditions.\\n\\n                \\n                  reCAPTCHA privacy and terms apply\\n                \\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/self_hosting/scripts', 'loc': 'https://docs.smith.langchain.com/self_hosting/scripts', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Delete workspaces - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewSetupInstall on KubernetesInstall on DockerInteract with an installationUpgrade an installationConfigure egress for subscription metricsView trace counts across an organizationLangSmith-managed ClickHouseConfigurationConfigure for scaleEnable TTL & data retentionCreate an Ingress for installations (Kubernetes)Mirror images for your installationUse environment variables for model providersTroubleshootingAuthentication & access controlSet up basic authenticationSet up SSO with OAuth2.0 & OIDCCustomize user managementConfigure custom TLS certificatesUse an existing secret for your installation (Kubernetes)Connect external servicesEnable blob storageConnect to an external ClickHouse databaseConnect to an external PostgreSQL databaseConnect to an external Redis databaseScriptsDelete workspacesDelete organizationsDelete tracesGenerate ClickHouse StatsGenerate query statsRun support queries against PostgreSQLRun support queries against ClickHouseObservabilityExport LangSmith telemetry to your observability backendConfigure your collector for telemetryDeploy an observability stackOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationScriptsDelete workspacesGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pagePrerequisitesRunning the deletion script for a single workspaceScriptsDelete workspacesCopy pageCopy pageDeleting a workspace is supported nativley in LangSmith Self-Hosted v0.10. View instructions for deleting a workspace.Follow the guide below for Self-Hosted versions before v0.10.\\nThe LangSmith UI does not currently support the deletion of an individual workspace from an organization. This, however, can be accomplished by directly removing all traces from all materialized views in ClickHouse (except the runs_history views) and the runs and feedbacks tables and then removing the Workspace from the Postgres tenants table.\\nThis command using the Workspace ID as an argument.\\n\\u200bPrerequisites\\nEnsure you have the following tools/items ready.\\n\\n\\nkubectl\\n\\nhttps://kubernetes.io/docs/tasks/tools/\\n\\n\\n\\nPostgreSQL client\\n\\nhttps://www.postgresql.org/download/\\n\\n\\n\\nPostgreSQL database connection:\\n\\nHost\\nPort\\nUsername\\n\\nIf using the bundled version, this is postgres\\n\\n\\nPassword\\n\\nIf using the bundled version, this is postgres\\n\\n\\nDatabase name\\n\\nIf using the bundled version, this is postgres\\n\\n\\n\\n\\n\\nClickhouse database credentials\\n\\nHost\\nPort\\nUsername\\n\\nIf using the bundled version, this is default\\n\\n\\nPassword\\n\\nIf using the bundled version, this is password\\n\\n\\nDatabase name\\n\\nIf using the bundled version, this is default\\n\\n\\n\\n\\n\\nConnectivity to the PostgreSQL database from the machine you will be running the migration script on.\\n\\nIf you are using the bundled version, you may need to port forward the postgresql service to your local machine.\\nRun kubectl port-forward svc/langsmith-postgres 5432:5432 to port forward the postgresql service to your local machine.\\n\\n\\n\\nConnectivity to the Clickhouse database from the machine you will be running the migration script on.\\n\\nIf you are using the bundled version, you may need to port forward the clickhouse service to your local machine.\\n\\nRun kubectl port-forward svc/langsmith-clickhouse 8123:8123 to port forward the clickhouse service to your local machine.\\n\\n\\nIf you are using Clickhouse Cloud you will want to specify the â€”ssl flag and use port 8443\\n\\n\\n\\nThe script to delete a workspace\\n\\nYou can download the script from here\\n\\n\\n\\n\\u200bRunning the deletion script for a single workspace\\nRun the following command to run the workspace removal script:\\nCopysh delete_workspace.sh <postgres_url> <clickhouse_url> --workspace_id <workspace_id>\\n\\nFor example, if you are using the bundled version with port-forwarding, the command would look like:\\nCopysh delete_workspace.sh \"postgres://postgres:postgres@localhost:5432/postgres\" \"clickhouse://default:password@localhost:8123/default\" --workspace_id 4ec70ec7-0808-416a-b836-7100aeec934b\\n\\nIf you visit the LangSmith UI, you should now see workspace is deleted.Was this page helpful?YesNoSuggest editsConnect to an external Redis databaseDelete organizationsâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/self_hosting/scripts/delete_a_workspace', 'loc': 'https://docs.smith.langchain.com/self_hosting/scripts/delete_a_workspace', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Delete workspaces - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewSetupInstall on KubernetesInstall on DockerInteract with an installationUpgrade an installationConfigure egress for subscription metricsView trace counts across an organizationLangSmith-managed ClickHouseConfigurationConfigure for scaleEnable TTL & data retentionCreate an Ingress for installations (Kubernetes)Mirror images for your installationUse environment variables for model providersTroubleshootingAuthentication & access controlSet up basic authenticationSet up SSO with OAuth2.0 & OIDCCustomize user managementConfigure custom TLS certificatesUse an existing secret for your installation (Kubernetes)Connect external servicesEnable blob storageConnect to an external ClickHouse databaseConnect to an external PostgreSQL databaseConnect to an external Redis databaseScriptsDelete workspacesDelete organizationsDelete tracesGenerate ClickHouse StatsGenerate query statsRun support queries against PostgreSQLRun support queries against ClickHouseObservabilityExport LangSmith telemetry to your observability backendConfigure your collector for telemetryDeploy an observability stackOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationScriptsDelete workspacesGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pagePrerequisitesRunning the deletion script for a single workspaceScriptsDelete workspacesCopy pageCopy pageDeleting a workspace is supported nativley in LangSmith Self-Hosted v0.10. View instructions for deleting a workspace.Follow the guide below for Self-Hosted versions before v0.10.\\nThe LangSmith UI does not currently support the deletion of an individual workspace from an organization. This, however, can be accomplished by directly removing all traces from all materialized views in ClickHouse (except the runs_history views) and the runs and feedbacks tables and then removing the Workspace from the Postgres tenants table.\\nThis command using the Workspace ID as an argument.\\n\\u200bPrerequisites\\nEnsure you have the following tools/items ready.\\n\\n\\nkubectl\\n\\nhttps://kubernetes.io/docs/tasks/tools/\\n\\n\\n\\nPostgreSQL client\\n\\nhttps://www.postgresql.org/download/\\n\\n\\n\\nPostgreSQL database connection:\\n\\nHost\\nPort\\nUsername\\n\\nIf using the bundled version, this is postgres\\n\\n\\nPassword\\n\\nIf using the bundled version, this is postgres\\n\\n\\nDatabase name\\n\\nIf using the bundled version, this is postgres\\n\\n\\n\\n\\n\\nClickhouse database credentials\\n\\nHost\\nPort\\nUsername\\n\\nIf using the bundled version, this is default\\n\\n\\nPassword\\n\\nIf using the bundled version, this is password\\n\\n\\nDatabase name\\n\\nIf using the bundled version, this is default\\n\\n\\n\\n\\n\\nConnectivity to the PostgreSQL database from the machine you will be running the migration script on.\\n\\nIf you are using the bundled version, you may need to port forward the postgresql service to your local machine.\\nRun kubectl port-forward svc/langsmith-postgres 5432:5432 to port forward the postgresql service to your local machine.\\n\\n\\n\\nConnectivity to the Clickhouse database from the machine you will be running the migration script on.\\n\\nIf you are using the bundled version, you may need to port forward the clickhouse service to your local machine.\\n\\nRun kubectl port-forward svc/langsmith-clickhouse 8123:8123 to port forward the clickhouse service to your local machine.\\n\\n\\nIf you are using Clickhouse Cloud you will want to specify the â€”ssl flag and use port 8443\\n\\n\\n\\nThe script to delete a workspace\\n\\nYou can download the script from here\\n\\n\\n\\n\\u200bRunning the deletion script for a single workspace\\nRun the following command to run the workspace removal script:\\nCopysh delete_workspace.sh <postgres_url> <clickhouse_url> --workspace_id <workspace_id>\\n\\nFor example, if you are using the bundled version with port-forwarding, the command would look like:\\nCopysh delete_workspace.sh \"postgres://postgres:postgres@localhost:5432/postgres\" \"clickhouse://default:password@localhost:8123/default\" --workspace_id 4ec70ec7-0808-416a-b836-7100aeec934b\\n\\nIf you visit the LangSmith UI, you should now see workspace is deleted.Was this page helpful?YesNoSuggest editsConnect to an external Redis databaseDelete organizationsâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/self_hosting/scripts/delete_an_organization', 'loc': 'https://docs.smith.langchain.com/self_hosting/scripts/delete_an_organization', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Delete organizations - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewSetupInstall on KubernetesInstall on DockerInteract with an installationUpgrade an installationConfigure egress for subscription metricsView trace counts across an organizationLangSmith-managed ClickHouseConfigurationConfigure for scaleEnable TTL & data retentionCreate an Ingress for installations (Kubernetes)Mirror images for your installationUse environment variables for model providersTroubleshootingAuthentication & access controlSet up basic authenticationSet up SSO with OAuth2.0 & OIDCCustomize user managementConfigure custom TLS certificatesUse an existing secret for your installation (Kubernetes)Connect external servicesEnable blob storageConnect to an external ClickHouse databaseConnect to an external PostgreSQL databaseConnect to an external Redis databaseScriptsDelete workspacesDelete organizationsDelete tracesGenerate ClickHouse StatsGenerate query statsRun support queries against PostgreSQLRun support queries against ClickHouseObservabilityExport LangSmith telemetry to your observability backendConfigure your collector for telemetryDeploy an observability stackOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationScriptsDelete organizationsGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pagePrerequisitesRunning the deletion script for a single organizationScriptsDelete organizationsCopy pageCopy pageThe LangSmith UI does not currently support the deletion of an individual organization from a self-hosted instance of LangSmith. This, however, can be accomplished by directly removing all traces from all materialized views in ClickHouse (except the runs_history views) and the runs and feedbacks tables and then removing the Organization from the Postgres tenants table.\\nThis command using the Organization ID as an argument.\\n\\u200bPrerequisites\\nEnsure you have the following tools/items ready.\\n\\n\\nkubectl\\n\\nhttps://kubernetes.io/docs/tasks/tools/\\n\\n\\n\\nPostgreSQL client\\n\\nhttps://www.postgresql.org/download/\\n\\n\\n\\nPostgreSQL database connection:\\n\\nHost\\nPort\\nUsername\\n\\nIf using the bundled version, this is postgres\\n\\n\\nPassword\\n\\nIf using the bundled version, this is postgres\\n\\n\\nDatabase name\\n\\nIf using the bundled version, this is postgres\\n\\n\\n\\n\\n\\nClickhouse database credentials\\n\\nHost\\nPort\\nUsername\\n\\nIf using the bundled version, this is default\\n\\n\\nPassword\\n\\nIf using the bundled version, this is password\\n\\n\\nDatabase name\\n\\nIf using the bundled version, this is default\\n\\n\\n\\n\\n\\nConnectivity to the PostgreSQL database from the machine you will be running the migration script on.\\n\\nIf you are using the bundled version, you may need to port forward the postgresql service to your local machine.\\nRun kubectl port-forward svc/langsmith-postgres 5432:5432 to port forward the postgresql service to your local machine.\\n\\n\\n\\nConnectivity to the Clickhouse database from the machine you will be running the migration script on.\\n\\nIf you are using the bundled version, you may need to port forward the clickhouse service to your local machine.\\n\\nRun kubectl port-forward svc/langsmith-clickhouse 8123:8123 to port forward the clickhouse service to your local machine.\\n\\n\\nIf you are using Clickhouse Cloud you will want to specify the â€”ssl flag and use port 8443\\n\\n\\n\\nThe script to delete an organization\\n\\nYou can download the script from here\\n\\n\\n\\n\\u200bRunning the deletion script for a single organization\\nRun the following command to run the organization removal script:\\nCopysh delete_organization.sh <postgres_url> <clickhouse_url> --organization_id <organization_id>\\n\\nFor example, if you are using the bundled version with port-forwarding, the command would look like:\\nCopysh delete_organization.sh \"postgres://postgres:postgres@localhost:5432/postgres\" \"clickhouse://default:password@localhost:8123/default\" --organization_id 4ec70ec7-0808-416a-b836-7100aeec934b\\n\\nIf you visit the LangSmith UI, you should now see organization is no longer present.Was this page helpful?YesNoSuggest editsDelete workspacesDelete tracesâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/self_hosting/scripts/delete_traces', 'loc': 'https://docs.smith.langchain.com/self_hosting/scripts/delete_traces', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Delete traces - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewSetupInstall on KubernetesInstall on DockerInteract with an installationUpgrade an installationConfigure egress for subscription metricsView trace counts across an organizationLangSmith-managed ClickHouseConfigurationConfigure for scaleEnable TTL & data retentionCreate an Ingress for installations (Kubernetes)Mirror images for your installationUse environment variables for model providersTroubleshootingAuthentication & access controlSet up basic authenticationSet up SSO with OAuth2.0 & OIDCCustomize user managementConfigure custom TLS certificatesUse an existing secret for your installation (Kubernetes)Connect external servicesEnable blob storageConnect to an external ClickHouse databaseConnect to an external PostgreSQL databaseConnect to an external Redis databaseScriptsDelete workspacesDelete organizationsDelete tracesGenerate ClickHouse StatsGenerate query statsRun support queries against PostgreSQLRun support queries against ClickHouseObservabilityExport LangSmith telemetry to your observability backendConfigure your collector for telemetryDeploy an observability stackOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationScriptsDelete tracesGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pagePrerequisitesRunning the deletion script for a single traceRunning the deletion script for a multiple traces from a file with one trace ID per lineScriptsDelete tracesCopy pageCopy pageThe LangSmith UI does not currently support the deletion of an individual trace. This, however, can be accomplished by directly removing the trace from all materialized views in ClickHouse (except the runs_history views) and the runs and feedback table themselves.\\nThis command can either be run using a trace ID as an argument or using a file that is a list of trace IDs.\\n\\u200bPrerequisites\\nEnsure you have the following tools/items ready.\\n\\n\\nkubectl\\n\\nhttps://kubernetes.io/docs/tasks/tools/\\n\\n\\n\\nClickhouse database credentials\\n\\nHost\\nPort\\nUsername\\n\\nIf using the bundled version, this is default\\n\\n\\nPassword\\n\\nIf using the bundled version, this is password\\n\\n\\nDatabase name\\n\\nIf using the bundled version, this is default\\n\\n\\n\\n\\n\\nConnectivity to the Clickhouse database from the machine you will be running the delete_trace_by_id script on.\\n\\nIf you are using the bundled version, you may need to port forward the clickhouse service to your local machine.\\nRun kubectl port-forward svc/langsmith-clickhouse 8123:8123 to port forward the clickhouse service to your local machine.\\n\\n\\n\\nThe script to delete a trace\\n\\nYou can download the script from here\\n\\n\\n\\n\\u200bRunning the deletion script for a single trace\\nRun the following command to run the trace deletion script using a single trace ID:\\nCopysh delete_trace_by_id.sh <clickhouse_url> --trace_id <trace_id>\\n\\nFor example, if you are using the bundled version with port-forwarding, the command would look like:\\nCopysh delete_trace_by_id.sh \"clickhouse://default:password@localhost:8123/default\" --trace_id 4ec70ec7-0808-416a-b836-7100aeec934b\\n\\nIf you visit the LangSmith UI, you should now see specified trace ID is no longer present nor reflected in stats.\\n\\u200bRunning the deletion script for a multiple traces from a file with one trace ID per line\\nRun the following command to run the trace deletion script using a list of trace IDs:\\nCopysh delete_trace_by_id.sh <clickhouse_url> --file <path/to/foo.txt>\\n\\nFor example, if you are using the bundled version with port-forwarding, the command would look like:\\nCopysh delete_trace_by_id.sh \"clickhouse://default:password@localhost:8123/default\" --file path/to/traces.txt\\n\\nIf you visit the LangSmith UI, you should now see all the specified traces have been removed.Was this page helpful?YesNoSuggest editsDelete organizationsGenerate ClickHouse StatsâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/self_hosting/scripts/generate_clickhouse_stats', 'loc': 'https://docs.smith.langchain.com/self_hosting/scripts/generate_clickhouse_stats', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Generate ClickHouse stats - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewSetupInstall on KubernetesInstall on DockerInteract with an installationUpgrade an installationConfigure egress for subscription metricsView trace counts across an organizationLangSmith-managed ClickHouseConfigurationConfigure for scaleEnable TTL & data retentionCreate an Ingress for installations (Kubernetes)Mirror images for your installationUse environment variables for model providersTroubleshootingAuthentication & access controlSet up basic authenticationSet up SSO with OAuth2.0 & OIDCCustomize user managementConfigure custom TLS certificatesUse an existing secret for your installation (Kubernetes)Connect external servicesEnable blob storageConnect to an external ClickHouse databaseConnect to an external PostgreSQL databaseConnect to an external Redis databaseScriptsDelete workspacesDelete organizationsDelete tracesGenerate ClickHouse StatsGenerate query statsRun support queries against PostgreSQLRun support queries against ClickHouseObservabilityExport LangSmith telemetry to your observability backendConfigure your collector for telemetryDeploy an observability stackOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationScriptsGenerate ClickHouse statsGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pagePrerequisitesRunning the clickhouse stats generation scriptScriptsGenerate ClickHouse statsCopy pageCopy pageAs part of troubleshooting your self-hosted instance of LangSmith, the LangChain team may ask you to generate Clickhouse statistics that will help us understand memory and CPU consumption and connection concurrency.\\nThis command will generate a CSV that can be shared with the LangChain team.\\n\\u200bPrerequisites\\nEnsure you have the following tools/items ready.\\n\\n\\nkubectl\\n\\nhttps://kubernetes.io/docs/tasks/tools/\\n\\n\\n\\nClickhouse database credentials\\n\\nHost\\nPort\\nUsername\\n\\nIf using the bundled version, this is default\\n\\n\\nPassword\\n\\nIf using the bundled version, this is password\\n\\n\\nDatabase name\\n\\nIf using the bundled version, this is default\\n\\n\\n\\n\\n\\nConnectivity to the Clickhouse database from the machine you will be running the get_clickhouse_stats script on.\\n\\nIf you are using the bundled version, you may need to port forward the clickhouse service to your local machine.\\nRun kubectl port-forward svc/langsmith-clickhouse 8123:8123 to port forward the clickhouse service to your local machine.\\n\\n\\n\\nThe script to generate ClickHouse stats\\n\\nYou can download the script from here\\n\\n\\n\\n\\u200bRunning the clickhouse stats generation script\\nRun the following command to run the stats generation script:\\nCopysh get_clickhouse_stats.sh <clickhouse_url> --output path/to/file.csv\\n\\nFor example, if you are using the bundled version with port-forwarding, the command would look like:\\nCopysh get_clickhouse_stats.sh \"clickhouse://default:password@localhost:8123/default\" --output clickhouse_stats.csv\\n\\nand after running this command you should see a file, clickhouse_stats.csv, has been created with Clickhouse statistics.Was this page helpful?YesNoSuggest editsDelete tracesGenerate query statsâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/self_hosting/scripts/generate_query_stats', 'loc': 'https://docs.smith.langchain.com/self_hosting/scripts/generate_query_stats', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Generate query stats - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewSetupInstall on KubernetesInstall on DockerInteract with an installationUpgrade an installationConfigure egress for subscription metricsView trace counts across an organizationLangSmith-managed ClickHouseConfigurationConfigure for scaleEnable TTL & data retentionCreate an Ingress for installations (Kubernetes)Mirror images for your installationUse environment variables for model providersTroubleshootingAuthentication & access controlSet up basic authenticationSet up SSO with OAuth2.0 & OIDCCustomize user managementConfigure custom TLS certificatesUse an existing secret for your installation (Kubernetes)Connect external servicesEnable blob storageConnect to an external ClickHouse databaseConnect to an external PostgreSQL databaseConnect to an external Redis databaseScriptsDelete workspacesDelete organizationsDelete tracesGenerate ClickHouse StatsGenerate query statsRun support queries against PostgreSQLRun support queries against ClickHouseObservabilityExport LangSmith telemetry to your observability backendConfigure your collector for telemetryDeploy an observability stackOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationScriptsGenerate query statsGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pagePrerequisitesRunning the query stats generation scriptScriptsGenerate query statsCopy pageCopy pageAs part of troubleshooting your self-hosted instance of LangSmith, the LangChain team may ask you to generate LangSmith query statistics that will help us understand the performance of various queries that drive the LangSmith product experience.\\nThis command will generate a CSV that can be shared with the LangChain team.\\n\\u200bPrerequisites\\nEnsure you have the following tools/items ready.\\n\\n\\nkubectl\\n\\nhttps://kubernetes.io/docs/tasks/tools/\\n\\n\\n\\nClickhouse database credentials\\n\\nHost\\nPort\\nUsername\\n\\nIf using the bundled version, this is default\\n\\n\\nPassword\\n\\nIf using the bundled version, this is password\\n\\n\\nDatabase name\\n\\nIf using the bundled version, this is default\\n\\n\\n\\n\\n\\nConnectivity to the Clickhouse database from the machine you will be running the get_query_stats script on.\\n\\nIf you are using the bundled version, you may need to port forward the clickhouse service to your local machine.\\nRun kubectl port-forward svc/langsmith-clickhouse 8123:8123 to port forward the clickhouse service to your local machine.\\n\\n\\n\\nThe script to generate query stats\\n\\nYou can download the script from here\\n\\n\\n\\n\\u200bRunning the query stats generation script\\nRun the following command to run the stats generation script:\\nCopysh get_query_stats.sh <clickhouse_url> --output path/to/file.csv\\n\\nFor example, if you are using the bundled version with port-forwarding, the command would look like:\\nCopysh get_query_stats.sh \"clickhouse://default:password@localhost:8123/default\" --output query_stats.csv\\n\\nand after running this command you should see a file, query_stats.csv, has been created with LangSmith query statistics.Was this page helpful?YesNoSuggest editsGenerate ClickHouse StatsRun support queries against PostgreSQLâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/self_hosting/scripts/running_ch_support_queries', 'loc': 'https://docs.smith.langchain.com/self_hosting/scripts/running_ch_support_queries', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Run support queries against ClickHouse - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewSetupInstall on KubernetesInstall on DockerInteract with an installationUpgrade an installationConfigure egress for subscription metricsView trace counts across an organizationLangSmith-managed ClickHouseConfigurationConfigure for scaleEnable TTL & data retentionCreate an Ingress for installations (Kubernetes)Mirror images for your installationUse environment variables for model providersTroubleshootingAuthentication & access controlSet up basic authenticationSet up SSO with OAuth2.0 & OIDCCustomize user managementConfigure custom TLS certificatesUse an existing secret for your installation (Kubernetes)Connect external servicesEnable blob storageConnect to an external ClickHouse databaseConnect to an external PostgreSQL databaseConnect to an external Redis databaseScriptsDelete workspacesDelete organizationsDelete tracesGenerate ClickHouse StatsGenerate query statsRun support queries against PostgreSQLRun support queries against ClickHouseObservabilityExport LangSmith telemetry to your observability backendConfigure your collector for telemetryDeploy an observability stackOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationScriptsRun support queries against ClickHouseGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pagePrerequisitesRunning the query scriptScriptsRun support queries against ClickHouseCopy pageCopy pageThis Helm repository contains queries to produce output that the LangSmith UI does not currently support directly (e.g. obtaining query exception logs from Clickhouse).\\nThis command takes a clickhouse connection string that contains an embedded name and password (which can be passed in from a call to a secrets manager) and executes a query from an input file. In the example below, we are using the ch_get_query_exceptions.sql input file in the support_queries/clickhouse directory.\\n\\u200bPrerequisites\\nEnsure you have the following tools/items ready.\\n\\n\\nkubectl\\n\\nhttps://kubernetes.io/docs/tasks/tools/\\n\\n\\n\\nClickhouse database credentials\\n\\nHost\\nPort\\nUsername\\n\\nIf using the bundled version, this is default\\n\\n\\nPassword\\n\\nIf using the bundled version, this is password\\n\\n\\nDatabase name\\n\\nIf using the bundled version, this is default\\n\\n\\n\\n\\n\\nConnectivity to the Clickhouse database from the machine you will be running the migration script on.\\n\\nIf you are using the bundled version, you may need to port forward the clickhouse service to your local machine.\\nRun kubectl port-forward svc/langsmith-clickhouse 8123:8123 to port forward the clickhouse service to your local machine.\\n\\n\\n\\nThe script to run a support query\\n\\nYou can download the script from here\\n\\n\\n\\n\\u200bRunning the query script\\nRun the following command to run the desired query:\\nCopysh run_support_query_ch.sh <clickhouse_url> --input path/to/query.sql\\n\\nFor example, if you are using the bundled version with port-forwarding, the command might look like:\\nCopysh run_support_query_ch.sh \"clickhouse://default:password@localhost:8123/default\" --input support_queries/clickhouse/ch_get_query_exceptions.sql\\n\\nwhich will output query logs for all queries that have thrown exceptions in Clickhouse in the last 7 days. To extract this to a file add the flag --output path/to/file.csvWas this page helpful?YesNoSuggest editsRun support queries against PostgreSQLExport LangSmith telemetry to your observability backendâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/self_hosting/scripts/running_pg_support_queries', 'loc': 'https://docs.smith.langchain.com/self_hosting/scripts/running_pg_support_queries', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Run support queries against PostgreSQL - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewSetupInstall on KubernetesInstall on DockerInteract with an installationUpgrade an installationConfigure egress for subscription metricsView trace counts across an organizationLangSmith-managed ClickHouseConfigurationConfigure for scaleEnable TTL & data retentionCreate an Ingress for installations (Kubernetes)Mirror images for your installationUse environment variables for model providersTroubleshootingAuthentication & access controlSet up basic authenticationSet up SSO with OAuth2.0 & OIDCCustomize user managementConfigure custom TLS certificatesUse an existing secret for your installation (Kubernetes)Connect external servicesEnable blob storageConnect to an external ClickHouse databaseConnect to an external PostgreSQL databaseConnect to an external Redis databaseScriptsDelete workspacesDelete organizationsDelete tracesGenerate ClickHouse StatsGenerate query statsRun support queries against PostgreSQLRun support queries against ClickHouseObservabilityExport LangSmith telemetry to your observability backendConfigure your collector for telemetryDeploy an observability stackOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationScriptsRun support queries against PostgreSQLGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pagePrerequisitesRunning the query scriptExport usage dataGet customer informationProcess the API response with jqInitial exportStatus updateScriptsRun support queries against PostgreSQLCopy pageCopy pageThis Helm repository contains queries to produce output that the LangSmith UI does not currently support directly (e.g. obtaining trace counts for multiple organizations in a single query).\\nThis command takes a postgres connection string that contains an embedded name and password (which can be passed in from a call to a secrets manager) and executes a query from an input file. In the example below, we are using the pg_get_trace_counts_daily.sql input file in the support_queries/postgres directory.\\n\\u200bPrerequisites\\nEnsure you have the following tools/items ready.\\n\\n\\nkubectl\\n\\nhttps://kubernetes.io/docs/tasks/tools/\\n\\n\\n\\nPostgreSQL client\\n\\nhttps://www.postgresql.org/download/\\n\\n\\n\\nPostgreSQL database connection:\\n\\nHost\\nPort\\nUsername\\n\\nIf using the bundled version, this is postgres\\n\\n\\nPassword\\n\\nIf using the bundled version, this is postgres\\n\\n\\nDatabase name\\n\\nIf using the bundled version, this is postgres\\n\\n\\n\\n\\n\\nConnectivity to the PostgreSQL database from the machine you will be running the migration script on.\\n\\nIf you are using the bundled version, you may need to port forward the postgresql service to your local machine.\\nRun kubectl port-forward svc/langsmith-postgres 5432:5432 to port forward the postgresql service to your local machine.\\n\\n\\n\\nThe script to run a support query\\n\\nYou can download the script from here\\n\\n\\n\\n\\u200bRunning the query script\\nRun the following command to run the desired query:\\nCopysh run_support_query_pg.sh <postgres_url> --input path/to/query.sql\\n\\nFor example, if you are using the bundled version with port-forwarding, the command might look like:\\nCopysh run_support_query_pg.sh \"postgres://postgres:postgres@localhost:5432/postgres\" --input support_queries/pg_get_trace_counts_daily.sql\\n\\nwhich will output the count of daily traces by workspace ID and organization ID. To extract this to a file add the flag --output path/to/file.csv\\n\\u200bExport usage data\\nExporting usage data requires running Helm chart version 0.11.4 or later.\\n\\u200bGet customer information\\nYou need to retrieve your customer information from the LangSmith API before running the export scripts. This information is required as input for the export scripts.\\nCopycurl https://<langsmith_url>/api/v1/info\\n# if configured with a subdomain / path prefix:\\ncurl http://<langsmith_url/prefix/api/v1/info\\n\\nThis will return a JSON response containing your customer information:\\nCopy{\\n  \"version\": \"0.11.4\",\\n  \"license_expiration_time\": \"2026-08-18T19:14:34Z\",\\n  \"customer_info\": {\\n    \"customer_id\": \"<id>\",\\n    \"customer_name\": \"<name>\"\\n  }\\n}\\n\\nExtract the customer_id and customer_name from this response to use as input for the export scripts.\\n\\u200bProcess the API response with jq\\nYou can use jq to parse the JSON response and set bash variables for use in your scripts:\\nCopy# Get the API response and extract customer information\\nexport LANGSMITH_URL=\"<your_langsmith_url>\"\\nresponse=$(curl -s $LANGSMITH_URL/api/v1/info)\\n\\n# Extract customer_id and customer_name using jq\\nexport CUSTOMER_ID=$(echo \"$response\" | jq -r \\'.customer_info.customer_id\\')\\nexport CUSTOMER_NAME=$(echo \"$response\" | jq -r \\'.customer_info.customer_name\\')\\n\\n# Verify the variables are set\\necho \"Customer ID: $CUSTOMER_ID\"\\necho \"Customer Name: $CUSTOMER_NAME\"\\n\\nYou can then use these environment variables in your export scripts or other commands.\\nIf you donâ€™t have jq, run these commands to set the environment variables based on the curl output:\\nCopycurl -s $LANGSMITH_URL/api/v1/info\\nexport CUSTOMER_ID=\"<id>\"\\nexport CUSTOMER_NAME=\"<name>\"\\n\\n\\u200bInitial export\\nThese scripts export usage data to a CSV for reporting to LangChain. They additionally track the export by assigning a backfill ID and timestamp.\\nTo export LangSmith trace usage:\\nCopy# Get customer information from the API\\nexport LANGSMITH_URL=\"<your_langsmith_url>\"\\nexport response=$(curl -s $LANGSMITH_URL/api/v1/info)\\nexport CUSTOMER_ID=$(echo \"$response\" | jq -r \\'.customer_info.customer_id\\') && echo \"Customer ID: $CUSTOMER_ID\"\\nexport CUSTOMER_NAME=$(echo \"$response\" | jq -r \\'.customer_info.customer_name\\') && echo \"Customer name: $CUSTOMER_NAME\"\\n\\n# Run the export script with customer information as variables\\nsh run_support_query_pg.sh <postgres_url> \\\\\\n  --input support_queries/postgres/pg_usage_traces_backfill_export.sql \\\\\\n  --output ls_export.csv \\\\\\n  -v customer_id=$CUSTOMER_ID \\\\\\n  -v customer_name=$CUSTOMER_NAME\\n\\nTo export LangGraph Platform usage:\\nCopysh run_support_query_pg.sh <postgres_url> \\\\\\n  --input support_queries/postgres/pg_usage_nodes_backfill_export.sql \\\\\\n  --output lgp_export.csv \\\\\\n  -v customer_id=$CUSTOMER_ID \\\\\\n  -v customer_name=$CUSTOMER_NAME\\n\\n\\u200bStatus update\\nThese scripts update the status of usage events in your installation to reflect that the events have been successfully processed by LangChain.\\nThe scripts require passing in the corresponding backfill_id, which will be confirmed by your LangChain rep.\\nTo update LangSmith trace usage:\\nCopysh run_support_query_pg.sh <postgres_url> --input support_queries/postgres/pg_usage_traces_backfill_update.sql --output export.csv -v backfill_id=<backfill_id>\\n\\nTo update LangGraph Platform usage:\\nCopysh run_support_query_pg.sh <postgres_url> --input support_queries/postgres/pg_usage_nodes_backfill_update.sql --output export.csv -v backfill_id=<backfill_id>\\nWas this page helpful?YesNoSuggest editsGenerate query statsRun support queries against ClickHouseâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/self_hosting/troubleshooting', 'loc': 'https://docs.smith.langchain.com/self_hosting/troubleshooting', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Troubleshooting - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewSetupInstall on KubernetesInstall on DockerInteract with an installationUpgrade an installationConfigure egress for subscription metricsView trace counts across an organizationLangSmith-managed ClickHouseConfigurationConfigure for scaleEnable TTL & data retentionCreate an Ingress for installations (Kubernetes)Mirror images for your installationUse environment variables for model providersTroubleshootingAuthentication & access controlSet up basic authenticationSet up SSO with OAuth2.0 & OIDCCustomize user managementConfigure custom TLS certificatesUse an existing secret for your installation (Kubernetes)Connect external servicesEnable blob storageConnect to an external ClickHouse databaseConnect to an external PostgreSQL databaseConnect to an external Redis databaseScriptsDelete workspacesDelete organizationsDelete tracesGenerate ClickHouse StatsGenerate query statsRun support queries against PostgreSQLRun support queries against ClickHouseObservabilityExport LangSmith telemetry to your observability backendConfigure your collector for telemetryDeploy an observability stackOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationConfigurationTroubleshootingGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageGetting helpful informationKubernetesDockerBrowser ErrorsCommon issuesDB::Exception: Cannot reserve 1.00 MiB, not enough space: While executing WaitForAsyncInsert. (NOT_ENOUGH_SPACE)KubernetesDockererror: Dirty database version â€˜versionâ€™. Fix and force versionKubernetesDocker413 - Request Entity Too LargeKubernetesDetails: code: 497, message: default: Not enough privileges. To execute this query, itâ€™s necessary to have the grant CREATE ROW POLICY ON default.feedbacks_rmtClickHouse fails to start up when running a cluster with AquaSecKubernetesDockerConfigurationTroubleshootingCopy pageCopy pageThis guide will walk you through common issues you may encounter when running a self-hosted instance of LangSmith.\\nWhile running LangSmith, you may encounter unexpected 500 errors, slow performance, or other issues. This guide will help you diagnose and resolve these issues.\\n\\u200bGetting helpful information\\nTo diagnose and resolve an issue, you will first need to retrieve some relevant information. Below, we explain how to do this for a kubernetes setup, a docker setup, as well as how to pull helpful browser info.\\nGenerally, the main services you will want to analyze are:\\n\\nlangsmith-backend: The main backend service.\\nlangsmith-platform-backend: Another important backend service.\\nlangsmith-queue: The queue service.\\n\\n\\u200bKubernetes\\nThe first step in troubleshooting is to gather important debugging information about your LangSmith deployment. Service logs, kubernetes events, and resource utilization of containers can help identify the root cause of an issue.\\nYou can run our k8s troubleshooting script which will pull all of the relevant kubernetes information and output it to a folder for investigation. The script also compresses this folder into a zip file for sharing. Here is an example of how to run this script, assuming your langsmith deployment was brought up in a langsmith namespace:\\nCopybash get_k8s_debugging_info.sh --namespace langsmith\\n\\nYou can then inspect the contents of the produced folder for any relevant errors or information. If you would like the LangSmith team to assist in debugging, please share this zip file with the team.\\n\\u200bDocker\\nIf running on Docker, you can check the logs your deployment by running the following command:\\nCopydocker compose logs >> logs.txt\\n\\n\\u200bBrowser Errors\\nIf you are experiencing an issue that surfaces as a browser error, it may also be helpful to inspect a HAR file which may include key information. To get the HAR file, you can follow this guide which explains the short process for various browsers.\\nYou can then use Googleâ€™s HAR analyzer to investigate. You can also send your HAR file to the LangSmith team to help with debugging.\\n\\u200bCommon issues\\n\\u200bDB::Exception: Cannot reserve 1.00 MiB, not enough space: While executing WaitForAsyncInsert. (NOT_ENOUGH_SPACE)\\nThis error occurs when ClickHouse runs out of disk space. You will need to increase the disk space available to ClickHouse.\\n\\u200bKubernetes\\nIn Kubernetes, you will need to increase the size of the ClickHouse PVC. To achieve this, you can perform the following steps:\\n\\n\\nGet the storage class of the PVC: kubectl get pvc data-langsmith-clickhouse-0 -n <namespace> -o jsonpath=\\'{.spec.storageClassName}\\'\\n\\n\\nEnsure the storage class has AllowVolumeExpansion: true: kubectl get sc <storage-class-name> -o jsonpath=\\'{.allowVolumeExpansion}\\'\\n\\nIf it is false, some storage classes can be updated to allow volume expansion.\\nTo update the storage class, you can run kubectl patch sc <storage-class-name> -p \\'{\"allowVolumeExpansion\": true}\\'\\nIf this fails, you may need to create a new storage class with the correct settings.\\n\\n\\n\\nEdit your pvc to have the new size: kubectl edit pvc data-langsmith-clickhouse-0 -n <namespace> or kubectl patch pvc data-langsmith-clickhouse-0 \\'{\"spec\":{\"resources\":{\"requests\":{\"storage\":\"100Gi\"}}}}\\' -n <namespace>\\n\\n\\nUpdate your helm chart langsmith_config.yaml to new size(e.g 100 Gi)\\n\\n\\nDelete the clickhouse statefulset kubectl delete statefulset langsmith-clickhouse --cascade=orphan -n <namespace>\\n\\n\\nApply helm chart with updated size (You can follow the upgrade guide here)\\n\\n\\nYour pvc should now have the new size. Verify by running kubectl get pvc and kubectl exec langsmith-clickhouse-0 -- bash -c \"df\"\\n\\n\\n\\u200bDocker\\nIn Docker, you will need to increase the size of the ClickHouse volume. To achieve this, you can perform the following steps:\\n\\nStop your instance of LangSmith. docker compose down\\nIf using bind mount, you will need to increase the size of the mount point.\\nIf using a docker volume, you will need to allocate more space to the volume/docker.\\n\\n\\u200berror: Dirty database version â€˜versionâ€™. Fix and force version\\nThis error occurs when the ClickHouse database is in an inconsistent state with our migrations. You will need to reset to an earlier database version and then rerun your upgrade/migrations.\\n\\u200bKubernetes\\n\\nForce migration to an earlier version, where version = dirty version - 1.\\n\\nCopykubectl exec -it deployments/langsmith-backend -- bash -c \\'migrate -source \"file://clickhouse/migrations\" -database \"clickhouse://$CLICKHOUSE_HOST:$CLICKHOUSE_NATIVE_PORT?username=$CLICKHOUSE_USER&password=$CLICKHOUSE_PASSWORD&database=$CLICKHOUSE_DB&x-multi-statement=true&x-migrations-table-engine=MergeTree&secure=$CLICKHOUSE_TLS\" force <version>\\'\\n\\n\\nRerun your upgrade/migrations.\\n\\n\\u200bDocker\\n\\nForce migration to an earlier version, where version = dirty version - 1.\\n\\nCopydocker compose exec langchain-backend migrate -source \"file://clickhouse/migrations\" -database \"clickhouse://$CLICKHOUSE_HOST:$CLICKHOUSE_NATIVE_PORT?username=$CLICKHOUSE_USER&password=$CLICKHOUSE_PASSWORD&database=$CLICKHOUSE_DB&x-multi-statement=true&x-migrations-table-engine=MergeTree&secure=$CLICKHOUSE_TLS\" force <version>\\n\\n\\nRerun your upgrade/migrations.\\n\\n\\u200b413 - Request Entity Too Large\\nThis error occurs when the request size exceeds the maximum allowed size. You will need to increase the maximum request size in your Nginx configuration.\\n\\u200bKubernetes\\n\\nEdit your langsmith_config.yaml and increase the frontend.maxBodySize value. This might look something like this:\\n\\nCopyfrontend:\\n  maxBodySize: \"100M\"\\n\\n\\nApply your changes to the cluster.\\n\\n\\u200bDetails: code: 497, message: default: Not enough privileges. To execute this query, itâ€™s necessary to have the grant CREATE ROW POLICY ON default.feedbacks_rmt\\nThis error occurs when your user does not have the necessary permissions to create row policies in Clickhouse. When deploying the Docker deployment, you need to copy the users.xml file from the github repo as well. This adds the <access_management> tag to the users.xml file, which allows the user to create row policies. Below is the default users.xml file that we expect to be used.\\nCopy<clickhouse>\\n    <users>\\n        <default>\\n            <access_management>1</access_management>\\n            <named_collection_control>1</named_collection_control>\\n            <show_named_collections>1</show_named_collections>\\n            <show_named_collections_secrets>1</show_named_collections_secrets>\\n            <profile>default</profile>\\n        </default>\\n    </users>\\n    <profiles>\\n        <default>\\n            <async_insert>1</async_insert>\\n            <async_insert_max_data_size>2000000</async_insert_max_data_size>\\n            <wait_for_async_insert>0</wait_for_async_insert>\\n            <parallel_view_processing>1</parallel_view_processing>\\n            <allow_simdjson>0</allow_simdjson>\\n            <lightweight_deletes_sync>0</lightweight_deletes_sync>\\n        </default>\\n    </profiles>\\n</clickhouse>\\n\\nIn some environments, your mount point may not be writable by the container. In these cases we suggest building a custom image with the users.xml file included.\\nExample Dockerfile:\\nCopyFROM clickhouse/clickhouse-server:24.8\\nCOPY ./users.xml /etc/clickhouse-server/users.d/users.xml\\n\\nThen take the following steps:\\n\\nBuild your custom image.\\n\\nCopydocker build -t <image-name> .\\n\\n\\nUpdate your docker-compose.yaml to use the custom image. Make sure to remove the users.xml mount point.\\n\\nCopylangchain-clickhouse:\\n  image: <image-name>\\n\\n\\nRestart your instance of LangSmith.\\n\\nCopydocker compose down --volumes\\ndocker compose up\\n\\n\\u200bClickHouse fails to start up when running a cluster with AquaSec\\nIn some environments, AquaSec may prevent ClickHouse from starting up correctly. This may manifest as the ClickHouse pod not emitting any logs and failing to get marked as ready.\\nGenerally this is due to LD_PRELOAD being set by AquaSec, which interferes with ClickHouse. To resolve this, you can add the following environment variable to your ClickHouse deployment:\\n\\u200bKubernetes\\nEdit your langsmith_config.yaml (or corresponding config file) and set the AQUA_SKIP_LD_PRELOAD environment variable:\\nCopyclickhouse:\\n  statefulSet:\\n    extraEnv:\\n      - name: AQUA_SKIP_LD_PRELOAD\\n        value: \"true\"\\n\\n\\u200bDocker\\nEdit your docker-compose.yaml and set the AQUA_SKIP_LD_PRELOAD environment variable:\\nCopylangchain-clickhouse:\\n  environment:\\n    - AQUA_SKIP_LD_PRELOAD=true\\nWas this page helpful?YesNoSuggest editsUse environment variables for model providersSet up basic authenticationâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/self_hosting/upgrades', 'loc': 'https://docs.smith.langchain.com/self_hosting/upgrades', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Upgrade an installation - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewSetupInstall on KubernetesInstall on DockerInteract with an installationUpgrade an installationConfigure egress for subscription metricsView trace counts across an organizationLangSmith-managed ClickHouseConfigurationConfigure for scaleEnable TTL & data retentionCreate an Ingress for installations (Kubernetes)Mirror images for your installationUse environment variables for model providersTroubleshootingAuthentication & access controlSet up basic authenticationSet up SSO with OAuth2.0 & OIDCCustomize user managementConfigure custom TLS certificatesUse an existing secret for your installation (Kubernetes)Connect external servicesEnable blob storageConnect to an external ClickHouse databaseConnect to an external PostgreSQL databaseConnect to an external Redis databaseScriptsDelete workspacesDelete organizationsDelete tracesGenerate ClickHouse StatsGenerate query statsRun support queries against PostgreSQLRun support queries against ClickHouseObservabilityExport LangSmith telemetry to your observability backendConfigure your collector for telemetryDeploy an observability stackOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationSetupUpgrade an installationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageKubernetes(Helm)Validate your deployment:DockerValidate your deployment:SetupUpgrade an installationCopy pageCopy pageFor general upgrade instructions, please follow the instructions below. Certain versions may have specific upgrade instructions, which will be detailed in more specific upgrade guides.\\n\\u200bKubernetes(Helm)\\nIf you donâ€™t have the repo added, run the following command to add it:\\nCopyhelm repo add langchain https://langchain-ai.github.io/helm/\\n\\nUpdate your local helm repo\\nCopyhelm repo update\\n\\nUpdate your helm chart config file with any updates that are needed in the new version. These will be detailed in the release notes for the new version.\\nRun the following command to upgrade the chart(replace version with the version you want to upgrade to):\\nIf you are using a namespace other than the default namespace, you will need to specify the namespace in the helm and kubectl commands by using the -n <namespace flag.\\nFind the latest version of the chart. You can find this in the LangSmith Helm Chart GitHub repository or by running the following command:\\nCopyhelm search repo langchain/langsmith --versions\\n\\nYou should see an output similar to this:\\nCopylangchain/langsmith     0.10.14         0.10.32         Helm chart to deploy the langsmith application ...\\nlangchain/langsmith     0.10.13         0.10.32         Helm chart to deploy the langsmith application ...\\nlangchain/langsmith     0.10.12         0.10.32         Helm chart to deploy the langsmith application ...\\nlangchain/langsmith     0.10.11         0.10.29         Helm chart to deploy the langsmith application ...\\nlangchain/langsmith     0.10.10         0.10.29         Helm chart to deploy the langsmith application ...\\nlangchain/langsmith     0.10.9          0.10.29         Helm chart to deploy the langsmith application ...\\n\\nChoose the version you want to upgrade to (generally the latest version is recommended) and note the version number.\\nCopyhelm upgrade <release-name> langchain/langsmith --version <version> --values <path-to-values-file> --wait --debug\\n\\nVerify that the upgrade was successful:\\nCopyhelm status <release-name>\\n\\nAll pods should be in the Running state. Verify that clickhouse is running and that both migrations jobs have completed.\\nCopykubectl get pods\\nNAME                                     READY   STATUS      RESTARTS   AGE\\nlangsmith-backend-95b6d54f5-gz48b        1/1     Running     0          15h\\nlangsmith-pg-migrations-d2z6k            0/1     Completed   0          5h48m\\nlangsmith-ch-migrations-gasvk            0/1     Completed   0          5h48m\\nlangsmith-clickhouse-0                   1/1     Running     0          26h\\nlangsmith-frontend-84687d9d45-6cg4r      1/1     Running     0          15h\\nlangsmith-hub-backend-66ffb75fb4-qg6kl   1/1     Running     0          15h\\nlangsmith-playground-85b444d8f7-pl589    1/1     Running     0          15h\\nlangsmith-queue-d58cb64f7-87d68          1/1     Running     0          15h\\n\\n\\u200bValidate your deployment:\\n\\n\\nRun kubectl get services\\nOutput should look something like:\\n\\n\\nCopyNAME                         TYPE           CLUSTER-IP       EXTERNAL-IP     PORT(S)                      AGE\\nkubernetes                   ClusterIP      172.20.0.1       <none>          443/TCP                      27d\\nlangsmith-backend            ClusterIP      172.20.22.34     <none>          1984/TCP                     21d\\nlangsmith-clickhouse         ClusterIP      172.20.117.62    <none>          8123/TCP,9000/TCP            21d\\nlangsmith-frontend           LoadBalancer   172.20.218.30    <external ip>   80:30093/TCP,443:31130/TCP   21d\\nlangsmith-platform-backend   ClusterIP      172.20.232.183   <none>          1986/TCP                     21d\\nlangsmith-playground         ClusterIP      172.20.167.132   <none>          3001/TCP                     21d\\nlangsmith-postgres           ClusterIP      172.20.59.63     <none>          5432/TCP                     21d\\nlangsmith-redis              ClusterIP      172.20.229.98    <none>          6379/TCP                     20d\\n\\n\\n\\nCurl the external ip of the langsmith-frontend service:\\nCopycurl <external ip>/api/info\\n{\"version\":\"0.5.7\",\"license_expiration_time\":\"2033-05-20T20:08:06\",\"batch_ingest_config\":{\"scale_up_qsize_trigger\":1000,\"scale_up_nthreads_limit\":16,\"scale_down_nempty_trigger\":4,\"size_limit\":100,\"size_limit_bytes\":20971520}}\\n\\n\\n\\nCheck that the version matches the version you upgraded to.\\n\\n\\nVisit the external ip for the langsmith-frontend service on your browser\\nThe LangSmith UI should be visible/operational\\n\\n\\n\\n\\u200bDocker\\nUpgrading the Docker version of LangSmith is a bit more involved than the Helm version and may require a small amount of downtime. Please follow the instructions below to upgrade your Docker version of LangSmith.\\n\\nUpdate your docker-compose.yml file to the file used in the latest release. You can find this in the LangSmith SDK GitHub repository\\nUpdate your .env file with any new environment variables that are required in the new version. These will be detailed in the release notes for the new version.\\nRun the following command to stop your current LangSmith instance:\\n\\nCopydocker-compose down\\n\\n\\nRun the following command to start your new LangSmith instance in the background:\\n\\nCopydocker-compose up -d\\n\\nIf everything ran successfully, you should see all the LangSmith containers running and healthy.\\nCopyCONTAINER ID   IMAGE                                  COMMAND                  CREATED        STATUS                        PORTS                                                      NAMES\\ne1c8f01a4ffc   langchain/langsmith-frontend:0.5.7     \"/entrypoint.sh nginâ€¦\"   10 hours ago   Up 40 seconds                 0.0.0.0:80->80/tcp, 8080/tcp                               cli-langchain-frontend-1\\n39e1394846b9   langchain/langsmith-backend:0.5.7      \"/bin/sh -c \\'exec uvâ€¦\"   10 hours ago   Up 40 seconds                 0.0.0.0:1984->1984/tcp                                     cli-langchain-backend-1\\nf8688dd58f2f   langchain/langsmith-go-backend:0.5.7   \"./smith-go\"             10 hours ago   Up 40 seconds                 0.0.0.0:1986->1986/tcp                                     cli-langchain-platform-backend-1\\n006f1303b04d   langchain/langsmith-backend:0.5.7      \"saq app.workers.queâ€¦\"   10 hours ago   Up 40 seconds                                                                            cli-langchain-queue-1\\n73a90242ed3a   redis:7                                \"docker-entrypoint.sâ€¦\"   10 hours ago   Up About a minute (healthy)   0.0.0.0:63791->6379/tcp                                    cli-langchain-redis-1\\neecf75ca672b   postgres:14.7                          \"docker-entrypoint.sâ€¦\"   10 hours ago   Up About a minute (healthy)   0.0.0.0:5433->5432/tcp                                     cli-langchain-db-1\\n3aa5652a864d   clickhouse/clickhouse-server:23.9      \"/entrypoint.sh\"         10 hours ago   Up About a minute (healthy)   9009/tcp, 0.0.0.0:8124->8123/tcp, 0.0.0.0:9001->9000/tcp   cli-langchain-clickhouse-1\\n84edc329a37f   langchain/langsmith-playground:0.5.7   \"docker-entrypoint.sâ€¦\"   10 hours ago   Up About a minute             0.0.0.0:3001->3001/tcp                                     cli-langchain-playground-1\\n\\n\\u200bValidate your deployment:\\n\\n\\nCurl the exposed port of the cli-langchain-frontend-1 container:\\nCopycurl localhost:80/info\\n{\"version\":\"0.5.7\",\"license_expiration_time\":\"2033-05-20T20:08:06\",\"batch_ingest_config\":{\"scale_up_qsize_trigger\":1000,\"scale_up_nthreads_limit\":16,\"scale_down_nempty_trigger\":4,\"size_limit\":100,\"size_limit_bytes\":20971520}}\\n\\n\\n\\nVisit the exposed port of the cli-langchain-frontend-1 container on your browser\\n\\n\\nThe LangSmith UI should be visible/operational\\nWas this page helpful?YesNoSuggest editsInteract with an installationConfigure egress for subscription metricsâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/self_hosting/usage', 'loc': 'https://docs.smith.langchain.com/self_hosting/usage', 'changefreq': 'weekly', 'priority': '0.5'}, page_content=\"Interact with your self-hosted instance of LangSmith - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewSetupInstall on KubernetesInstall on DockerInteract with an installationUpgrade an installationConfigure egress for subscription metricsView trace counts across an organizationLangSmith-managed ClickHouseConfigurationConfigure for scaleEnable TTL & data retentionCreate an Ingress for installations (Kubernetes)Mirror images for your installationUse environment variables for model providersTroubleshootingAuthentication & access controlSet up basic authenticationSet up SSO with OAuth2.0 & OIDCCustomize user managementConfigure custom TLS certificatesUse an existing secret for your installation (Kubernetes)Connect external servicesEnable blob storageConnect to an external ClickHouse databaseConnect to an external PostgreSQL databaseConnect to an external Redis databaseScriptsDelete workspacesDelete organizationsDelete tracesGenerate ClickHouse StatsGenerate query statsRun support queries against PostgreSQLRun support queries against ClickHouseObservabilityExport LangSmith telemetry to your observability backendConfigure your collector for telemetryDeploy an observability stackOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationSetupInteract with your self-hosted instance of LangSmithGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageConfiguring the application you want to use with LangSmithSelf-Signed CertificatesAPI ReferenceSetupInteract with your self-hosted instance of LangSmithCopy pageCopy pageThis guide will walk you through the process of using your self-hosted instance of LangSmith.\\nThis guide assumes you have already deployed a self-hosted LangSmith instance. If you have not, please refer to the kubernetes deployment guide or the docker deployment guide.\\n\\u200bConfiguring the application you want to use with LangSmith\\nLangSmith has a single API for interacting with both the hub and the LangSmith backend.\\n\\nOnce you have deployed your instance, you can access the LangSmith UI at http(s)://<host>.\\nThe LangSmith API will be available at http(s)://<host>/api/v1\\nThe LangGraph Platform Control Plane will be available at http(s)://<host>/api-host\\n\\nTo use the API of your instance, you will need to set the following environment variables in your application:\\nCopyLANGSMITH_ENDPOINT=http://<host>/api/v1\\nLANGSMITH_API_KEY=foo # Set to a legitimate API key if using OAuth\\n\\nYou can also configure these variables directly in the LangSmith SDK client:\\nCopyimport langsmith\\nlangsmith_client = langsmith.Client(\\n    api_key='<api_key>',\\n    api_url='http(s)://<host>/api/v1',\\n)\\n\\nAfter setting the above, you should be able to run your code and see the results in your self-hosted instance. We recommend running through the quickstart guide to get a feel for how to use LangSmith.\\n\\u200bSelf-Signed Certificates\\nIf you are using self-signed certificates for your self-hosted LangSmith instance, this can be problematic as Python comes with its own set of trusted certificates, which may not include your self-signed certificate. To resolve this, you may need to use something like truststore to load system certificates into your Python environment.\\nYou can do this like so:\\n\\npip install truststore (or similar depending on the package manager you are using)\\n\\nThen use the following code to load the system certificates:\\nCopyimport truststore\\ntruststore.inject_into_ssl()\\n# The rest of your code\\nimport langsmith\\nlangsmith_client = langsmith.Client(\\n    api_key='<api_key>',\\n    api_url='http(s)://<host>/api/v1',\\n)\\n\\n\\u200bAPI Reference\\nTo access the API reference, navigate to http://<host>/api/docs in your browser.Was this page helpful?YesNoSuggest editsInstall on DockerUpgrade an installationâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'loc': 'https://docs.smith.langchain.com/', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Get started with LangSmith - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KLangSmithPlatform for LLM observability and evaluationOverviewQuickstartsTrace an applicationEvaluate an applicationTest promptsAPI & SDKsAPI referencePython SDKJS/TS SDKPricingPlansPricing FAQOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...âŒ˜KGitHubForumForumSearch...NavigationGet started with LangSmithGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumGet started with LangSmithCopy pageCopy pageLangSmith is a platform for building production-grade LLM applications. Monitor and evaluate your application, so you can ship quickly and with confidence.\\nLangSmith is framework agnostic â€”\\xa0you can use it with or without LangChainâ€™s open source frameworks langchain and langgraph.\\n\\nStart tracingGain visibility into each step your application takes when handling a request to debug faster.Learn moreEvaluate your applicationMeasure quality of your applications over time to build more reliable AI applications.Learn moreTest your promptsIterate on prompts, with automatic version control and collaboration features.Learn moreSet up your workspaceSet up your workspace, configure admin settings, and invite your team to collaborate.Learn moreWas this page helpful?YesNoSuggest editsTrace an applicationâŒ˜IAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T13:49:31.736932Z",
     "start_time": "2025-10-04T13:49:31.725695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_voyageai import VoyageAIEmbeddings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "embd = VoyageAIEmbeddings(model=\"voyage-2\", api_key=os.getenv(\"VOYAGE_API_KEY\"))"
   ],
   "id": "77de7212a2064ab9",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T21:10:01.416673Z",
     "start_time": "2025-10-04T21:09:33.983318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embd = HuggingFaceEmbeddings(\n",
    "        model_name=\"all-MiniLM-l6-v2\",\n",
    "        model_kwargs={'device': 'cpu'},\n",
    "        encode_kwargs={'normalize_embeddings': False}\n",
    "    )"
   ],
   "id": "c7a527067017a0d2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manan/PycharmProjects/langsmith-course/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9a45a089bdced963"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
